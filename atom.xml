<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>我的梦想是星辰大海</title>
  
  <subtitle>知识源于积累,登峰造极源于自律</subtitle>
  <link href="http://xubatian.cn/atom.xml" rel="self"/>
  
  <link href="http://xubatian.cn/"/>
  <updated>2022-02-15T02:21:55.314Z</updated>
  <id>http://xubatian.cn/</id>
  
  <author>
    <name>xubatian</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Flink 原理与实现: Flink流计算常用算子（Flink算子大全）</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E6%B5%81%E8%AE%A1%E7%AE%97%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90%EF%BC%88Flink%E7%AE%97%E5%AD%90%E5%A4%A7%E5%85%A8%EF%BC%89/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E6%B5%81%E8%AE%A1%E7%AE%97%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90%EF%BC%88Flink%E7%AE%97%E5%AD%90%E5%A4%A7%E5%85%A8%EF%BC%89/</id>
    <published>2022-02-15T01:20:27.000Z</published>
    <updated>2022-02-15T02:21:55.314Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215092137.png"></p><p>Flink和Spark类似，也是一种一站式处理的框架；既可以进行批处理（DataSet），也可以进行实时处理（DataStream）。</p><p>所以下面将Flink的算子分为两大类：一类是DataSet，一类是DataStream。</p><p><strong>Flink官网:</strong> <a href="https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/dev/datastream/operators/overview/">https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/dev/datastream/operators/overview/</a></p><p><strong>案例代码:</strong> <a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/tree/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/tree/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo</a></p><span id="more"></span><p>我们列举了一些Flink自带且常用的transformation算子，例如map、flatMap等。在Flink的编程体系中，我们获取到数据源之后，需要经过一系列的处理即transformation操作，再将最终结果输出到目的Sink（ES、mysql或者hdfs），使数据落地。因此，除了正确的继承重写RichSourceFunction&lt;&gt;和RichSinkFunction&lt;&gt;之外，最终要的就是实时处理这部分，下面的图介绍了Flink代码执行流程以及各模块的组成部分。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215092857.png"></p><p>在Flink中，Transformation算子就是将一个或多个DataStream转换为新的DataStream，可以将多个转换组合成复杂的数据流拓扑。如下图所示，DataStream会由不同的Transformation操作，转换、过滤、聚合成其他不同的流，从而完成我们的业务要求。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215092803.png"></p><h1 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h1><h3 id="一、Source算子"><a href="#一、Source算子" class="headerlink" title="一、Source算子"></a><strong>一、Source算子</strong></h3><h3 id="1-fromCollection"><a href="#1-fromCollection" class="headerlink" title="1. fromCollection"></a><strong>1. fromCollection</strong></h3><p>fromCollection：从本地集合读取数据</p><p>例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val env = ExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">val textDataSet: DataSet[String] = env.fromCollection(</span><br><span class="line">  List(&quot;1,张三&quot;, &quot;2,李四&quot;, &quot;3,王五&quot;, &quot;4,赵六&quot;)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="2-readTextFile"><a href="#2-readTextFile" class="headerlink" title="2. readTextFile"></a><strong>2. readTextFile</strong></h3><p>readTextFile：从文件中读取</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val textDataSet: DataSet[String]  = env.readTextFile(&quot;/data/a.txt&quot;)</span><br></pre></td></tr></table></figure><h3 id="3-readTextFile：遍历目录"><a href="#3-readTextFile：遍历目录" class="headerlink" title="3. readTextFile：遍历目录"></a><strong>3. readTextFile：遍历目录</strong></h3><p>readTextFile可以对一个文件目录内的所有文件，包括所有子目录中的所有文件的遍历访问方式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val parameters = new Configuration</span><br><span class="line">// recursive.file.enumeration 开启递归</span><br><span class="line">parameters.setBoolean(&quot;recursive.file.enumeration&quot;, true)</span><br><span class="line">val file = env.readTextFile(&quot;/data&quot;).withParameters(parameters)</span><br></pre></td></tr></table></figure><h3 id="4-readTextFile：读取压缩文件"><a href="#4-readTextFile：读取压缩文件" class="headerlink" title="4. readTextFile：读取压缩文件"></a><strong>4. readTextFile：读取压缩文件</strong></h3><p>对于以下压缩类型，不需要指定任何额外的inputformat方法，flink可以自动识别并且解压。但是，压缩文件可能不会并行读取，可能是顺序读取的，这样可能会影响作业的可伸缩性。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val file = env.readTextFile(&quot;/data/file.gz&quot;)</span><br></pre></td></tr></table></figure><h3 id="二、Transform转换算子"><a href="#二、Transform转换算子" class="headerlink" title="二、Transform转换算子"></a><strong>二、Transform转换算子</strong></h3><p>因为Transform算子基于Source算子操作，所以首先构建Flink执行环境及Source算子，后续Transform算子操作基于此：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val env = ExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">val textDataSet: DataSet[String] = env.fromCollection(</span><br><span class="line">  List(&quot;张三,1&quot;, &quot;李四,2&quot;, &quot;王五,3&quot;, &quot;张三,4&quot;)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="1-map"><a href="#1-map" class="headerlink" title="1. map"></a><strong>1. map</strong></h3><p>将DataSet中的每一个元素转换为另外一个元素</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// 使用map将List转换为一个Scala的样例类</span><br><span class="line"></span><br><span class="line">case class User(name: String, id: String)</span><br><span class="line"></span><br><span class="line">val userDataSet: DataSet[User] = textDataSet.map &#123;</span><br><span class="line">  text =&gt;</span><br><span class="line">    val fieldArr = text.split(&quot;,&quot;)</span><br><span class="line">    User(fieldArr(0), fieldArr(1))</span><br><span class="line">&#125;</span><br><span class="line">userDataSet.print()</span><br></pre></td></tr></table></figure><h3 id="2-flatMap"><a href="#2-flatMap" class="headerlink" title="2. flatMap"></a><strong>2. flatMap</strong></h3><p>将DataSet中的每一个元素转换为0…n个元素。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 使用flatMap操作，将集合中的数据：</span><br><span class="line">// 根据第一个元素，进行分组</span><br><span class="line">// 根据第二个元素，进行聚合求值 </span><br><span class="line"></span><br><span class="line">val result = textDataSet.flatMap(line =&gt; line)</span><br><span class="line">      .groupBy(0) // 根据第一个元素，进行分组</span><br><span class="line">      .sum(1) // 根据第二个元素，进行聚合求值</span><br><span class="line">      </span><br><span class="line">result.print()</span><br></pre></td></tr></table></figure><h3 id="3-mapPartition"><a href="#3-mapPartition" class="headerlink" title="3. mapPartition"></a><strong>3. mapPartition</strong></h3><p>将一个分区中的元素转换为另一个元素</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 使用mapPartition操作，将List转换为一个scala的样例类</span><br><span class="line"></span><br><span class="line">case class User(name: String, id: String)</span><br><span class="line"></span><br><span class="line">val result: DataSet[User] = textDataSet.mapPartition(line =&gt; &#123;</span><br><span class="line">      line.map(index =&gt; User(index._1, index._2))</span><br><span class="line">    &#125;)</span><br><span class="line">    </span><br><span class="line">result.print()</span><br></pre></td></tr></table></figure><h3 id="4-filter"><a href="#4-filter" class="headerlink" title="4. filter"></a><strong>4. filter</strong></h3><p>过滤出来一些符合条件的元素，返回<strong>boolean值为true</strong>的元素</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val source: DataSet[String] = env.fromElements(&quot;java&quot;, &quot;scala&quot;, &quot;java&quot;)</span><br><span class="line">val filter:DataSet[String] = source.filter(line =&gt; line.contains(&quot;java&quot;))//过滤出带java的数据</span><br><span class="line">filter.print()</span><br></pre></td></tr></table></figure><h3 id="5-reduce"><a href="#5-reduce" class="headerlink" title="5. reduce"></a><strong>5. reduce</strong></h3><p>可以对一个dataset或者一个group来进行聚合计算，最终<strong>聚合成一个元素</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// 使用 fromElements 构建数据源</span><br><span class="line">val source = env.fromElements((&quot;java&quot;, 1), (&quot;scala&quot;, 1), (&quot;java&quot;, 1))</span><br><span class="line">// 使用map转换成DataSet元组</span><br><span class="line">val mapData: DataSet[(String, Int)] = source.map(line =&gt; line)</span><br><span class="line">// 根据首个元素分组</span><br><span class="line">val groupData = mapData.groupBy(_._1)</span><br><span class="line">// 使用reduce聚合</span><br><span class="line">val reduceData = groupData.reduce((x, y) =&gt; (x._1, x._2 + y._2))</span><br><span class="line">// 打印测试</span><br><span class="line">reduceData.print()</span><br></pre></td></tr></table></figure><h3 id="6-reduceGroup"><a href="#6-reduceGroup" class="headerlink" title="6. reduceGroup"></a><strong>6. reduceGroup</strong></h3><p>将一个dataset或者一个group<strong>聚合成一个或多个元素</strong>。<br>reduceGroup是reduce的一种优化方案；<br>它会先分组reduce，然后在做整体的reduce；这样做的好处就是可以减少网络IO</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// 使用 fromElements 构建数据源</span><br><span class="line">val source: DataSet[(String, Int)] = env.fromElements((&quot;java&quot;, 1), (&quot;scala&quot;, 1), (&quot;java&quot;, 1))</span><br><span class="line">// 根据首个元素分组</span><br><span class="line">val groupData = source.groupBy(_._1)</span><br><span class="line">// 使用reduceGroup聚合</span><br><span class="line">val result: DataSet[(String, Int)] = groupData.reduceGroup &#123;</span><br><span class="line">      (in: Iterator[(String, Int)], out: Collector[(String, Int)]) =&gt;</span><br><span class="line">        val tuple = in.reduce((x, y) =&gt; (x._1, x._2 + y._2))</span><br><span class="line">        out.collect(tuple)</span><br><span class="line">    &#125;</span><br><span class="line">// 打印测试</span><br><span class="line">result.print()</span><br></pre></td></tr></table></figure><h3 id="7-minBy和maxBy"><a href="#7-minBy和maxBy" class="headerlink" title="7. minBy和maxBy"></a><strong>7. minBy和maxBy</strong></h3><p>选择具有最小值或最大值的<strong>元素</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// 使用minBy操作，求List中每个人的最小值</span><br><span class="line">// List(&quot;张三,1&quot;, &quot;李四,2&quot;, &quot;王五,3&quot;, &quot;张三,4&quot;)</span><br><span class="line"></span><br><span class="line">case class User(name: String, id: String)</span><br><span class="line">// 将List转换为一个scala的样例类</span><br><span class="line">val text: DataSet[User] = textDataSet.mapPartition(line =&gt; &#123;</span><br><span class="line">      line.map(index =&gt; User(index._1, index._2))</span><br><span class="line">    &#125;)</span><br><span class="line">    </span><br><span class="line">val result = text</span><br><span class="line">          .groupBy(0) // 按照姓名分组</span><br><span class="line">          .minBy(1)   // 每个人的最小值</span><br></pre></td></tr></table></figure><h3 id="8-Aggregate"><a href="#8-Aggregate" class="headerlink" title="8. Aggregate"></a><strong>8. Aggregate</strong></h3><p>在数据集上进行聚合求<strong>最值</strong>（最大值、最小值）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">val data = new mutable.MutableList[(Int, String, Double)]</span><br><span class="line">    data.+=((1, &quot;yuwen&quot;, 89.0))</span><br><span class="line">    data.+=((2, &quot;shuxue&quot;, 92.2))</span><br><span class="line">    data.+=((3, &quot;yuwen&quot;, 89.99))</span><br><span class="line">// 使用 fromElements 构建数据源</span><br><span class="line">val input: DataSet[(Int, String, Double)] = env.fromCollection(data)</span><br><span class="line">// 使用group执行分组操作</span><br><span class="line">val value = input.groupBy(1)</span><br><span class="line">            // 使用aggregate求最大值元素</span><br><span class="line">            .aggregate(Aggregations.MAX, 2) </span><br><span class="line">// 打印测试</span><br><span class="line">value.print()       </span><br></pre></td></tr></table></figure><p><strong>Aggregate只能作用于元组上</strong></p><blockquote><p> 注意：<br>要使用aggregate，只能使用字段索引名或索引名称来进行分组 <code>groupBy(0)</code> ，否则会报一下错误:<br>Exception in thread “main” java.lang.UnsupportedOperationException: Aggregate  does not support grouping with KeySelector functions, yet.</p></blockquote><h3 id="9-distinct"><a href="#9-distinct" class="headerlink" title="9. distinct"></a><strong>9. distinct</strong></h3><p>去除重复的数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 数据源使用上一题的</span><br><span class="line">// 使用distinct操作，根据科目去除集合中重复的元组数据</span><br><span class="line"></span><br><span class="line">val value: DataSet[(Int, String, Double)] = input.distinct(1)</span><br><span class="line">value.print()</span><br></pre></td></tr></table></figure><h3 id="10-first"><a href="#10-first" class="headerlink" title="10. first"></a><strong>10. first</strong></h3><p>取前N个数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input.first(2) // 取前两个数</span><br></pre></td></tr></table></figure><h3 id="11-join"><a href="#11-join" class="headerlink" title="11. join"></a><strong>11. join</strong></h3><p>将两个DataSet按照一定条件连接到一起，形成新的DataSet</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// s1 和 s2 数据集格式如下：</span><br><span class="line">// DataSet[(Int, String,String, Double)]</span><br><span class="line"></span><br><span class="line"> val joinData = s1.join(s2)  // s1数据集 join s2数据集</span><br><span class="line">             .where(0).equalTo(0) &#123;     // join的条件</span><br><span class="line">      (s1, s2) =&gt; (s1._1, s1._2, s2._2, s1._3)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="12-leftOuterJoin"><a href="#12-leftOuterJoin" class="headerlink" title="12. leftOuterJoin"></a><strong>12. leftOuterJoin</strong></h3><p>左外连接,左边的Dataset中的每一个元素，去连接右边的元素</p><p>此外还有：</p><p>rightOuterJoin：右外连接,左边的Dataset中的每一个元素，去连接左边的元素</p><p>fullOuterJoin：全外连接,左右两边的元素，全部连接</p><p>下面以 leftOuterJoin 进行示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> val data1 = ListBuffer[Tuple2[Int,String]]()</span><br><span class="line">    data1.append((1,&quot;zhangsan&quot;))</span><br><span class="line">    data1.append((2,&quot;lisi&quot;))</span><br><span class="line">    data1.append((3,&quot;wangwu&quot;))</span><br><span class="line">    data1.append((4,&quot;zhaoliu&quot;))</span><br><span class="line"></span><br><span class="line">val data2 = ListBuffer[Tuple2[Int,String]]()</span><br><span class="line">    data2.append((1,&quot;beijing&quot;))</span><br><span class="line">    data2.append((2,&quot;shanghai&quot;))</span><br><span class="line">    data2.append((4,&quot;guangzhou&quot;))</span><br><span class="line"></span><br><span class="line">val text1 = env.fromCollection(data1)</span><br><span class="line">val text2 = env.fromCollection(data2)</span><br><span class="line"></span><br><span class="line">text1.leftOuterJoin(text2).where(0).equalTo(0).apply((first,second)=&gt;&#123;</span><br><span class="line">      if(second==null)&#123;</span><br><span class="line">        (first._1,first._2,&quot;null&quot;)</span><br><span class="line">      &#125;else&#123;</span><br><span class="line">        (first._1,first._2,second._2)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;).print()</span><br></pre></td></tr></table></figure><h3 id="13-cross"><a href="#13-cross" class="headerlink" title="13. cross"></a><strong>13. cross</strong></h3><p>交叉操作，通过形成这个数据集和其他数据集的笛卡尔积，创建一个新的数据集</p><p>和join类似，但是这种交叉操作会产生笛卡尔积，在<strong>数据比较大的时候，是非常消耗内存的操作</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val cross = input1.cross(input2)&#123;</span><br><span class="line">      (input1 , input2) =&gt; (input1._1,input1._2,input1._3,input2._2)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">cross.print()</span><br></pre></td></tr></table></figure><h3 id="14-union"><a href="#14-union" class="headerlink" title="14. union"></a><strong>14. union</strong></h3><p>联合操作，创建包含来自该数据集和其他数据集的元素的新数据集,<strong>不会去重</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val unionData: DataSet[String] = elements1.union(elements2).union(elements3)</span><br><span class="line">// 去除重复数据</span><br><span class="line">val value = unionData.distinct(line =&gt; line)</span><br></pre></td></tr></table></figure><h3 id="15-rebalance"><a href="#15-rebalance" class="headerlink" title="15. rebalance"></a><strong>15. rebalance</strong></h3><p>Flink也有数据倾斜的时候，比如当前有数据量大概10亿条数据需要处理，在处理过程中可能会发生如图所示的状况：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215092303.png"></p><p>这个时候本来总体数据量只需要10分钟解决的问题，出现了数据倾斜，机器1上的任务需要4个小时才能完成，那么其他3台机器执行完毕也要等待机器1执行完毕后才算整体将任务完成； 所以在实际的工作中，出现这种情况比较好的解决方案就是接下来要介绍的—<strong>rebalance</strong>（内部使用round robin方法将数据均匀打散。这对于数据倾斜时是很好的选择。）</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215092318.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 使用rebalance操作，避免数据倾斜</span><br><span class="line">val rebalance = filterData.rebalance()</span><br></pre></td></tr></table></figure><h3 id="16-partitionByHash"><a href="#16-partitionByHash" class="headerlink" title="16. partitionByHash"></a><strong>16. partitionByHash</strong></h3><p>按照指定的key进行hash分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">val data = new mutable.MutableList[(Int, Long, String)]</span><br><span class="line">data.+=((1, 1L, &quot;Hi&quot;))</span><br><span class="line">data.+=((2, 2L, &quot;Hello&quot;))</span><br><span class="line">data.+=((3, 2L, &quot;Hello world&quot;))</span><br><span class="line"></span><br><span class="line">val collection = env.fromCollection(data)</span><br><span class="line">val unique = collection.partitionByHash(1).mapPartition&#123;</span><br><span class="line">  line =&gt;</span><br><span class="line">    line.map(x =&gt; (x._1 , x._2 , x._3))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">unique.writeAsText(&quot;hashPartition&quot;, WriteMode.NO_OVERWRITE)</span><br><span class="line">env.execute()</span><br></pre></td></tr></table></figure><h3 id="17-partitionByRange"><a href="#17-partitionByRange" class="headerlink" title="17. partitionByRange"></a><strong>17. partitionByRange</strong></h3><p>根据指定的key对数据集进行范围分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">val data = new mutable.MutableList[(Int, Long, String)]</span><br><span class="line">data.+=((1, 1L, &quot;Hi&quot;))</span><br><span class="line">data.+=((2, 2L, &quot;Hello&quot;))</span><br><span class="line">data.+=((3, 2L, &quot;Hello world&quot;))</span><br><span class="line">data.+=((4, 3L, &quot;Hello world, how are you?&quot;))</span><br><span class="line"></span><br><span class="line">val collection = env.fromCollection(data)</span><br><span class="line">val unique = collection.partitionByRange(x =&gt; x._1).mapPartition(line =&gt; line.map&#123;</span><br><span class="line">  x=&gt;</span><br><span class="line">    (x._1 , x._2 , x._3)</span><br><span class="line">&#125;)</span><br><span class="line">unique.writeAsText(&quot;rangePartition&quot;, WriteMode.OVERWRITE)</span><br><span class="line">env.execute()</span><br></pre></td></tr></table></figure><h3 id="18-sortPartition"><a href="#18-sortPartition" class="headerlink" title="18. sortPartition"></a><strong>18. sortPartition</strong></h3><p>根据指定的字段值进行分区的排序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">val data = new mutable.MutableList[(Int, Long, String)]</span><br><span class="line">    data.+=((1, 1L, &quot;Hi&quot;))</span><br><span class="line">    data.+=((2, 2L, &quot;Hello&quot;))</span><br><span class="line">    data.+=((3, 2L, &quot;Hello world&quot;))</span><br><span class="line">    data.+=((4, 3L, &quot;Hello world, how are you?&quot;))</span><br><span class="line"></span><br><span class="line">val ds = env.fromCollection(data)</span><br><span class="line">    val result = ds</span><br><span class="line">      .map &#123; x =&gt; x &#125;.setParallelism(2)</span><br><span class="line">      .sortPartition(1, Order.DESCENDING)//第一个参数代表按照哪个字段进行分区</span><br><span class="line">      .mapPartition(line =&gt; line)</span><br><span class="line">      .collect()</span><br><span class="line"></span><br><span class="line">println(result)</span><br></pre></td></tr></table></figure><h3 id="三、Sink算子"><a href="#三、Sink算子" class="headerlink" title="三、Sink算子"></a><strong>三、Sink算子</strong></h3><h3 id="1-collect"><a href="#1-collect" class="headerlink" title="1. collect"></a><strong>1. collect</strong></h3><p>将数据输出到本地集合</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result.collect()</span><br></pre></td></tr></table></figure><h3 id="2-writeAsText"><a href="#2-writeAsText" class="headerlink" title="2. writeAsText"></a><strong>2. writeAsText</strong></h3><p>将数据输出到文件</p><p>Flink支持多种存储设备上的文件，包括本地文件，hdfs文件等</p><p>Flink支持多种文件的存储格式，包括text文件，CSV文件等</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 将数据写入本地文件</span><br><span class="line">result.writeAsText(&quot;/data/a&quot;, WriteMode.OVERWRITE)</span><br><span class="line"></span><br><span class="line">// 将数据写入HDFS</span><br><span class="line">result.writeAsText(&quot;hdfs://node01:9000/data/a&quot;, WriteMode.OVERWRITE)</span><br></pre></td></tr></table></figure><h1 id="DataStream"><a href="#DataStream" class="headerlink" title="DataStream"></a><strong>DataStream</strong></h1><p>和DataSet一样，DataStream也包括一系列的Transformation操作</p><h3 id="一、Source算子-1"><a href="#一、Source算子-1" class="headerlink" title="一、Source算子"></a><strong>一、Source算子</strong></h3><p>Flink可以使用 StreamExecutionEnvironment.addSource(source) 来为我们的程序添加数据来源。<br>Flink 已经提供了若干实现好了的 source functions，当然我们也可以通过实现 SourceFunction  来自定义非并行的source或者实现 ParallelSourceFunction 接口或者扩展  RichParallelSourceFunction 来自定义并行的 source。</p><p>Flink在流处理上的source和在批处理上的source基本一致。大致有4大类：</p><ul><li>基于<strong>本地集合</strong>的source（Collection-based-source）</li><li>基于<strong>文件</strong>的source（File-based-source）- 读取文本文件，即符合 TextInputFormat 规范的文件，并将其作为字符串返回</li><li>基于<strong>网络套接字</strong>的source（Socket-based-source）- 从 socket 读取。元素可以用分隔符切分。</li><li><strong>自定义</strong>的source（Custom-source）</li></ul><p>下面使用addSource将Kafka数据写入Flink为例：</p><p>如果需要外部数据源对接，可使用addSource，如将Kafka数据写入Flink， 先引入依赖：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>将Kafka数据写入Flink：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val properties = new Properties()</span><br><span class="line">properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class="line">properties.setProperty(&quot;group.id&quot;, &quot;consumer-group&quot;)</span><br><span class="line">properties.setProperty(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;)</span><br><span class="line">properties.setProperty(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;)</span><br><span class="line">properties.setProperty(&quot;auto.offset.reset&quot;, &quot;latest&quot;)</span><br><span class="line"></span><br><span class="line">val source = env.addSource(new FlinkKafkaConsumer011[String](&quot;sensor&quot;, new SimpleStringSchema(), properties))</span><br></pre></td></tr></table></figure><p>基于网络套接字的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val source = env.socketTextStream(&quot;IP&quot;, PORT)</span><br></pre></td></tr></table></figure><h3 id="二、Transform转换算子-1"><a href="#二、Transform转换算子-1" class="headerlink" title="二、Transform转换算子"></a><strong>二、Transform转换算子</strong></h3><h3 id="1-map-1"><a href="#1-map-1" class="headerlink" title="1. map"></a><strong>1. map</strong></h3><p>将DataSet中的每一个元素转换为另外一个元素</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.map &#123; x =&gt; x * 2 &#125;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215093723.png"></p><h3 id="2-FlatMap"><a href="#2-FlatMap" class="headerlink" title="2. FlatMap"></a><strong>2. FlatMap</strong></h3><p>采用一个数据元并生成零个，一个或多个数据元。将句子分割为单词的flatmap函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.flatMap &#123; str =&gt; str.split(&quot; &quot;) &#125;</span><br></pre></td></tr></table></figure><h3 id="3-Filter"><a href="#3-Filter" class="headerlink" title="3. Filter"></a><strong>3. Filter</strong></h3><p>计算每个数据元的布尔函数，并保存函数返回true的数据元。过滤掉零值的过滤器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.filter &#123; _ != 0 &#125;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215093751.png"></p><h3 id="4-KeyBy"><a href="#4-KeyBy" class="headerlink" title="4. KeyBy"></a><strong>4. KeyBy</strong></h3><p>逻辑上将流分区为不相交的分区。具有相同Keys的所有记录都分配给同一分区。在内部，keyBy（）是使用散列分区实现的。指定键有不同的方法。</p><p>此转换返回KeyedStream，其中包括使用被Keys化状态所需的KeyedStream。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(0) </span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215093914.png"></p><p><strong>注意:</strong></p><p>​        这个keyBy,你给我一个dataStream,他调用keyBy之后得到一个KeyedStream,这个KeyedStream就相当于是Spark里面的,包含键值对的RDD.说白了就是元素是二元组的那个RDD.不过在我们Flink里面,没有所谓二元组的说法.不一定说二元组就是键值对.在Flink中没有这个说法.只有在Spark RDD中才有这个说法.意思就是说:你看到一个Dstream里面的元素是二元组类型,千万别认为这就是一个键值对的Dstream.这是一个非常普通的Dstream.除非他不是Dstream,而是一个KeyedStream.所以在<strong>Flink中到底判断他是不是一个键值对的Stream,就看他类型是不是KeyedStream,跟里面的元素没有关系.</strong> </p><h3 id="5-滚动聚合算子（Rolling-Aggregation）"><a href="#5-滚动聚合算子（Rolling-Aggregation）" class="headerlink" title="5. 滚动聚合算子（Rolling Aggregation）"></a>5. 滚动聚合算子（Rolling Aggregation）</h3><p>Aggregations 是 KeyedDataStream 接口提供的聚合算子，根据指定的字段进行聚合操作，滚动地产生一系列数据聚合结果。其实是将 Reduce 算子中的函数进行了封装，封装的聚合操作有sum,min,max 等，这样就不需要用户自己定义 Reduce 函数。<br>如下代码所示，指定数据集中第一个字段作为 key，用第二个字段作为累加字段，然后滚动地对第二个字段的数值进行累加并输出。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215100316.png"></p><p><strong>这些算子可以针对KeyedStream的每一个支流做聚合。</strong></p><ul><li><p> sum()</p></li><li><p> min()</p></li><li><p> max()</p></li><li><p> minBy()</p></li><li><p> maxBy()</p></li></ul><p>  <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215094055.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215100711.png"></p><h3 id="6-Reduce"><a href="#6-Reduce" class="headerlink" title="6. Reduce"></a><strong>6. Reduce</strong></h3><p>被Keys化数据流上的“滚动”Reduce。将当前数据元与最后一个Reduce的值组合并发出新值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keyedStream.reduce &#123; _ + _ &#125;  </span><br></pre></td></tr></table></figure><p><strong>KeyedStream → DataStream</strong>：一个分组数据流的聚合操作，合并当前的元素和上次聚合的结果，产生一个新的值，返回的流中包含每一次聚合的结果，而不是只返回最后一次聚合的最终结果。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val stream2 = env.readTextFile(&quot;YOUR_PATH\\sensor.txt&quot;)</span><br><span class="line">  .map( data =&gt; &#123;</span><br><span class="line">    val dataArray = data.split(&quot;,&quot;)</span><br><span class="line">    SensorReading(dataArray(0).trim, dataArray(1).trim.toLong, dataArray(2).trim.toDouble)</span><br><span class="line">  &#125;)</span><br><span class="line">  .keyBy(&quot;id&quot;)</span><br><span class="line">  .reduce( (x, y) =&gt; SensorReading(x.id, x.timestamp + 1, y.temperature) )</span><br></pre></td></tr></table></figure><h3 id="7-Fold"><a href="#7-Fold" class="headerlink" title="7. Fold"></a><strong>7. Fold</strong></h3><p>具有初始值的被Keys化数据流上的“滚动”折叠。将当前数据元与最后折叠的值组合并发出新值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val result: DataStream[String] =  keyedStream.fold(&quot;start&quot;)((str, i) =&gt; &#123; str + &quot;-&quot; + i &#125;) </span><br><span class="line"></span><br><span class="line">// 解释：当上述代码应用于序列（1,2,3,4,5）时，输出结果“start-1”，“start-1-2”，“start-1-2-3”，...</span><br></pre></td></tr></table></figure><h3 id="8-Aggregations"><a href="#8-Aggregations" class="headerlink" title="8. Aggregations"></a><strong>8. Aggregations</strong></h3><p>在被Keys化数据流上滚动聚合。min和minBy之间的差异是min返回最小值，而minBy返回该字段中具有最小值的数据元（max和maxBy相同）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">keyedStream.sum(0);</span><br><span class="line"></span><br><span class="line">keyedStream.min(0);</span><br><span class="line"></span><br><span class="line">keyedStream.max(0);</span><br><span class="line"></span><br><span class="line">keyedStream.minBy(0);</span><br><span class="line"></span><br><span class="line">keyedStream.maxBy(0);</span><br></pre></td></tr></table></figure><h3 id="9-Window"><a href="#9-Window" class="headerlink" title="9. Window"></a><strong>9. Window</strong></h3><p>可以在已经分区的KeyedStream上定义Windows。Windows根据某些特征（例如，在最后5秒内到达的数据）对每个Keys中的数据进行分组。这里不再对窗口进行详解，有关窗口的完整说明，请查看这篇文章： <strong><a href="https://mp.weixin.qq.com/s/S-RmP5OWiGqwn-C_TZNO5A">Flink 中极其重要的 Time 与 Window 详细解析</a></strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(5))); </span><br></pre></td></tr></table></figure><h3 id="10-WindowAll"><a href="#10-WindowAll" class="headerlink" title="10. WindowAll"></a><strong>10. WindowAll</strong></h3><p>Windows可以在常规DataStream上定义。Windows根据某些特征（例如，在最后5秒内到达的数据）对所有流事件进行分组。</p><p>注意：在许多情况下，这是非并行转换。所有记录将收集在windowAll 算子的一个任务中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(5)))</span><br></pre></td></tr></table></figure><h3 id="11-Window-Apply"><a href="#11-Window-Apply" class="headerlink" title="11. Window Apply"></a><strong>11. Window Apply</strong></h3><p>将一般函数应用于整个窗口。</p><p>注意：如果您正在使用windowAll转换，则需要使用AllWindowFunction。</p><p>下面是一个手动求和窗口数据元的函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">windowedStream.apply &#123; WindowFunction &#125;</span><br><span class="line"></span><br><span class="line">allWindowedStream.apply &#123; AllWindowFunction &#125;</span><br></pre></td></tr></table></figure><h3 id="12-Window-Reduce"><a href="#12-Window-Reduce" class="headerlink" title="12. Window Reduce"></a><strong>12. Window Reduce</strong></h3><p>将函数缩减函数应用于窗口并返回缩小的值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">windowedStream.reduce &#123; _ + _ &#125;</span><br></pre></td></tr></table></figure><h3 id="13-Window-Fold"><a href="#13-Window-Fold" class="headerlink" title="13. Window Fold"></a><strong>13. Window Fold</strong></h3><p>将函数折叠函数应用于窗口并返回折叠值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val result: DataStream[String] = windowedStream.fold(&quot;start&quot;, (str, i) =&gt; &#123; str + &quot;-&quot; + i &#125;) </span><br><span class="line"></span><br><span class="line">// 上述代码应用于序列（1,2,3,4,5）时，将序列折叠为字符串“start-1-2-3-4-5”</span><br></pre></td></tr></table></figure><h3 id="14-Union-真正意义上的汇合"><a href="#14-Union-真正意义上的汇合" class="headerlink" title="14. Union(真正意义上的汇合)"></a><strong>14. Union</strong>(真正意义上的汇合)</h3><p>两个或多个数据流的联合，创建包含来自所有流的所有数据元的新流。注意：如果将数据流与自身联合，则会在结果流中获取两次数据元</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.union(otherStream1, otherStream2, ...)</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215094828.png"></p><p><strong>DataStream → DataStream</strong>：对两个或者两个以上的DataStream进行union操作，产生一个包含所有DataStream元素的新DataStream。</p><h4 id="Connect与-Union-区别："><a href="#Connect与-Union-区别：" class="headerlink" title="Connect与 Union 区别："></a>Connect与 Union 区别：</h4><p>  1． Union之前两个流的类型必须是一样，Connect可以不一样，在之后的coMap中再去调整成为一样的。</p><ol start="2"><li>  Connect只能操作两个流，Union可以操作多个。</li></ol><h3 id="15-Window-Join"><a href="#15-Window-Join" class="headerlink" title="15. Window Join"></a><strong>15. Window Join</strong></h3><p>在给定Keys和公共窗口上连接两个数据流</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dataStream.join(otherStream)</span><br><span class="line">    .where(&lt;key selector&gt;).equalTo(&lt;key selector&gt;)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.seconds(3)))</span><br><span class="line">    .apply (new JoinFunction () &#123;...&#125;)</span><br></pre></td></tr></table></figure><h3 id="16-Interval-Join"><a href="#16-Interval-Join" class="headerlink" title="16. Interval Join"></a><strong>16. Interval Join</strong></h3><p>在给定的时间间隔内使用公共Keys关联两个被Key化的数据流的两个数据元e1和e2，以便e1.timestamp + lowerBound &lt;= e2.timestamp &lt;= e1.timestamp + upperBound</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">am.intervalJoin(otherKeyedStream)</span><br><span class="line">    .between(Time.milliseconds(-2), Time.milliseconds(2)) </span><br><span class="line">    .upperBoundExclusive(true) </span><br><span class="line">    .lowerBoundExclusive(true) </span><br><span class="line">    .process(new IntervalJoinFunction() &#123;...&#125;)</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214232600.png"></p><p><strong>案例代码:</strong></p><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/Flink-shangbaishuyao-RTDW-gmall-realtime/src/main/java/com/shangbaishuyao/gmall/realtime/app/DWM/OrderWideApp.java">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/Flink-shangbaishuyao-RTDW-gmall-realtime/src/main/java/com/shangbaishuyao/gmall/realtime/app/DWM/OrderWideApp.java</a></p><h3 id="17-Window-CoGroup"><a href="#17-Window-CoGroup" class="headerlink" title="17. Window CoGroup"></a><strong>17. Window CoGroup</strong></h3><p>在给定Keys和公共窗口上对两个数据流进行Cogroup</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dataStream.coGroup(otherStream)</span><br><span class="line">    .where(0).equalTo(1)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.seconds(3)))</span><br><span class="line">    .apply (new CoGroupFunction () &#123;...&#125;)</span><br></pre></td></tr></table></figure><h3 id="18-Connect"><a href="#18-Connect" class="headerlink" title="18. Connect"></a><strong>18. Connect</strong></h3><p>“连接”两个保存其类型的数据流。连接允许两个流之间的共享状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Integer&gt; someStream = ... DataStream&lt;String&gt; otherStream = ... ConnectedStreams&lt;Integer, String&gt; connectedStreams = someStream.connect(otherStream)</span><br><span class="line"></span><br><span class="line">// ... 代表省略中间操作</span><br></pre></td></tr></table></figure><h3 id="19-CoMap，CoFlatMap"><a href="#19-CoMap，CoFlatMap" class="headerlink" title="19. CoMap，CoFlatMap"></a><strong>19. CoMap，CoFlatMap</strong></h3><p>Connect做连接的,把两个流连在一起.也可以理解为做回合,把两个流汇合在一起.</p><p>Connect把两个流汇合在一起有条件吗?stream1是整形,stream2是字符串的这样的两个流可以汇合在一起吗?他是可以的.他不管你两个流是否类型一致.他都可以汇合在一起.connect会和其实和上面的splitStream一样,都没有被真正的汇合或切分.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215095927.png"></p><p>类似于连接数据流上的map和flatMap</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">connectedStreams.map(</span><br><span class="line">    (_ : Int) =&gt; true,</span><br><span class="line">    (_ : String) =&gt; false)connectedStreams.flatMap(</span><br><span class="line">    (_ : Int) =&gt; true,</span><br><span class="line">    (_ : String) =&gt; false)</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215094623.png"></p><p><strong>DataStream,DataStream → ConnectedStreams</strong>：连接两个保持他们类型的数据流，两个数据流被Connect之后，只是被放在了一个同一个流中，内部依然保持各自的数据和形式不发生任何变化，两个流相互独立。</p><p> <strong>CoMap,CoFlatMap</strong><br><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215094718.png"></p><p>从这张图可以看出,他只有调用map或者FlatMap才真正将两个流汇合起来的.<br>为什么要执行map或者FlatMap呢?因为执行map算子或者flatmap算子之后他就可以在map算子或者FlatMap算子中把这两个流分别做处理.分别处理成同样类型的这时返回得到一个新的对象,他就不会出现两个流了,他就变成一个流了.</p><p><strong>ConnectedStreams → DataStream</strong>：作用于ConnectedStreams上，功能与map和flatMap一样，对ConnectedStreams中的每一个Stream分别进行map和flatMap处理。</p><h3 id="20-Split-不是真正意义上切分流-只是打了标记"><a href="#20-Split-不是真正意义上切分流-只是打了标记" class="headerlink" title="20. Split(不是真正意义上切分流,只是打了标记)"></a><strong>20. Split(不是真正意义上切分流,只是打了标记)</strong></h3><p>根据某些标准将流拆分为两个或更多个流</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val split = someDataStream.split(</span><br><span class="line">  (num: Int) =&gt;</span><br><span class="line">    (num % 2) match &#123;</span><br><span class="line">      case 0 =&gt; List(&quot;even&quot;)</span><br><span class="line">      case 1 =&gt; List(&quot;odd&quot;)</span><br><span class="line">    &#125;)      </span><br></pre></td></tr></table></figure><h3 id="21-Select-真正意义上的切分流"><a href="#21-Select-真正意义上的切分流" class="headerlink" title="21. Select(真正意义上的切分流)"></a><strong>21. Select(真正意义上的切分流)</strong></h3><p>从拆分流中选择一个或多个流</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SplitStream&lt;Integer&gt; split;DataStream&lt;Integer&gt; even = split.select(&quot;even&quot;);DataStream&lt;Integer&gt; odd = split.select(&quot;odd&quot;);DataStream&lt;Integer&gt; all = split.select(&quot;even&quot;,&quot;odd&quot;)</span><br></pre></td></tr></table></figure><h4 id="Split-和-Select"><a href="#Split-和-Select" class="headerlink" title="Split 和 Select:"></a>Split 和 Select:</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215095211.png"></p><p><strong>DataStream → SplitStream</strong>：根据某些特征把一个DataStream拆分成两个或者多个DataStream。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215095246.png"></p><p><strong>SplitStream→DataStream</strong>：从一个SplitStream中获取一个或者多个DataStream。</p><p>需求：传感器数据按照温度高低（以30度为界），拆分成两个流。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val splitStream = stream2</span><br><span class="line">  .split( sensorData =&gt; &#123;</span><br><span class="line">    if (sensorData.temperature &gt; 30) Seq(&quot;high&quot;) else Seq(&quot;low&quot;)</span><br><span class="line">  &#125; )</span><br><span class="line"></span><br><span class="line">val high = splitStream.select(&quot;high&quot;)</span><br><span class="line">val low = splitStream.select(&quot;low&quot;)</span><br><span class="line">val all = splitStream.select(&quot;high&quot;, &quot;low&quot;)</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215095354.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215095336.png"></p><h3 id="三、Sink算子-1"><a href="#三、Sink算子-1" class="headerlink" title="三、Sink算子"></a><strong>三、Sink算子</strong></h3><p>支持将数据输出到：</p><ul><li>本地文件(参考批处理)</li><li>本地集合(参考批处理)</li><li>HDFS(参考批处理)</li></ul><p>除此之外，还支持：</p><ul><li>sink到kafka</li><li>sink到mysql</li><li>sink到redis</li></ul><p>下面以sink到kafka为例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">val sinkTopic = &quot;test&quot;</span><br><span class="line"></span><br><span class="line">//样例类</span><br><span class="line">case class Student(id: Int, name: String, addr: String, sex: String)</span><br><span class="line">val mapper: ObjectMapper = new ObjectMapper()</span><br><span class="line"></span><br><span class="line">//将对象转换成字符串</span><br><span class="line">def toJsonString(T: Object): String = &#123;</span><br><span class="line">    mapper.registerModule(DefaultScalaModule)</span><br><span class="line">    mapper.writeValueAsString(T)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    //1.创建流执行环境</span><br><span class="line">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    //2.准备数据</span><br><span class="line">    val dataStream: DataStream[Student] = env.fromElements(</span><br><span class="line">      Student(8, &quot;xiaoming&quot;, &quot;beijing biejing&quot;, &quot;female&quot;)</span><br><span class="line">    )</span><br><span class="line">    //将student转换成字符串</span><br><span class="line">    val studentStream: DataStream[String] = dataStream.map(student =&gt;</span><br><span class="line">      toJsonString(student) // 这里需要显示SerializerFeature中的某一个，否则会报同时匹配两个方法的错误</span><br><span class="line">    )</span><br><span class="line">    //studentStream.print()</span><br><span class="line">    val prop = new Properties()</span><br><span class="line">    prop.setProperty(&quot;bootstrap.servers&quot;, &quot;node01:9092&quot;)</span><br><span class="line"></span><br><span class="line">    val myProducer = new FlinkKafkaProducer011[String](sinkTopic, new KeyedSerializationSchemaWrapper[String](new SimpleStringSchema()), prop)</span><br><span class="line">    studentStream.addSink(myProducer)</span><br><span class="line">    studentStream.print()</span><br><span class="line">    env.execute(&quot;Flink add sink&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>知识源于积累,登峰造极源于自律!</p><p>好文章就得收藏慢慢品, 文章转载于:<a href="https://zhuanlan.zhihu.com/p/356616078">https://zhuanlan.zhihu.com/p/356616078</a> 在此基础上做的CRUD.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215092137.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Flink和Spark类似，也是一种一站式处理的框架；既可以进行批处理（DataSet），也可以进行实时处理（DataStream）。&lt;/p&gt;
&lt;p&gt;所以下面将Flink的算子分为两大类：一类是DataSet，一类是DataStream。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flink官网:&lt;/strong&gt; &lt;a href=&quot;https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/dev/datastream/operators/overview/&quot;&gt;https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/dev/datastream/operators/overview/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;案例代码:&lt;/strong&gt; &lt;a href=&quot;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/tree/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo&quot;&gt;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/tree/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink实现UDF函数——更细粒度的控制流</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E5%AE%9E%E7%8E%B0UDF%E5%87%BD%E6%95%B0%E2%80%94%E2%80%94%E6%9B%B4%E7%BB%86%E7%B2%92%E5%BA%A6%E7%9A%84%E6%8E%A7%E5%88%B6%E6%B5%81/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E5%AE%9E%E7%8E%B0UDF%E5%87%BD%E6%95%B0%E2%80%94%E2%80%94%E6%9B%B4%E7%BB%86%E7%B2%92%E5%BA%A6%E7%9A%84%E6%8E%A7%E5%88%B6%E6%B5%81/</id>
    <published>2022-02-15T00:58:42.000Z</published>
    <updated>2022-02-15T01:16:29.101Z</updated>
    
    <content type="html"><![CDATA[<p>Flink的一个优势,是其他计算引擎所做不到的.或者说能做到,但是代码特别麻烦,我们的Flink中每一个算子,他都给你提供了一个函数对象作为参数</p><p><strong>好记心烂笔头</strong>:</p><p>为什么我们不用匿名函数(lambda function)去写Flink代码呢?</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">因为Flink中花括号里面的代码都是运行在slot中的,但是那么多slot,每个slot里面都可能执行,那数据库就被搞死了.所以我们就不用匿名函数的方式写代码了.而是使用函数类的方式.</span><br></pre></td></tr></table></figure><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215090044.png"></p><h1 id="函数类（Function-Classes）"><a href="#函数类（Function-Classes）" class="headerlink" title="函数类（Function Classes）"></a>函数类（Function Classes）</h1><p>Flink暴露了所有<strong>udf函数的接口</strong>(实现方式为接口或者抽象类)。例如MapFunction做转换的, FilterFunction做过滤的, ProcessFunction不知道做转换,做过滤还是做其他的,<strong>但是我肯定要处理数据,则使用processFunction,他是没有限制的,你想做什么你就在这里面写什么就好了</strong>等等。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215090143.png"></p><p>下面例子实现了FilterFunction接口：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class FilterFilter extends FilterFunction[String] &#123;</span><br><span class="line">      override def filter(value: String): Boolean = &#123;</span><br><span class="line">        value.contains(&quot;flink&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br><span class="line">val flinkTweets = tweets.filter(new FlinkFilter)</span><br></pre></td></tr></table></figure><p>还可以将函数实现成匿名类</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val flinkTweets = tweets.filter(</span><br><span class="line">new RichFilterFunction[String] &#123;</span><br><span class="line">override def filter(value: String): Boolean = &#123;</span><br><span class="line">value.contains(&quot;flink&quot;)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>我们filter的字符串”flink”还可以当作参数传进去。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val tweets: DataStream[String] = ...</span><br><span class="line">val flinkTweets = tweets.filter(new KeywordFilter(&quot;flink&quot;))</span><br><span class="line"></span><br><span class="line">class KeywordFilter(keyWord: String) extends FilterFunction[String] &#123;</span><br><span class="line">override def filter(value: String): Boolean = &#123;</span><br><span class="line">value.contains(keyWord)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="匿名函数（Lambda-Functions）"><a href="#匿名函数（Lambda-Functions）" class="headerlink" title="匿名函数（Lambda Functions）"></a>匿名函数（Lambda Functions）</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val tweets: DataStream[String] = ...</span><br><span class="line">val flinkTweets = tweets.filter(_.contains(&quot;flink&quot;))</span><br></pre></td></tr></table></figure><h1 id="富函数（Rich-Functions"><a href="#富函数（Rich-Functions" class="headerlink" title="富函数（Rich Functions)"></a>富函数（Rich Functions)</h1><p><strong>富函数和函数类是一样的,不过他有一个特点.他增加了生命周期的管理.什么叫生命周期?就是    XXXFunction()什么时候初始化.什么时候销毁.什么时候执行你们的代码等就是所谓的生命周期的管理.XXXFunction()本身是没有生命周期的管理的.如果你需要增加生命周期的管理,你需要继承RichMapFunction()</strong></p><p>“富函数”是DataStream API提供的一个函数类的接口，所有Flink函数类都有其Rich版本。它与常规函数的不同在于，可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。</p><ul><li> RichMapFunction</li><li> RichFlatMapFunction</li><li> RichFilterFunction</li><li> …</li></ul><p>Rich Function有一个生命周期的概念。典型的生命周期方法有：</p><ul><li> open()方法是rich function的初始化方法，当一个算子例如map或者filter被调用之前open()会被调用。</li><li> close()方法是生命周期中的最后一个调用的方法，做一些清理工作。</li><li> getRuntimeContext()方法提供了函数的RuntimeContext的一些信息，例如函数执行的并行度，任务的名字，以及state状态</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class MyFlatMap extends RichFlatMapFunction[Int, (Int, Int)] &#123;</span><br><span class="line">var subTaskIndex = 0</span><br><span class="line"></span><br><span class="line">override def open(configuration: Configuration): Unit = &#123;</span><br><span class="line">subTaskIndex = getRuntimeContext.getIndexOfThisSubtask</span><br><span class="line">// 以下可以做一些初始化工作，例如建立一个和HDFS的连接</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">override def flatMap(in: Int, out: Collector[(Int, Int)]): Unit = &#123;</span><br><span class="line">if (in % 2 == subTaskIndex) &#123;</span><br><span class="line">out.collect((subTaskIndex, in))</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">override def close(): Unit = &#123;</span><br><span class="line">// 以下做一些清理工作，例如断开和HDFS的连接。</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="案例代码"><a href="#案例代码" class="headerlink" title="案例代码"></a>案例代码</h1><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-from-shangbaishuyao-wordCount/src/main/scala/com/shangbaishuyao/processFunction/">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-from-shangbaishuyao-wordCount/src/main/scala/com/shangbaishuyao/processFunction/</a></p><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo03/Flink04_Transform_Reduce.java">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo03/Flink04_Transform_Reduce.java</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Flink的一个优势,是其他计算引擎所做不到的.或者说能做到,但是代码特别麻烦,我们的Flink中每一个算子,他都给你提供了一个函数对象作为参数&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;好记心烂笔头&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;为什么我们不用匿名函数(lambda function)去写Flink代码呢?&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;因为Flink中花括号里面的代码都是运行在slot中的,但是那么多slot,每个slot里面都可能执行,那数据库就被搞死了.所以我们就不用匿名函数的方式写代码了.而是使用函数类的方式.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink CEP复杂事件处理</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink-CEP%E5%A4%8D%E6%9D%82%E4%BA%8B%E4%BB%B6%E5%A4%84%E7%90%86/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink-CEP%E5%A4%8D%E6%9D%82%E4%BA%8B%E4%BB%B6%E5%A4%84%E7%90%86/</id>
    <published>2022-02-14T14:52:35.000Z</published>
    <updated>2022-02-14T15:31:08.053Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flink-CEP-代码案例"><a href="#Flink-CEP-代码案例" class="headerlink" title="Flink CEP 代码案例"></a>Flink CEP 代码案例</h1><p>登录告警系统:  一堆的登录日志从，匹配一个恶意登录的模式（如果一个用户连续失败三次，则是恶意登录），从而找到哪些用户名是用于恶意 登录</p><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-from-shangbaishuyao-wordCount/src/main/scala/com/shangbaishuyao/cep/TestCepDemo.scala">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-from-shangbaishuyao-wordCount/src/main/scala/com/shangbaishuyao/cep/TestCepDemo.scala</a></p><p>登录失败CEP模型:</p><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo09/Flink03_Practice_LoginFailWithCEP.java">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo09/Flink03_Practice_LoginFailWithCEP.java</a></p><p>支付失败CEP模型:</p><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo09/Flink04_Practice_OrderPayWithCEP.java">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo09/Flink04_Practice_OrderPayWithCEP.java</a></p><span id="more"></span><h1 id="什么是复杂事件处理CEP"><a href="#什么是复杂事件处理CEP" class="headerlink" title="什么是复杂事件处理CEP"></a>什么是复杂事件处理CEP</h1><p>​        CEP是做复杂事件处理的.如果你碰到一个业务需求非常之复杂,而且他的条件也是非常复杂的.比如说处理某一个,符合某一个条件,做什么事情.这个条件是非常复杂的.那么处理复杂的事件,Flink专门有一套API.这套API的名字叫CEP. 而spark是没有CEP的.</p><p>​    一个或多个由简单事件构成的事件流通过一定的规则匹配，然后输出用户想得到的数据，满足规则的复杂事件。</p><p>​    特征：</p><p>​        Ø 目标：从有序的简单事件流中发现一些高阶特征(无序的就得加延时操作)</p><p>​        Ø 输入：一个或多个由简单事件构成的事件流</p><p>​        Ø 处理：识别简单事件之间的内在联系，多个符合一定规则的简单事件构成复杂事件</p><p>​        Ø 输出：满足规则的复杂事件</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214225403.png"></p><p>​        如上图所示: 如果一个用户短时间内频繁登录失败，就有可能是出现了程序的恶意攻击，比如密码暴力破解。因此我们考虑，应该对用户的登录失败动作进行统计，具体来说，如果同一用户（可以是不同IP）在2秒之内连续两次登录失败，就认为存在恶意登录的风险，输出相关的信息进行报警提示。</p><p>这个恶意登录监控有两种解决办法. 一.状态编程. 二.CEP编程.  </p><p>​        比如同一个用户(可以是不同ip)在2秒内连续2次登录失败,我们就触发报警.打一个告警信息. 我们可以引入一个listState. 将上一次用户登录的数据放到这个列表中. 后面还出现相同数据,继续放到列表中. 列表的长度等于2了或者大于2了. 就开始发出一个告警. 不过用状态编程会有问题. 会造成内存的容量的增加.还可能造成一些精准的业务无法做到. 比如说我不要连续两秒内两次登录了. 改成两秒内连续5次登录失败.才将这样的用户找出来. 这种需求改起来特别复杂.因为中间很有可能穿插一些其他东西. 如上图: 比如我要删选长方形和紧跟着圆形的数据. 后来需求改成长方形后面有圆形的数据了. 这两个之间很可能穿插了一些其他的三角形的一些东西. 那这些穿插的需不需要考虑放到listState中去. 而且还要考虑,这个时间是否到达5秒. 所以用状态编程不适合做恶意登录风险.所以当前业务我们应该使用Flink的CEP的库来做.</p><p>​        CEP用于分析低延迟、频繁产生的不同来源的事件流。CEP可以帮助在复杂的、不相关的事件流中找出有意义的模式和复杂的关系，以接近实时或准实时的获得通知并阻止一些行为。</p><p>​        CEP支持在流上进行模式匹配，根据模式的条件不同，分为连续的条件或不连续的条件；模式的条件允许有时间的限制，当在条件范围内没有达到满足的条件时，会导致模式匹配超时。</p><p>看起来很简单，但是它有很多不同的功能：</p><p>​        Ø 输入的流数据，尽快产生结果</p><p>​        Ø 在2个event流上，基于时间进行聚合类的计算</p><p>​        Ø 提供实时/准实时的警告和通知</p><p>​        Ø 在多样的数据源中产生关联并分析模式</p><p>​        Ø 高吞吐、低延迟的处理</p><p>市场上有多种CEP的解决方案，例如Spark、Samza、Beam等，但他们都没有提供专门的library支持。但是Flink提供了专门的CEP library。</p><h1 id="Flink-CEP"><a href="#Flink-CEP" class="headerlink" title="Flink CEP"></a>Flink CEP</h1><p>Flink为CEP提供了专门的Flink CEP library，它包含如下组件：</p><p>FlinkCEP的步骤就四个. 一是准备好数据.该分组分组.改设置waterMark的设置Watermark.该过滤过滤. 因为CEP本质上就是一个窗口函数.它里面封装了开窗.因为一般情况下,我们会设置在某一个时间内满足这个条件的.如果说没有时间限制的话.那这个符合这个规则的数据找到一堆. 或者找不到. 比如找到一本矩形开头的,但是后面数据源源不断的来.因为他是一个无线的流.所以会造成他匹配不成功.所以一般来说我们一定要限制一个时间范围.当你限制一个时间范围的话,实际上就是开窗了.所以前面你要定义时间语义,定义是否有水位线之类的.</p><p>第二个就是定义我们的模式. 即定义我们的pattern. 定义好了之后,我们的pattern会帮我们检测.模式就是我们的规则. 检测实际上会出现两种情况.一种情况符合这个规则. 还有种情况是不符合这个规则.其实就是一种匹配成功了.一种没有匹配成功. 但是一般情况下我们只是处理匹配成功的. 匹配成功的数据怎么拿出来呢? 就是生成Alert.  这里检测的时候为什么要说能够帮我检测根据规则去匹配成功的,也可以匹配哪些不成功的呢?因为我们以后可能遇到一些用排除法做模式匹配. 如果不用排除法发现一些业务太复杂了,我们不好加一些条件去设置规则. 到时候发现这个业务可以用排除法的话.而且反证的条件是容易定义的.那我就把反证的模式定义好.定义好之后我只需要找那些没有匹配上的.没有匹配上的就是我所需要的.</p><p>​        Ø Event Stream</p><p>​        Ø pattern定义</p><p>​        Ø pattern检测</p><p>​        Ø 生成Alert</p><p><strong>Flink CEP 四步骤:</strong></p><p>定义模式,只要匹配订单在创建之后15分钟内有支付的事件.单一模式</p><p>模式检测</p><p>生产Alter. 找到所有的在15分钟内支付的订单事件, 并且还要找到没有支付的订单事件</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214225700.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214231635.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214231644.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214231739.png"></p><p>notFollowedBy的意思就是不想让某一个是事件在两个事件之间发生.</p><p>因为notFollowedBy()绝对不可能放在最后面. 他只会放在两个个体模式之间. 既然是在两个个体模式的意思.就意味着表示在两个个体不同的事件之间不出现一个什么样的事件. 所以称之为叫不想要某个事件在两个事件之间发生.这个模式也是可以指定时间的. 用within里面指定时间.</p><p>超时事件的提取: 就是提取匹配的反的模式</p><p>超时事件的提取. 假设我有一个窗口里面有许多数据, 我要匹配 :</p><p>矩形 followBy 圆形 的.  我只要是矩形开头圆形结尾就可以匹配到.</p><p>但是有这么一种情况就是: 我在窗口最后位置有一个矩形,后面就没数据了.  就是在这个5分钟窗口内找不到矩形开头后面紧跟着圆圈的了.像这样的一个数据事件我们也可以在后面选取的时候将他选着出来.所以他有两种事件的选取. 一种是匹配事件的选取. 一种是超时事件的提取. 超时事件提取的话,会用到另外一个函数类叫patternTimeOutFunction</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214232124.png"></p><p>首先，开发人员要在DataStream流上定义出模式条件，之后Flink CEP引擎进行模式检测，必要时生成告警。</p><p>为了使用Flink CEP，我们需要导入依赖：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-cep_$&#123;scala.binary.version&#125;&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h2 id="Event-Streams"><a href="#Event-Streams" class="headerlink" title="Event Streams"></a><strong>Event Streams</strong></h2><p>以登陆事件流为例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">case class LoginEvent(userId: String, ip: String, eventType: String, eventTime: String)</span><br><span class="line"></span><br><span class="line">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line">env.setParallelism(1)</span><br><span class="line"></span><br><span class="line">val loginEventStream = env.fromCollection(List(</span><br><span class="line">  LoginEvent(&quot;1&quot;, &quot;192.168.0.1&quot;, &quot;fail&quot;, &quot;1558430842&quot;),</span><br><span class="line">  LoginEvent(&quot;1&quot;, &quot;192.168.0.2&quot;, &quot;fail&quot;, &quot;1558430843&quot;),</span><br><span class="line">  LoginEvent(&quot;1&quot;, &quot;192.168.0.3&quot;, &quot;fail&quot;, &quot;1558430844&quot;),</span><br><span class="line">  LoginEvent(&quot;2&quot;, &quot;192.168.10.10&quot;, &quot;success&quot;, &quot;1558430845&quot;)</span><br><span class="line">)).assignAscendingTimestamps(_.eventTime.toLong)</span><br></pre></td></tr></table></figure><h2 id="Pattern-API"><a href="#Pattern-API" class="headerlink" title="Pattern API"></a><strong>Pattern API</strong></h2><p>每个Pattern都应该包含几个步骤，或者叫做state。从一个state到另一个state，通常我们需要定义一些条件，例如下列的代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val loginFailPattern = Pattern.begin[LoginEvent](&quot;begin&quot;)</span><br><span class="line">  .where(_.eventType.equals(&quot;fail&quot;))</span><br><span class="line">  .next(&quot;next&quot;)</span><br><span class="line">  .where(_.eventType.equals(&quot;fail&quot;))</span><br><span class="line">  .within(Time.seconds(10)</span><br></pre></td></tr></table></figure><p>每个state都应该有一个标示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例如 .begin[LoginEvent](&quot;begin&quot;)中的&quot;begin&quot;</span><br></pre></td></tr></table></figure><p>每个state都需要有一个唯一的名字，而且需要一个filter来过滤条件，这个过滤条件定义事件需要符合的条件，例如: </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.where(_.eventType.equals(&quot;fail&quot;))</span><br></pre></td></tr></table></figure><p>我们也可以通过subtype来限制event的子类型:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start.subtype(SubEvent.class).where(...);</span><br></pre></td></tr></table></figure><p>事实上，你可以多次调用subtype和where方法；而且如果where条件是不相关的，你可以通过or来指定一个单独的filter函数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pattern.where(...).or(...);</span><br></pre></td></tr></table></figure><p>之后，我们可以在此条件基础上，通过next或者followedBy方法切换到下一个state，next的意思是说上一步符合条件的元素之后紧挨着的元素；而followedBy并不要求一定是挨着的元素。这两者分别称为严格近邻和非严格近邻。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val strictNext = start.next(&quot;middle&quot;)</span><br><span class="line">val nonStrictNext = start.followedBy(&quot;middle&quot;)</span><br></pre></td></tr></table></figure><p>最后，我们可以将所有的Pattern的条件限定在一定的时间范围内：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">next.within(Time.seconds(10))</span><br></pre></td></tr></table></figure><p>这个时间可以是Processing Time，也可以是Event Time。</p><h2 id="Pattern-检测"><a href="#Pattern-检测" class="headerlink" title="Pattern 检测"></a><strong>Pattern 检测</strong></h2><p>通过一个input DataStream以及刚刚我们定义的Pattern，我们可以创建一个PatternStream：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val input = ...</span><br><span class="line">val pattern = ...</span><br><span class="line"></span><br><span class="line">val patternStream = CEP.pattern(input, pattern)</span><br><span class="line">val patternStream = CEP.pattern(loginEventStream.keyBy(_.userId), loginFailPattern)</span><br></pre></td></tr></table></figure><p>一旦获得PatternStream，我们就可以通过select或flatSelect，从一个Map序列找到我们需要的警告信息。</p><h2 id="select"><a href="#select" class="headerlink" title="select"></a><strong>select</strong></h2><p>select方法需要实现一个PatternSelectFunction，通过select方法来输出需要的警告。它接受一个Map对，包含string/event，其中key为state的名字，event则为真实的Event。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val loginFailDataStream = patternStream</span><br><span class="line">  .select((pattern: Map[String, Iterable[LoginEvent]]) =&gt; &#123;</span><br><span class="line">    val first = pattern.getOrElse(&quot;begin&quot;, null).iterator.next()</span><br><span class="line">    val second = pattern.getOrElse(&quot;next&quot;, null).iterator.next()</span><br><span class="line"></span><br><span class="line">    Warning(first.userId, first.eventTime, second.eventTime, &quot;warning&quot;)</span><br><span class="line">  &#125;)</span><br></pre></td></tr></table></figure><p>其返回值仅为1条记录。</p><h2 id="flatSelect"><a href="#flatSelect" class="headerlink" title="flatSelect"></a><strong>flatSelect</strong></h2><p>通过实现PatternFlatSelectFunction，实现与select相似的功能。唯一的区别就是flatSelect方法可以返回多条记录，它通过一个Collector[OUT]类型的参数来将要输出的数据传递到下游。</p><h2 id="超时事件的处理"><a href="#超时事件的处理" class="headerlink" title="超时事件的处理"></a><strong>超时事件的处理</strong></h2><p>通过within方法，我们的parttern规则将匹配的事件限定在一定的窗口范围内。当有超过窗口时间之后到达的event，我们可以通过在select或flatSelect中，实现PatternTimeoutFunction和PatternFlatTimeoutFunction来处理这种情况。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">val patternStream: PatternStream[Event] = CEP.pattern(input, pattern)</span><br><span class="line"></span><br><span class="line">val outputTag = OutputTag[String](&quot;side-output&quot;)</span><br><span class="line"></span><br><span class="line">val result: SingleOutputStreamOperator[ComplexEvent] = patternStream.select(outputTag)&#123;</span><br><span class="line">    (pattern: Map[String, Iterable[Event]], timestamp: Long) =&gt; TimeoutEvent()</span><br><span class="line">&#125; &#123;</span><br><span class="line">    pattern: Map[String, Iterable[Event]] =&gt; ComplexEvent()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val timeoutResult: DataStream&lt;TimeoutEvent&gt; = result.getSideOutput(outputTag)</span><br></pre></td></tr></table></figure><h1 id="附录-Flink的双流join"><a href="#附录-Flink的双流join" class="headerlink" title="附录:Flink的双流join"></a>附录:Flink的双流join</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214232600.png"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Flink-CEP-代码案例&quot;&gt;&lt;a href=&quot;#Flink-CEP-代码案例&quot; class=&quot;headerlink&quot; title=&quot;Flink CEP 代码案例&quot;&gt;&lt;/a&gt;Flink CEP 代码案例&lt;/h1&gt;&lt;p&gt;登录告警系统:  一堆的登录日志从，匹配一个恶意登录的模式（如果一个用户连续失败三次，则是恶意登录），从而找到哪些用户名是用于恶意 登录&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-from-shangbaishuyao-wordCount/src/main/scala/com/shangbaishuyao/cep/TestCepDemo.scala&quot;&gt;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-from-shangbaishuyao-wordCount/src/main/scala/com/shangbaishuyao/cep/TestCepDemo.scala&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;登录失败CEP模型:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo09/Flink03_Practice_LoginFailWithCEP.java&quot;&gt;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo09/Flink03_Practice_LoginFailWithCEP.java&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;支付失败CEP模型:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo09/Flink04_Practice_OrderPayWithCEP.java&quot;&gt;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo09/Flink04_Practice_OrderPayWithCEP.java&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: FlinkSQL的Table API 与SQL之函数（Functions）</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-FlinkSQL%E7%9A%84Table-API-%E4%B8%8ESQL%E4%B9%8B%E5%87%BD%E6%95%B0%EF%BC%88Functions%EF%BC%89/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-FlinkSQL%E7%9A%84Table-API-%E4%B8%8ESQL%E4%B9%8B%E5%87%BD%E6%95%B0%EF%BC%88Functions%EF%BC%89/</id>
    <published>2022-02-14T14:00:35.000Z</published>
    <updated>2022-02-14T14:14:24.887Z</updated>
    
    <content type="html"><![CDATA[<p>Flink Table 和 SQL内置了很多SQL中支持的函数；如果有无法满足的需要，则可以实现用户自定义的函数（UDF）来解决。</p><span id="more"></span><h2 id="系统内置函数"><a href="#系统内置函数" class="headerlink" title="系统内置函数"></a><strong>系统内置函数</strong></h2><p>Flink Table API 和 SQL为用户提供了一组用于数据转换的内置函数。SQL中支持的很多函数，Table API和SQL都已经做了实现，其它还在快速开发扩展中。</p><p>以下是一些典型函数的举例，全部的内置函数，可以参考官网介绍。</p><h3 id="1-比较函数"><a href="#1-比较函数" class="headerlink" title="1.比较函数"></a>1.比较函数</h3><p>SQL：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">value1 = value2</span><br><span class="line"></span><br><span class="line">value1 &gt; value2</span><br></pre></td></tr></table></figure><p>Table API：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ANY1 === ANY2</span><br><span class="line"></span><br><span class="line">ANY1 &gt; ANY2</span><br></pre></td></tr></table></figure><h3 id="2-逻辑函数"><a href="#2-逻辑函数" class="headerlink" title="2.逻辑函数"></a>2.逻辑函数</h3><p>SQL：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">boolean1 OR boolean2</span><br><span class="line"></span><br><span class="line">boolean IS FALSE</span><br><span class="line"></span><br><span class="line">NOT boolean</span><br></pre></td></tr></table></figure><p>Table API：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">BOOLEAN1 || BOOLEAN2</span><br><span class="line"></span><br><span class="line">BOOLEAN.isFalse</span><br><span class="line"></span><br><span class="line">!BOOLEAN</span><br></pre></td></tr></table></figure><h3 id="3-算术函数"><a href="#3-算术函数" class="headerlink" title="3.算术函数"></a>3.算术函数</h3><p>SQL：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">numeric1 + numeric2</span><br><span class="line"></span><br><span class="line">POWER(numeric1, numeric2)</span><br></pre></td></tr></table></figure><p>Table API：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">NUMERIC1 + NUMERIC2</span><br><span class="line"></span><br><span class="line">NUMERIC1.power(NUMERIC2)</span><br></pre></td></tr></table></figure><h3 id="4-字符串函数"><a href="#4-字符串函数" class="headerlink" title="4.字符串函数"></a>4.字符串函数</h3><p>SQL：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">string1 || string2</span><br><span class="line"></span><br><span class="line">UPPER(string)</span><br><span class="line"></span><br><span class="line">CHAR_LENGTH(string)</span><br></pre></td></tr></table></figure><p>Table API：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">STRING1 + STRING2</span><br><span class="line"></span><br><span class="line">STRING.upperCase()</span><br><span class="line"></span><br><span class="line">STRING.charLength()</span><br></pre></td></tr></table></figure><h3 id="5-时间函数"><a href="#5-时间函数" class="headerlink" title="5.时间函数"></a>5.时间函数</h3><p>SQL：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DATE string</span><br><span class="line"></span><br><span class="line">TIMESTAMP string</span><br><span class="line"></span><br><span class="line">CURRENT_TIME</span><br><span class="line"></span><br><span class="line">INTERVAL string range</span><br></pre></td></tr></table></figure><p>Table API：    </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">STRING.toDate</span><br><span class="line"></span><br><span class="line">STRING.toTimestamp</span><br><span class="line"></span><br><span class="line">currentTime()</span><br><span class="line"></span><br><span class="line">NUMERIC.days</span><br><span class="line"></span><br><span class="line">NUMERIC.minutes</span><br></pre></td></tr></table></figure><h3 id="6-聚合函数"><a href="#6-聚合函数" class="headerlink" title="6.聚合函数"></a>6.聚合函数</h3><p>SQL：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">COUNT(*)</span><br><span class="line"></span><br><span class="line">SUM([ ALL | DISTINCT ] expression)</span><br><span class="line"></span><br><span class="line">RANK()</span><br><span class="line"></span><br><span class="line">ROW_NUMBER()</span><br></pre></td></tr></table></figure><p>Table API：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FIELD.count</span><br><span class="line"></span><br><span class="line">FIELD.sum0  </span><br></pre></td></tr></table></figure><h2 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a><strong>UDF</strong></h2><p>​        用户定义函数（User-defined Functions，UDF）是一个重要的特性，因为它们显著地扩展了查询（Query）的表达能力。一些系统内置函数无法解决的需求，我们可以用UDF来自定义实现。</p><h3 id="注册用户自定义函数UDF"><a href="#注册用户自定义函数UDF" class="headerlink" title="注册用户自定义函数UDF"></a><strong>注册用户自定义函数UDF</strong></h3><p>​        在大多数情况下，用户定义的函数必须先注册，然后才能在查询中使用。不需要专门为Scala 的Table API注册函数。</p><p>函数通过调用registerFunction（）方法在TableEnvironment中注册。当用户定义的函数被注册时，它被插入到TableEnvironment的函数目录中，这样Table API或SQL解析器就可以识别并正确地解释它。</p><h3 id="标量函数（Scalar-Functions）"><a href="#标量函数（Scalar-Functions）" class="headerlink" title="标量函数（Scalar Functions）"></a><strong>标量函数（Scalar Functions）</strong></h3><p>​        用户定义的标量函数，可以将0、1或多个标量值，映射到新的标量值。</p><p>​        为了定义标量函数，必须在org.apache.flink.table.functions中扩展基类Scalar Function，并实现（一个或多个）求值（evaluation，eval）方法。标量函数的行为由求值方法决定，求值方法必须公开声明并命名为eval（直接def声明，没有override）。求值方法的参数类型和返回类型，确定了标量函数的参数和返回类型。</p><p>在下面的代码中，我们定义自己的HashCode函数，在TableEnvironment中注册它，并在查询中调用它。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// 自定义一个标量函数</span><br><span class="line">public static class HashCode extends ScalarFunction &#123;</span><br><span class="line">    private int factor = 13;</span><br><span class="line"></span><br><span class="line">    public HashCode(int factor) &#123;</span><br><span class="line">        this.factor = factor;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public int eval(String s) &#123;</span><br><span class="line">        return s.hashCode() * factor;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>主函数中调用，计算sensor id的哈希值（前面部分照抄，流环境、表环境、读取source、建表）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String[] args) throws Exception &#123;</span><br><span class="line">    // 1. 创建环境</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(1);</span><br><span class="line"></span><br><span class="line">    StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">    // 2. 读取文件，得到 DataStream</span><br><span class="line">    String filePath = &quot;..\\sensor.txt&quot;;</span><br><span class="line"></span><br><span class="line">    DataStream&lt;String&gt; inputStream = env.readTextFile(filePath);</span><br><span class="line"></span><br><span class="line">    // 3. 转换成 Java Bean，并指定timestamp和watermark</span><br><span class="line">    DataStream&lt;SensorReading&gt; dataStream = inputStream</span><br><span class="line">            .map( line -&gt; &#123;</span><br><span class="line">                String[] fields = line.split(&quot;,&quot;);</span><br><span class="line">                return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2]));</span><br><span class="line">            &#125; );</span><br><span class="line"></span><br><span class="line">    // 4. 将 DataStream 转换为 Table</span><br><span class="line">    Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;id, timestamp as ts, temperature&quot;);</span><br><span class="line"></span><br><span class="line">    // 5. 调用自定义hash函数，对id进行hash运算</span><br><span class="line">    HashCode hashCode = new HashCode(23);</span><br><span class="line">    tableEnv.registerFunction(&quot;hashCode&quot;, hashCode);</span><br><span class="line">    Table resultTable = sensorTable</span><br><span class="line">            .select(&quot;id, ts, hashCode(id)&quot;);</span><br><span class="line"></span><br><span class="line">    //  sql</span><br><span class="line">    tableEnv.createTemporaryView(&quot;sensor&quot;, sensorTable);</span><br><span class="line">    Table resultSqlTable = tableEnv.sqlQuery(&quot;select id, ts, hashCode(id) from sensor&quot;);</span><br><span class="line"></span><br><span class="line">    tableEnv.toAppendStream(resultTable, Row.class).print(&quot;result&quot;);</span><br><span class="line">    tableEnv.toRetractStream(resultSqlTable, Row.class).print(&quot;sql&quot;);</span><br><span class="line"></span><br><span class="line">    env.execute(&quot;scalar function test&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="表函数（Table-Functions）"><a href="#表函数（Table-Functions）" class="headerlink" title="表函数（Table Functions）"></a><strong>表函数（Table Functions）</strong></h3><p>与用户定义的标量函数类似，用户定义的表函数，可以将0、1或多个标量值作为输入参数；与标量函数不同的是，它可以返回任意数量的行作为输出，而不是单个值。</p><p>为了定义一个表函数，必须扩展org.apache.flink.table.functions中的基类TableFunction并实现（一个或多个）求值方法。表函数的行为由其求值方法决定，求值方法必须是public的，并命名为eval。求值方法的参数类型，决定表函数的所有有效参数。</p><p>返回表的类型由TableFunction的泛型类型确定。求值方法使用protected collect（T）方法发出输出行。</p><p>在Table API中，Table函数需要与.joinLateral或.leftOuterJoinLateral一起使用。</p><p>joinLateral算子，会将外部表中的每一行，与表函数（TableFunction，算子的参数是它的表达式）计算得到的所有行连接起来。</p><p>而leftOuterJoinLateral算子，则是左外连接，它同样会将外部表中的每一行与表函数计算生成的所有行连接起来；并且，对于表函数返回的是空表的外部行，也要保留下来。</p><p>在SQL中，则需要使用Lateral Table（<TableFunction>），或者带有ON TRUE条件的左连接。</p><p>下面的代码中，我们将定义一个表函数，在表环境中注册它，并在查询中调用它。</p><p>自定义TableFunction：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// 自定义TableFunction</span><br><span class="line">public static class Split extends TableFunction&lt;Tuple2&lt;String, Integer&gt;&gt; &#123;</span><br><span class="line">    private String separator = &quot;,&quot;;</span><br><span class="line"></span><br><span class="line">    public Split(String separator) &#123;</span><br><span class="line">        this.separator = separator;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 类似flatmap，没有返回值</span><br><span class="line">    public void eval(String str) &#123;</span><br><span class="line">        for (String s : str.split(separator)) &#123;</span><br><span class="line">            collect(new Tuple2&lt;String, Integer&gt;(s, s.length()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来，就是在代码中调用。首先是Table API的方式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Split split = new Split(&quot;_&quot;);</span><br><span class="line">tableEnv.registerFunction(&quot;split&quot;, split);</span><br><span class="line">Table resultTable = sensorTable</span><br><span class="line">        .joinLateral( &quot;split(id) as (word, length)&quot;)</span><br><span class="line">        .select(&quot;id, ts, word, length&quot;);</span><br></pre></td></tr></table></figure><p>然后是SQL的方式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.createTemporaryView(&quot;sensor&quot;, sensorTable);</span><br><span class="line">Table resultSqlTable = tableEnv.sqlQuery(&quot;select id, ts, word, length &quot; +</span><br><span class="line">        &quot;from sensor, lateral table( split(id) ) as splitId(word, length)&quot;);</span><br></pre></td></tr></table></figure><h3 id="聚合函数（Aggregate-Functions）"><a href="#聚合函数（Aggregate-Functions）" class="headerlink" title="聚合函数（Aggregate Functions）"></a><strong>聚合函数（Aggregate Functions）</strong></h3><p>​        用户自定义聚合函数（User-Defined Aggregate Functions，UDAGGs）可以把一个表中的数据，聚合成一个标量值。用户定义的聚合函数，是通过继承AggregateFunction抽象类实现的。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214220859.png"></p><p>上图中显示了一个聚合的例子。</p><p>假设现在有一张表，包含了各种饮料的数据。该表由三列（id、name和price）、五行组成数据。现在我们需要找到表中所有饮料的最高价格，即执行max（）聚合，结果将是一个数值。</p><p>AggregateFunction的工作原理如下。</p><ul><li> 首先，它需要一个累加器，用来保存聚合中间结果的数据结构（状态）。可以通过调用AggregateFunction的createAccumulator（）方法创建空累加器。</li><li> 随后，对每个输入行调用函数的accumulate（）方法来更新累加器。</li><li> 处理完所有行后，将调用函数的getValue（）方法来计算并返回最终结果。</li></ul><p>AggregationFunction要求必须实现的方法：</p><ul><li> createAccumulator()</li><li> accumulate()</li><li> getValue()</li></ul><p>​    除了上述方法之外，还有一些可选择实现的方法。其中一些方法，可以让系统执行查询更有效率，而另一些方法，对于某些场景是必需的。例如，如果聚合函数应用在会话窗口（session group window）的上下文中，则merge（）方法是必需的。</p><ul><li> retract() </li><li> merge() </li><li> resetAccumulator()</li></ul><p>接下来我们写一个自定义AggregateFunction，计算一下每个sensor的平均温度值.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// 定义AggregateFunction的Accumulator</span><br><span class="line">public static class AvgTempAcc &#123;</span><br><span class="line">    double sum = 0.0;</span><br><span class="line">    int count = 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 自定义一个聚合函数，求每个传感器的平均温度值，保存状态(tempSum, tempCount)</span><br><span class="line">public static class AvgTemp extends AggregateFunction&lt;Double, AvgTempAcc&gt;&#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Double getValue(AvgTempAcc accumulator) &#123;</span><br><span class="line">        return accumulator.sum / accumulator.count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public AvgTempAcc createAccumulator() &#123;</span><br><span class="line">        return new AvgTempAcc();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 实现一个具体的处理计算函数，accumulate</span><br><span class="line">    public void accumulate( AvgTempAcc accumulator, Double temp) &#123;</span><br><span class="line">        accumulator.sum += temp;</span><br><span class="line">        accumulator.count += 1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来就可以在代码中调用了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// 创建一个聚合函数实例</span><br><span class="line">AvgTemp avgTemp = new AvgTemp();</span><br><span class="line"></span><br><span class="line">// Table API的调用 </span><br><span class="line">tableEnv.registerFunction(&quot;avgTemp&quot;, avgTemp);</span><br><span class="line">Table resultTable = sensorTable</span><br><span class="line">        .groupBy(&quot;id&quot;)</span><br><span class="line">        .aggregate(&quot;avgTemp(temperature) as avgTemp&quot;)</span><br><span class="line">        .select(&quot;id, avgTemp&quot;);</span><br><span class="line">// sql</span><br><span class="line">tableEnv.createTemporaryView(&quot;sensor&quot;, sensorTable);</span><br><span class="line">Table resultSqlTable = tableEnv.sqlQuery(&quot;select id, avgTemp(temperature) &quot; +</span><br><span class="line">        &quot;from sensor group by id&quot;);</span><br><span class="line"></span><br><span class="line">tableEnv.toRetractStream(resultTable, Row.class).print(&quot;result&quot;);</span><br><span class="line">tableEnv.toRetractStream(resultSqlTable, Row.class).print(&quot;sql&quot;);</span><br></pre></td></tr></table></figure><h3 id="表聚合函数（Table-Aggregate-Functions）"><a href="#表聚合函数（Table-Aggregate-Functions）" class="headerlink" title="表聚合函数（Table Aggregate Functions）"></a><strong>表聚合函数（Table Aggregate Functions）</strong></h3><p>​        用户定义的表聚合函数（User-Defined Table Aggregate Functions，UDTAGGs），可以把一个表中数据，聚合为具有多行和多列的结果表。这跟AggregateFunction非常类似，只是之前聚合结果是一个标量值，现在变成了一张表。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214221100.png"></p><p>​        比如现在我们需要找到表中所有饮料的前2个最高价格，即执行top2（）表聚合。我们需要检查5行中的每一行，得到的结果将是一个具有排序后前2个值的表。</p><p>用户定义的表聚合函数，是通过继承TableAggregateFunction抽象类来实现的。</p><p>TableAggregateFunction的工作原理如下。</p><ul><li><p> 首先，它同样需要一个累加器（Accumulator），它是保存聚合中间结果的数据结构。通过调用TableAggregateFunction的createAccumulator（）方法可以创建空累加器。</p></li><li><p> 随后，对每个输入行调用函数的accumulate（）方法来更新累加器。</p></li><li><p>处理完所有行后，将调用函数的emitValue（）方法来计算并返回最终结果。</p></li></ul><p>AggregationFunction要求必须实现的方法：</p><ul><li> createAccumulator()</li><li> accumulate()</li></ul><p>除了上述方法之外，还有一些可选择实现的方法。</p><ul><li><p> retract() </p></li><li><p> merge() </p></li><li><p> resetAccumulator() </p></li><li><p> emitValue() </p></li><li><p>emitUpdateWithRetract()</p></li></ul><p>接下来我们写一个自定义TableAggregateFunction，用来提取每个sensor最高的两个温度值。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">// 先定义一个 Accumulator </span><br><span class="line">public static class Top2TempAcc &#123;</span><br><span class="line">    double highestTemp = Double.MIN_VALUE;</span><br><span class="line">    double secondHighestTemp = Double.MIN_VALUE;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 自定义表聚合函数</span><br><span class="line">public static class Top2Temp extends TableAggregateFunction&lt;Tuple2&lt;Double, Integer&gt;, Top2TempAcc&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Top2TempAcc createAccumulator() &#123;</span><br><span class="line">        return new Top2TempAcc();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 实现计算聚合结果的函数accumulate</span><br><span class="line">    public void accumulate(Top2TempAcc acc, Double temp) &#123;</span><br><span class="line">        if (temp &gt; acc.highestTemp) &#123;</span><br><span class="line">            acc.secondHighestTemp = acc.highestTemp;</span><br><span class="line">            acc.highestTemp = temp;</span><br><span class="line">        &#125; else if (temp &gt; acc.secondHighestTemp) &#123;</span><br><span class="line">            acc.secondHighestTemp = temp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    // 实现一个输出结果的方法，最终处理完表中所有数据时调用</span><br><span class="line">    public void emitValue(Top2TempAcc acc, Collector&lt;Tuple2&lt;Double, Integer&gt;&gt; out) &#123;</span><br><span class="line">        out.collect(new Tuple2&lt;&gt;(acc.highestTemp, 1));</span><br><span class="line">        out.collect(new Tuple2&lt;&gt;(acc.secondHighestTemp, 2));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来就可以在代码中调用了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 创建一个表聚合函数实例</span><br><span class="line">Top2Temp top2Temp = new Top2Temp();</span><br><span class="line">tableEnv.registerFunction(&quot;top2Temp&quot;, top2Temp);</span><br><span class="line">Table resultTable = sensorTable</span><br><span class="line">        .groupBy(&quot;id&quot;)</span><br><span class="line">        .flatAggregate(&quot;top2Temp(temperature) as (temp, rank)&quot;)</span><br><span class="line">        .select(&quot;id, temp, rank&quot;);</span><br><span class="line"></span><br><span class="line">tableEnv.toRetractStream(resultTable, Row.class).print(&quot;result&quot;);</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;Flink Table 和 SQL内置了很多SQL中支持的函数；如果有无法满足的需要，则可以实现用户自定义的函数（UDF）来解决。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: FlinkSQL的Table API 与SQL之窗口（Windows）</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-FlinkSQL%E7%9A%84Table-API-%E4%B8%8ESQL%E4%B9%8B%E7%AA%97%E5%8F%A3%EF%BC%88Windows%EF%BC%89/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-FlinkSQL%E7%9A%84Table-API-%E4%B8%8ESQL%E4%B9%8B%E7%AA%97%E5%8F%A3%EF%BC%88Windows%EF%BC%89/</id>
    <published>2022-02-14T12:21:59.000Z</published>
    <updated>2022-02-14T13:59:27.017Z</updated>
    
    <content type="html"><![CDATA[<p>​        时间语义，要配合窗口操作才能发挥作用。最主要的用途，当然就是开窗口、根据时间段做计算了。下面我们就来看看Table API和SQL中，怎么利用时间字段做窗口操作。</p><p>在Table API和SQL中，主要有两种窗口：Group Windows和Over Windows</p><span id="more"></span><h2 id="分组窗口（Group-Windows）"><a href="#分组窗口（Group-Windows）" class="headerlink" title="分组窗口（Group Windows）"></a><strong>分组窗口（Group Windows）</strong></h2><p>分组窗口（Group Windows）会根据时间或行计数间隔，将行聚合到有限的组（Group）中，并对每个组的数据执行一次聚合函数。</p><p>Table API中的Group Windows都是使用.window（w:GroupWindow）子句定义的，并且必须由as子句指定一个别名。为了按窗口对表进行分组，窗口的别名必须在group by子句中，像常规的分组字段一样引用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Table table = input</span><br><span class="line">  .window([w: GroupWindow] as &quot;w&quot;) // 定义窗口，别名 w</span><br><span class="line">  .groupBy(&quot;w, a&quot;)  // 以属性a和窗口w作为分组的key </span><br><span class="line">  .select(&quot;a, b.sum&quot;)  // 聚合字段b的值，求和</span><br></pre></td></tr></table></figure><p>或者，还可以把窗口的相关信息，作为字段添加到结果表中：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Table table = input</span><br><span class="line">  .window([w: GroupWindow] as &quot;w&quot;) </span><br><span class="line">  .groupBy(&quot;w, a&quot;) </span><br><span class="line">  .select(&quot;a, w.start, w.end, w.rowtime, b.count&quot;)</span><br></pre></td></tr></table></figure><p>Table API提供了一组具有特定语义的预定义Window类，这些类会被转换为底层DataStream或DataSet的窗口操作。</p><p>Table API支持的窗口定义，和我们熟悉的一样，主要也是三种：滚动（Tumbling）、滑动（Sliding）和会话（Session）。</p><h3 id="滚动窗口"><a href="#滚动窗口" class="headerlink" title="滚动窗口"></a><strong>滚动窗口</strong></h3><p>滚动窗口（Tumbling windows）要用Tumble类来定义，另外还有三个方法：</p><ul><li> over：定义窗口长度</li><li> on：用来分组（按时间间隔）或者排序（按行数）的时间字段</li><li> as：别名，必须出现在后面的groupBy中</li></ul><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// Tumbling Event-time Window</span><br><span class="line">.window(Tumble.over(&quot;10.minutes&quot;).on(&quot;rowtime&quot;).as(&quot;w&quot;))</span><br><span class="line"></span><br><span class="line">// Tumbling Processing-time Window</span><br><span class="line">.window(Tumble.over(&quot;10.minutes&quot;).on(&quot;proctime&quot;).as(&quot;w&quot;))</span><br><span class="line"></span><br><span class="line">// Tumbling Row-count Window</span><br><span class="line">.window(Tumble.over(&quot;10.rows&quot;).on(&quot;proctime&quot;).as(&quot;w&quot;))</span><br></pre></td></tr></table></figure><h3 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h3><p>滑动窗口（Sliding windows）要用Slide类来定义，另外还有四个方法：</p><ul><li> over：定义窗口长度</li><li> every：定义滑动步长</li><li> on：用来分组（按时间间隔）或者排序（按行数）的时间字段</li><li> as：别名，必须出现在后面的groupBy中</li></ul><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// Sliding Event-time Window</span><br><span class="line">.window(Slide.over(&quot;10.minutes&quot;).every(&quot;5.minutes&quot;).on(&quot;rowtime&quot;).as(&quot;w&quot;))</span><br><span class="line"></span><br><span class="line">// Sliding Processing-time window </span><br><span class="line">.window(Slide.over(&quot;10.minutes&quot;).every(&quot;5.minutes&quot;).on(&quot;proctime&quot;).as(&quot;w&quot;))</span><br><span class="line"></span><br><span class="line">// Sliding Row-count window</span><br><span class="line">.window(Slide.over(&quot;10.rows&quot;).every(&quot;5.rows&quot;).on(&quot;proctime&quot;).as(&quot;w&quot;))</span><br></pre></td></tr></table></figure><h3 id="会话窗口"><a href="#会话窗口" class="headerlink" title="会话窗口"></a><strong>会话窗口</strong></h3><p>会话窗口（Session windows）要用Session类来定义，另外还有三个方法：</p><ul><li> withGap：会话时间间隔</li><li> on：用来分组（按时间间隔）或者排序（按行数）的时间字段</li><li> as：别名，必须出现在后面的groupBy中</li></ul><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Session Event-time Window</span><br><span class="line">.window(Session.withGap.(&quot;10.minutes&quot;).on(&quot;rowtime&quot;).as(&quot;w&quot;))</span><br><span class="line"></span><br><span class="line">// Session Processing-time Window</span><br><span class="line">.window(Session.withGap.(&quot;10.minutes&quot;).on(“proctime&quot;).as(&quot;w&quot;))</span><br></pre></td></tr></table></figure><h2 id="Over-Windows"><a href="#Over-Windows" class="headerlink" title="Over Windows"></a><strong>Over Windows</strong></h2><p>Over window聚合是标准SQL中已有的（Over子句），可以在查询的SELECT子句中定义。Over window 聚合，会针对每个输入行，计算相邻行范围内的聚合。Over windows</p><p>使用.window（w:overwindows*）子句定义，并在select（）方法中通过别名来引用。</p><p>比如这样：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Table table = input</span><br><span class="line">  .window([w: OverWindow] as &quot;w&quot;)</span><br><span class="line">  .select(&quot;a, b.sum over w, c.min over w&quot;)</span><br></pre></td></tr></table></figure><p>Table API提供了Over类，来配置Over窗口的属性。可以在事件时间或处理时间，以及指定为时间间隔、或行计数的范围内，定义Over windows。</p><p>无界的over window是使用常量指定的。也就是说，时间间隔要指定UNBOUNDED_RANGE，或者行计数间隔要指定UNBOUNDED_ROW。而有界的over window是用间隔的大小指定的。</p><p>实际代码应用如下：</p><p>1） 无界的 over window</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 无界的事件时间 over window</span><br><span class="line">.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;rowtime&quot;).preceding.(UNBOUNDED_RANGE).as(&quot;w&quot;))</span><br><span class="line"></span><br><span class="line">// 无界的处理时间 over window</span><br><span class="line">.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;proctime&quot;).preceding.(UNBOUNDED_RANGE).as(&quot;w&quot;))</span><br><span class="line"></span><br><span class="line">// 无界的事件时间 Row-count over window</span><br><span class="line">.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;rowtime&quot;).preceding.(UNBOUNDED_ROW).as(&quot;w&quot;))</span><br><span class="line"></span><br><span class="line">//无界的处理时间 Row-count over window</span><br><span class="line">.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;proctime&quot;).preceding.(UNBOUNDED_ROW).as(&quot;w&quot;))</span><br></pre></td></tr></table></figure><p>2） 有界的over window</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 有界的事件时间 over window</span><br><span class="line">.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;rowtime&quot;).preceding(&quot;1.minutes&quot;).as(&quot;w&quot;))</span><br><span class="line">        </span><br><span class="line">// 有界的处理时间 over window</span><br><span class="line">.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;proctime&quot;).preceding(&quot;1.minutes&quot;).as(&quot;w&quot;))</span><br><span class="line">        </span><br><span class="line">// 有界的事件时间 Row-count over window</span><br><span class="line">.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;rowtime&quot;).preceding(&quot;10.rows&quot;).as(&quot;w&quot;))</span><br><span class="line">        </span><br><span class="line">// 有界的处理时间 Row-count over window</span><br><span class="line">.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;procime&quot;).preceding(&quot;10.rows&quot;).as(&quot;w&quot;))</span><br></pre></td></tr></table></figure><h2 id="SQL中窗口的定义"><a href="#SQL中窗口的定义" class="headerlink" title="SQL中窗口的定义"></a>SQL中窗口的定义</h2><p>我们已经了解了在Table API里window的调用方式，同样，我们也可以在SQL中直接加入窗口的定义和使用。</p><h3 id="Group-Windows"><a href="#Group-Windows" class="headerlink" title="Group Windows"></a><strong>Group Windows</strong></h3><p>Group Windows在SQL查询的Group BY子句中定义。与使用常规GROUP BY子句的查询一样，使用GROUP BY子句的查询会计算每个组的单个结果行。</p><p>SQL支持以下Group窗口函数:</p><ol><li>TUMBLE(time_attr, interval)</li></ol><p>​        定义一个滚动窗口，第一个参数是时间字段，第二个参数是窗口长度。</p><ol start="2"><li>HOP(time_attr, interval, interval)</li></ol><p>​        定义一个滑动窗口，第一个参数是时间字段，第二个参数是窗口滑动步长，第三个是窗口长度。</p><ol start="3"><li>SESSION(time_attr, interval)</li></ol><p>​        定义一个会话窗口，第一个参数是时间字段，第二个参数是窗口间隔（Gap）。</p><p>另外还有一些辅助函数，可以用来选择Group Window的开始和结束时间戳，以及时间属性。</p><p>这里只写TUMBLE_*，滑动和会话窗口是类似的（HOP_*，SESSION_*）。</p><ul><li> TUMBLE_START(time_attr, interval)</li><li> TUMBLE_END(time_attr, interval)</li><li> TUMBLE_ROWTIME(time_attr, interval)</li><li> TUMBLE_PROCTIME(time_attr, interval)</li></ul><h3 id="Over-Windows-1"><a href="#Over-Windows-1" class="headerlink" title="Over Windows"></a><strong>Over Windows</strong></h3><p>​        由于Over本来就是SQL内置支持的语法，所以这在SQL中属于基本的聚合操作。所有聚合必须在同一窗口上定义，也就是说，必须是相同的分区、排序和范围。目前仅支持在当前行范围之前的窗口（无边界和有边界）。</p><p>注意，ORDER BY必须在单一的时间属性上指定。</p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">SELECT COUNT(amount) OVER (</span><br><span class="line">  PARTITION BY user</span><br><span class="line">  ORDER BY proctime</span><br><span class="line">  ROWS BETWEEN 2 PRECEDING AND CURRENT ROW)</span><br><span class="line">FROM Orders</span><br><span class="line"></span><br><span class="line">// 也可以做多个聚合</span><br><span class="line">SELECT COUNT(amount) OVER w, SUM(amount) OVER w</span><br><span class="line">FROM Orders</span><br><span class="line">WINDOW w AS (</span><br><span class="line">  PARTITION BY user</span><br><span class="line">  ORDER BY proctime</span><br><span class="line">  ROWS BETWEEN 2 PRECEDING AND CURRENT ROW)</span><br></pre></td></tr></table></figure><h3 id="代码练习（以分组滚动窗口为例）"><a href="#代码练习（以分组滚动窗口为例）" class="headerlink" title="代码练习（以分组滚动窗口为例）"></a><strong>代码练习（以分组滚动窗口为例）</strong></h3><p>我们可以综合学习过的内容，用一段完整的代码实现一个具体的需求。例如，可以开一个滚动窗口，统计10秒内出现的每个sensor的个数。</p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        // 1. 创建环境</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(1);</span><br><span class="line">        // 设置事件时间</span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">        // 2. 读取文件，得到 DataStream</span><br><span class="line">        String filePath = &quot;..\\sensor.txt&quot;;</span><br><span class="line">        DataStream&lt;String&gt; inputStream = env.readTextFile(filePath);</span><br><span class="line"></span><br><span class="line">        // 3. 转换成 Java Bean，并指定timestamp和watermark</span><br><span class="line">        DataStream&lt;SensorReading&gt; dataStream = inputStream</span><br><span class="line">                .map( line -&gt; &#123;</span><br><span class="line">                    String[] fields = line.split(&quot;,&quot;);</span><br><span class="line">                    return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2]));</span><br><span class="line">                &#125; )</span><br><span class="line">                .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;SensorReading&gt;(Time.seconds(1)) &#123;</span><br><span class="line">                    @Override</span><br><span class="line">                    public long extractTimestamp(SensorReading element) &#123;</span><br><span class="line">                        return element.getTimestamp() * 1000L;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        // 4. 将 DataStream 转换为 Table</span><br><span class="line">        Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;id, timestamp.rowtime as ts, temperature&quot;);</span><br><span class="line"></span><br><span class="line">        // 5. 开窗聚合</span><br><span class="line">        Table resultTable = sensorTable</span><br><span class="line">                .window(Tumble.over(&quot;10.seconds&quot;).on(&quot;ts&quot;).as(&quot;tw&quot;))</span><br><span class="line">                // 每10秒统计一次，滚动时间窗口</span><br><span class="line">                .groupBy(&quot;id, tw&quot;)</span><br><span class="line">                .select(&quot;id, id.count, temperature.avg, tw.end&quot;);</span><br><span class="line"></span><br><span class="line">        //  sql</span><br><span class="line">        tableEnv.createTemporaryView(&quot;sensor&quot;, sensorTable);</span><br><span class="line">        Table resultSqlTable = tableEnv.sqlQuery(</span><br><span class="line">                &quot;select id, count(id), avg(temperature), tumble_end(ts, interval &#x27;10&#x27; second) &quot; +</span><br><span class="line">                        &quot;from sensor group by id, tumble(ts, interval &#x27;10&#x27; second)&quot;);</span><br><span class="line"></span><br><span class="line">        // 转换成流打印输出</span><br><span class="line">        tableEnv.toAppendStream(resultTable, Row.class).print(&quot;result&quot;);</span><br><span class="line">        tableEnv.toAppendStream(resultSqlTable, Row.class).print(&quot;sql&quot;);</span><br><span class="line"></span><br><span class="line">        env.execute(&quot;time and window test&quot;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;​        时间语义，要配合窗口操作才能发挥作用。最主要的用途，当然就是开窗口、根据时间段做计算了。下面我们就来看看Table API和SQL中，怎么利用时间字段做窗口操作。&lt;/p&gt;
&lt;p&gt;在Table API和SQL中，主要有两种窗口：Group Windows和Over Windows&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: FlinkSQL的Table API 与SQL之流处理中的特殊概念</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-FlinkSQL%E7%9A%84Table-API-%E4%B8%8ESQL%E4%B9%8B%E6%B5%81%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E7%89%B9%E6%AE%8A%E6%A6%82%E5%BF%B5/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-FlinkSQL%E7%9A%84Table-API-%E4%B8%8ESQL%E4%B9%8B%E6%B5%81%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E7%89%B9%E6%AE%8A%E6%A6%82%E5%BF%B5/</id>
    <published>2022-02-14T12:07:03.000Z</published>
    <updated>2022-02-15T02:27:43.722Z</updated>
    
    <content type="html"><![CDATA[<p>​            Table API和SQL，本质上还是基于关系型表的操作方式；而关系型表、关系代数，以及SQL本身，一般是有界的，更适合批处理的场景。这就导致在进行流处理的过程中，理解会稍微复杂一些，需要引入一些特殊概念。</p><span id="more"></span><h2 id="流处理和关系代数（表，及SQL）的区别"><a href="#流处理和关系代数（表，及SQL）的区别" class="headerlink" title="流处理和关系代数（表，及SQL）的区别"></a><strong>流处理和关系代数（表，及SQL）的区别</strong></h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214200752.png"></p><p>可以看到，其实关系代数（主要就是指关系型数据库中的表）和SQL，主要就是针对批处理的，这和流处理有天生的隔阂。</p><h2 id="动态表（Dynamic-Tables）"><a href="#动态表（Dynamic-Tables）" class="headerlink" title="动态表（Dynamic Tables）"></a><strong>动态表（Dynamic Tables）</strong></h2><p>因为流处理面对的数据，是连续不断的，这和我们熟悉的关系型数据库中保存的“表”完全不同。所以，如果我们把流数据转换成Table，然后执行类似于table的select操作，结果就不是一成不变的，而是随着新数据的到来，会不停更新。</p><p>我们可以随着新数据的到来，不停地在之前的基础上更新结果。这样得到的表，在Flink Table API概念里，就叫做“<strong>动态表</strong>”（Dynamic Tables）。</p><p>动态表是Flink对流数据的Table API和SQL支持的核心概念。与表示批处理数据的静态表不同，动态表是随时间变化的。动态表可以像静态的批处理表一样进行查询，查询一个动态表会产生持续查询（Continuous Query）。连续查询永远不会终止，并会生成另一个动态表。查询（Query）会不断更新其动态结果表，以反映其动态输入表上的更改。</p><h2 id="流式持续查询的过程"><a href="#流式持续查询的过程" class="headerlink" title="流式持续查询的过程"></a><strong>流式持续查询的过程</strong></h2><p>下图显示了流、动态表和连续查询的关系：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214200844.png"></p><p>流式持续查询的过程为：</p><ol><li><p>流被转换为动态表。</p></li><li><p>对动态表计算连续查询，生成新的动态表。</p></li><li><p>生成的动态表被转换回流。</p></li></ol><h3 id="将流转换成表（Table）"><a href="#将流转换成表（Table）" class="headerlink" title="将流转换成表（Table）"></a><strong>将流转换成表（Table）</strong></h3><p>为了处理带有关系查询的流，必须先将其转换为表。</p><p>从概念上讲，流的每个数据记录，都被解释为对结果表的插入（Insert）修改。因为流式持续不断的，而且之前的输出结果无法改变。本质上，我们其实是从一个、只有插入操作的changelog（更新日志）流，来构建一个表。</p><p>为了更好地说明动态表和持续查询的概念，我们来举一个具体的例子。</p><p>比如，我们现在的输入数据，就是用户在网站上的访问行为，数据类型（Schema）如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  user:  VARCHAR,   // 用户名</span><br><span class="line">  cTime: TIMESTAMP, // 访问某个URL的时间戳</span><br><span class="line">  url:   VARCHAR    // 用户访问的URL</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>下图显示了如何将访问URL事件流，或者叫点击事件流（左侧）转换为表（右侧）。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214200937.png"></p><p>随着插入更多的访问事件流记录，生成的表将不断增长。</p><h3 id="持续查询（Continuous-Query）"><a href="#持续查询（Continuous-Query）" class="headerlink" title="持续查询（Continuous Query）"></a><strong>持续查询（Continuous Query）</strong></h3><p>持续查询，会在动态表上做计算处理，并作为结果生成新的动态表。与批处理查询不同，连续查询从不终止，并根据输入表上的更新更新其结果表。</p><p>在任何时间点，连续查询的结果在语义上，等同于在输入表的快照上，以批处理模式执行的同一查询的结果。</p><p>在下面的示例中，我们展示了对点击事件流中的一个持续查询。</p><p>这个Query很简单，是一个分组聚合做count统计的查询。它将用户字段上的clicks表分组，并统计访问的url数。图中显示了随着时间的推移，当clicks表被其他行更新时如何计算查询。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214201010.png"></p><h3 id="将动态表转换成流"><a href="#将动态表转换成流" class="headerlink" title="将动态表转换成流"></a><strong>将动态表转换成流</strong></h3><p>与常规的数据库表一样，动态表可以通过插入（Insert）、更新（Update）和删除（Delete）更改，进行持续的修改。将动态表转换为流或将其写入外部系统时，需要对这些更改进行编码。Flink的Table API和SQL支持三种方式对动态表的更改进行编码：</p><h4 id="1）仅追加（Append-only）流"><a href="#1）仅追加（Append-only）流" class="headerlink" title="1）仅追加（Append-only）流"></a><strong>1）仅追加（Append-only）流</strong></h4><p>仅通过插入（Insert）更改，来修改的动态表，可以直接转换为“仅追加”流。这个流中发出的数据，就是动态表中新增的每一行。</p><h4 id="2）撤回（Retract）流"><a href="#2）撤回（Retract）流" class="headerlink" title="2）撤回（Retract）流"></a><strong>2）撤回（Retract）流</strong></h4><p>Retract流是包含两类消息的流，添加（Add）消息和撤回（Retract）消息。</p><p>动态表通过将INSERT 编码为add消息、DELETE 编码为retract消息、UPDATE编码为被更改行（前一行）的retract消息和更新后行（新行）的add消息，转换为retract流。</p><p>下图显示了将动态表转换为Retract流的过程。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214201708.png"> </p><h4 id="3）Upsert（更新插入）流"><a href="#3）Upsert（更新插入）流" class="headerlink" title="3）Upsert（更新插入）流"></a><strong>3）Upsert（更新插入）流</strong></h4><p>Upsert流包含两种类型的消息：Upsert消息和delete消息。转换为upsert流的动态表，需要有唯一的键（key）。</p><p>通过将INSERT和UPDATE更改编码为upsert消息，将DELETE更改编码为DELETE消息，就可以将具有唯一键（Unique Key）的动态表转换为流。</p><p>下图显示了将动态表转换为upsert流的过程。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214201105.png"></p><p>这些概念我们之前都已提到过。需要注意的是，在代码里将动态表转换为DataStream时，仅支持Append和Retract流。而向外部系统输出动态表的TableSink接口，则可以有不同的实现，比如之前我们讲到的ES，就可以有Upsert模式。</p><h2 id="时间特性"><a href="#时间特性" class="headerlink" title="时间特性"></a><strong>时间特性</strong></h2><p>基于时间的操作（比如Table API和SQL中窗口操作），需要定义相关的时间语义和时间数据来源的信息。所以，Table可以提供一个逻辑上的时间字段，用于在表处理程序中，指示时间和访问相应的时间戳。</p><p>时间属性，可以是每个表schema的一部分。一旦定义了时间属性，它就可以作为一个字段引用，并且可以在基于时间的操作中使用。</p><p>时间属性的行为类似于常规时间戳，可以访问，并且进行计算。</p><h3 id="处理时间（Processing-Time）"><a href="#处理时间（Processing-Time）" class="headerlink" title="处理时间（Processing Time）"></a><strong>处理时间（Processing Time）</strong></h3><p>处理时间语义下，允许表处理程序根据机器的本地时间生成结果。它是时间的最简单概念。它既不需要提取时间戳，也不需要生成watermark。</p><p>定义处理时间属性有三种方法：在DataStream转化时直接指定；在定义Table Schema时指定；在创建表的DDL中指定。</p><h4 id="1-DataStream转化成Table时指定"><a href="#1-DataStream转化成Table时指定" class="headerlink" title="1) DataStream转化成Table时指定"></a><strong>1)</strong> <em>DataStream转化成Table时指定</em></h4><p>由DataStream转换成表时，可以在后面指定字段名来定义Schema。在定义Schema期间，可以使用.proctime，定义处理时间字段。</p><p>注意，这个proctime属性只能通过附加逻辑字段，来扩展物理schema。因此，只能在schema定义的末尾定义它。</p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// 定义好 DataStream</span><br><span class="line">DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;\\sensor.txt&quot;)</span><br><span class="line">DataStream&lt;SensorReading&gt; dataStream = inputStream</span><br><span class="line">        .map( line -&gt; &#123;</span><br><span class="line">            String[] fields = line.split(&quot;,&quot;);</span><br><span class="line">            return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2]));</span><br><span class="line">        &#125; );</span><br><span class="line"></span><br><span class="line">// 将 DataStream转换为 Table，并指定时间字段</span><br><span class="line">Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;id, temperature, timestamp, pt.proctime&quot;);</span><br></pre></td></tr></table></figure><h4 id="2-定义Table-Schema时指定"><a href="#2-定义Table-Schema时指定" class="headerlink" title="2) 定义Table Schema时指定"></a><strong>2)</strong> <strong>定义Table Schema时指定</strong></h4><p>这种方法其实也很简单，只要在定义Schema的时候，加上一个新的字段，并指定成proctime就可以了。</p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.connect(</span><br><span class="line">  new FileSystem().path(&quot;..\\sensor.txt&quot;))</span><br><span class="line">  .withFormat(new Csv())</span><br><span class="line">  .withSchema(new Schema()</span><br><span class="line">    .field(&quot;id&quot;, DataTypes.STRING())</span><br><span class="line">    .field(&quot;timestamp&quot;, DataTypes.BIGINT())</span><br><span class="line">    .field(&quot;temperature&quot;, DataTypes.DOUBLE())</span><br><span class="line">    .field(&quot;pt&quot;, DataTypes.TIMESTAMP(3))</span><br><span class="line">      .proctime()    // 指定 pt字段为处理时间</span><br><span class="line">  ) // 定义表结构</span><br><span class="line">  .createTemporaryTable(&quot;inputTable&quot;); // 创建临时表</span><br></pre></td></tr></table></figure><h4 id="3-创建表的DDL中指定"><a href="#3-创建表的DDL中指定" class="headerlink" title="3) 创建表的DDL中指定"></a><strong>3)</strong> <strong>创建表的DDL中指定</strong></h4><p>在创建表的DDL中，增加一个字段并指定成proctime，也可以指定当前的时间字段。</p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">String sinkDDL = &quot;create table dataTable (&quot; +</span><br><span class="line">        &quot; id varchar(20) not null, &quot; +</span><br><span class="line">        &quot; ts bigint, &quot; +</span><br><span class="line">        &quot; temperature double, &quot; +</span><br><span class="line">        &quot; pt AS PROCTIME() &quot; +</span><br><span class="line">        &quot;) with (&quot; +</span><br><span class="line">        &quot; &#x27;connector.type&#x27; = &#x27;filesystem&#x27;, &quot; +</span><br><span class="line">        &quot; &#x27;connector.path&#x27; = &#x27;/sensor.txt&#x27;, &quot; +</span><br><span class="line">        &quot; &#x27;format.type&#x27; = &#x27;csv&#x27;)&quot;;</span><br><span class="line"></span><br><span class="line">tableEnv.sqlUpdate(sinkDDL);</span><br></pre></td></tr></table></figure><p>注意：运行这段DDL，必须使用Blink Planner。</p><h3 id="事件时间（Event-Time）"><a href="#事件时间（Event-Time）" class="headerlink" title="事件时间（Event Time）"></a><strong>事件时间（Event Time）</strong></h3><p>事件时间语义，允许表处理程序根据每个记录中包含的时间生成结果。这样即使在有乱序事件或者延迟事件时，也可以获得正确的结果。</p><p>为了处理无序事件，并区分流中的准时和迟到事件；Flink需要从事件数据中，提取时间戳，并用来推进事件时间的进展（watermark）。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214201354.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;\\sensor.txt&quot;)</span><br><span class="line">DataStream&lt;SensorReading&gt; dataStream = inputStream</span><br><span class="line">        .map( line -&gt; &#123;</span><br><span class="line">            String[] fields = line.split(&quot;,&quot;);</span><br><span class="line">            return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2]));</span><br><span class="line">        &#125; )</span><br><span class="line">        .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;SensorReading&gt;(Time.seconds(1)) &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public long extractTimestamp(SensorReading element) &#123;</span><br><span class="line">                return element.getTimestamp() * 1000L;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;id, timestamp.rowtime as ts, temperature&quot;);</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214201454.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.connect(</span><br><span class="line">  new FileSystem().path(&quot;sensor.txt&quot;))</span><br><span class="line">  .withFormat(new Csv())</span><br><span class="line">  .withSchema(new Schema()</span><br><span class="line">    .field(&quot;id&quot;, DataTypes.STRING())</span><br><span class="line">    .field(&quot;timestamp&quot;, DataTypes.BIGINT())</span><br><span class="line">      .rowtime(</span><br><span class="line">        new Rowtime()</span><br><span class="line">          .timestampsFromField(&quot;timestamp&quot;)    // 从字段中提取时间戳</span><br><span class="line">          .watermarksPeriodicBounded(1000)    // watermark延迟1秒</span><br><span class="line">      )</span><br><span class="line">    .field(&quot;temperature&quot;, DataTypes.DOUBLE())</span><br><span class="line">  ) // 定义表结构</span><br><span class="line">  .createTemporaryTable(&quot;inputTable&quot;); // 创建临时表</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214201528.png"></p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">String sinkDDL = &quot;create table dataTable (&quot; +</span><br><span class="line">        &quot; id varchar(20) not null, &quot; +</span><br><span class="line">        &quot; ts bigint, &quot; +</span><br><span class="line">        &quot; temperature double, &quot; +</span><br><span class="line">        &quot; rt AS TO_TIMESTAMP( FROM_UNIXTIME(ts) ), &quot; +</span><br><span class="line">        &quot; watermark for rt as rt - interval &#x27;1&#x27; second&quot; +</span><br><span class="line">        &quot;) with (&quot; +</span><br><span class="line">        &quot; &#x27;connector.type&#x27; = &#x27;filesystem&#x27;, &quot; +</span><br><span class="line">        &quot; &#x27;connector.path&#x27; = &#x27;/sensor.txt&#x27;, &quot; +</span><br><span class="line">        &quot; &#x27;format.type&#x27; = &#x27;csv&#x27;)&quot;;</span><br><span class="line"></span><br><span class="line">tableEnv.sqlUpdate(sinkDDL);</span><br></pre></td></tr></table></figure><p>这里<strong>FROM_UNIXTIME</strong>是系统内置的时间函数，用来将一个整数（秒数）转换成“YYYY-MM-DD hh:mm:ss”格式（默认，也可以作为第二个String参数传入）的日期时间字符串（date time string）；然后再用<strong>TO_TIMESTAMP</strong>将其转换成Timestamp。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;​            Table API和SQL，本质上还是基于关系型表的操作方式；而关系型表、关系代数，以及SQL本身，一般是有界的，更适合批处理的场景。这就导致在进行流处理的过程中，理解会稍微复杂一些，需要引入一些特殊概念。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: FlinkSQL的Table API 与SQL概念</title>
    <link href="http://xubatian.cn/Flink%20%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0:%20FlinkSQL%E7%9A%84Table%20API%20%E4%B8%8ESQL%E6%A6%82%E5%BF%B5/"/>
    <id>http://xubatian.cn/Flink%20%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0:%20FlinkSQL%E7%9A%84Table%20API%20%E4%B8%8ESQL%E6%A6%82%E5%BF%B5/</id>
    <published>2022-02-14T11:00:03.000Z</published>
    <updated>2022-02-14T11:55:02.130Z</updated>
    
    <content type="html"><![CDATA[<h1 id="整体介绍"><a href="#整体介绍" class="headerlink" title="整体介绍"></a>整体介绍</h1><h2 id="Flink-SQL-源码案例"><a href="#Flink-SQL-源码案例" class="headerlink" title="Flink SQL 源码案例"></a>Flink SQL 源码案例</h2><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo10/">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo10/</a>  从FlinkDemo10往后都是</p><h2 id="什么是-Table-API-和-Flink-SQL"><a href="#什么是-Table-API-和-Flink-SQL" class="headerlink" title="什么是 Table API 和 Flink SQL"></a>什么是 Table API 和 Flink SQL</h2><p>Flink本身是批流统一的处理框架，所以Table API和SQL，就是批流统一的上层处理API。<br>目前功能尚未完善，处于活跃的开发阶段。<br>Table API是一套内嵌在Java和Scala语言中的查询API，它允许我们以非常直观的方式，组合来自一些关系运算符的查询（比如select、filter和join）。而对于Flink SQL，就是直接可以在代码中写SQL，来实现一些查询（Query）操作。Flink的SQL支持，基于实现了SQL标准的Apache Calcite（Apache开源SQL解析工具）。<br>无论输入是批输入还是流式输入，在这两套API中，指定的查询都具有相同的语义，得到相同的结果。</p><span id="more"></span><p>Table API和SQL: </p><p>实际上就是把我们一个流变成一个关系型的API.</p><p>如果我们有关系型的API的话.我们的代码变成就非常方便一些.这个和我们所说的hive是一样的.为什么要用hive呢?<strong>本来我们使用SparkCore和MapReduce也是能替代hive的</strong>.但是我们为什么用hive呢?因为我们就是不想写复杂的代码呀!当然了,也是为了以后的数据管理和数据分析方便一些.</p><p>所以我们把这个流变成关系型的API的话.把Data Stream变成关系型的API后,实际上就是方便以后我们对数据进行分析和处理的.而且他的可读性也强一些.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214190035.png"></p><p>​        Table API是流处理和批处理通用的关系型API，Table API可以基于流输入或者批输入来运行而不需要进行任何修改。Table API是SQL语言的超集并专门为Apache Flink设计的，Table API是Scala 和Java语言集成式的API。与常规SQL语言中将查询指定为字符串不同，Table API查询是以Java或Scala中的语言嵌入样式来定义的，具有IDE支持如:自动完成和语法检测。</p><h2 id="需要引入的依赖"><a href="#需要引入的依赖" class="headerlink" title="需要引入的依赖"></a>需要引入的依赖</h2><p>Table API和SQL需要引入的依赖有两个：planner和bridge。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-table-planner_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.10.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-table-api-java-bridge_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.10.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p><strong>flink-table-planner</strong>：planner计划器，是table API最主要的部分，提供了运行时环境和生成程序执行计划的planner；</p><p><strong>flink-table-api-java-bridge</strong>：bridge桥接器，主要负责table API和 DataStream/DataSet API的连接支持，按照语言分java和scala。</p><p>这里的两个依赖，是IDE环境下运行需要添加的；如果是生产环境，lib目录下默认已经有了planner，就只需要有bridge就可以了。</p><p>当然，如果想使用用户自定义函数，或是跟kafka做连接，需要有一个SQL client，这个包含在flink-table-common里。</p><h2 id="两种planner（old-amp-blink）的区别"><a href="#两种planner（old-amp-blink）的区别" class="headerlink" title="两种planner（old &amp; blink）的区别"></a>两种planner（old &amp; blink）的区别</h2><ol><li><p>批流统一：Blink将批处理作业，视为流式处理的特殊情况。所以，blink不支持表和DataSet之间的转换，批处理作业将不转换为DataSet应用程序，而是跟流处理一样，转换为DataStream程序来处理。</p></li><li><p>因为批流统一，Blink planner也不支持BatchTableSource，而使用有界的StreamTableSource代替。</p></li><li><p>Blink planner只支持全新的目录，不支持已弃用的ExternalCatalog。</p></li><li><p>旧planner和Blink planner的FilterableTableSource实现不兼容。旧的planner会把PlannerExpressions下推到filterableTableSource中，而blink planner则会把Expressions下推。</p></li><li><p>基于字符串的键值配置选项仅适用于Blink planner。</p></li><li><p>PlannerConfig在两个planner中的实现不同。</p></li><li><p>Blink planner会将多个sink优化在一个DAG中（仅在TableEnvironment上受支持，而在StreamTableEnvironment上不受支持）。而旧planner的优化总是将每一个sink放在一个新的DAG中，其中所有DAG彼此独立。</p></li><li><p>旧的planner不支持目录统计，而Blink planner支持。</p></li></ol><h1 id="API调用"><a href="#API调用" class="headerlink" title="API调用"></a><strong>API调用</strong></h1><h2 id="基本程序结构"><a href="#基本程序结构" class="headerlink" title="基本程序结构"></a>基本程序结构</h2><p>​        Table API 和 SQL 的程序结构，与流式处理的程序结构类似；也可以近似地认为有这么几步：首先创建执行环境，然后定义source、transform和sink。</p><p>​        具体操作流程如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment  tableEnv = ...     // 创建表的执行环境</span><br><span class="line"></span><br><span class="line">// 创建一张表，用于读取数据</span><br><span class="line">tableEnv.connect(...).createTemporaryTable(&quot;inputTable&quot;);</span><br><span class="line">// 注册一张表，用于把计算结果输出</span><br><span class="line">tableEnv.connect(...).createTemporaryTable(&quot;outputTable&quot;);</span><br><span class="line"></span><br><span class="line">// 通过 Table API 查询算子，得到一张结果表</span><br><span class="line">Table result = tableEnv.from(&quot;inputTable&quot;).select(...);</span><br><span class="line">// 通过 SQL查询语句，得到一张结果表</span><br><span class="line">Table sqlResult  = tableEnv.sqlQuery(&quot;SELECT ... FROM inputTable ...&quot;);</span><br><span class="line"></span><br><span class="line">// 将结果表写入输出表中</span><br><span class="line">result.insertInto(&quot;outputTable&quot;);</span><br></pre></td></tr></table></figure><h2 id="创建表环境"><a href="#创建表环境" class="headerlink" title="创建表环境"></a><strong>创建表环境</strong></h2><p>创建表环境最简单的方式，就是基于流处理执行环境，调create方法直接创建：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br></pre></td></tr></table></figure><p>表环境（TableEnvironment）是flink中集成Table API &amp; SQL的核心概念。它负责:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">注册catalog</span><br><span class="line"></span><br><span class="line">在内部 catalog 中注册表</span><br><span class="line"></span><br><span class="line">执行 SQL 查询</span><br><span class="line"></span><br><span class="line">注册用户自定义函数</span><br><span class="line"></span><br><span class="line">将 DataStream 或 DataSet 转换为表</span><br><span class="line"></span><br><span class="line">保存对 ExecutionEnvironment 或 StreamExecutionEnvironment 的引用</span><br></pre></td></tr></table></figure><p>在创建TableEnv的时候，可以多传入一个EnvironmentSettings或者TableConfig参数，可以用来配置 TableEnvironment的一些特性。</p><p>​        比如，配置老版本的流式查询（Flink-Streaming-Query）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">EnvironmentSettings settings = EnvironmentSettings.newInstance()</span><br><span class="line">  .useOldPlanner()      // 使用老版本planner</span><br><span class="line">  .inStreamingMode()    // 流处理模式</span><br><span class="line">  .build();</span><br><span class="line">StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);</span><br></pre></td></tr></table></figure><p>基于老版本的批处理环境（Flink-Batch-Query）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment batchEnv = ExecutionEnvironment.getExecutionEnvironment;</span><br><span class="line">BatchTableEnvironment batchTableEnv = BatchTableEnvironment.create(batchEnv);</span><br></pre></td></tr></table></figure><p>基于blink版本的流处理环境（Blink-Streaming-Query）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">EnvironmentSettings bsSettings = EnvironmentSettings.newInstance()</span><br><span class="line">.useBlinkPlanner()</span><br><span class="line">.inStreamingMode().build();</span><br><span class="line">StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(env, bsSettings);</span><br></pre></td></tr></table></figure><p>基于blink版本的批处理环境（Blink-Batch-Query）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">EnvironmentSettings bbSettings = EnvironmentSettings.newInstance()</span><br><span class="line">.useBlinkPlanner()</span><br><span class="line">.inBatchMode().build();</span><br><span class="line">TableEnvironment bbTableEnv = TableEnvironment.create(bbSettings);</span><br></pre></td></tr></table></figure><h2 id="在Catalog中注册表"><a href="#在Catalog中注册表" class="headerlink" title="在Catalog中注册表"></a>在Catalog中注册表</h2><h3 id="表（Table）的概念"><a href="#表（Table）的概念" class="headerlink" title="表（Table）的概念"></a><strong>表（Table）的概念</strong></h3><p>​        TableEnvironment可以注册目录Catalog，并可以基于Catalog注册表。它会维护一个Catalog-Table表之间的map。</p><p>表（Table）是由一个“标识符”来指定的，由3部分组成：Catalog名、数据库（database）名和对象名（表名）。如果没有指定目录或数据库，就使用当前的默认值。</p><p>表可以是常规的（Table，表），或者虚拟的（View，视图）。常规表（Table）一般可以用来描述外部数据，比如文件、数据库表或消息队列的数据，也可以直接从 DataStream转换而来。视图可以从现有的表中创建，通常是table API或者SQL查询的一个结果。</p><h3 id="连接到文件系统（Csv格式）"><a href="#连接到文件系统（Csv格式）" class="headerlink" title="连接到文件系统（Csv格式）"></a><strong>连接到文件系统（Csv格式）</strong></h3><p>​        连接外部系统在Catalog中注册表，直接调用tableEnv.connect()就可以，里面参数要传入一个ConnectorDescriptor，也就是connector描述器。对于文件系统的connector而言，flink内部已经提供了，就叫做FileSystem()。</p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tableEnv</span><br><span class="line">.connect( new FileSystem().path(&quot;sensor.txt&quot;))  // 定义表数据来源，外部连接</span><br><span class="line">  .withFormat(new OldCsv())    // 定义从外部系统读取数据之后的格式化方法</span><br><span class="line">  .withSchema( new Schema()</span><br><span class="line">    .field(&quot;id&quot;, DataTypes.STRING())</span><br><span class="line">    .field(&quot;timestamp&quot;, DataTypes.BIGINT())</span><br><span class="line">    .field(&quot;temperature&quot;, DataTypes.DOUBLE())</span><br><span class="line">  )    // 定义表结构</span><br><span class="line">  .createTemporaryTable(&quot;inputTable&quot;);    // 创建临时表</span><br></pre></td></tr></table></figure><p>这是旧版本的csv格式描述器。由于它是非标的，跟外部系统对接并不通用，所以将被弃用，以后会被一个符合RFC-4180标准的新format描述器取代。新的描述器就叫Csv()，但flink没有直接提供，需要引入依赖flink-csv：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-csv&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.10.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>代码非常类似，只需要把withFormat里的OldCsv改成Csv就可以了。</p><h3 id="连接到Kafka"><a href="#连接到Kafka" class="headerlink" title="连接到Kafka"></a><strong>连接到Kafka</strong></h3><p>​        kafka的连接器flink-kafka-connector中，1.10版本的已经提供了Table API的支持。我们可以在 connect方法中直接传入一个叫做Kafka的类，这就是kafka连接器的描述器ConnectorDescriptor。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.connect(</span><br><span class="line">  new Kafka()</span><br><span class="line">    .version(&quot;0.11&quot;) // 定义kafka的版本</span><br><span class="line">    .topic(&quot;sensor&quot;) // 定义主题</span><br><span class="line">    .property(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;) </span><br><span class="line">    .property(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class="line">)</span><br><span class="line">  .withFormat(new Csv())</span><br><span class="line">  .withSchema(new Schema()</span><br><span class="line">  .field(&quot;id&quot;, DataTypes.STRING())</span><br><span class="line">  .field(&quot;timestamp&quot;, DataTypes.BIGINT())</span><br><span class="line">  .field(&quot;temperature&quot;, DataTypes.DOUBLE())</span><br><span class="line">)</span><br><span class="line">  .createTemporaryTable(&quot;kafkaInputTable&quot;);</span><br></pre></td></tr></table></figure><p>​        当然也可以连接到ElasticSearch、MySql、HBase、Hive等外部系统，实现方式基本上是类似的。</p><h2 id="表的查询"><a href="#表的查询" class="headerlink" title="表的查询"></a><strong>表的查询</strong></h2><p>利用外部系统的连接器connector，我们可以读写数据，并在环境的Catalog中注册表。接下来就可以对表做查询转换了。</p><p>Flink给我们提供了两种查询方式：Table API和 SQL。</p><h3 id="Table-API的调用"><a href="#Table-API的调用" class="headerlink" title="Table API的调用"></a><strong>Table API的调用</strong></h3><p>Table API是集成在Scala和Java语言内的查询API。与SQL不同，Table API的查询不会用字符串表示，而是在宿主语言中一步一步调用完成的。</p><p>Table API基于代表一张“表”的Table类，并提供一整套操作处理的方法API。这些方法会返回一个新的Table对象，这个对象就表示对输入表应用转换操作的结果。有些关系型转换操作，可以由多个方法调用组成，构成链式调用结构。例如table.select(…).filter(…)，其中select（…）表示选择表中指定的字段，filter(…)表示筛选条件。</p><p>代码中的实现如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Table sensorTable = tableEnv.from(&quot;inputTable&quot;);</span><br><span class="line"></span><br><span class="line">Table resultTable = senorTable</span><br><span class="line">.select(&quot;id, temperature&quot;)</span><br><span class="line">.filter(&quot;id =&#x27;sensor_1&#x27;&quot;);</span><br></pre></td></tr></table></figure><h3 id="SQL查询"><a href="#SQL查询" class="headerlink" title="SQL查询"></a><strong>SQL查询</strong></h3><p>Flink的SQL集成，基于的是ApacheCalcite，它实现了SQL标准。在Flink中，用常规字符串来定义SQL查询语句。SQL 查询的结果，是一个新的 Table。</p><p>代码实现如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Table resultSqlTable = tableEnv.sqlQuery(&quot;select id, temperature from inputTable where id =&#x27;sensor_1&#x27;&quot;);</span><br></pre></td></tr></table></figure><p>当然，也可以加上聚合操作，比如我们统计每个sensor温度数据出现的个数，做个count统计：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Table aggResultTable = sensorTable</span><br><span class="line">.groupBy(&quot;id&quot;)</span><br><span class="line">.select(&quot;id, id.count as count&quot;);</span><br></pre></td></tr></table></figure><p>SQL的实现：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Table aggResultSqlTable = tableEnv.sqlQuery(&quot;select id, count(id) as cnt from inputTable group by id&quot;);</span><br></pre></td></tr></table></figure><p>这里Table API里指定的字段，前面加了一个单引号’，这是Table API中定义的Expression类型的写法，可以很方便地表示一个表中的字段。</p><p>字段可以直接全部用双引号引起来，也可以用半边单引号+字段名的方式。以后的代码中，一般都用后一种形式。</p><h2 id="将DataStream-转换成表"><a href="#将DataStream-转换成表" class="headerlink" title="将DataStream 转换成表"></a><strong>将DataStream 转换成表</strong></h2><p>Flink允许我们把Table和DataStream做转换：我们可以基于一个DataStream，先流式地读取数据源，然后map成POJO，再把它转成Table。Table的列字段（column fields），就是POJO里的字段，这样就不用再麻烦地定义schema了。</p><h3 id="代码表达"><a href="#代码表达" class="headerlink" title="代码表达"></a><strong>代码表达</strong></h3><p>代码中实现非常简单，直接用tableEnv.fromDataStream()就可以了。默认转换后的 Table schema 和 DataStream 中的字段定义一一对应，也可以单独指定出来。</p><p>这就允许我们更换字段的顺序、重命名，或者只选取某些字段出来，相当于做了一次map操作（或者Table API的 select操作）。</p><p>代码具体如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;sensor.txt&quot;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;SensorReading&gt; dataStream = inputStream</span><br><span class="line">        .map( line -&gt; &#123;</span><br><span class="line">            String[] fields = line.split(&quot;,&quot;);</span><br><span class="line">            return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2]));</span><br><span class="line">        &#125; );</span><br><span class="line"></span><br><span class="line">Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;id, timestamp.rowtime as ts, temperature&quot;);</span><br></pre></td></tr></table></figure><h3 id="数据类型与-Table-schema的对应"><a href="#数据类型与-Table-schema的对应" class="headerlink" title="数据类型与 Table schema的对应"></a><strong>数据类型与 Table schema的对应</strong></h3><p>在上节的例子中，DataStream 中的数据类型，与表的 Schema 之间的对应关系，是按照类中的字段名来对应的（name-based mapping），所以还可以用as做重命名。</p><p>基于名称的对应：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;timestamp as ts, id as myId, temperature&quot;);</span><br></pre></td></tr></table></figure><p>Flink的DataStream和 DataSet API支持多种类型。</p><p>组合类型，比如元组（内置Scala和Java元组）、POJO、Scala case类和Flink的Row类型等，允许具有多个字段的嵌套数据结构，这些字段可以在Table的表达式中访问。其他类型，则被视为原子类型。</p><h2 id="创建临时视图（Temporary-View）"><a href="#创建临时视图（Temporary-View）" class="headerlink" title="创建临时视图（Temporary View）"></a><strong>创建临时视图（Temporary View）</strong></h2><p>创建临时视图的第一种方式，就是直接从DataStream转换而来。同样，可以直接对应字段转换；也可以在转换的时候，指定相应的字段。</p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.createTemporaryView(&quot;sensorView&quot;, dataStream);</span><br><span class="line">tableEnv.createTemporaryView(&quot;sensorView&quot;, dataStream, &quot;id, temperature, timestamp as ts&quot;);</span><br></pre></td></tr></table></figure><p>另外，当然还可以基于Table创建视图：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.createTemporaryView(&quot;sensorView&quot;, sensorTable);</span><br></pre></td></tr></table></figure><p>View和Table的Schema完全相同。事实上，在Table API中，可以认为View和Table是等价的。</p><h2 id="输出表"><a href="#输出表" class="headerlink" title="输出表"></a><strong>输出表</strong></h2><p>表的输出，是通过将数据写入 TableSink 来实现的。TableSink 是一个通用接口，可以支持不同的文件格式、存储数据库和消息队列。</p><p>具体实现，输出表最直接的方法，就是通过 Table.insertInto() 方法将一个 Table 写入注册过的 TableSink 中。</p><h3 id="输出到文件"><a href="#输出到文件" class="headerlink" title="输出到文件"></a><strong>输出到文件</strong></h3><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// 注册输出表</span><br><span class="line">tableEnv.connect(</span><br><span class="line">  new FileSystem().path(&quot;…\\resources\\out.txt&quot;)</span><br><span class="line">) // 定义到文件系统的连接</span><br><span class="line">  .withFormat(new Csv()) // 定义格式化方法，Csv格式</span><br><span class="line">  .withSchema(new Schema()</span><br><span class="line">  .field(&quot;id&quot;, DataTypes.STRING())</span><br><span class="line">  .field(&quot;temp&quot;, DataTypes.DOUBLE())</span><br><span class="line">) // 定义表结构</span><br><span class="line">  .createTemporaryTable(&quot;outputTable&quot;); // 创建临时表</span><br><span class="line"></span><br><span class="line">resultSqlTable.insertInto(&quot;outputTable&quot;);</span><br></pre></td></tr></table></figure><h3 id="更新模式（Update-Mode）"><a href="#更新模式（Update-Mode）" class="headerlink" title="更新模式（Update Mode）"></a><strong>更新模式（Update Mode）</strong></h3><p>在流处理过程中，表的处理并不像传统定义的那样简单。</p><p>对于流式查询（Streaming Queries），需要声明如何在（动态）表和外部连接器之间执行转换。与外部系统交换的消息类型，由<strong>更新模式</strong>（update mode）指定。</p><p>Flink Table API中的更新模式有以下三种：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214192920.png"></p><h3 id="输出到Kafka"><a href="#输出到Kafka" class="headerlink" title="输出到Kafka"></a><strong>输出到Kafka</strong></h3><p>除了输出到文件，也可以输出到Kafka。我们可以结合前面Kafka作为输入数据，构建数据管道，kafka进，kafka出。</p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// 输出到 kafka</span><br><span class="line">tableEnv.connect(</span><br><span class="line">  new Kafka()</span><br><span class="line">    .version(&quot;0.11&quot;)</span><br><span class="line">    .topic(&quot;sinkTest&quot;)</span><br><span class="line">    .property(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;)</span><br><span class="line">    .property(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class="line">)</span><br><span class="line">  .withFormat( new Csv() )</span><br><span class="line">  .withSchema( new Schema()</span><br><span class="line">    .field(&quot;id&quot;, DataTypes.STRING())</span><br><span class="line">    .field(&quot;temp&quot;, DataTypes.DOUBLE())</span><br><span class="line">  )</span><br><span class="line">  .createTemporaryTable(&quot;kafkaOutputTable&quot;);</span><br><span class="line"></span><br><span class="line">resultTable.insertInto(&quot;kafkaOutputTable&quot;);</span><br></pre></td></tr></table></figure><h3 id="输出到ElasticSearch"><a href="#输出到ElasticSearch" class="headerlink" title="输出到ElasticSearch"></a><strong>输出到ElasticSearch</strong></h3><p>ElasticSearch的connector可以在upsert（update+insert，更新插入）模式下操作，这样就可以使用Query定义的键（key）与外部系统交换UPSERT/DELETE消息。</p><p>另外，对于“仅追加”（append-only）的查询，connector还可以在append 模式下操作，这样就可以与外部系统只交换insert消息。</p><p>es目前支持的数据格式，只有Json，而flink本身并没有对应的支持，所以还需要引入依赖：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-json&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.10.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>代码实现如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// 输出到es</span><br><span class="line">tableEnv.connect(</span><br><span class="line">  new Elasticsearch()</span><br><span class="line">    .version(&quot;6&quot;)</span><br><span class="line">    .host(&quot;localhost&quot;, 9200, &quot;http&quot;)</span><br><span class="line">    .index(&quot;sensor&quot;)</span><br><span class="line">    .documentType(&quot;temp&quot;)</span><br><span class="line">)</span><br><span class="line">  .inUpsertMode()           // 指定是 Upsert 模式</span><br><span class="line">  .withFormat(new Json())</span><br><span class="line">  .withSchema( new Schema()</span><br><span class="line">    .field(&quot;id&quot;, DataTypes.STRING())</span><br><span class="line">    .field(&quot;count&quot;, DataTypes.BIGINT())</span><br><span class="line">  )</span><br><span class="line">  .createTemporaryTable(&quot;esOutputTable&quot;);</span><br><span class="line"></span><br><span class="line">aggResultTable.insertInto(&quot;esOutputTable&quot;);</span><br></pre></td></tr></table></figure><h3 id="输出到MySql"><a href="#输出到MySql" class="headerlink" title="输出到MySql"></a><strong>输出到MySql</strong></h3><p>Flink专门为Table API的jdbc连接提供了flink-jdbc连接器，我们需要先引入依赖：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-jdbc_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.10.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>jdbc连接的代码实现比较特殊，因为没有对应的java/scala类实现ConnectorDescriptor，所以不能直接tableEnv.connect()。不过Flink SQL留下了执行DDL的接口：tableEnv.sqlUpdate()。</p><p>对于jdbc的创建表操作，天生就适合直接写DDL来实现，所以我们的代码可以这样写：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// 输出到 Mysql</span><br><span class="line">String sinkDDL= &quot;create table jdbcOutputTable (&quot; +</span><br><span class="line">        &quot; id varchar(20) not null, &quot; +</span><br><span class="line">        &quot; cnt bigint not null &quot; +</span><br><span class="line">        &quot;) with (&quot; +</span><br><span class="line">        &quot; &#x27;connector.type&#x27; = &#x27;jdbc&#x27;, &quot; +</span><br><span class="line">        &quot; &#x27;connector.url&#x27; = &#x27;jdbc:mysql://localhost:3306/test&#x27;, &quot; +</span><br><span class="line">        &quot; &#x27;connector.table&#x27; = &#x27;sensor_count&#x27;, &quot; +</span><br><span class="line">        &quot; &#x27;connector.driver&#x27; = &#x27;com.mysql.jdbc.Driver&#x27;, &quot; +</span><br><span class="line">        &quot; &#x27;connector.username&#x27; = &#x27;root&#x27;, &quot; +</span><br><span class="line">        &quot; &#x27;connector.password&#x27; = &#x27;123456&#x27; )&quot;;</span><br><span class="line"></span><br><span class="line">tableEnv.sqlUpdate(sinkDDL);    // 执行 DDL创建表</span><br><span class="line">aggResultSqlTable.insertInto(&quot;jdbcOutputTable&quot;);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="将表转换成DataStream"><a href="#将表转换成DataStream" class="headerlink" title="将表转换成DataStream"></a><strong>将表转换成DataStream</strong></h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214193149.png"></p><p>代码实现如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Row&gt; resultStream = tableEnv.toAppendStream(resultTable, Row.class);</span><br><span class="line">DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; aggResultStream = tableEnv.toRetractStream(aggResultTable, Row.class);</span><br><span class="line"></span><br><span class="line">resultStream.print(&quot;result&quot;);</span><br><span class="line">aggResultStream.print(&quot;aggResult&quot;);</span><br></pre></td></tr></table></figure><p>所以，没有经过groupby之类聚合操作，可以直接用 toAppendStream 来转换；而如果经过了聚合，有更新操作，一般就必须用 toRetractDstream。</p><h2 id="Query的解释和执行"><a href="#Query的解释和执行" class="headerlink" title="Query的解释和执行"></a>Query的解释和执行</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214193238.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">String explaination = tableEnv.explain(resultTable);</span><br><span class="line">System.out.println(explaination);</span><br></pre></td></tr></table></figure><p>Query的解释和执行过程，老planner和blink planner大体是一致的，又有所不同。整体来讲，Query都会表示成一个逻辑查询计划，然后分两步解释：</p><ol><li><p>优化查询计划</p></li><li><p>解释成 DataStream 或者 DataSet程序</p></li></ol><p>而Blink版本是批流统一的，所以所有的Query，只会被解释成DataStream程序；另外在批处理环境TableEnvironment下，Blink版本要到tableEnv.execute()执行调用才开始解释。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;整体介绍&quot;&gt;&lt;a href=&quot;#整体介绍&quot; class=&quot;headerlink&quot; title=&quot;整体介绍&quot;&gt;&lt;/a&gt;整体介绍&lt;/h1&gt;&lt;h2 id=&quot;Flink-SQL-源码案例&quot;&gt;&lt;a href=&quot;#Flink-SQL-源码案例&quot; class=&quot;headerlink&quot; title=&quot;Flink SQL 源码案例&quot;&gt;&lt;/a&gt;Flink SQL 源码案例&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo10/&quot;&gt;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo10/&lt;/a&gt;  从FlinkDemo10往后都是&lt;/p&gt;
&lt;h2 id=&quot;什么是-Table-API-和-Flink-SQL&quot;&gt;&lt;a href=&quot;#什么是-Table-API-和-Flink-SQL&quot; class=&quot;headerlink&quot; title=&quot;什么是 Table API 和 Flink SQL&quot;&gt;&lt;/a&gt;什么是 Table API 和 Flink SQL&lt;/h2&gt;&lt;p&gt;Flink本身是批流统一的处理框架，所以Table API和SQL，就是批流统一的上层处理API。&lt;br&gt;目前功能尚未完善，处于活跃的开发阶段。&lt;br&gt;Table API是一套内嵌在Java和Scala语言中的查询API，它允许我们以非常直观的方式，组合来自一些关系运算符的查询（比如select、filter和join）。而对于Flink SQL，就是直接可以在代码中写SQL，来实现一些查询（Query）操作。Flink的SQL支持，基于实现了SQL标准的Apache Calcite（Apache开源SQL解析工具）。&lt;br&gt;无论输入是批输入还是流式输入，在这两套API中，指定的查询都具有相同的语义，得到相同的结果。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink+Kafka如何实现端到端的exactly-once语义</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink-Kafka%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84exactly-once%E8%AF%AD%E4%B9%89/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink-Kafka%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84exactly-once%E8%AF%AD%E4%B9%89/</id>
    <published>2022-02-14T10:27:01.000Z</published>
    <updated>2022-02-14T15:13:21.039Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214231316.png"></p><h1 id="Flink和kafka是如何实现端到端的exactly-once的呢"><a href="#Flink和kafka是如何实现端到端的exactly-once的呢" class="headerlink" title="Flink和kafka是如何实现端到端的exactly-once的呢?"></a>Flink和kafka是如何实现端到端的exactly-once的呢?</h1><p>​        ①首先Flink的内部一定是exactly-once的.因为他利用了CheckPoint机制,把状态存盘,发生故障的时候可以从HDFS文件系统中恢复.<br>​        ②如果你的Source是kafka的话.可不可以做到呢?<br>​            可以的.因为kafka的source是可以做到偏移量(offset)重置的.而且可以随意重置.甚至Flink他会自动帮我重置的.就是在故障恢复的时候他会自动帮我重置的.所以代码我都可以不用写.<br>​        ③那Sink,kafka支持什么呢?<br>​            kafka支持两阶段提交.实际上说白了就是真正意义上的事务.<br>这个一个前提条件,就是你必须在kafka的配置文件中设置隔离级别和开启我们kafka的事务.这是kafka的配置,是需要我们自己去配的.他和我们Flink的代码是没有关系的.    </p><p>​           而且我们kafka 的producer的sink本身就继承了这个TwoPhaseCommitSinkFunction(两阶段提交函数). 上面我们说过有两个父类.一个支持预写日志(WAL)的父类GenriceWriteAheadSink模板类,这个模板类就是父类.还有一个是两阶段提价(2PC)的父类TwoPhaseCommitSinkFunction接口.实际上我们kafka的Producer默认就是支持的.如下图所示:</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214183024.png"></p><p>所以这个对象<strong>FlinkKafkaProducer011默认就支持两阶段提交</strong>.你只需要把kafka的属性设置好.他就能做到两阶段提交了.所以实际上Flink天生就和kafka结合的.</p><p>​    我们知道，端到端的状态一致性的实现，需要每一个组件都实现，对于Flink + Kafka的数据管道系统（Kafka进、Kafka出）而言，各组件怎样保证exactly-once语义呢？</p><ul><li>内部 —— 利用checkpoint机制，把状态存盘，发生故障的时候可以恢复，保证内部的状态一致性</li><li>source —— kafka consumer作为source，可以将偏移量保存下来，如果后续任务出现了故障，恢复的时候可以由连接器重置偏移量，重新消费数据，保证一致性</li><li>sink —— kafka producer作为sink，采用两阶段提交 sink，需要实现一个 TwoPhaseCommitSinkFunction</li></ul><p>内部的checkpoint机制我们已经有了了解，那source和sink具体又是怎样运行的呢？接下来我们逐步做一个分析。</p><p>我们知道Flink由JobManager协调各个TaskManager进行checkpoint存储，checkpoint保存在 StateBackend中，默认StateBackend是内存级的，也可以改为文件级的进行持久化保存。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214183208.png"></p><p>当 checkpoint 启动时，JobManager 会将检查点分界线（barrier）注入数据流；barrier会在算子间传递下去。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214183224.png"></p><p>每个算子会对当前的状态做个快照，保存到状态后端。对于source任务而言，就会把当前的offset作为状态保存起来。下次从checkpoint恢复时，source任务可以重新提交偏移量，从上次保存的位置开始重新消费数据。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214183240.png"></p><p>每个内部的 transform 任务遇到 barrier 时，都会把状态存到 checkpoint 里。</p><p>sink 任务首先把数据写入外部 kafka，这些数据都属于预提交的事务（还不能被消费）；当遇到 barrier 时，把状态保存到状态后端，并开启新的预提交事务。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214183300.png"></p><p>当所有算子任务的快照完成，也就是这次的 checkpoint 完成时，JobManager 会向所有任务发通知，确认这次 checkpoint 完成。</p><p>当sink 任务收到确认通知，就会正式提交之前的事务，kafka 中未确认的数据就改为“已确认”，数据就真正可以被消费了。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214183318.png"></p><p>所以我们看到，执行过程实际上是一个两段式提交，每个算子执行完成，会进行“预提交”，直到执行完sink操作，会发起“确认提交”，如果执行失败，预提交会放弃掉。</p><p>具体的两阶段提交步骤总结如下：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214183346.png"></p><p>所以我们也可以看到，如果宕机需要通过StateBackend进行恢复，只能恢复所有确认提交的操作。</p><h1 id="选择一个状态后端-state-backend"><a href="#选择一个状态后端-state-backend" class="headerlink" title="选择一个状态后端(state backend)"></a>选择一个状态后端(state backend)</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214183451.png"></p><p>注意：RocksDB的支持并不直接包含在flink中，需要引入依赖：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-statebackend-rocksdb_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.7.2&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>设置状态后端为FsStateBackend：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">val checkpointPath: String = ???</span><br><span class="line">val backend = new RocksDBStateBackend(checkpointPath)</span><br><span class="line"></span><br><span class="line">env.setStateBackend(backend)</span><br><span class="line">env.setStateBackend(new FsStateBackend(&quot;file:///tmp/checkpoints&quot;))</span><br><span class="line">env.enableCheckpointing(1000)</span><br><span class="line">// 配置重启策略</span><br><span class="line">env.setRestartStrategy(RestartStrategies.fixedDelayRestart(60, Time.of(10, TimeUnit.SECONDS)))</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214231316.png&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Flink和kafka是如何实现端到端的exactly-once的呢&quot;&gt;&lt;a href=&quot;#Flink和kafka是如何实现端到端的exactly-once的呢&quot; class=&quot;headerlink&quot; title=&quot;Flink和kafka是如何实现端到端的exactly-once的呢?&quot;&gt;&lt;/a&gt;Flink和kafka是如何实现端到端的exactly-once的呢?&lt;/h1&gt;&lt;p&gt;​        ①首先Flink的内部一定是exactly-once的.因为他利用了CheckPoint机制,把状态存盘,发生故障的时候可以从HDFS文件系统中恢复.&lt;br&gt;​        ②如果你的Source是kafka的话.可不可以做到呢?&lt;br&gt;​            可以的.因为kafka的source是可以做到偏移量(offset)重置的.而且可以随意重置.甚至Flink他会自动帮我重置的.就是在故障恢复的时候他会自动帮我重置的.所以代码我都可以不用写.&lt;br&gt;​        ③那Sink,kafka支持什么呢?&lt;br&gt;​            kafka支持两阶段提交.实际上说白了就是真正意义上的事务.&lt;br&gt;这个一个前提条件,就是你必须在kafka的配置文件中设置隔离级别和开启我们kafka的事务.这是kafka的配置,是需要我们自己去配的.他和我们Flink的代码是没有关系的.    &lt;/p&gt;
&lt;p&gt;​           而且我们kafka 的producer的sink本身就继承了这个TwoPhaseCommitSinkFunction(两阶段提交函数). 上面我们说过有两个父类.一个支持预写日志(WAL)的父类GenriceWriteAheadSink模板类,这个模板类就是父类.还有一个是两阶段提价(2PC)的父类TwoPhaseCommitSinkFunction接口.实际上我们kafka的Producer默认就是支持的.如下图所示:&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink的状态编程和容错机制之检查点（checkpoint）</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E7%9A%84%E7%8A%B6%E6%80%81%E7%BC%96%E7%A8%8B%E5%92%8C%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6%E4%B9%8B%E6%A3%80%E6%9F%A5%E7%82%B9%EF%BC%88checkpoint%EF%BC%89/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E7%9A%84%E7%8A%B6%E6%80%81%E7%BC%96%E7%A8%8B%E5%92%8C%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6%E4%B9%8B%E6%A3%80%E6%9F%A5%E7%82%B9%EF%BC%88checkpoint%EF%BC%89/</id>
    <published>2022-02-14T09:03:12.000Z</published>
    <updated>2022-02-14T14:28:15.594Z</updated>
    
    <content type="html"><![CDATA[<p>​        Flink具体如何保证exactly-once呢? 它使用一种被称为”检查点”（checkpoint）的特性，在出现故障时将系统重置回正确状态。下面通过简单的类比来解释检查点的作用。</p><p>​        假设你和两位朋友正在数项链上有多少颗珠子，如下图所示。你捏住珠子，边数边拨，每拨过一颗珠子就给总数加一。你的朋友也这样数他们手中的珠子。当你分神忘记数到哪里时，怎么办呢? 如果项链上有很多珠子，你显然不想从头再数一遍，尤其是当三人的速度不一样却又试图合作的时候，更是如此(比如想记录前一分钟三人一共数了多少颗珠子，回想一下一分钟滚动窗口)。</p><span id="more"></span><p>​    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214170606.png">        </p><p>​        于是，你想了一个更好的办法: 在项链上每隔一段就松松地系上一根有色皮筋，将珠子分隔开; 当珠子被拨动的时候，皮筋也可以被拨动; 然后，你安排一个助手，让他在你和朋友拨到皮筋时记录总数。用这种方法，当有人数错时，就不必从头开始数。相反，你向其他人发出错误警示，然后你们都从上一根皮筋处开始重数，助手则会告诉每个人重数时的起始数值，例如在粉色皮筋处的数值是多少。</p><p>​        Flink检查点的作用就类似于皮筋标记。数珠子这个类比的关键点是: 对于指定的皮筋而言，珠子的相对位置是确定的; 这让皮筋成为重新计数的参考点。总状态(珠子的总数)在每颗珠子被拨动之后更新一次，助手则会保存与每根皮筋对应的检查点状态，如当遇到粉色皮筋时一共数了多少珠子，当遇到橙色皮筋时又是多少。当问题出现时，这种方法使得重新计数变得简单。</p><h1 id="Flink的检查点算法"><a href="#Flink的检查点算法" class="headerlink" title="Flink的检查点算法"></a>Flink的检查点算法</h1><p>Flink检查点:</p><p>①隔一段时间(程序员设置),由JobManager来触发CheckPoint.<br>②CheckPoint保存在JobManager内存里面.</p><p>保存CheckPoint之后就意味着我们程序有一份快照了.比如source5,sum奇数9,偶数6.保存了</p><p>某个子任务挂掉造成整个Job挂掉.重启Job,需要保证状态一致.<br>一致性分为三种级别: ①最多一次②至少一次③精确一次<br>如何保证精确一次? 从上一个检查点开始恢复.从source5,sum奇数9,偶数6.开始恢复.<br>由于你offset是5,那就从5开始重新读.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214222509.png"></p><p>Jobmanger发送Barrier,Barrier里面封装id号, JobManager将barrier发送给每一个source.</p><p>只是将Barrier发送到原来source流里面的数据中了.只是将Barrier插入到流里面了.</p><p>算子接收到Barrier之后马上进行CheckPoint, 即将Barrier之前的offset保存到CheckPoint中. 保存在哪里呢? 保存在状态中,或者说保存在JobManager的内存中.<br>接收到Barrier之后,将offset保存到状态中,同时还要将Barrier以广播的形式发送出去.下面的几个节点都发. 下面的节点,如果有个source的情况下, 其中一个算子接收到Barrier之后,这个算子先暂停工作,将barrier后的数据缓存起来, 到了第二个source的Barrier来了之后在进行CheckPoint.即将offset保存到状态中.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214222807.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214171442.png"></p><p>①CheckPoint的Barrier检查点机制确实可以做到Flink内部算子的精确一致. 但是无法做到端到端的一致.<br>如果想要保证端到端的一致就要考虑,source是哪一种source. 比如: source支不支持重置offset.  Kafka可以,kafka是支持offset重置的.  如果是kafka, 再结合CheckPoint就能做到end-to-end,即端到端的精确一致.</p><p>我们到底把CheckPoint保存在哪里?<br><strong>checkPoint数据保存在哪里是由状态后端(state backend)的机制决定的.</strong></p><p>Flink有三种状态后端.<br>实际上有两种情况:<br>没有checkPoint的本地状态保存在TaskManager内存中<br>CheckPoint之后由TaskManager内存上转移到JobManager内存中.<br>即Flink的默认状态保存在TaskManager的JVM内存上,  而CheckPoint的状态保存在JobManager的JVM内存 上. 这是默认情况.</p><p>第一: <strong>memoryStateBackend(内存级状态后端)</strong> ,保存内存中, 将键控状态作为内存中的对象进行管理. 一开始键控状态是存储在TaskManager的JVM堆内存中.<br>因为TaskManager才真正执行任务. 当触发CheckPoint的时候, 他会将键控状态保存到JobManager的JVM内存中.  所以说内存级状态后端是保存在JobManager内存中的.<br>第二:<strong>FsStateBackend(持久化到远程文件系统中,最常用的是HDFS文件系统).</strong><br>第三:<strong>RocksDBStateBackend(RockDB 数据库的状态后端)</strong></p><p>我们用是内存级状态后端比较多,虽然存储在状态中不稳定,但是因为我们JobManager使用zookeeper做了高可用.还有另外一台JobManager存放CheckPoint呀.<br>如果你认为这种还要风险的话,那就使用第二种,存放在文件系统中.</p><p><strong>内存级状态后端和文件系统状态状态后端</strong><br>共同点:<br>他们的算子状态和键控状态都保存在TaskManager的堆内存上.<br>不同点:<br>当触发checkPoint时,内存级状态后端保存在JobManager内存上; 而文件系统状态后端保存在文件系统上.<br>注意: checkPoint之后,TaskManager内存上的还有状态的, 他不会动原来的状态,只是又存了一份而已. 可以理解为存了两份状态.<br>但是RockDB状态后端就不保存在TaskManager的内存上了,而是直接保存在RockDB数据库中.  RockDB是键值对数据库,类似redis,有Flink自己维护.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214171657.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214171719.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214171738.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214171801.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214171819.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214171838.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214171904.png"></p><p><strong>总结CheckPoint原理</strong></p><p>​        首先CheckPoint是由JobManager来发起来开始CheckPoint.由他来发起的话,他就将Barrier发到流里面去.把Barrier发给流.比如:如果当且这个Barrier就插在3的后面4的前面. 而且所有的流都可以插.所以这里暂时不用考虑两个流的情况.因为两个流和一个流实际上是一样的.如下图:</p><p>​    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214171952.png"></p><p>​        直接当Source2不存在就可以了.然后我们也暂时不考虑分组.就直接每一个数据我们做聚合累加.就做这么一个简单的事情.我们就先将一下这个简单的过程.将他讲清楚.如下图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214172014.png"></p><p>​        现在我们是这种情况:我们不做奇偶的分组了.而是来一个数据就累加,来一个数据就累加.所以这个sum直接做累加.他不是什么累加偶数,累加奇数什么的.而是来一条数据就累加,来一条数据就累加.<br>​        假设JobManager触发了一个CheckPoint,然后他就把这个三角形的Barrier插到数据里面去即插到流里面去.source1呢,刚刚把3收到了.并且把3发出去了.假设把3发出去了.这个之后就收到了一个三角形即Barrier了.<br>收到这个三角形(Barrier)之后呢,马上把当前状态中保存的Offset,再把它保存到CheckPoint里面去.实际上就是把这个Offset保存到CheckPoint里面.<br>​            保存完了之后,他需要做两件事情.<br>​            第一件事情就是把Barrier往外广播.往下一步广播.<br>​            第二件事情就是告诉通知我们的JobManager,我当前Source的这个CheckPoint结束了.<br>​        然后由于他往外广播了,那这个barrier可能是不是又被这个Sum 算子收到了呢?(sum even )那么请问这个Sum算子收到的Barrier一定是在那一条数据后面才能收到的.因为这里不考虑奇偶,所以一定是在3后面才能收到.所以,一开始他收到3的时候他直接做累加.1+2+3=6所以他的状态就是6.如下图所示:</p><p>​    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214172114.png"></p><p>​        因为3进来之后就是1+2+3=6 ,所以状态就是6. 收到3之后马上是不是收到一个Barrier呀.收到Barrier之后.他把这个6(这个6本来就在状态里面了)<br>也是CheckPoint一下.保存到我们的状态后端.然后也是一样.做两件事情.第一件事情:对外广播.第二件事情:通知我的JobManager,我的Sum聚合算子完成了CheckPoint.<br>​        那这里可能会有疑问,就是这个sum这个节点正在做CheckPoint的时候这个source1又有数据来了,即4过来了.4来了怎么办呢?<br>他会把这个4缓存起来.因为我这个CheckPoint还没有做完,所以他把4缓存起来.缓存到哪里呢?缓存到map集合中.那么这个map集合保存到哪里呢?当然是保存到JVM的内存里面.除非这个sum节点的CheckPoint完成了.完成之后呢.他就会从缓存中去把4读进来,再次做聚合.做4的聚合.那你可能会问,这时候5也来了呀?他是按照队列来的,先拿4,再拿5.  5那完之后再次从source里面拿下一个.以此类推.那接下来开始 写到sink里面.写到Sink他也是一样的.他也是按照上面所说的这个过程.一直到所有的节点CheckPoint都完成了最后一次通知我这个JobManager之后,如下图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214172157.png"></p><p>最后一次通知我这个JobManager之后.我这个JobManager就知道了,整个过程的CheckPoint就已经结束了.也就是说,ID为2的这个CheckPoint就已经结束了.就是这么一个过程.所以两条流和一条流实际上都是一样的.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214172238.png"></p><p>Flink检查点的核心作用是确保状态正确，即使遇到程序中断，也要正确。记住这一基本点之后，我们用一个例子来看检查点是如何运行的。Flink为用户提供了用来定义状态的工具。例如，以下这个Scala程序按照输入记录的第一个字段(一个字符串)进行分组并维护第二个字段的计数状态。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val stream: DataStream[(String, Int)] = ... </span><br><span class="line">val counts: DataStream[(String, Int)] = stream</span><br><span class="line">.keyBy(record =&gt; record._1)</span><br><span class="line">.mapWithState(  (in: (String, Int), state: Option[Int])  =&gt; </span><br><span class="line">state match &#123; </span><br><span class="line">case Some(c) =&gt; ( (in._1, c + in._2), Some(c + in._2) ) </span><br><span class="line">case None =&gt; ( (in._1, in._2), Some(in._2) )</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>该程序有两个算子: keyBy算子用来将记录按照第一个元素(一个字符串)进行分组，根据该key将数据进行重新分区，然后将记录再发送给下一个算子: 有状态的map算子(mapWithState)。map算子在接收到每个元素后，将输入记录的第二个字段的数据加到现有总数中，再将更新过的元素发射出去。下图表示程序的初始状态: 输入流中的6条记录被检查点分割线(checkpoint barrier)隔开，所有的map算子状态均为0(计数还未开始)。所有key为a的记录将被顶层的map算子处理，所有key为b的记录将被中间层的map算子处理，所有key为c的记录则将被底层的map算子处理。</p><p>图 按key累加计数程序初始状态</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214172446.png"></p><p>上图是程序的初始状态。注意，a、b、c三组的初始计数状态都是0，即三个圆柱上的值。ckpt表示检查点分割线（checkpoint barriers）。每条记录在处理顺序上严格地遵守在检查点之前或之后的规定，例如[“b”,2]在检查点之前被处理，[“a”,2]则在检查点之后被处理。</p><p>当该程序处理输入流中的6条记录时，涉及的操作遍布3个并行实例(节点、CPU内核等)。那么，检查点该如何保证exactly-once呢?</p><p>检查点分割线和普通数据记录类似。它们由算子处理，但并不参与计算，而是会触发与检查点相关的行为。当读取输入流的数据源(在本例中与keyBy算子内联)遇到检查点屏障时，它将其在输入流中的位置保存到持久化存储中。如果输入流来自消息传输系统(Kafka)，这个位置就是偏移量。Flink的存储机制是插件化的，持久化存储可以是分布式文件系统，如HDFS。下图展示了这个过程。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214172524.png"></p><p>当Flink数据源(在本例中与keyBy算子内联)遇到检查点分界线（barrier）时，它会将其在输入流中的位置保存到持久化存储中。这让 Flink可以根据该位置重启。</p><p>检查点像普通数据记录一样在算子之间流动。当map算子处理完前3条数据并收到检查点分界线时，它们会将状态以异步的方式写入持久化存储，如下图所示</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214172548.png"></p><p>位于检查点之前的所有记录([“b”,2]、[“b”,3]和[“c”,1])被map算子处理之后的情况。此时，持久化存储已经备份了检查点分界线在输入流中的位置(备份操作发生在barrier被输入算子处理的时候)。map算子接着开始处理检查点分界线，并触发将状态异步备份到稳定存储中这个动作。</p><p>当map算子的状态备份和检查点分界线的位置备份被确认之后，该检查点操作就可以被标记为完成，如下图所示。我们在无须停止或者阻断计算的条件下，在一个逻辑时间点(对应检查点屏障在输入流中的位置)为计算状态拍了快照。通过确保备份的状态和位置指向同一个逻辑时间点，后文将解释如何基于备份恢复计算，从而保证exactly-once。值得注意的是，当没有出现故障时，Flink检查点的开销极小，检查点操作的速度由持久化存储的可用带宽决定。回顾数珠子的例子: 除了因为数错而需要用到皮筋之外，皮筋会被很快地拨过。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214172608.png"></p><p>检查点操作完成，状态和位置均已备份到稳定存储中。输入流中的所有数据记录都已处理完成。值得注意的是，备份的状态值与实际的状态值是不同的。备份反映的是检查点的状态。</p><p>如果检查点操作失败，Flink可以丢弃该检查点并继续正常执行，因为之后的某一个检查点可能会成功。虽然恢复时间可能更长，但是对于状态的保证依旧很有力。只有在一系列连续的检查点操作失败之后，Flink才会抛出错误，因为这通常预示着发生了严重且持久的错误。</p><p>现在来看看下图所示的情况: 检查点操作已经完成，但故障紧随其后。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214172719.png"></p><p>在这种情况下，Flink会重新拓扑(可能会获取新的执行资源)，将输入流倒回到上一个检查点，然后恢复状态值并从该处开始继续计算。在本例中，[“a”,2]、[“a”,2]和[“c”,2]这几条记录将被重播。</p><p>下图展示了这一重新处理过程。从上一个检查点开始重新计算，可以保证在剩下的记录被处理之后，得到的map算子的状态值与没有发生故障时的状态值一致。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214172742.png"></p><p>Flink将输入流倒回到上一个检查点屏障的位置，同时恢复map算子的状态值。然后，Flink从此处开始重新处理。这样做保证了在记录被处理之后，map算子的状态值与没有发生故障时的一致。<br>Flink检查点算法的正式名称是异步分界线快照(asynchronous barrier snapshotting)。该算法大致基于Chandy-Lamport分布式快照算法。<br><strong>检查点是Flink最有价值的创新之一，因为它使Flink可以保证exactly-once，并且不需要牺牲性能。</strong></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;​        Flink具体如何保证exactly-once呢? 它使用一种被称为”检查点”（checkpoint）的特性，在出现故障时将系统重置回正确状态。下面通过简单的类比来解释检查点的作用。&lt;/p&gt;
&lt;p&gt;​        假设你和两位朋友正在数项链上有多少颗珠子，如下图所示。你捏住珠子，边数边拨，每拨过一颗珠子就给总数加一。你的朋友也这样数他们手中的珠子。当你分神忘记数到哪里时，怎么办呢? 如果项链上有很多珠子，你显然不想从头再数一遍，尤其是当三人的速度不一样却又试图合作的时候，更是如此(比如想记录前一分钟三人一共数了多少颗珠子，回想一下一分钟滚动窗口)。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink的状态编程和容错机制之状态一致性</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E7%9A%84%E7%8A%B6%E6%80%81%E7%BC%96%E7%A8%8B%E5%92%8C%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6%E4%B9%8B%E7%8A%B6%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E7%9A%84%E7%8A%B6%E6%80%81%E7%BC%96%E7%A8%8B%E5%92%8C%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6%E4%B9%8B%E7%8A%B6%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7/</id>
    <published>2022-02-14T08:40:53.000Z</published>
    <updated>2022-02-14T08:52:56.661Z</updated>
    
    <content type="html"><![CDATA[<p>当在分布式系统中引入状态时，自然也会导致一系列的问题.如:</p><p>​    一致性问题.</p><p>什么叫一致性问题呢?</p><p><strong>就是不管你这个任务是失败了,还是中间出错了,还是暂停了,还是重复启动了.我要保证所有数据,不管在那种情况下发生,数据都是准确的或者说结果一定是准确的.这就叫所谓的状态一致.</strong></p><p>你要想保证结果准确,首先你就要保证的就是状态一致.所以状态一致就是为了保证最后结果的准确.</p><p>一致性的级别在我们Flink中分为三种.</p><span id="more"></span><h1 id="一致性-级别分为3种"><a href="#一致性-级别分为3种" class="headerlink" title="一致性(级别分为3种)"></a>一致性(级别分为3种)</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214164400.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214164425.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214164506.png"></p><h1 id="端到端（end-to-end）状态一致性"><a href="#端到端（end-to-end）状态一致性" class="headerlink" title="端到端（end-to-end）状态一致性"></a>端到端（end-to-end）状态一致性</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214164634.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214164658.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214164829.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214164852.png"></p><p>如下图所示:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214164911.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214164935.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;当在分布式系统中引入状态时，自然也会导致一系列的问题.如:&lt;/p&gt;
&lt;p&gt;​    一致性问题.&lt;/p&gt;
&lt;p&gt;什么叫一致性问题呢?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;就是不管你这个任务是失败了,还是中间出错了,还是暂停了,还是重复启动了.我要保证所有数据,不管在那种情况下发生,数据都是准确的或者说结果一定是准确的.这就叫所谓的状态一致.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;你要想保证结果准确,首先你就要保证的就是状态一致.所以状态一致就是为了保证最后结果的准确.&lt;/p&gt;
&lt;p&gt;一致性的级别在我们Flink中分为三种.&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink的状态编程和容错机制之算子状态和键控状态</title>
    <link href="http://xubatian.cn/Flink%20%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0:%20Flink%E7%9A%84%E7%8A%B6%E6%80%81%E7%BC%96%E7%A8%8B%E5%92%8C%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6%E4%B9%8B%E7%AE%97%E5%AD%90%E7%8A%B6%E6%80%81%E5%92%8C%E9%94%AE%E6%8E%A7%E7%8A%B6%E6%80%81/"/>
    <id>http://xubatian.cn/Flink%20%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0:%20Flink%E7%9A%84%E7%8A%B6%E6%80%81%E7%BC%96%E7%A8%8B%E5%92%8C%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6%E4%B9%8B%E7%AE%97%E5%AD%90%E7%8A%B6%E6%80%81%E5%92%8C%E9%94%AE%E6%8E%A7%E7%8A%B6%E6%80%81/</id>
    <published>2022-02-14T08:08:20.000Z</published>
    <updated>2022-02-14T08:39:24.246Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flink的状态编程概述"><a href="#Flink的状态编程概述" class="headerlink" title="Flink的状态编程概述"></a>Flink的状态编程概述</h1><p>​    在我们的Flink中,他默认就是有状态的.他和spark的一个本质的区别.有状态,但是他的状态是如何分布的呢?<br>​            <strong>状态分为两类: 算子状态(operator state)和键控状态(keyed state)</strong></p><p>​    算子状态: </p><p>​            是由Flink每一个子任务自己把任务运行过程中的一些业务或者逻辑或者数据,由自己来保存的或者说由自己来管理的,这样的状态称为算子状态.</p><p>​            所以算子状态的作用范围是限定为当前的算子任务的.</p><p>​            什么叫算子任务啊?</p><span id="more"></span><p>​            比如如下图:<img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214160955.png"></p><p>​            如图所示: sum()就是一个算子.这个算子里面在真正运行的过程中要看你这里写的并行度是多少.假设我这里定义的并行度setParallelism(2)是2.如图所示:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214161028.png"></p><p>​        如图所示:图上这句话的意思就是表示,我的这个sum()算子他有两个子任务.因为他有两个并行度.所以他就有两个子任务.这意味着由同一并行任务所处理的所有数据都可以访问到相同的状态，<strong>状态对于同一任务而言是共享的。但是算子状态不能由相同或不同算子的另一个任务访问</strong>。这是什么意思呢?如下图所以:就以下图sum()为例,因为我们知道下图这个sum()一定是用到状态了.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214161059.png"></p><p>​        如图所示: 如果我给这个sum()的并行度设置为2.就意味着这个sum()这个算子有两个subtask即子任务.他是这样子的: 就是他每一个subtask(子任务)都有各自所管理的算子状态.</p><p>​        就算你这两个subtask(子任务)都属于同一个算子.他也是不能够相互访问的.他是不能访问另外一个的.所以所谓的作用范围其实就是限定当前算子的子任务的.如下图所示:</p><p>​        <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214161121.png"></p><p>而我们这样的<strong>算子状态</strong>呢,他有三种基本数据的存储结构.这所说的是算子管理的这些状态有三个存储方式.</p><p>​        第一种是<strong>列表状态(List state)</strong>:就是说我们的算子状态中的数据是使用列表的方式.是存在一个列表里面的.把整个算子的所有数据以一组列表的方式存储起来.</p><p>​        第二种是<strong>联合列表状态(Union List State)</strong>:他也是以列表来存放算子状态的所有的数据.他和前面列表状态的区别是: 在发生故障时,或者从保存点启动应用程序去恢复数据的时候,他的运行代码不同.</p><p>​        第三种是<strong>广播状态(Broadcast state)</strong>:广播状态的意思就是说.我现在有一个算子.这个算子里面呢,他有一些数据或者说有一些逻辑,这个逻辑呢,其他的算子也是会用得到这个逻辑的.或者说这个数据,其他的算子也会用得到.那怎么办呢? 我们前面讲过,算子和算子之间的任务是不能共享的.这个时候呢,我们可以把这个状态存为一种广播状态,存为广播状态的话,这种情况下,他会把状态数据往其他的子任务上去发.这样的话,其他任务上也会有这个所谓的状态数据了.</p><p>​        不管怎么说,这个算子状态一般情况下是不能由程序员来控制的.(这个我们只需要知道就OK了). 而真正能由程序员控制的状态是键控状态(Keyed state),所谓键控状态(Keyed State)就是说我们这个状态只会依赖于数据中的键来进行维护和存储的.这种称之为键控状态.那我们马上就想到,我们sum()这个算子里面存的每一个单词,存的每一个单词里面的数据,实际上就是一种键控状态.如图所示:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214161229.png"></p><p>​        因为他是根据数据流中定义的键来进行维护和管理的,那我们访问的时候也是根据键来进行访问.<strong>Flink为每一个键值维护一个状态实例,让你每次可以修改或者你可以根据键去访问</strong>,<strong>当然删除实际上也是可以删除,删除实际上我们调用一下clear()就可以了.这样的一个键控状态(Keyed state )的数据有点类似于一个分布式的键值对(keyed-value)的map数据结构</strong>.为什么叫分布式的呢?因为我们有很多并行度,每个并行度很有可能一直在多个不同的slot上运行.slot是一个线程.那么这个slot这个线程所在的JVM里面就会保存这个状态信息.保存这个键控状态(Keyed state)的值.键控状态的值是以键值对进行存储的. 你的键就是你数据中定义的那个键. <strong>你这个键在某一个JVM中存放了,他有可能还会在另外一个TaskManager的JVM上再存一个吗? 不会的</strong>.所以,其他的TaskManager上所存放的这些状态是可能有其他的键.那么对整个集群而言就是一个分布式的Key-Value数据结构.而且是惟一的Key.就算是分布式的也是一个唯一的Key.刚才我们已经说了,一个key他只会在某一台TaskManager上的JVM所管理的内存里面存放的.至于这个数据存放,后面会说.</p><p>​        当然,我们的Key-Value(键控状态)也是有所谓的数据结构的.它存储的数据结构呢有这么几种:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214161422.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214161438.png"></p><p>​            <strong>流式计算分为无状态和有状态两种情况</strong>。无状态的计算观察每个独立事件，并根据最后一个事件输出结果。例如，流处理应用程序从传感器接收温度读数，并在温度超过90度时发出警告。有状态的计算则会基于多个事件输出结果。以下是一些例子。</p><p>​            所有类型的窗口。例如，计算过去一小时的平均温度，就是有状态的计算。</p><p>​            所有用于复杂事件处理的状态机。例如，若在一分钟内收到两个相差20度以上的温度读数，则发出警告，这是有状态的计算。</p><p>​            流与流之间的所有关联操作，以及流与静态表或动态表之间的关联操作，都是有状态的计算。</p><p>​            下图展示了无状态流处理和有状态流处理的主要区别。无状态流处理分别接收每条数据记录(图中的黑条)，然后根据最新输入的数据生成输出数据(白条)。有状态流处理会维护状态(根据每条输入记录进行更新)，并基于最新输入的记录和当前的状态值生成输出记录(灰条)。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214161603.png"></p><p>上图中输入数据由黑条表示。无状 态流处理每次只转换一条输入记录，并且仅根据最新的输入记录输出结果(白条)。有状态 流处理维护所有已处理记录的状态值，并根据每条新输入的记录更新状态，因此输出记录(灰条)反映的是综合考虑多个事件之后的结果。</p><p>尽管无状态的计算很重要，但是流处理对有状态的计算更感兴趣。事实上，正确地实现有状态的计算比实现无状态的计算难得多。旧的流处理系统并不支持有状态的计算，而新一代的流处理系统则将状态及其正确性视为重中之重。</p><h1 id="有状态的算子和应用程序"><a href="#有状态的算子和应用程序" class="headerlink" title="有状态的算子和应用程序"></a>有状态的算子和应用程序</h1><p>​        Flink内置的很多算子，数据源source，数据存储sink都是有状态的，流中的数据都是buffer records，会保存一定的元素或者元数据。例如: ProcessWindowFunction会缓存输入流的数据，ProcessFunction会保存设置的定时器信息等等。</p><p>在Flink中，状态始终与特定算子相关联。总的来说，有两种类型的状态：</p><pre><code>     算子状态（operator state）     键控状态（keyed state）</code></pre><h2 id="算子状态（operator-state）"><a href="#算子状态（operator-state）" class="headerlink" title="算子状态（operator state）"></a>算子状态（operator state）</h2><p>​        算子状态的作用范围限定为算子任务。这意味着由同一并行任务所处理的所有数据都可以访问到相同的状态，状态对于同一任务而言是共享的。算子状态不能由相同或不同算子的另一个任务访问。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214161855.png"></p><h2 id="键控状态（keyed-state）"><a href="#键控状态（keyed-state）" class="headerlink" title="键控状态（keyed state）"></a>键控状态（keyed state）</h2><p>​        键控状态是根据输入数据流中定义的键（key）来维护和访问的。Flink为每个键值维护一个状态实例，并将具有相同键的所有数据，都分区到同一个算子任务中，这个任务会维护和处理这个key对应的状态。当任务处理一条数据时，它会自动将状态的访问范围限定为当前数据的key。因此，具有相同key的所有数据都会访问相同的状态。Keyed State很类似于一个分布式的key-value map数据结构，只能用于KeyedStream（keyBy算子处理之后）。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214162021.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214162040.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">val sensorData: DataStream[SensorReading] = ...</span><br><span class="line">val keyedData: KeyedStream[SensorReading, String] = sensorData.keyBy(_.id)</span><br><span class="line">val alerts: DataStream[(String, Double, Double)] = keyedData</span><br><span class="line">.flatMap(new TemperatureAlertFunction(1.7))</span><br><span class="line">class TemperatureAlertFunction(val threshold: Double) extends RichFlatMapFunction[SensorReading, (String, Double, Double)] &#123;</span><br><span class="line">//定义一个值状态</span><br><span class="line">private var lastTempState: ValueState[Double] = _</span><br><span class="line">override def open(parameters: Configuration): Unit = &#123;</span><br><span class="line">val lastTempDescriptor = new ValueStateDescriptor[Double](&quot;lastTemp&quot;, classOf[Double])</span><br><span class="line">lastTempState = getRuntimeContext.getState[Double](lastTempDescriptor)</span><br><span class="line">&#125;</span><br><span class="line">override def flatMap(reading: SensorReading,</span><br><span class="line">out: Collector[(String, Double, Double)]): Unit = &#123;</span><br><span class="line">val lastTemp = lastTempState.value()</span><br><span class="line">val tempDiff = (reading.temperature - lastTemp).abs</span><br><span class="line">if (tempDiff &gt; threshold) &#123;</span><br><span class="line">out.collect((reading.id, reading.temperature, tempDiff))</span><br><span class="line">&#125;</span><br><span class="line">this.lastTempState.update(reading.temperature)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过RuntimeContext注册StateDescriptor。StateDescriptor以状态state的名字和存储的数据类型为参数。</p><p>在open()方法中创建state变量。注意复习之前的RichFunction相关知识。</p><p>接下来我们使用了FlatMap with keyed ValueState的快捷方式flatMapWithState实现以上需求。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">val alerts: DataStream[(String, Double, Double)] = keyedSensorData</span><br><span class="line">  .flatMapWithState[(String, Double, Double), Double] &#123;</span><br><span class="line">    case (in: SensorReading, None) =&gt;</span><br><span class="line">      (List.empty, Some(in.temperature))</span><br><span class="line">    case (r: SensorReading, lastTemp: Some[Double]) =&gt;</span><br><span class="line">      val tempDiff = (r.temperature - lastTemp.get).abs</span><br><span class="line">      if (tempDiff &gt; 1.7) &#123;</span><br><span class="line">        (List((r.id, r.temperature, tempDiff)), Some(r.temperature))</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        (List.empty, Some(r.temperature))</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Flink的状态编程概述&quot;&gt;&lt;a href=&quot;#Flink的状态编程概述&quot; class=&quot;headerlink&quot; title=&quot;Flink的状态编程概述&quot;&gt;&lt;/a&gt;Flink的状态编程概述&lt;/h1&gt;&lt;p&gt;​    在我们的Flink中,他默认就是有状态的.他和spark的一个本质的区别.有状态,但是他的状态是如何分布的呢?&lt;br&gt;​            &lt;strong&gt;状态分为两类: 算子状态(operator state)和键控状态(keyed state)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;​    算子状态: &lt;/p&gt;
&lt;p&gt;​            是由Flink每一个子任务自己把任务运行过程中的一些业务或者逻辑或者数据,由自己来保存的或者说由自己来管理的,这样的状态称为算子状态.&lt;/p&gt;
&lt;p&gt;​            所以算子状态的作用范围是限定为当前的算子任务的.&lt;/p&gt;
&lt;p&gt;​            什么叫算子任务啊?&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink的ProcessFunction API（底层API）</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E7%9A%84ProcessFunction-API%EF%BC%88%E5%BA%95%E5%B1%82API%EF%BC%89/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E7%9A%84ProcessFunction-API%EF%BC%88%E5%BA%95%E5%B1%82API%EF%BC%89/</id>
    <published>2022-02-14T07:44:22.000Z</published>
    <updated>2022-02-14T08:05:53.264Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flink底层API概述"><a href="#Flink底层API概述" class="headerlink" title="Flink底层API概述"></a>Flink底层API概述</h1><p>ProcessFunction API是属于Flink三层API中最底层的一层API.最底层的API意味着什么呢?</p><p>​        顾名思义就是我们可以做任何你想要做的任何事情.你可以理解为底层的API我们想要处理一些细腻化的操作,或者要处理一些特殊的业务.我如果使用高级的API ,DataStream API已经搞定不了的情况下.最后呢,我们就可以是用ProcessFunction API. ProcessFunction API是属于我们Flink里面最底层的转换算子.在这个最底层的转换算子中.我们可以有很多的工作可以做.或者有很多的功能可以完成.可以访问时间戳.那我们访问的时间戳是什么时间戳呢?是处理的时间戳.也可以是当前进来的那条数据的时间戳.那么当前进来的时间戳和处理的时间戳我们不是可以使用system.currenttimemillis()吗?但是这个时候我们使用这个是不准确的.<strong>你要想访问准确的时间的话.我们就可以使用底层的API.他就给我们提供了一个方法.我们可以访问watermark,还可以查看当前的水位线.水位线实际上是这样的,就是他每个两百毫秒调用一次.是需要更新这个水位线的.因为水位线有两种.</strong></p><p>一种是周期性的水位线.<br>还有一种是间断性的水位线.</p><p>周期性的水位线就是说我每隔多长时间来更新一下或者设置一下这个新的水位线.就是这个意思.那么当前<strong>最新的水位线是多少我们都可以通过在底层API中去拿到</strong>.甚至我们还可以<strong>注册定时的事件.</strong></p><p>什么叫注册定时的事件呢?</p><p>​    比如说:我们到达某一个条件之后我想触发一个事件.触发一个新的事件.这个事件我规定你在三秒钟之后你给我执行.那么在三秒钟还没到的时候我就有必要把这个事件注册一下.注册之后,从现在开始到三秒钟之后他会自动触发.所以你可以理解为定义,我们定义一个马上过一段时间要触发的一个事件.</p><p>​        还可以输出一个特定的一些事件.比如说超时的一些事件.什么意思呢?比如说我们现在正在处理数据.假设处理数据的时候我们发现可能某一个条件还没成熟.那么没关系,我们可以在几秒钟之后设置一个超时时间.在几秒钟之后我们再进行处理.这就是所谓的超时时间.</p><p><strong>实际上几秒钟之后再处理的话,也是需要注册一个定时事件的</strong>.因为你要注册一个处理的时间,多长时间之后再进行处理.这些都是原来我们的API所无法做到的.那我们就可以通过ProcessFunction API这个底层的API来进行处理.</p><span id="more"></span><p><strong>Flink给我们提供了8中ProcessFunction API.</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214154851.png"></p><p>这些窗口函数说白了就是一个个的对象,看着像函数,实际上他是对象.或者说就是一个类.那我们要用的话是声明一个匿名方法吗?不是的,是声明一个匿名函数吗?不是的.而是你需要写一个类去继承这8个ProcessFunction.他是我们真正的窗口函数类.如果你要把前面的知识点串联起来的话.</p><p>Flink里面我们提供了三种函数类</p><p>匿名函数不好</p><p>一个是普通的函数类.</p><p>第二个是富函数类.</p><p>第三个是底层处理的函数类.</p><p>所以我们有三种函数类.这块儿函数类有一个特点,他不是说你要声明写一个匿名函数就可以了.而是你必须要写一个类去继承他.</p><p>这里最常用的是ProcessFunction和KeyedProcessFunction.</p><p>下面以KeyedProcessFunction举例子.</p><p>​        我们之前学习的转换算子是无法访问事件的时间戳信息和水位线信息的。而这在一些应用场景下，极为重要。例如MapFunction这样的map转换算子就无法访问时间戳或者当前事件的事件时间。<br>​        基于此，DataStream API提供了一系列的Low-Level(底层的)转换算子。可以访问时间戳、watermark以及注册定时事件。还可以输出特定的一些事件，例如超时事件等。Process Function用来构建事件驱动的应用以及实现自定义的业务逻辑(使用之前的window函数和转换算子无法实现)。例如，Flink SQL就是使用Process Function实现的。<br>Flink提供了8个Process Function：<br>ProcessFunction<br>KeyedProcessFunction<br>CoProcessFunction<br>ProcessJoinFunction<br>BroadcastProcessFunction<br>KeyedBroadcastProcessFunction<br>ProcessWindowFunction<br>ProcessAllWindowFunction</p><h1 id="KeyedProcessFunction"><a href="#KeyedProcessFunction" class="headerlink" title="KeyedProcessFunction"></a>KeyedProcessFunction</h1><p><strong>这里以KeyedProcessFunction举例,因为他用的最多.上面的这8个ProcessFunction API即底层API,都是按照下面连个案例去操作的.</strong></p><p>​        以下例子说明,就是你可以通过这个Process Function中,拿到你想要拿到的任何东西,包括你对侧输出流的管理,包括你对当前的运行时间还有当前的watermark,当前的TimerService(时间服务),TimerService里面又可以注册一个触发器,并且还可以删除一个触发器.总之一句话,就是底层的ProcessFunction提供了所有你想要拿到的东西.都可以在底层的ProcessFunction API里面拿</p><p>​        KeyedProcessFunction是专门用来操作KeyedStream的.只会操作KeyedStream.</p><p>​        KeyedProcessFunction他会处理我们流的每一个元素.可以输出多个元素.也可以输出0个,也可以输出1个.这个我们之前说的DataStream API里面的处理函数或者转换函数不一样的.比如我们在DataStream中说的FlatMap这个处理函数(注意:处理函数就是所谓的转换函数),这里FlatMap他是把一条数据变成n条数据,即把一个元素转换成多个元素.还有一个,比如说map算子,map算子就是把一条数据通过map输出一条数据.我们可不可以用map算子来把一条数输出0条数据呢?(0个元素就代表不输出的意思.) 请问可以这样做吗?不可以的.那为什么我们后面不能输出null呢?输出null不就是代表不输出了呢?因为null也是等于0的呀!因为我们的map是要申明类型的,一个输入类型,一个输出类型.那我们写代码的时候输出一个null不就行了嘛?你什么代码也不写,这是不行的,因为这个代码是必须有返回值的.你这个方法声明的返回值你却没有返回任何东西的话,你的语法就通不过,然后他的编译也通不过.那什么都不输出的话编译不通过,那我就输出个null不行吗?不行,输出null的话,在执行过程中是会报错的.为什么?因为他要把输出的数据还要反序列化一下.在我们Flink中,流的处理,尤其在后面写到Sink里面他要进行序列化的.也就是说,他会造成后面的算子报错的现象.可能你这个map算子不会报错,但是会造成后面的算子报错.因为后面的算子他要反序列化你这个map算子输出的对象.那这个时候因为你为null,那他就会报一个空指针异常的错误.所以说,他做不到.</p><p>​        但是我们当前的底层API即KeyedProcessFunction这个底层API他就可以做到,他可以不输出.因为它里面的方法是没有返回值的.他既然没有返回值,我不输出,我不写代码就可以了.他可以输出1个元素也可以输出0个元素.他是非常灵活的.这就是所谓的底层API.当然,灵活归灵活,只不过代码写的稍微多一点.以前是调一个函数就ok了.现在我们用底层API我们还得写一个类.去继承这个父类.然后重写父类的所有抽象方法.而且我们所有的ProcessFunction都继承自RichFunction接口.RichFunction接口的特点是,有生命周期的管理.你可以知道我们这个subtask,就是我们这算子所对应的subtask在什么时候初始化,什么时候关闭.什么时候初始化的回调方法就是Open()方法.什么时候关闭的回调方法就是close()方法.这就是所谓的声明周期.还可以的到当前运行的上下文环境getRuntimeContext().这个上下文环境以后是要用的.为什么呢?因为我们前面说,我们的底层API可以访问时间,可以访问watermark(水位线),可以注册事件.那么可以通过谁来注册事件呢?就是通过我们运行时的上下环境,就是getRuntimeContext()来进行注册事件.</p><p>​        归纳总结: 我们整个Flink学了三种函数类.普通的函数类.富函数类和底层处理的函数类.那我们是不是可以认为底层处理的函数类可以做前面两种所有能做的事情.除此之外,我们的底层处理的函数还能做额外的一些事情,比如说注册新的事件,访问时间戳等等.或者说底层处理的函数类,就是我们所谓的写代码过程中发现一个业务数据做不到的我们就可以放大招了,使用我们底层处理的函数类来完成我们想要的需求.那如果底层处理的函数类都搞定不了的话,那就是我们技术选型有问题了.我们就不该选用Flink了.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214155410.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214155648.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214160025.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214160123.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214160218.png"></p><h1 id="时间服务-TimerService-和-定时器（Timers）"><a href="#时间服务-TimerService-和-定时器（Timers）" class="headerlink" title="时间服务 (TimerService )和 定时器（Timers）"></a>时间服务 (TimerService )和 定时器（Timers）</h1><p>Context和OnTimerContext所持有的TimerService对象拥有以下方法:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214160312.png"></p><p>当定时器timer触发时，会执行回调函数onTimer()。注意定时器timer只能在keyed streams上面使用。</p><p>下面举个例子说明KeyedProcessFunction如何操作KeyedStream。</p><p>需求：监控温度传感器的温度值，如果温度值在一秒钟之内(processing time)连续上升，则报警。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val warnings = readings</span><br><span class="line">.keyBy(_.id)</span><br><span class="line">.process(new TempIncreaseAlertFunction)</span><br></pre></td></tr></table></figure><p>看一下TempIncreaseAlertFunction如何实现, 程序中使用了ValueState这样一个状态变量。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">class TempIncreaseAlertFunction extends KeyedProcessFunction[String, SensorReading, String] &#123;</span><br><span class="line">  // 保存上一个传感器温度值</span><br><span class="line">  lazy val lastTemp: ValueState[Double] = getRuntimeContext.getState(</span><br><span class="line">    new ValueStateDescriptor[Double](&quot;lastTemp&quot;, Types.of[Double])</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  // 保存注册的定时器的时间戳</span><br><span class="line">  lazy val currentTimer: ValueState[Long] = getRuntimeContext.getState(</span><br><span class="line">    new ValueStateDescriptor[Long](&quot;timer&quot;, Types.of[Long])</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  override def processElement(r: SensorReading,</span><br><span class="line">                          ctx: KeyedProcessFunction[String, SensorReading, String]#Context,</span><br><span class="line">                          out: Collector[String]): Unit = &#123;</span><br><span class="line">    // 取出上一次的温度</span><br><span class="line">    val prevTemp = lastTemp.value()</span><br><span class="line">    // 将当前温度更新到上一次的温度这个变量中</span><br><span class="line">    lastTemp.update(r.temperature)</span><br><span class="line"></span><br><span class="line">    val curTimerTimestamp = currentTimer.value()</span><br><span class="line">    if (prevTemp == 0.0 || r.temperature &lt; prevTemp) &#123;</span><br><span class="line">      // 温度下降或者是第一个温度值，删除定时器</span><br><span class="line">      ctx.timerService().deleteProcessingTimeTimer(curTimerTimestamp)</span><br><span class="line">      // 清空状态变量</span><br><span class="line">      currentTimer.clear()</span><br><span class="line">    &#125; else if (r.temperature &gt; prevTemp &amp;&amp; curTimerTimestamp == 0) &#123;</span><br><span class="line">      // 温度上升且我们并没有设置定时器</span><br><span class="line">      val timerTs = ctx.timerService().currentProcessingTime() + 1000</span><br><span class="line">      ctx.timerService().registerProcessingTimeTimer(timerTs)</span><br><span class="line"></span><br><span class="line">      currentTimer.update(timerTs)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def onTimer(ts: Long,</span><br><span class="line">                    ctx: KeyedProcessFunction[String, SensorReading, String]#OnTimerContext,</span><br><span class="line">                    out: Collector[String]): Unit = &#123;</span><br><span class="line">    out.collect(&quot;传感器id为: &quot; + ctx.getCurrentKey + &quot;的传感器温度值已经连续1s上升了。&quot;)</span><br><span class="line">    currentTimer.clear()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="侧输出流（SideOutput）【注意：测输出流是用来替代Split算子的。Split算子已经过时了。】"><a href="#侧输出流（SideOutput）【注意：测输出流是用来替代Split算子的。Split算子已经过时了。】" class="headerlink" title="侧输出流（SideOutput）【注意：测输出流是用来替代Split算子的。Split算子已经过时了。】"></a>侧输出流（SideOutput）【注意：测输出流是用来替代Split算子的。Split算子已经过时了。】</h1><p>​        侧输出流是用来替代Split算子的,split算子实际上已经过时了.侧输出流的意思就是说,我们把一条流分成多个流.这个侧输出流可以这样子的.就是一条主流两个测流.或者一条主流,三个测流.这就根据你自己的意思来决定.但是有一个特点就是你的类型必须相同.那么,如果你输出侧输出流的话.你必须需要用到我们的底层,也需要用到底层的function才行的.所以这个侧输出流他也是属于我们的ProcessFunction API里面的.就是底层API里面的.<br>​        由于我们可以输出多条测流,那么每一个测流呢,我们可以定义一个OutputTag[X]对象来区分.其中的X就是他每条测流里面的每个元素的类型.所以需要在前面就得定义一个OutputTag[X].有多少个侧输出流,你就需要定义多少个OutputTag[X].实际上你可以理解为就是我当前测流的一个标记.或者我当前测流的一个标签.到时候你去拿我这个测流的时候,你也是根据这个标签去拿.</p><p>​        大部分的DataStream API的算子的输出是单一输出，也就是某种数据类型的流。除了split算子，可以将一条流分成多条流，这些流的数据类型也都相同。process function的side outputs功能可以产生多条流，并且这些流的数据类型可以不一样。一个side output可以定义为OutputTag[X]对象，X是输出流的数据类型。process function可以通过Context对象发射一个事件到一个或者多个side outputs。</p><p>下面是一个示例程序:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val monitoredReadings: DataStream[SensorReading] = readings</span><br><span class="line">  .process(new FreezingMonitor)</span><br><span class="line"></span><br><span class="line">monitoredReadings</span><br><span class="line">  .getSideOutput(new OutputTag[String](&quot;freezing-alarms&quot;))</span><br><span class="line">  .print()</span><br><span class="line"></span><br><span class="line">readings.print()</span><br></pre></td></tr></table></figure><p>接下来我们实现FreezingMonitor函数，用来监控传感器温度值，将温度值低于32F的温度输出到side output。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class FreezingMonitor extends ProcessFunction[SensorReading, SensorReading] &#123;</span><br><span class="line">  // 定义一个侧输出标签</span><br><span class="line">  lazy val freezingAlarmOutput: OutputTag[String] =</span><br><span class="line">    new OutputTag[String](&quot;freezing-alarms&quot;)</span><br><span class="line"></span><br><span class="line">  override def processElement(r: SensorReading,</span><br><span class="line">                              ctx: ProcessFunction[SensorReading, SensorReading]#Context,</span><br><span class="line">                              out: Collector[SensorReading]): Unit = &#123;</span><br><span class="line">    // 温度在32F以下时，输出警告信息</span><br><span class="line">    if (r.temperature &lt; 32.0) &#123;</span><br><span class="line">      ctx.output(freezingAlarmOutput, s&quot;Freezing Alarm for $&#123;r.id&#125;&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    // 所有数据直接常规输出到主流</span><br><span class="line">    out.collect(r)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="CoProcessFunction-这是将两条流汇成一个流的"><a href="#CoProcessFunction-这是将两条流汇成一个流的" class="headerlink" title="CoProcessFunction(这是将两条流汇成一个流的)"></a>CoProcessFunction(这是将两条流汇成一个流的)</h1><p>CoProcessFunction,这个是将两个流汇成一个流的.CoProcessFunction和我们之前讲的两个底层的函数不一样.它里面有两个需要实现的方法.如果你需要把两条流汇成一个流的话.你就要分别处理,所以要<strong>重写CoProcessFunction里面的两个方法:ProcessElement1()和ProcessElement2().那如果我想要将他汇成一个流的话,我只需要在这两个方法中用同样的代码去删除就可以了.</strong></p><p>对于两条输入流，DataStream API提供了CoProcessFunction这样的low-level操作。CoProcessFunction提供了操作每一个输入流的方法: processElement1()和processElement2()。</p><p>类似于ProcessFunction，这两种方法都通过Context对象来调用。这个Context对象可以访问事件数据，定时器时间戳，TimerService，以及side outputs。CoProcessFunction也提供了onTimer()回调函数。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Flink底层API概述&quot;&gt;&lt;a href=&quot;#Flink底层API概述&quot; class=&quot;headerlink&quot; title=&quot;Flink底层API概述&quot;&gt;&lt;/a&gt;Flink底层API概述&lt;/h1&gt;&lt;p&gt;ProcessFunction API是属于Flink三层API中最底层的一层API.最底层的API意味着什么呢?&lt;/p&gt;
&lt;p&gt;​        顾名思义就是我们可以做任何你想要做的任何事情.你可以理解为底层的API我们想要处理一些细腻化的操作,或者要处理一些特殊的业务.我如果使用高级的API ,DataStream API已经搞定不了的情况下.最后呢,我们就可以是用ProcessFunction API. ProcessFunction API是属于我们Flink里面最底层的转换算子.在这个最底层的转换算子中.我们可以有很多的工作可以做.或者有很多的功能可以完成.可以访问时间戳.那我们访问的时间戳是什么时间戳呢?是处理的时间戳.也可以是当前进来的那条数据的时间戳.那么当前进来的时间戳和处理的时间戳我们不是可以使用system.currenttimemillis()吗?但是这个时候我们使用这个是不准确的.&lt;strong&gt;你要想访问准确的时间的话.我们就可以使用底层的API.他就给我们提供了一个方法.我们可以访问watermark,还可以查看当前的水位线.水位线实际上是这样的,就是他每个两百毫秒调用一次.是需要更新这个水位线的.因为水位线有两种.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一种是周期性的水位线.&lt;br&gt;还有一种是间断性的水位线.&lt;/p&gt;
&lt;p&gt;周期性的水位线就是说我每隔多长时间来更新一下或者设置一下这个新的水位线.就是这个意思.那么当前&lt;strong&gt;最新的水位线是多少我们都可以通过在底层API中去拿到&lt;/strong&gt;.甚至我们还可以&lt;strong&gt;注册定时的事件.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;什么叫注册定时的事件呢?&lt;/p&gt;
&lt;p&gt;​    比如说:我们到达某一个条件之后我想触发一个事件.触发一个新的事件.这个事件我规定你在三秒钟之后你给我执行.那么在三秒钟还没到的时候我就有必要把这个事件注册一下.注册之后,从现在开始到三秒钟之后他会自动触发.所以你可以理解为定义,我们定义一个马上过一段时间要触发的一个事件.&lt;/p&gt;
&lt;p&gt;​        还可以输出一个特定的一些事件.比如说超时的一些事件.什么意思呢?比如说我们现在正在处理数据.假设处理数据的时候我们发现可能某一个条件还没成熟.那么没关系,我们可以在几秒钟之后设置一个超时时间.在几秒钟之后我们再进行处理.这就是所谓的超时时间.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;实际上几秒钟之后再处理的话,也是需要注册一个定时事件的&lt;/strong&gt;.因为你要注册一个处理的时间,多长时间之后再进行处理.这些都是原来我们的API所无法做到的.那我们就可以通过ProcessFunction API这个底层的API来进行处理.&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink 的时间语义与Wartermark(水位线)机制</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink-%E7%9A%84%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E4%B8%8EWartermark-%E6%B0%B4%E4%BD%8D%E7%BA%BF-%E6%9C%BA%E5%88%B6/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink-%E7%9A%84%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E4%B8%8EWartermark-%E6%B0%B4%E4%BD%8D%E7%BA%BF-%E6%9C%BA%E5%88%B6/</id>
    <published>2022-02-14T07:05:57.000Z</published>
    <updated>2022-02-14T07:33:17.678Z</updated>
    
    <content type="html"><![CDATA[<h1 id="时间语义与Wartermark-水位线-阐述"><a href="#时间语义与Wartermark-水位线-阐述" class="headerlink" title="时间语义与Wartermark(水位线)阐述:"></a>时间语义与Wartermark(水位线)阐述:</h1><p>用到了时间语义和watermark,所以Flink和sparkStreaming是有一定的区别的.</p><p><strong>Flink中的时间语义有三种:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Event Time(事件触发的时间或者是事件创建的时间),</span><br><span class="line"></span><br><span class="line">Ingestion Time(进入Flink的时间),</span><br><span class="line"></span><br><span class="line">Processing Time(Flink在处理这条数据的时间),</span><br></pre></td></tr></table></figure><span id="more"></span>        <p>​        在生产环境中,大多数我们都关心Event Time,我们可能会把Event Time来作为我们开窗的时间参数.就是说我们任何一个开窗都会有一个开窗时间,我们的开窗时间是基于这三个时间的哪一种呢?他是基于Event Time这一种的.但是默认情况下,他的时间语义是基于Processing Time这种的.这是所谓的Flink中的时间语义.所以,如果我们不使用Flink默认的时间语义的话,而是使用Event Time的话,我首先还得想清楚,如何去引入这个Event Time.引入Event Time的两行代码,第一行时先声明,告诉我们的Flink,我不用processing Time,我想要使用Event Time. 如下所示:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">// 从调用时刻开始给env创建的每一个stream追加时间特征</span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br></pre></td></tr></table></figure><p>​        第二行代码就是说,到底我们的每一条数据中的哪一个属性,数据中的哪一个小段,数据中的哪一个字段,他是属于Event Time的,或者说,具体的Event Time的值是多少,你得告诉我.所以第二行代码,如上图所示.</p><p>​        但是具体的Event Time的值是多少呢？ <strong>这个时候我还要你考虑一下,是否我们的数据严格按照我的Event Time的顺序进来呢,还是按照我的Event Time无序(乱序)的进来.所以我们必须考虑这两种情况</strong>.我们的开窗函数事实上在以后的业务中很有可能会考虑这两种情况.无<strong>论你是按照Event Time还是按照Processing Time,我们很有可能都需要考虑这两种情况</strong>.第一种情况就是我的数据是严格按照某一个时间语义升序进来的.还有一种情况就是按照某一个时间语义乱序进来的.升序进来的话就比较简单,但是如果是乱序进来的话.就需要考虑一个延迟的问题了.为了保证我们延迟的时候,可以再固定的时间触发.或者说在某一个条件下去触发,那我们就需要引入一个叫watermark(水位线)</p><p>​        <strong>watermark的作用其实只有两个作用: 一个是规定我们延迟之后(因为我们每一个窗口都要做延迟,为何要延迟?因为数据是乱序的)我到底在什么条件下去触发呢?那是由watermark来决定的.第二个作用是,watermark还可以确定,我们在触发的时候有哪些时间的数据已经进到我们的窗口里面来了.这就是watermark的两个具体的作用.</strong> </p><h1 id="Flink中的时间语义"><a href="#Flink中的时间语义" class="headerlink" title="Flink中的时间语义"></a>Flink中的时间语义</h1><p>在Flink的流式处理中，会涉及到时间的不同概念，如下图所示(Flink时间概念)：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214150638.png"></p><p><strong>Event Time</strong>：是事件创建的时间。它通常由事件中的时间戳描述，例如采集的日志数据中，每一条日志都会记录自己的生成时间，Flink通过时间戳分配器访问事件时间戳。</p><p><strong>Ingestion Time</strong>：是数据进入Flink的时间。</p><p><strong>Processing Time</strong>：是每一个执行基于时间操作的算子的本地系统时间，与机器相关，默认的时间属性就是Processing Time。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214151213.png"></p><p>一个例子——电影《星球大战》：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214151321.png"></p><p>例如，一条日志进入Flink的时间为2017-11-12 10:00:00.123，到达Window的系统时间为2017-11-12 10:00:01.234，日志的内容如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2017-11-02 18:37:15.624 INFO Fail over to rm2</span><br></pre></td></tr></table></figure><p>对于业务来说，要统计1min内的故障日志个数，哪个时间是最有意义的？—— eventTime，因为我们要根据日志的生成时间进行统计。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214151401.png"></p><h1 id="EventTime的引入"><a href="#EventTime的引入" class="headerlink" title="EventTime的引入"></a>EventTime的引入</h1><p>​        在Flink的流式处理中，绝大部分的业务都会使用eventTime，一般只在eventTime无法使用时，才会被迫使用ProcessingTime或者IngestionTime。<br>​        如果要使用EventTime，那么需要引入EventTime的时间属性，引入方式如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">// 从调用时刻开始给env创建的每一个stream追加时间特征</span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line"></span><br><span class="line">解析:</span><br><span class="line">其实这TimeCharacteristic.EventTime整个是一个常量,里面有三种常量对应着三种时间.</span><br><span class="line">这句话的意思就是说,未来我要采用eventTime作为我真正要计算的时间.但是问题是你的具体的某一条数据中(这里,假设我们定义的sessionRead使我们的某一条具体数据),但是问题是,我这个eventTime在我sessionRead这条数据中,是哪个属性才是eventTime呢?你还没告诉我,所以这一行代码很明显还是不够的,我们肯定还有明确的代码,告诉我们的Flink, 到底具体的时间是哪一个时间.这里只是告诉你,你如果要用,你必须首先申明.</span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line">这行代码你可以理解为,申明告诉我们的Flink,我未来要使用不是Processing Time而是EventTime.至于EventTime中的哪一个,在后面慢慢详解.</span><br></pre></td></tr></table></figure><h1 id="Watermark"><a href="#Watermark" class="headerlink" title="Watermark"></a>Watermark</h1><p>​        Watermark翻译成中文叫做水位线.他就是用来规定,什么样的数据算是迟到的数据.他就是定义一个标准,低于这个标准了,我们就认为是迟到的数据.这个就是所谓的水位线.这个水位线一般来说就是处理乱序的数据,这里我们得清楚,是基于哪一个时间乱序.在第七章我们探讨的是基于Event Time乱序.因为处理时间是不肯定乱序的.因为我是实时处理,所以我们严格按照时间的顺序来进行处理的.但是我进来的数据,他是有可能根据我们事件产生的时间产生一个乱序的行为.这个乱序我们如何处理呢?这就需要定义一个watermark.不但要定义一个Watermark,还要让这个watermark实时的,或者隔一段时间或者达到什么样的条件.他的水位线要跟着变化.但是一般情况下这个水位线是单调递增还是单调递减呢?还是保持不变呢?</p><p>​        你想一下,数据源源不断的从一个管道里面进来,这个数据总体上我们一般认为他是有序的.他不可能总体上无序,比如说我要处理1999年到2020年的数据,最后发现,1999年的数据最后再来.等我全部处理完成之后再来.那这个总体上就无序了.那如果我要处理1999年到2020年数据,这数据有一点乱序的,什么叫有一点乱序呀?就是1999年的某一天,假设是1月1号的某一天.有可能是10:05分的数据来了,05分的数据来了之后,又来了03分的数据.那发现这个数据就不对了,顺序变了.这种情况不止一处的话,说明总体上还是有序的,细节上还是不对的.</p><p>​        但是我的数据总体上完全乱序的,这个就不在我们的watermark的处理范围之内.</p><p>​        Watermark的具体的机制是:<strong>Watermark可以理解成一个延迟触发机制，我们可以设置Watermark的延时时长t，每次系统会校验已经到达的数据中最大的maxEventTime，然后认定eventTime小于maxEventTime - t的所有数据都已经到达，如果有窗口的停止时间等于maxEventTime – t，那么这个窗口被触发执行。</strong></p><p>​        我们设置watermark的一个延时的时长为T,这个T实际上是我们根据数据的特点来设置的.什么叫数据的特点呢?就是数据是进入窗口是乱序进来的,你这数据乱到什么程度.根据你数据乱到什么程度来确定我的T该设置为多少.比如说:本来按道理说是进来的是9:05分的数据了,可是你还进来的是8:05分的数据.这就很明显延迟了1个小时.那么,既然我的数据延迟了一个小时了,所以我的T也应该是要延迟一个小时.这就是所谓的根据你数据的乱序程度.</p><p>​        但是如果我数据应该进来了9:05分,可是呢.我进来的确是9:00的数据,那只是延迟5秒就可以了.所以这个T就要设置为5秒.实际上T是根据你数据乱序的特点来确定我T设置为多少.当你确定好T之后,我们的系统会校验已经到达的数据中最大的Event Time,然后认定我们的eventTime是否小于maxEventTime - t,如果我们的Evnet Time小于我们的maxEventTime-t,那这就说明我们所有的数据都已经到达.这个时候我们就要开始触发了.触发的时候需要判断我们窗口的停止时间是不是小于等于maxEventTime-t,窗口的停止时间就是窗口的结束时间.</p><p>​        我们窗口的起始时间和结束时间是有一个时间范围的.只要是范围的话我们就要考虑一个问题.就是到底是左闭右开,还是左开右闭,或者左闭右也是闭合的,或者包头不包尾的呢?</p><p>​        其实我们的时间范围就是包头不包尾的.啥意思呢?就是包括了起始时间,不包括结束时间.比如说的起始时间是9:00分到9:05分,那结束时间就是9:05分,比如说我现在有一条数据,他就是属于9:00的,那么请问这条数据是不是属于我当前这个窗口的呢?是的.那如果我这条数据是9:05分的数据呢?那他就不属于我们当前这个窗口的了,因为包头不包尾.他应该是属于下一个窗口的.所以我的窗口的起始时间小于等于maxEventTime-t那么我这个窗口就会被触发.</p><p>​        这样的话就有一个watermark这么一个机制来控制我的延迟触发的问题. </p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>​        我们知道，流处理从事件产生，到流经source，再到operator，中间是有一个过程和时间的，虽然大部分情况下，流到operator的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络、分布式等原因，导致乱序的产生，所谓乱序，就是指Flink接收到的事件的先后顺序不是严格按照事件的Event Time顺序排列的。</p><h3 id="数据的乱序图-示"><a href="#数据的乱序图-示" class="headerlink" title="数据的乱序图 示:"></a><strong>数据的乱序图 示:</strong></h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214151816.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那么此时出现一个问题，一旦出现乱序，如果只根据eventTime决定window的运行，我们不能明确数据是否全部到位，但又不能无限期的等下去，此时必须要有个机制来保证一个特定的时间后，必须触发window去进行计算了，这个特别的机制，就是Watermark。</span><br><span class="line">Watermark是一种衡量Event Time进展的机制。</span><br><span class="line">Watermark是用于处理乱序事件的，而正确的处理乱序事件，通常用Watermark机制结合window来实现。</span><br><span class="line">数据流中的Watermark用于表示timestamp小于Watermark的数据，都已经到达了，因此，window的执行也是由Watermark触发的。</span><br><span class="line">Watermark可以理解成一个延迟触发机制，我们可以设置Watermark的延时时长t，每次系统会校验已经到达的数据中最大的maxEventTime，然后认定eventTime小于maxEventTime - t的所有数据都已经到达，如果有窗口的停止时间等于maxEventTime – t，那么这个窗口被触发执行。</span><br></pre></td></tr></table></figure><h3 id="有序流的Watermarker如下图所示："><a href="#有序流的Watermarker如下图所示：" class="headerlink" title="有序流的Watermarker如下图所示："></a>有序流的Watermarker如下图所示：</h3><p><strong>Watermark设置为0</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214151946.png"></p><h3 id="乱序流的Watermarker如下图所示："><a href="#乱序流的Watermarker如下图所示：" class="headerlink" title="乱序流的Watermarker如下图所示："></a>乱序流的Watermarker如下图所示：</h3><p><strong>Watermark设置为2</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214152129.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214152151.png"></p><p>​        当Flink接收到数据时，会按照一定的规则去生成Watermark，这条Watermark就等于当前所有到达数据中的maxEventTime - 延迟时长，也就是说，Watermark是由数据携带的，一旦数据携带的Watermark比当前未触发的窗口的停止时间要晚，那么就会触发相应窗口的执行。由于Watermark是由数据携带的，因此，如果运行过程中无法获取新的数据，那么没有被触发的窗口将永远都不被触发。</p><p>​        上图中，我们设置的允许最大延迟到达时间为2s，所以时间戳为7s的事件对应的Watermark是5s，时间戳为12s的事件的Watermark是10s，如果我们的窗口1是1s<del>5s，窗口2是6s</del>10s，那么时间戳为7s的事件到达时的Watermarker恰好触发窗口1，时间戳为12s的事件到达时的Watermark恰好触发窗口2。</p><pre><code>     Watermark 就是触发前一窗口的“关窗时间”，一旦触发关门那么以当前时刻为准在窗口范围内的所有所有数据都会收入窗中。</code></pre><p>只要没有达到水位那么不管现实中的时间推进了多久都不会触发关窗。</p><h2 id="Watermark的引入"><a href="#Watermark的引入" class="headerlink" title="Watermark的引入"></a>Watermark的引入</h2><h3 id="水位线有两种-周期性水位线和间断性的水位线"><a href="#水位线有两种-周期性水位线和间断性的水位线" class="headerlink" title="水位线有两种:周期性水位线和间断性的水位线"></a><strong>水位线有两种:周期性水位线和间断性的水位线</strong></h3><p>watermark的引入很简单，对于乱序数据，最常见的引用方式如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">//上面只说了要引入EventTime,但是没说到底什么时候引入eventTime. 这里就告诉你到底什么时候引入Event</span><br><span class="line">dataStream.assignTimestampsAndWatermarks( new BoundedOutOfOrdernessTimestampExtractor[SensorReading](Time.milliseconds(1000)) &#123;</span><br><span class="line">  override def extractTimestamp(element: SensorReading): Long = &#123;</span><br><span class="line">    element.timestamp * 1000    //EventTime一定要设置毫秒为单位的</span><br><span class="line">  &#125;</span><br><span class="line">&#125; )</span><br></pre></td></tr></table></figure><p>​        Event Time的使用一定要<strong>指定数据源中的时间戳</strong>。否则程序无法知道事件的事件时间是什么(数据源里的数据没有时间戳的话，就只能使用Processing Time了)。</p><p>​        我们看到上面的例子中创建了一个看起来有点复杂的类，这个类实现的其实就是分配时间戳的接口。Flink暴露了TimestampAssigner接口供我们实现，使我们可以自定义如何从事件数据中抽取时间戳。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">// 从调用时刻开始给env创建的每一个stream追加时间特性</span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line"></span><br><span class="line">val readings: DataStream[SensorReading] = env</span><br><span class="line">        .addSource(new SensorSource)</span><br><span class="line">        .assignTimestampsAndWatermarks(new MyAssigner())</span><br></pre></td></tr></table></figure><h3 id="MyAssigner有两种类型"><a href="#MyAssigner有两种类型" class="headerlink" title="MyAssigner有两种类型"></a>MyAssigner有两种类型</h3><p>​            AssignerWithPeriodicWatermarks   按照周期来设置分配水位线<br>​            AssignerWithPunctuatedWatermarks  根据间断的特点来分配水位线<br>以上两个接口都继承自TimestampAssigner。</p><h4 id="Assigner-with-periodic-watermarks-根据周期来设置分配水位线"><a href="#Assigner-with-periodic-watermarks-根据周期来设置分配水位线" class="headerlink" title="Assigner with periodic watermarks(根据周期来设置分配水位线)"></a>Assigner with periodic watermarks(根据周期来设置分配水位线)</h4><p>Assigner with periodic watermarks 这是一个接口,一定要自己写一个类去实现这个接口,这是一种复杂的代码写法</p><p>如果你想要通过周期性来生成我的watermark的话,实际上你得告诉我周期的间隔时间.默认情况下,周期的时间是200毫秒.也就是说没隔200毫秒会插入水位线.会在数据流中插入水位线.插入一次一定就是调用一个方法去插入.如果我肯定要写一个类去实现AssignerWithPeriodicWatermarks(根据周期来设置分配水位线)这个接口.</p><p>当然这个200毫秒我也可以自己设置,在ExecutionConfig.setAutoWatermark<br>Interval()这个方法中,我可以指定一个时间.那么这个时间就是由我来设置这个周期性.这叫周期性的水位线.就是隔一段时间我给你水位线加上去,隔一段时间我就给你水位线加上去.</p><p>水位线有两个作用:</p><p>第一个作用是,我可以判断小于等于当前水位线的数据已经来了.</p><p>第二个作用是,我可以用他来判断他在什么时候应该触发我们window的function的执行.</p><p>周期性的生成watermark：系统会周期性的将watermark插入到流中(水位线也是一种特殊的事件!)。默认周期是200毫秒。可以使用ExecutionConfig.setAutoWatermarkInterval()方法进行设置。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line"></span><br><span class="line">// 每隔5秒产生一个watermark</span><br><span class="line">env.getConfig.setAutoWatermarkInterval(5000)</span><br></pre></td></tr></table></figure><p>​        产生watermark的逻辑：每隔5秒钟，Flink会调用</p><p>AssignerWithPeriodicWatermarks的getCurrentWatermark()方法。如果方法返回一个时间戳大于之前水位的时间戳，新的watermark会被插入到流中。这个检查保证了水位线是单调递增的。如果方法返回的时间戳小于等于之前水位的时间戳，则不会产生新的watermark。</p><p>例子，自定义一个周期性的时间戳抽取：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class PeriodicAssigner extends AssignerWithPeriodicWatermarks[SensorReading] &#123;</span><br><span class="line">val bound: Long = 60 * 1000 // 延时为1分钟</span><br><span class="line">var maxTs: Long = Long.MinValue // 观察到的最大时间戳</span><br><span class="line">//返回当前的水位线</span><br><span class="line">override def getCurrentWatermark: Watermark = &#123;</span><br><span class="line">new Watermark(maxTs - bound)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">override def extractTimestamp(r: SensorReading, previousTS: Long) = &#123;</span><br><span class="line">maxTs = maxTs.max(r.timestamp)</span><br><span class="line">r.timestamp</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​        一种简单的特殊情况是，如果我们事先得知数据流的时间戳是单调递增的，也就是说没有乱序，那我们可以使用assignAscendingTimestamps，这个方法会直接使用数据的时间戳生成watermark。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val stream: DataStream[SensorReading] = ...</span><br><span class="line">val withTimestampsAndWatermarks = stream</span><br><span class="line">.assignAscendingTimestamps(e =&gt; e.timestamp)</span><br><span class="line"></span><br><span class="line">&gt;&gt; result:  E(1), W(1), E(2), W(2), ...</span><br></pre></td></tr></table></figure><p>​        而对于乱序数据流，如果我们能大致估算出数据流中的事件的最大延迟时间，就可以使用如下代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">val stream: DataStream[SensorReading] = ...</span><br><span class="line">val withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(</span><br><span class="line">new SensorTimeAssigner</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">class SensorTimeAssigner extends BoundedOutOfOrdernessTimestampExtractor[SensorReading](Time.seconds(5)) &#123;</span><br><span class="line">// 抽取时间戳</span><br><span class="line">override def extractTimestamp(r: SensorReading): Long = r.timestamp</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&gt;&gt; relust:  E(10), W(0), E(8), E(7), E(11), W(1), ...</span><br></pre></td></tr></table></figure><h4 id="Assigner-with-punctuated-watermarks-间断式的生产水位线"><a href="#Assigner-with-punctuated-watermarks-间断式的生产水位线" class="headerlink" title="Assigner with punctuated watermarks(间断式的生产水位线)"></a>Assigner with punctuated watermarks(间断式的生产水位线)</h4><p>​        什么叫间断式的生产watermark(水位线)呢?本质上来说就是根据你指定的条件,他不是按照时间来的,而是让你来决定.你觉得可以加上一个watermark(水位线)了,那你就加.就是你写代码到达某一个条件了你就加一个水位线.当然你写代码时也可以判断时间.比如说你到达某一个时间,你加一个水位线.当然也可以不以时间为判断.比如说我判断你的键值对.键值对中的某一个键等于某个值的时候,我给你加个水位线.或者你的值等于什么什么的时候.我给你加一个水位线.所以,这叫健壮式的生产watermark(水位线).说白了这个所谓的健壮式的watermark(水位线),就是不以这个时间周期性来生成的.就是由我自己根据业务查询,自己根据条件去设置.</p><p>​        </p><p>​    间断式地生成watermark。和周期性生成的方式不同，这种方式不是固定时间的，而是可以根据需要对每条数据进行筛选和处理。直接上代码来举个例子，我们只给sensor_1的传感器的数据流插入watermark：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks[SensorReading] &#123;</span><br><span class="line">val bound: Long = 60 * 1000</span><br><span class="line"></span><br><span class="line">override def checkAndGetNextWatermark(r: SensorReading, extractedTS: Long): Watermark = &#123;</span><br><span class="line">if (r.id == &quot;sensor_1&quot;) &#123;</span><br><span class="line">new Watermark(extractedTS - bound)</span><br><span class="line">&#125; else &#123;</span><br><span class="line">null</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">override def extractTimestamp(r: SensorReading, previousTS: Long): Long = &#123;</span><br><span class="line">r.timestamp</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="EvnetTime在window中的使用"><a href="#EvnetTime在window中的使用" class="headerlink" title="EvnetTime在window中的使用"></a>EvnetTime在window中的使用</h1><h2 id="滚动窗口（TumblingEventTimeWindows）"><a href="#滚动窗口（TumblingEventTimeWindows）" class="headerlink" title="滚动窗口（TumblingEventTimeWindows）"></a>滚动窗口（TumblingEventTimeWindows）</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    //  环境</span><br><span class="line">    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line">    env.setParallelism(1)</span><br><span class="line"></span><br><span class="line">    val dstream: DataStream[String] = env.socketTextStream(&quot;localhost&quot;,7777)</span><br><span class="line"></span><br><span class="line">    val textWithTsDstream: DataStream[(String, Long, Int)] = dstream.map &#123; text =&gt;</span><br><span class="line">      val arr: Array[String] = text.split(&quot; &quot;)</span><br><span class="line">      (arr(0), arr(1).toLong, 1)</span><br><span class="line">    &#125;</span><br><span class="line">    val textWithEventTimeDstream: DataStream[(String, Long, Int)] = textWithTsDstream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[(String, Long, Int)](Time.milliseconds(1000)) &#123;</span><br><span class="line">      override def extractTimestamp(element: (String, Long, Int)): Long = &#123;</span><br><span class="line"></span><br><span class="line">       return  element._2</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    val textKeyStream: KeyedStream[(String, Long, Int), Tuple] = textWithEventTimeDstream.keyBy(0)</span><br><span class="line">    textKeyStream.print(&quot;textkey:&quot;)</span><br><span class="line"></span><br><span class="line">    val windowStream: WindowedStream[(String, Long, Int), Tuple, TimeWindow] = textKeyStream.window(TumblingEventTimeWindows.of(Time.seconds(2)))</span><br><span class="line"></span><br><span class="line">    val groupDstream: DataStream[mutable.HashSet[Long]] = windowStream.fold(new mutable.HashSet[Long]()) &#123; case (set, (key, ts, count)) =&gt;</span><br><span class="line">      set += ts</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    groupDstream.print(&quot;window::::&quot;).setParallelism(1)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果是按照Event Time的时间窗口计算得出的，而无关系统的时间（包括输入的快慢）</p><h2 id="滑动窗口（SlidingEventTimeWindows）"><a href="#滑动窗口（SlidingEventTimeWindows）" class="headerlink" title="滑动窗口（SlidingEventTimeWindows）"></a>滑动窗口（SlidingEventTimeWindows）</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def main(args: Array[String]): Unit = &#123;</span><br><span class="line">  //  环境</span><br><span class="line">  val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">  env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line">  env.setParallelism(1)</span><br><span class="line"></span><br><span class="line">  val dstream: DataStream[String] = env.socketTextStream(&quot;localhost&quot;,7777)</span><br><span class="line"></span><br><span class="line">  val textWithTsDstream: DataStream[(String, Long, Int)] = dstream.map &#123; text =&gt;</span><br><span class="line">    val arr: Array[String] = text.split(&quot; &quot;)</span><br><span class="line">    (arr(0), arr(1).toLong, 1)</span><br><span class="line">  &#125;</span><br><span class="line">  val textWithEventTimeDstream: DataStream[(String, Long, Int)] = textWithTsDstream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[(String, Long, Int)](Time.milliseconds(1000)) &#123;</span><br><span class="line">    override def extractTimestamp(element: (String, Long, Int)): Long = &#123;</span><br><span class="line"></span><br><span class="line">     return  element._2</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  val textKeyStream: KeyedStream[(String, Long, Int), Tuple] = textWithEventTimeDstream.keyBy(0)</span><br><span class="line">  textKeyStream.print(&quot;textkey:&quot;)</span><br><span class="line"></span><br><span class="line">  val windowStream: WindowedStream[(String, Long, Int), Tuple, TimeWindow] = textKeyStream.window(SlidingEventTimeWindows.of(Time.seconds(2),Time.milliseconds(500)))</span><br><span class="line"></span><br><span class="line">  val groupDstream: DataStream[mutable.HashSet[Long]] = windowStream.fold(new mutable.HashSet[Long]()) &#123; case (set, (key, ts, count)) =&gt;</span><br><span class="line">    set += ts</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  groupDstream.print(&quot;window::::&quot;).setParallelism(1)</span><br><span class="line"></span><br><span class="line">  env.execute()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="会话窗口（EventTimeSessionWindows）"><a href="#会话窗口（EventTimeSessionWindows）" class="headerlink" title="会话窗口（EventTimeSessionWindows）"></a>会话窗口（EventTimeSessionWindows）</h2><p>​        相邻两次数据的EventTime的时间差超过指定的时间间隔就会触发执行。如果加入Watermark， 会在符合窗口触发的情况下进行延迟。到达延迟水位再进行窗口触发。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    //  环境</span><br><span class="line">    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line">    env.setParallelism(1)</span><br><span class="line"></span><br><span class="line">    val dstream: DataStream[String] = env.socketTextStream(&quot;localhost&quot;,7777)</span><br><span class="line"></span><br><span class="line">    val textWithTsDstream: DataStream[(String, Long, Int)] = dstream.map &#123; text =&gt;</span><br><span class="line">      val arr: Array[String] = text.split(&quot; &quot;)</span><br><span class="line">      (arr(0), arr(1).toLong, 1)</span><br><span class="line">    &#125;</span><br><span class="line">    val textWithEventTimeDstream: DataStream[(String, Long, Int)] = textWithTsDstream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[(String, Long, Int)](Time.milliseconds(1000)) &#123;</span><br><span class="line">      override def extractTimestamp(element: (String, Long, Int)): Long = &#123;</span><br><span class="line"></span><br><span class="line">       return  element._2</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    val textKeyStream: KeyedStream[(String, Long, Int), Tuple] = textWithEventTimeDstream.keyBy(0)</span><br><span class="line">    textKeyStream.print(&quot;textkey:&quot;)</span><br><span class="line"></span><br><span class="line">    val windowStream: WindowedStream[(String, Long, Int), Tuple, TimeWindow] = textKeyStream.window(EventTimeSessionWindows.withGap(Time.milliseconds(500)) )</span><br><span class="line"></span><br><span class="line">    windowStream.reduce((text1,text2)=&gt;</span><br><span class="line">      (  text1._1,0L,text1._3+text2._3)</span><br><span class="line">    )  .map(_._3).print(&quot;windows:::&quot;).setParallelism(1)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;时间语义与Wartermark-水位线-阐述&quot;&gt;&lt;a href=&quot;#时间语义与Wartermark-水位线-阐述&quot; class=&quot;headerlink&quot; title=&quot;时间语义与Wartermark(水位线)阐述:&quot;&gt;&lt;/a&gt;时间语义与Wartermark(水位线)阐述:&lt;/h1&gt;&lt;p&gt;用到了时间语义和watermark,所以Flink和sparkStreaming是有一定的区别的.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flink中的时间语义有三种:&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Event Time(事件触发的时间或者是事件创建的时间),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Ingestion Time(进入Flink的时间),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Processing Time(Flink在处理这条数据的时间),&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink中的Window API</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E4%B8%AD%E7%9A%84Window-API/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E4%B8%AD%E7%9A%84Window-API/</id>
    <published>2022-02-14T06:26:50.000Z</published>
    <updated>2022-02-14T07:00:18.604Z</updated>
    
    <content type="html"><![CDATA[<p>附录算子俯瞰图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214142001.png"></p><span id="more"></span><h1 id="TimeWindow-时间窗口"><a href="#TimeWindow-时间窗口" class="headerlink" title="TimeWindow(时间窗口)"></a>TimeWindow(时间窗口)</h1><p><strong>时间窗口有三种:滚动窗口,滑动窗口, 会话窗口</strong></p><p>​        <strong>TimeWindow是将指定时间范围内的所有数据组成一个window，一次对一个window里面的所有数据进行计算。</strong></p><p>​        注意：timeWindow函数必须在keyBy之后，timeWindowAll则不需要</p><p><strong>不管你是三种窗口中的哪一种，如果你的窗口函数的后面没有加All的话，那就是基于我们的keyedStream的，加了All就是基于我们的DataStream的.</strong></p><h2 id="1-滚动窗口（Tumbling-Window）"><a href="#1-滚动窗口（Tumbling-Window）" class="headerlink" title="1.滚动窗口（Tumbling Window）"></a>1.滚动窗口（Tumbling Window）</h2><p>Flink默认的时间窗口根据Processing Time 进行窗口的划分，将Flink获取到的数据根据进入Flink的时间划分到不同的窗口中</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val minTempPerWindow = dataStream</span><br><span class="line">  .map(r =&gt; (r.id, r.temperature))</span><br><span class="line">  .keyBy(_._1)</span><br><span class="line">  .timeWindow(Time.seconds(15))</span><br><span class="line">  .reduce((r1, r2) =&gt; (r1._1, r1._2.min(r2._2)))</span><br></pre></td></tr></table></figure><p>注意:</p><p>​        滚动窗口一定要在keyBy之后去调用.keyBy之后调用的话,由于它是滚动窗口.我们只需要传一个参数,就是窗口的长度就可以了.<br>​        如果你仔细想的话,你发现有一个问题:  就是假设我窗口的长度为15秒,那么当前某一个窗口他的起始的时间是怎么算的?是不是就是把你当前的时间除以15呀.直接除以15取模,取模之后的到一个什么?</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214143417.png"></p><p>​            时间间隔可以通过<strong>Time.milliseconds(x)毫秒，Time.seconds(x)秒，Time.minutes(x)分钟</strong>,等其中的一个来指定。</p><p>​            TimeWindow()实际上就是设置我们的窗口.窗口设置好之后,要么做聚合要么做其他操做,一般来说就是做聚合.</p><h2 id="2-滑动窗口（SlidingEventTimeWindows）"><a href="#2-滑动窗口（SlidingEventTimeWindows）" class="headerlink" title="2.滑动窗口（SlidingEventTimeWindows）"></a>2.滑动窗口（SlidingEventTimeWindows）</h2><p>​        <strong>滑动窗口和滚动窗口的函数名是完全一致的</strong>，只是在传参数时需要传入两个参数，一个是window_size，一个是sliding_size。</p><p>​        下面代码中的sliding_size设置为了5s，也就是说，窗口每5s就计算一次，每一次计算的window范围是15s内的所有元素。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val minTempPerWindow: DataStream[(String, Double)] = dataStream</span><br><span class="line">  .map(r =&gt; (r.id, r.temperature))</span><br><span class="line">  .keyBy(_._1)</span><br><span class="line">  .timeWindow(Time.seconds(15), Time.seconds(5))</span><br><span class="line">  .reduce((r1, r2) =&gt; (r1._1, r1._2.min(r2._2)))</span><br><span class="line">.window(EventTimeSessionWindows.withGap(Time.minutes(10))</span><br></pre></td></tr></table></figure><p><strong>时间间隔可以通过Time.milliseconds(x)，Time.seconds(x)，Time.minutes(x)等其中的一个来指定。</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214143631.png"></p><h1 id="CountWindow-计数窗口"><a href="#CountWindow-计数窗口" class="headerlink" title="CountWindow(计数窗口)"></a>CountWindow(计数窗口)</h1><p>​        **CountWindow(计数窗口)**也有所谓的滚动窗口和滑动窗口,但是他只有这两个,没有所谓的会话窗口</p><p>​        CountWindow根据窗口中相同key元素的数量来触发执行，执行时只计算元素数量达到窗口大小的key对应的结果。</p><p>​        注意：CountWindow的window_size指的是相同Key的元素的个数，不是输入的所有元素的总数。</p><h2 id="1-滚动窗口（Tumbling-Window）-1"><a href="#1-滚动窗口（Tumbling-Window）-1" class="headerlink" title="1.滚动窗口（Tumbling Window）"></a>1.滚动窗口（Tumbling Window）</h2><p>​       <strong>默认的CountWindow是一个滚动窗口，只需要指定窗口大小即可，当元素数量达到窗口大小时，就会触发窗口的执行。</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val minTempPerWindow: DataStream[(String, Double)] = dataStream</span><br><span class="line">  .map(r =&gt; (r.id, r.temperature))</span><br><span class="line">  .keyBy(_._1)</span><br><span class="line">  .countWindow(5)</span><br><span class="line">  .reduce((r1, r2) =&gt; (r1._1, r1._2.max(r2._2)))</span><br></pre></td></tr></table></figure><h2 id="2-滑动窗口（SlidingEventTimeWindows）-1"><a href="#2-滑动窗口（SlidingEventTimeWindows）-1" class="headerlink" title="2.滑动窗口（SlidingEventTimeWindows）"></a>2.滑动窗口（SlidingEventTimeWindows）</h2><p>​        滑动窗口和滚动窗口的函数名是完全一致的，只是在传参数时需要传入两个参数，一个是window_size，一个是sliding_size。</p><p>​        下面代码中的sliding_size设置为了2，也就是说，每收到两个相同key的数据就计算一次，每一次计算的window范围是5个元素。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val keyedStream: KeyedStream[(String, Int), Tuple] = dataStream.map(r =&gt; (r.id, r.temperature)).keyBy(0)</span><br><span class="line">//每当某一个key的个数达到2的时候,触发计算，计算最近该key最近10个元素的内容</span><br><span class="line">val windowedStream: WindowedStream[(String, Int), Tuple, GlobalWindow] = keyedStream.countWindow(10,2)</span><br><span class="line">val sumDstream: DataStream[(String, Int)] = windowedStream.sum(1)</span><br></pre></td></tr></table></figure><h1 id="window-function-窗口函数"><a href="#window-function-窗口函数" class="headerlink" title="window function(窗口函数)"></a>window function(窗口函数)</h1><p> <strong>window function(窗口函数) :  这里面有增量聚合函数和全窗口函数</strong></p><p>​        <strong>window function 定义了要对窗口中收集的数据做的计算操作，主要可以分为两类：</strong></p><h2 id="1-增量聚合函数（incremental-aggregation-functions）"><a href="#1-增量聚合函数（incremental-aggregation-functions）" class="headerlink" title="1.增量聚合函数（incremental aggregation functions）"></a>1.增量聚合函数（incremental aggregation functions）</h2><p>​                        每条数据到来就进行计算，保持一个简单的状态。典型的增量聚合函数有ReduceFunction, AggregateFunction。</p><h2 id="2-全窗口函数（full-window-functions）"><a href="#2-全窗口函数（full-window-functions）" class="headerlink" title="2.全窗口函数（full window functions）"></a>2.全窗口函数（full window functions）</h2><p>​                        先把窗口所有数据收集起来，等到计算的时候会遍历所有数据。</p><p>​                        ProcessWindowFunction和ProcessAllWindowFunction就是一个全窗口函数。</p><p>​                        Apply也是全量的.</p><h1 id="其它可选API"><a href="#其它可选API" class="headerlink" title="其它可选API"></a>其它可选API</h1><p><strong>.trigger() —— 触发器</strong></p><p>定义 window 什么时候关闭，触发计算并输出结果</p><p><strong>.evitor() —— 移除器</strong></p><p>定义移除某些数据的逻辑</p><p><strong>.allowedLateness()</strong> —— 允许处理迟到的数据</p><p><strong>.sideOutputLateData()</strong> —— 将迟到的数据放入侧输出流</p><p><strong>.getSideOutput()</strong> —— 获取侧输出流</p><h2 id="什么叫允许处理迟到的数据"><a href="#什么叫允许处理迟到的数据" class="headerlink" title="什么叫允许处理迟到的数据?"></a>什么叫允许处理迟到的数据?</h2><p>这个时候就得考虑,我们的开窗的规则.他到底是按照什么时间来确定的.如果你开窗的规则不是按照执行时间,而是按照数据生成的时间.那就有可能出现所谓迟到的数据.举例:假设我数据的产生是在2:05分产生的.产生数据之后通过kafka再到我们的flink里面.中间是不是有一个过程呀?中间可能隔离一分钟等时间.然后到了我们的FlinkJob里面之后,Flink Job默认情况下他不是探讨你的数据生成的时间,而是探讨执行时间.执行时间肯定是按照顺序来的.但有没有可能是2:05产生的数据先来,下一条数据是2:04分生成的数据呢?当然有可能.所以假设以后要做这个流处理,要严格按照这个数据的生成时间,那这个时候就会出现所谓的迟到的数据.比如说2:05分的数据来了之后发现2:04分的数据还没来.他是真正执行完之后可能下一个窗口才来.那这个就叫所谓的迟到的数据.如果你要严格按照时间顺序的话,你就需要将2:05分的数据等一下.就要想办法让2:04分的这个迟到的数据放到2:05分的数据之前处理.那有什么解决办法呢?</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214145930.png"></p><p>**这三个操作就是其中一个解决方法.就是把迟到的数据单独放到一个分支流数据中.叫侧输出流.放到侧输出流中,我们还得取到侧输出流中的数据.这就是一个不太好的办法.**这个实际上也没法做到实时性.而且你也不知道到底哪个是迟到的数据.什么样的数据才算做迟到的数据.没有一个标准.所以这种处理迟到数据的办法不是最好的.只是他是其中一种.当然还有其他的一些参数,这里只介绍到这里.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214150000.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;附录算子俯瞰图:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214142001.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink中的Window</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E4%B8%AD%E7%9A%84Window/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E4%B8%AD%E7%9A%84Window/</id>
    <published>2022-02-14T06:03:51.000Z</published>
    <updated>2022-02-14T06:22:43.400Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Window主要分为哪几种?</strong><br>window主要分为<br><strong>CountWindow</strong>:  计数窗口:按照指定的数据条数生成一个Window，与时间无关。</p><p><strong>TimeWindow</strong>:    时间窗口:按照指定的数据条数生成一个Window，与时间无关。</p><p>而我们最关心的就是<strong>TimeWindow</strong>,因为他和时间有关系.<br><strong>TimeWindow又分为三种: 滑动窗口,滚动窗口,会话窗口.</strong></p><p>滚动窗口和滑动窗口有一个特点,就是他们的时间是对齐的.</p><p>所谓的时间对齐是什么意思呢?</p><span id="more"></span><p>就是你不管是哪一个并行度,里面所有的时间是对齐的.对齐说的是数据的时间对齐.比如说我这窗口的起始时间是10:05分,那我所有的,只要是10:05分的数据,都属于我接下来这新开的窗口.因为我假设我开窗的起始时间,实际上说的就是 我的开窗时间是10:05分的话,那么我不管你是哪个组的,也不管你是哪个分区的.我们的数据只要到达10:05分,就属于当前我们这个窗口.这就叫所谓的时间对齐.滚动窗口是没有滑动步长的,只有滑动窗口才有滑动步长.</p><p>​    如果我们在选择窗口开窗的时候,还需要对窗口里面的数据进行处理.对数据进行处理的话,我们称之为windowFunction,也称之为窗口函数.<br>​    <strong>WindowFunction(窗口函数)分为两类,一类叫增量聚合函数,另一种叫全窗口函数</strong><br>​    增量聚合函数的意思就是说,在窗口里面,因为一个窗口是一个起始时间到结束时间.这个时间段内,源源不断的会有数据过来.那么你来一条数据我就处理一条数据,来一条数据我就处理一条数据.这是一种情况.还有一种情况就是全窗口函数,等这个窗口结束了,所有数据都来了,我一次性处理.这两种处理方式就是所谓的增量聚合函数和全窗口函数.在大多数的</p><p>​    生产环境中,我们使用增量聚合函数比较合适.因为假设你后面一个窗口里面数据量特别多的话.你如果一次性全部处理的话,你处理时间是比较长的.但是如果我数据量很多,我来一条就处理一条,来一条就处理一条,那我事实上我就把我处理的时间均匀的分散到这个很长的时间段里面去了.所以他实际上比不会耽误我们的时间.</p><p>所以我们重点关心增量聚合函数.他有两大类,<br><strong>一个叫ReduceFunction,另一个叫AggregateFunction,都是属于增量聚合函数.</strong></p><h1 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h1><p>Window开窗,开往窗口之后怎么办?是由windowFunctions来决定的</p><p><strong>Window就是把无限的流，按照时间或者条数或者会话切成有限的流。本来你的流是无限的,我们按照一定规则切成一段一段的有限的流.而每一小段就是一段窗口(window)</strong></p><h2 id="Window概述"><a href="#Window概述" class="headerlink" title="Window概述"></a>Window概述</h2><p>​        Spark streaming流式计算是一种被设计用于处理无限数据集的数据处理引擎，而无限数据集是指一种不断增长的本质上无限的数据集，而<strong>window是一种切割无限数据为有限块进行处理的手段</strong>。</p><p>​        Window是无限数据流处理的核心，Window将一个无限的stream拆分成有限大小的”buckets”桶，我们可以在这些桶上做计算操作。</p><h2 id="Window类型"><a href="#Window类型" class="headerlink" title="Window类型"></a>Window类型</h2><p><strong>Window可以分成两类</strong>：<br>        CountWindow(计数窗口)：按照指定的数据条数生成一个Window，与时间无关。<br>        TimeWindow(时间窗口)：按照时间生成Window。<br>对于TimeWindow，可以根据窗口实现原理的不同分成三类：<strong>滚动窗口（Tumbling Window）、滑动窗口（Sliding Window）和会话窗口(Session Window）。</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214141018.png"></p><p><strong>滑动窗口有可能有某两条数据进到第一个窗口同时又进入到第二个窗口</strong></p><p><strong>滚动窗口只要设置窗口的长度就可以了.</strong></p><h3 id="1-滚动窗口（Tumbling-Windows）"><a href="#1-滚动窗口（Tumbling-Windows）" class="headerlink" title="1.滚动窗口（Tumbling Windows）"></a>1.滚动窗口（Tumbling Windows）</h3><p>将数据依据固定的窗口长度对数据进行切片。<br><strong>特点：时间对齐，窗口长度固定，没有重叠。</strong></p><p><strong>时间对齐是啥意思?什么情况下时间对齐?如果你的窗口没有加ALL的话</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214141145.png"></p><p>​        我们的窗口是基于KeyedStream的.就是分完组之后再使用窗口函数.你既然是KeyedStream那就是有很多一组一组的数据.如下图所示: user1,user2,user3你可以理解为三个组.这三个组他的时间一定是一致的.所以称之为时间对齐.就是说,如果Window1的时间是2019年11月11日11点11分11秒的话,那么window2,window3,window4等等所有的滑动窗口的组都是这个时间.所以说是时间对齐.窗口长度固定,没有重叠.没有重叠就是说任何一条数据不可能重复进入到两个窗口.随意说,我们定义这样的一个滚动窗口只需要定义窗口长度就可以了.</p><p>滚动窗口分配器将每个元素分配到一个指定窗口大小的窗口中，滚动窗口有一个固定的大小，并且不会出现重叠。例如：如果你指定了一个5分钟大小的滚动窗口，窗口的创建如下图所示：</p><p><strong>滚动窗口</strong>示例图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214141305.png"></p><p><strong>适用场景</strong>：适合做BI统计等（做每个时间段的聚合计算）。</p><h3 id="2-滑动窗口（Sliding-Windows）"><a href="#2-滑动窗口（Sliding-Windows）" class="headerlink" title="2.滑动窗口（Sliding Windows）"></a>2.滑动窗口（Sliding Windows）</h3><p>滑动窗口是固定窗口的更广义的一种形式，滑动窗口由固定的窗口长度和滑动间隔组成。</p><p><strong>特点：时间对齐，窗口长度固定,增加滑动步长，可以有重叠。</strong></p><p>​    由于他增加了滑动步长,所以说,他可以出现数据的重叠.<br>​    滑动窗口分配器将元素分配到固定长度的窗口中，与滚动窗口类似，窗口的大小由窗口大小参数来配置，另一个窗口滑动参数控制滑动窗口开始的频率。因此，滑动窗口如果滑动参数小于窗口大小的话，窗口是可以重叠的，在这种情况下元素会被分配到多个窗口中。<br>​    例如，你有10分钟的窗口和5分钟的滑动，那么每个窗口中5分钟的窗口里包含着上个10分钟产生的数据，如下图所示：</p><p><strong>滑动窗口</strong>示例图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214141539.png"></p><p><strong>适用场景</strong>：对最近一个时间段内的统计（求某接口最近5min的失败率来决定是否要报警）。</p><h3 id="3-会话窗口（Session-Windows）"><a href="#3-会话窗口（Session-Windows）" class="headerlink" title="3.会话窗口（Session Windows）"></a>3.会话窗口（Session Windows）</h3><p>​        会话窗口就是说在<strong>某一个你指定的时间长度内.没有数据来了.假设我规定这个时间长度为5秒.那么上一个窗口结束之后5秒内都没有数据来.5秒钟之后,到第六秒开始有数据来了.那么我们就认为,从第六秒开始,就是一个新的窗口了</strong>.所以他中间有一个时间的间隙.这就称之为会话窗口.为什么称之为会话窗口呢?因为你发现没有,既然他有时间的间隙的话,就好比一次一次的会话一样.</p><p>​        假如你开发了一个web项目跑在Tomcat里面的话.默认情况下,会话的超时时间是多少?半小时.所谓默认的会话超时时间是什么意思?他这个半小时是从什么时刻开始算的?是从你这个会话的最后一次访问的时间开始算的.半个小时之后你这个会话就超时了. 会话超时的话,说白了你这个会话就结束了.那么以后来的数据就是另外一个会话.或者以后来的请求就是另外一个会话.这里我们的timeout就是所谓的会话超时时间.这个会话超时时间假设你设定为5秒.那么这5秒从哪里开始算呢?就是从window1的最后一条数据进来的时间开始算.5秒钟之后,如果你还是没有数据来.那么就会产生一个窗口2;但是假设你这5秒钟内还一直有数据来呢?有数据来还是属于window1.而这个window1的边界就开始往后延伸.因为他是严格按照会话的超时时间来进行切割window.来切割widow的话,这个特点就是他这个时间没有对齐的.</p><p>​        很明显时间没有对齐呀!因为很有可能某一个window他的数据量比较大,且时间跨度比较长.那么下一个窗口的量比较大,时间跨度比较短,这也是有可能的.所以时间无对齐的. </p><p>​         所以会话窗口（Session Windows）由一系列事件组合一个指定时间长度的timeout间隙组成，类似于web应用的session，也就是一段时间没有接收到新数据就会生成新的窗口。</p><p>​        特点：时间无对齐。</p><p>​        session窗口分配器通过session活动来对元素进行分组，session窗口跟滚动窗口和滑动窗口相比，不会有重叠和固定的开始时间和结束时间的情况，相反，当它在一个固定的时间周期内不再收到元素，即非活动间隔产生，那个这个窗口就会关闭。一个session窗口通过一个session间隔来配置，这个session间隔定义了非活跃周期的长度，当这个非活跃周期产生，那么当前的session将关闭并且后续的元素将被分配到新的session窗口中去。</p><p><strong>会话窗口</strong>示例图</p><p>​    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214141811.png"></p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214142001.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;Window主要分为哪几种?&lt;/strong&gt;&lt;br&gt;window主要分为&lt;br&gt;&lt;strong&gt;CountWindow&lt;/strong&gt;:  计数窗口:按照指定的数据条数生成一个Window，与时间无关。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TimeWindow&lt;/strong&gt;:    时间窗口:按照指定的数据条数生成一个Window，与时间无关。&lt;/p&gt;
&lt;p&gt;而我们最关心的就是&lt;strong&gt;TimeWindow&lt;/strong&gt;,因为他和时间有关系.&lt;br&gt;&lt;strong&gt;TimeWindow又分为三种: 滑动窗口,滚动窗口,会话窗口.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;滚动窗口和滑动窗口有一个特点,就是他们的时间是对齐的.&lt;/p&gt;
&lt;p&gt;所谓的时间对齐是什么意思呢?&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>YARN调度器【capacity-scheduler.xml】默认配置</title>
    <link href="http://xubatian.cn/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/YARN%E8%B0%83%E5%BA%A6%E5%99%A8%E3%80%90capacity-scheduler-xml%E3%80%91%E9%BB%98%E8%AE%A4%E9%85%8D%E7%BD%AE/"/>
    <id>http://xubatian.cn/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/YARN%E8%B0%83%E5%BA%A6%E5%99%A8%E3%80%90capacity-scheduler-xml%E3%80%91%E9%BB%98%E8%AE%A4%E9%85%8D%E7%BD%AE/</id>
    <published>2022-02-12T10:11:21.000Z</published>
    <updated>2022-02-12T10:20:31.134Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop版本：3.1.3</p><pre><code>1、三种常见调度器    1.1、先进先出调度器    1.2、容量调度器    1.3、公平调度器2、容量调度器 多队列配置3、单词4、默认配置【capacity-scheduler.xml】</code></pre><span id="more"></span><h1 id="1、三种常见调度器"><a href="#1、三种常见调度器" class="headerlink" title="1、三种常见调度器"></a>1、三种常见调度器</h1><h2 id="1-1、先进先出调度器"><a href="#1-1、先进先出调度器" class="headerlink" title="1.1、先进先出调度器"></a>1.1、先进先出调度器</h2><pre><code>first-in first-out schedulerFIFO Scheduler后入队的任务 要等待 前入队的任务 出队</code></pre><p>可配置：<br>    1、每个用户的最大资源占比，防止单个用户把资源占满<br>    2、限制哪些用户可以提交应用到本队列 </p><h2 id="1-2、容量调度器"><a href="#1-2、容量调度器" class="headerlink" title="1.2、容量调度器"></a>1.2、容量调度器</h2><pre><code>Capacity Scheduler相当于 多个 FIFO Scheduler不同队列上的任务可以并行（比如 3个队列就可以并行3个任务）相同队列上的任务不能并行</code></pre><p>可配置：<br>    1、默认容量占比：各个队列占据一定百分比的资源<br>    （如：a队列40% b队列60%）<br>    2、最大容量占比：队列占据的资源百分比的最大值<br>    （如：a队列最大70%，当超出40%时，可以借b队列的空闲资源，最多借30%） </p><pre><code>划分方式：按业务划分（更常用）：下单、支付、物流…按技术划分：HIVE、Spark、Flink…</code></pre><h2 id="1-3、公平调度器"><a href="#1-3、公平调度器" class="headerlink" title="1.3、公平调度器"></a>1.3、公平调度器</h2><pre><code>Fair Scheduler和Capacity Scheduler类似，可以多队列配置；不同的是，叶子队列不是FIFO的在同一条叶子队列上，所有作业可以并发；资源分配的依据：时间尺度、优先级、资源缺额…</code></pre><p>在时间尺度上获得公平的资源</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220212181307.png"></p><p>最大最小公平分配算法</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220212181328.png"></p><h1 id="2、容量调度器-多队列配置"><a href="#2、容量调度器-多队列配置" class="headerlink" title="2、容量调度器 多队列配置"></a>2、容量调度器 多队列配置</h1><h2 id="1、编辑配置文件"><a href="#1、编辑配置文件" class="headerlink" title="1、编辑配置文件"></a>1、编辑配置文件</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim $HADOOP_HOME/etc/hadoop/capacity-scheduler.xml</span><br></pre></td></tr></table></figure><h2 id="2、修改根队列下面的叶队列名称，逗号分隔（此处新增队列名称为hive）"><a href="#2、修改根队列下面的叶队列名称，逗号分隔（此处新增队列名称为hive）" class="headerlink" title="2、修改根队列下面的叶队列名称，逗号分隔（此处新增队列名称为hive）"></a>2、修改<strong>根队列</strong>下面的<strong>叶队列</strong>名称，逗号分隔（此处新增队列名称为<code>hive</code>）</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;default,hive&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;在根队列下设置叶队列名称&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h2 id="3、修改-名为default的队列-的容量占比"><a href="#3、修改-名为default的队列-的容量占比" class="headerlink" title="3、修改 名为default的队列 的容量占比"></a>3、修改 名为default的队列 的容量占比</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.default.capacity&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;40&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;设置root下名为default队列的容量占比&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h2 id="4、给新队列配置"><a href="#4、给新队列配置" class="headerlink" title="4、给新队列配置"></a>4、给新队列配置</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.capacity&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;60&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;root下名为hive的队列 的 容量占比&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.user-limit-factor&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;每个用户可以占据该队列资源占比的上限（防止某用户把资源占满）&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.maximum-capacity&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;80&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;该队列的最大容量占比（可外借80-60=20）&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.state&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;RUNNING&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;该队列状态（RUNNING或STOPPED）&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.acl_submit_applications&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;访问控制列表：限定哪些用户 能访问该队列&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.acl_administer_queue&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;访问控制列表：限定哪些用户 可以管理该队列上的作业&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.acl_application_max_priority&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.maximum-application-lifetime&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-1&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;提交到该队列的应用 的 最大生存时间（-1表示无限时间）&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.default-application-lifetime&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-1&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;提交到该队列的应用 的 默认生存时间（-1表示无限时间；要求小于最大生存时间）&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h2 id="5、分发配置"><a href="#5、分发配置" class="headerlink" title="5、分发配置"></a>5、分发配置</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync.py $HADOOP_HOME/etc/hadoop/capacity-scheduler.xml</span><br></pre></td></tr></table></figure><h2 id="6、查看浏览器，端口8088"><a href="#6、查看浏览器，端口8088" class="headerlink" title="6、查看浏览器，端口8088"></a>6、查看浏览器，端口<code>8088</code></h2><p><img src="https://img-blog.csdnimg.cn/20210423104042285.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1llbGxvd19weXRob24=,size_16,color_FFFFFF,t_70"></p><p>7、提交到指定队列</p><ul><li>Java代码的<code>org.apache.hadoop.conf.Configuration</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">configuration.set(&quot;mapred.job.queuename&quot;, &quot;hive&quot;);</span><br></pre></td></tr></table></figure><ul><li>HIVE</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapred.job.queue.name=hive</span><br></pre></td></tr></table></figure><h1 id="3、单词"><a href="#3、单词" class="headerlink" title="3、单词"></a>3、单词</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220212181655.png"></p><h1 id="4、默认配置【capacity-scheduler-xml】"><a href="#4、默认配置【capacity-scheduler-xml】" class="headerlink" title="4、默认配置【capacity-scheduler.xml】"></a>4、默认配置【capacity-<a href="https://so.csdn.net/so/search?q=scheduler&spm=1001.2101.3001.7020">scheduler</a>.xml】</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 容量调度器中 挂起和运行的应用程序 的 最大数量 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.maximum-applications&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;Maximum number of applications that can be pending and running.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 可以用来运行【Application Masters】的最大资源占比 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;0.1&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">Maximum percent of resources in the cluster which can be used to run </span><br><span class="line">application masters i.e. controls number of concurrent running applications.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 容量调度器中的【资源计算器】，它用来 比较资源，默认比较资源的内存，</span><br><span class="line">另外可以选择别的资源计算器，从资源的多个维度（不仅内存，还有CPU等）来比较 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.resource-calculator&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">The ResourceCalculator implementation to be used to compare Resources in the scheduler.</span><br><span class="line">The default i.e. DefaultResourceCalculator only uses Memory while</span><br><span class="line">DominantResourceCalculator uses dominant-resource to compare </span><br><span class="line">multi-dimensional resources such as Memory, CPU etc.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 在 名为root的队列 下 设置队列名称（默认default一条队列，可设置多队列） --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;default&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;The queues at the this level (root is the root queue).&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- root下名为default的队列 的 容量占比 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.default.capacity&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;100&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;Default queue target capacity.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 每个用户可以占据该队列资源占比的上限（防止某用户把资源占满） --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.default.user-limit-factor&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;Default queue user limit a percentage from 0.0 to 1.0.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 该队列的最大容量占比 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.default.maximum-capacity&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;100&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 该队列状态（RUNNING or STOPPED） --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.default.state&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;RUNNING&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 访问控制列表：限定哪些用户 能访问该队列 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.default.acl_submit_applications&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 访问控制列表：限定哪些用户 可以管理该队列上的作业 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.default.acl_administer_queue&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;The ACL of who can administer jobs on the default queue.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.default.acl_application_max_priority&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">The ACL of who can submit applications with configured priority.</span><br><span class="line">For e.g, [user=&#123;name&#125; group=&#123;name&#125; max_priority=&#123;priority&#125; default_priority=&#123;priority&#125;]</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 提交到该队列的应用 的 最大生存时间（-1表示无限时间） --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.default.maximum-application-lifetime&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-1&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">Maximum lifetime of an application which is submitted to a queue</span><br><span class="line">in seconds. Any value less than or equal to zero will be considered as disabled.</span><br><span class="line">This will be a hard time limit for all applications in this</span><br><span class="line">queue. If positive value is configured then any application submitted</span><br><span class="line">to this queue will be killed after exceeds the configured lifetime.</span><br><span class="line">User can also specify lifetime per application basis in</span><br><span class="line">application submission context. But user lifetime will be</span><br><span class="line">overridden if it exceeds queue maximum lifetime. It is point-in-time</span><br><span class="line">configuration.</span><br><span class="line">Note : Configuring too low value will result in killing application</span><br><span class="line">sooner. This feature is applicable only for leaf queue.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 提交到该队列的应用 的 默认生存时间（-1表示无限时间；要求小于最大生存时间） --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.default.default-application-lifetime&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-1&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">Default lifetime of an application which is submitted to a queue</span><br><span class="line">in seconds. Any value less than or equal to zero will be considered as</span><br><span class="line">disabled.</span><br><span class="line">If the user has not submitted application with lifetime value then this</span><br><span class="line">value will be taken. It is point-in-time configuration.</span><br><span class="line">Note : Default lifetime can&#x27;t exceed maximum lifetime. This feature is</span><br><span class="line">applicable only for leaf queue.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.node-locality-delay&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;40&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">Number of missed scheduling opportunities after which the CapacityScheduler </span><br><span class="line">attempts to schedule rack-local containers.</span><br><span class="line">When setting this parameter, the size of the cluster should be taken into account.</span><br><span class="line">We use 40 as the default value, which is approximately the number of nodes in one rack.</span><br><span class="line">Note, if this value is -1, the locality constraint in the container request</span><br><span class="line">will be ignored, which disables the delay scheduling.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.rack-locality-additional-delay&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-1&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">Number of additional missed scheduling opportunities over the node-locality-delay</span><br><span class="line">ones, after which the CapacityScheduler attempts to schedule off-switch containers,</span><br><span class="line">instead of rack-local ones.</span><br><span class="line">Example: with node-locality-delay=40 and rack-locality-delay=20, the scheduler will</span><br><span class="line">attempt rack-local assignments after 40 missed opportunities, and off-switch assignments</span><br><span class="line">after 40+20=60 missed opportunities.</span><br><span class="line">When setting this parameter, the size of the cluster should be taken into account.</span><br><span class="line">We use -1 as the default value, which disables this feature. In this case, the number</span><br><span class="line">of missed opportunities for assigning off-switch containers is calculated based on</span><br><span class="line">the number of containers and unique locations specified in the resource request,</span><br><span class="line">as well as the size of the cluster.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.queue-mappings&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">A list of mappings that will be used to assign jobs to queues</span><br><span class="line">The syntax for this list is [u|g]:[name]:[queue_name][,next mapping]*</span><br><span class="line">Typically this list will be used to map users to queues,</span><br><span class="line">for example, u:%user:%user maps all users to queues with the same name</span><br><span class="line">as the user.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.queue-mappings-override.enable&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">If a queue mapping is present, will it override the value specified</span><br><span class="line">by the user? This can be used by administrators to place jobs in queues</span><br><span class="line">that are different than the one specified by the user.</span><br><span class="line">The default is false.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.per-node-heartbeat.maximum-offswitch-assignments&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">Controls the number of OFF_SWITCH assignments allowed</span><br><span class="line">during a node&#x27;s heartbeat. Increasing this value can improve</span><br><span class="line">scheduling rate for OFF_SWITCH containers. Lower values reduce</span><br><span class="line">&quot;clumping&quot; of applications on particular nodes. The default is 1.</span><br><span class="line">Legal values are 1-MAX_INT. This config is refreshable.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.application.fail-fast&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">Whether RM should fail during recovery if previous applications&#x27;</span><br><span class="line">queue is no longer valid.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220212181909.png"></p><p>知识源于积累,登峰造极源于自律!</p><p>好文章就得收藏慢慢品, 文章转载于: <a href="https://blog.csdn.net/Yellow_python/article/details/116021592">https://blog.csdn.net/Yellow_python/article/details/116021592</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Hadoop版本：3.1.3&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、三种常见调度器
    1.1、先进先出调度器
    1.2、容量调度器
    1.3、公平调度器
2、容量调度器 多队列配置
3、单词
4、默认配置【capacity-scheduler.xml】
&lt;/code&gt;&lt;/pre&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="hadoop" scheme="http://xubatian.cn/tags/hadoop/"/>
    
    <category term="yarn" scheme="http://xubatian.cn/tags/yarn/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink核心概念之并行度（Parallelism）</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8B%E5%B9%B6%E8%A1%8C%E5%BA%A6%EF%BC%88Parallelism%EF%BC%89/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8B%E5%B9%B6%E8%A1%8C%E5%BA%A6%EF%BC%88Parallelism%EF%BC%89/</id>
    <published>2022-02-11T05:04:57.000Z</published>
    <updated>2022-02-11T05:13:29.938Z</updated>
    
    <content type="html"><![CDATA[<p>Flink程序的执行具有并行、分布式的特性。<br>在执行过程中，一个流（stream）包含一个或多个分区（stream partition），而每一个算子（operator）可以包含一个或多个子任务（operator subtask），这些子任务在不同的线程、不同的物理机或不同的容器中彼此互不依赖地执行。</p><pre><code>一个特定算子的子任务（subtask）的个数被称之为其并行度（parallelism）。一般情况下，一个流程序的并行度，可以认为就是其所有算子中最大的并行度。一个程序中，不同的算子可能具有不同的并行度。streamEnv.setParallelism(1)//加在ENV上表示默认所有的算子平行度都是1</code></pre><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211130627.png"></p><p>Stream在算子之间传输数据的形式可以是one-to-one(forwarding)的模式也可以是redistributing的模式，具体是哪一种形式，取决于算子的种类。</p><p>​        <strong>One-to-one</strong>：stream(比如在source和map operator之间)维护着分区以及元素的顺序。那意味着map 算子的子任务看到的元素的个数以及顺序跟source 算子的子任务生产的元素的个数、顺序相同，map、fliter、flatMap等算子都是one-to-one的对应关系。</p><p>​        <strong>类似于spark中的窄依赖</strong></p><p>​        <strong>Redistributing</strong>：stream(map()跟keyBy/window之间或者keyBy/window跟sink之间)的分区会发生改变。每一个算子的子任务依据所选择的transformation发送数据到不同的目标任务。例如，keyBy() 基于hashCode重分区、broadcast和rebalance会随机重新分区，这些算子都会引起redistribute(重新分配过程)过程，而redistribute过程就<strong>类似于Spark中的shuffle过程</strong>。</p><p>​        <strong>类似于spark中的宽依赖</strong></p><h2 id="博主解析"><a href="#博主解析" class="headerlink" title="博主解析:"></a>博主解析:</h2><p>​        <strong>并行度就是在执行过程中,尤其是我们Flink的Job执行过程中,一个流(Stream)包含一个或者多个分区.由于他包含一个或多个分区,从而就造成了一个算子他就包含一个或者多个子任务</strong>.实际上这流(Stream)和算子,这两个是因果关系.就因为你这流包含了一个或者两个或者多个分区.从而造成流所对应的算子就会出现一个或者多个子任务.这些子任务在不同的Slot上运行.而且也可能是不同的物理机.或者是不同的不同的容器,彼此之间相互互不依赖地执行.但是实际上他们有依赖关系吗?实际上他们也有依赖关系的.完全没有依赖关系是不可能的,因为我们还有一个数据的依赖.因为我们的数据一定是从上一个子任务到下一个子任务的.</p><p>​        一个特定的算子它到底有多少个子任务呢?就是由我们这个并行度来决定的.而并行度就是由我们parallelism来设置的.我们可以在代码的后面通过setParallelism()方法来设置并行度.也可以统一的在ENV,来设置算子的并行度.在ENV上设置算子的并行度就是所谓默认的.由于我们设置了并行度,所以就造成了两种情况.就是造成了我们的Stream和Stream之间有两种关系存在.</p><p>​        <strong>一种是One-to-one(一对一对应关系),还有一种就是Redistributing(重新分配)</strong></p><p>​        One-to-one:就是一个对应一个.什么情况下才是一个对应一个呢?</p><p>如图所示:</p><p>​        <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211131006.png"></p><p>​            <strong>Redistribute</strong>:重新分配的意思,实际上就是变了.本来可能是只有一个并行度,通过一个算子之后变成了两个或者三个等并行度了</p><p>如图所示:</p><p>​    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211131056.png"></p><h2 id="目前Flink的dataStream支持8种物理分区策略"><a href="#目前Flink的dataStream支持8种物理分区策略" class="headerlink" title="目前Flink的dataStream支持8种物理分区策略"></a>目前Flink的dataStream支持8种物理分区策略</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">①GlobalPartitioner(全局分区)： 数据会被分发到下游算子的第一个实例中进行处理。</span><br><span class="line">②ShufflePartitioner(shuffle分区) ：数据会被随机分发到下游算子的每一个实例中进行。</span><br><span class="line">③RebalancePartitioner(重新平衡分区)： 数据会被循环发送到下游的每一个实例中进行处理。</span><br><span class="line">④RescalePartitioner (重新调节分区)：这种分区器会根据上下游算子的并行度，循环的方式输出到下游算子的每个实例。这里有点难以理解，假设上游并行度为 2，编号为 A 和 B。下游并行度为 4，编号为 1，2，3，4。那么 A 则把数据循环发送给 1 和 2，B 则把数据循环发送给 3 和 4。假设上游并行度为 4，编号为 A，B，C，D。下游并行度为 2，编号为 1，2。那么 A 和 B 则把数据发送给 1，C 和 D 则把数据发送给 2。</span><br><span class="line">⑤BroadcastPartitioner (广播分区) ：广播分区会将上游数据输出到下游算子的每个实例中。适合于大数据集和小数据集做Jion的场景。</span><br><span class="line">⑥ForwardPartitioner (forward分区)：用于将记录输出到下游本地的算子实例。它要求上下游算子并行度一样。简单的说，ForwardPartitioner用来做数据的控制台打印。</span><br><span class="line">⑦KeyGroupStreamPartitioner ：Hash 分区器。会将数据按Key的Hash值输出到下游算子实例中。​​​​</span><br><span class="line">⑧CustomPartitionerWrapper：用户自定义分区器。需要用户自己实现 Partitioner 接口，来定义自己的分区逻辑。</span><br><span class="line">static class CustomPartitioner implements Partitioner&lt;String&gt; &#123; </span><br><span class="line">    @Override </span><br><span class="line">    public int partition(String key, int numPartitions) &#123; </span><br><span class="line">        switch (key)&#123; </span><br><span class="line">            case &quot;1&quot;: return 1;</span><br><span class="line">            case &quot;2&quot;: return 2;</span><br><span class="line">            case &quot;3&quot;: return 3;</span><br><span class="line">            default : return 4;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211131134.png"></p><h2 id="Flink8种传输数据操作"><a href="#Flink8种传输数据操作" class="headerlink" title="Flink8种传输数据操作"></a>Flink8种传输数据操作</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">KeyBy: 按照key的hash值分区, 相同的key一定会分到相同的分区.即相同的key进入相同的并行度,也就是说进入同一个子任务.</span><br><span class="line">Shuffle: 随机分的.</span><br><span class="line">Rebalance: 全局轮询,即上游的所有并行度都对下游所有并行度轮询.</span><br><span class="line">Rescale: 局部轮询,也就是说上游两个,下游四个.我上游两个在两个里面轮询了,一个人在两个里面轮询.</span><br><span class="line">Global: 所有数据发送到下游同一个并行度.</span><br><span class="line">Broadcast:所有数据发送到下游所有并行度.</span><br><span class="line">Forward:要求并行度相同,因为他是一对一的.</span><br><span class="line">Customer: 自定义.</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211131309.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Flink程序的执行具有并行、分布式的特性。&lt;br&gt;在执行过程中，一个流（stream）包含一个或多个分区（stream partition），而每一个算子（operator）可以包含一个或多个子任务（operator subtask），这些子任务在不同的线程、不同的物理机或不同的容器中彼此互不依赖地执行。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;一个特定算子的子任务（subtask）的个数被称之为其并行度（parallelism）。一般情况下，一个流程序的并行度，可以认为就是其所有算子中最大的并行度。一个程序中，不同的算子可能具有不同的并行度。
streamEnv.setParallelism(1)//加在ENV上表示默认所有的算子平行度都是1
&lt;/code&gt;&lt;/pre&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink核心概念之执行图（ExecutionGraph）</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8B%E6%89%A7%E8%A1%8C%E5%9B%BE%EF%BC%88ExecutionGraph%EF%BC%89/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8B%E6%89%A7%E8%A1%8C%E5%9B%BE%EF%BC%88ExecutionGraph%EF%BC%89/</id>
    <published>2022-02-11T04:56:45.000Z</published>
    <updated>2022-02-11T05:03:45.814Z</updated>
    
    <content type="html"><![CDATA[<p>​        由Flink程序直接映射成的数据流图是StreamGraph，也被称为逻辑流图，因为它们表示的是计算逻辑的高级视图。为了执行一个流处理程序，Flink需要将逻辑流图转换为物理数据流图（也叫执行图），详细说明程序的执行方式。</p><p>​        <strong>Flink 中的执行图可以分成四层：StreamGraph(数据流图) -&gt; JobGraph(工作图) -&gt; ExecutionGraph (执行图)-&gt; 物理执行图。</strong></p><p>​        </p><p>​        <strong>StreamGraph</strong>：(数据流图)是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构有效。</p><p>​        <strong>JobGraph</strong>：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。</p><p>​        <strong>ExecutionGraph</strong>：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。</p><p>​        <strong>物理执行图</strong>：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构</p><span id="more"></span><h2 id="四个流图的转换过程"><a href="#四个流图的转换过程" class="headerlink" title="四个流图的转换过程"></a>四个流图的转换过程</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211130057.png"></p><p>​        StreamGraph称之为数据流图,数据流图是根据用户通过API的编程,尤其是通过dataStream API的编程,编程就是你写好的代码.就是根据你写好的代码来生成一个最初的图,这个图用来描述你代码或者说你程序的拓扑结构,这样的图就称之为StreamGraph(数据流图)</p><p>​        举例:如图所示:</p><p>​        <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211130206.png"></p><p>​        假设我写的代码里面第一个写的是Source读数据,读完数据之后调用FlatMap,接下来调用第三个Map算子;这些完全是根据自己的代码而定的.我这里先不管代码中我Map算子设置的并行度是几或者我的FlatMap中设置的并行度是几.我就把我写好的代码根据算子调用的顺序得到上面这张图,这张图就称之为数据流图.这张数据流图就是用来展示你写的代码的拓扑结构.这张数据流图是我们看不到的,因为Web-UI界面是不会给我展示这张图的.当然,自己写代码的人自己心里面是有这样图的.然后根据我的数据流图,优化之后的到一个JobGraph,这个我们称之为工作图.</p><p>​        JobGraph是什么意思呢?</p><p>​        他实际上是将前面的StreamGraph(数据流图)进行优化,优化之后呢.他会把多个符合条件的节点,通过operatorChain(任务链)连在一起.他时通过任务链把多个符合条件的节点连在一起.如下图所示:</p><p>​    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211130241.png"></p><p>​            上面两个图都是在客户端完成的,客户端完成之后就要开始要提交了.客户端提交给JobManager,JobManager在接收到你提交过来的Job之后,他首先会根据你提交过来的图,将这个图变成执行图.执行图里面有什么特点呢?</p><p>​            他是根据你这个执行图看看你有几个管道,找到你有几个管道,每个管道之间的数据通信是什么样子的.这就是所谓的执行图.变成执行图之后,每个管道还要得到不同的任务,因为你已经得到管道了,得到管道之后你就可以得到各个不同的任务.这个每个任务在哪个Slot上运行.后面这个图就叫物理执行图了.因为物理执行图就是实施去哪个TaskManager上的Slot去运行的.所以这四个图是这么来的.</p><pre><code>    Flink 中的执行图可以分成四层：    StreamGraph(数据流图) -&gt; JobGraph(工作图) -&gt; ExecutionGraph (执行图)-&gt; 物理执行图</code></pre><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211130331.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;​        由Flink程序直接映射成的数据流图是StreamGraph，也被称为逻辑流图，因为它们表示的是计算逻辑的高级视图。为了执行一个流处理程序，Flink需要将逻辑流图转换为物理数据流图（也叫执行图），详细说明程序的执行方式。&lt;/p&gt;
&lt;p&gt;​        &lt;strong&gt;Flink 中的执行图可以分成四层：StreamGraph(数据流图) -&amp;gt; JobGraph(工作图) -&amp;gt; ExecutionGraph (执行图)-&amp;gt; 物理执行图。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;​        &lt;/p&gt;
&lt;p&gt;​        &lt;strong&gt;StreamGraph&lt;/strong&gt;：(数据流图)是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构有效。&lt;/p&gt;
&lt;p&gt;​        &lt;strong&gt;JobGraph&lt;/strong&gt;：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。&lt;/p&gt;
&lt;p&gt;​        &lt;strong&gt;ExecutionGraph&lt;/strong&gt;：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。&lt;/p&gt;
&lt;p&gt;​        &lt;strong&gt;物理执行图&lt;/strong&gt;：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink核心概念之TaskManger与Slots</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8BTaskManger%E4%B8%8ESlots/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8BTaskManger%E4%B8%8ESlots/</id>
    <published>2022-02-11T04:45:08.000Z</published>
    <updated>2022-02-11T04:50:06.695Z</updated>
    
    <content type="html"><![CDATA[<p>​        在Flink job的运行过程中,Flink Job中每一个worker(TaskManager)都是一个JVM进程，它可能会在独立的Slot(线程)上执行一个或多个subtask(子任务)。为了控制一个worker能接收多少个task，worker通过task slot来进行控制（一个worker至少有一个task slot）。</p><p>​        每个task slot表示TaskManager拥有资源的一个固定大小的子集。假如一个TaskManager有三个slot，那么它会将其管理的内存分成三份给各个slot。资源slot化意味着一个subtask将不需要跟来自其他job的subtask竞争被管理的内存，取而代之的是它将拥有一定数量的内存储备。需要注意的是，这里不会涉及到CPU的隔离，slot目前仅仅用来隔离task的受管理的内存。</p><p>​        通过调整task slot的数量，允许用户定义subtask之间如何互相隔离。如果一个TaskManager一个slot，那将意味着每个task group运行在独立的JVM中（该JVM可能是通过一个特定的容器启动的），而一个TaskManager多个slot意味着更多的subtask可以共享同一个JVM。而在同一个JVM进程中的task将共享TCP连接（基于多路复用）和心跳消息。它们也可能共享数据集和数据结构，因此这减少了每个task的负载。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211124632.png"></p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211124731.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211124817.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211124840.png"></p><p>默认情况下，Flink允许子任务共享slot，即使它们是不同任务的子任务（前提是它们来自同一个job）。 这样的结果是，一个slot可以保存作业的整个管道。<br><strong>Task Slot是静态的概念，是指TaskManager具有的并发执行能力</strong>，可以通过参数taskmanager.numberOfTaskSlots进行配置；<strong>而并行度parallelism是动态概念</strong>，即TaskManager运行程序时实际使用的并发能力，可以通过参数parallelism.default进行配置。<br>也就是说，假设一共有3个TaskManager，每一个TaskManager中的分配3个TaskSlot，也就是每个TaskManager可以接收3个task，一共9个TaskSlot，如果我们设置parallelism.default=1，即运行程序默认的并行度为1，9个TaskSlot只用了1个，有8个空闲，因此，设置合适的并行度才能提高效率。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211124935.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211124954.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;​        在Flink job的运行过程中,Flink Job中每一个worker(TaskManager)都是一个JVM进程，它可能会在独立的Slot(线程)上执行一个或多个subtask(子任务)。为了控制一个worker能接收多少个task，worker通过task slot来进行控制（一个worker至少有一个task slot）。&lt;/p&gt;
&lt;p&gt;​        每个task slot表示TaskManager拥有资源的一个固定大小的子集。假如一个TaskManager有三个slot，那么它会将其管理的内存分成三份给各个slot。资源slot化意味着一个subtask将不需要跟来自其他job的subtask竞争被管理的内存，取而代之的是它将拥有一定数量的内存储备。需要注意的是，这里不会涉及到CPU的隔离，slot目前仅仅用来隔离task的受管理的内存。&lt;/p&gt;
&lt;p&gt;​        通过调整task slot的数量，允许用户定义subtask之间如何互相隔离。如果一个TaskManager一个slot，那将意味着每个task group运行在独立的JVM中（该JVM可能是通过一个特定的容器启动的），而一个TaskManager多个slot意味着更多的subtask可以共享同一个JVM。而在同一个JVM进程中的task将共享TCP连接（基于多路复用）和心跳消息。它们也可能共享数据集和数据结构，因此这减少了每个task的负载。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211124632.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink任务调度原理概念</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E5%8E%9F%E7%90%86%E6%A6%82%E5%BF%B5/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E5%8E%9F%E7%90%86%E6%A6%82%E5%BF%B5/</id>
    <published>2022-02-11T04:33:07.000Z</published>
    <updated>2022-02-11T04:36:00.355Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211123327.png"></p><span id="more"></span><p>​        客户端不是运行时和程序执行的一部分，但它用于准备并发送dataflow(JobGraph)给Master(JobManager)，然后，客户端断开连接或者维持连接以等待接收计算结果。</p><p>​        当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程.</p><p>​        <strong>Client</strong> 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境连通即可）。提交 Job 后，Client 可以结束进程（Streaming的任务），也可以不结束并等待结果返回。</p><p>​        <strong>JobManager</strong> 主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。</p><p>​        <strong>TaskManager</strong> 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220211123327.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
</feed>
