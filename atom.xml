<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>我的梦想是星辰大海</title>
  
  <subtitle>知识源于积累,登峰造极源于自律</subtitle>
  <link href="http://xubatian.cn/atom.xml" rel="self"/>
  
  <link href="http://xubatian.cn/"/>
  <updated>2022-02-10T03:37:24.339Z</updated>
  <id>http://xubatian.cn/</id>
  
  <author>
    <name>xubatian</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>kafka面试常遇问题</title>
    <link href="http://xubatian.cn/kafka%E9%9D%A2%E8%AF%95%E5%B8%B8%E9%81%87%E9%97%AE%E9%A2%98/"/>
    <id>http://xubatian.cn/kafka%E9%9D%A2%E8%AF%95%E5%B8%B8%E9%81%87%E9%97%AE%E9%A2%98/</id>
    <published>2022-02-10T03:16:03.000Z</published>
    <updated>2022-02-10T03:37:24.339Z</updated>
    
    <content type="html"><![CDATA[<ol><li>为什么要用消息队列？为什么选择了kafka?</li><li>kafka的组件与作用(架构)？</li><li>kafka为什么要分区？</li><li>Kafka生产者分区策略？</li><li>kafka的数据可靠性怎么保证？(丢，重)</li><li>kafka的副本机制？</li><li>kafka的消费分区分配策略？</li><li>kafka的offset怎么维护？</li><li>kafka为什么这么快？(高效读写数据)</li><li>Kafka消息数据积压，Kafka消费能力不足怎么处理？</li><li>kafka事务是怎么实现的？</li><li>Kafka中的数据是有序的吗？</li><li>Kafka可以按照时间消费数据？</li><li>Kafka单条日志传输大小？</li><li>Kafka参数优化？</li><li>Kafka适合以下应用场景？</li><li>Exactly Once语义？在流式计算中怎么保持？</li></ol><span id="more"></span><h2 id="解析参考"><a href="#解析参考" class="headerlink" title="解析参考"></a>解析参考</h2><h3 id="为什么要用消息队列"><a href="#为什么要用消息队列" class="headerlink" title="为什么要用消息队列"></a>为什么要用消息队列</h3><ol><li>解耦</li></ol><p>允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p><ol><li>可恢复性</li></ol><p>系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。</p><ol><li>缓冲</li></ol><p>有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。</p><ol><li>灵活性与峰值处理能力</li></ol><p>在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。</p><ol><li>异步通信</li></ol><p>很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</p><h3 id="为什么选择了kafka"><a href="#为什么选择了kafka" class="headerlink" title="为什么选择了kafka"></a>为什么选择了kafka</h3><ol><li>高吞吐量、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒。</li><li>可扩展性：kafka集群支持热扩展。</li><li>持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失。</li><li>容错性：允许集群中节点故障（若副本数量为n,则允许n-1个节点故障）。</li><li>高并发：支持数千个客户端同时读写。</li></ol><h3 id="kafka的组件与作用-架构"><a href="#kafka的组件与作用-架构" class="headerlink" title="kafka的组件与作用(架构)"></a>kafka的组件与作用(架构)</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220210112112.png"></p><ol><li>Producer ：消息生产者，就是向kafka broker发消息的客户端。</li><li>Consumer ：消息消费者，向kafka broker取消息的客户端。</li><li>Consumer Group （CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</li><li>Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。</li><li>Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic。</li><li>Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。</li><li>Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。</li><li>leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。</li><li>follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的follower。</li></ol><h3 id="kafka为什么要分区"><a href="#kafka为什么要分区" class="headerlink" title="kafka为什么要分区"></a>kafka为什么要分区</h3><ol><li>方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了。</li><li>可以提高并发，因为可以以Partition为单位读写。</li></ol><h3 id="Kafka生产者分区策略"><a href="#Kafka生产者分区策略" class="headerlink" title="Kafka生产者分区策略"></a>Kafka生产者分区策略</h3><ol><li>指明 partition 的情况下，直接将指明的值直接作为partiton值。</li><li>没有指明partition值但有key的情况下，将key的hash值与topic的partition数进行取余得到partition值。</li><li>既没有partition值又没有key值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与topic可用的partition总数取余得到partition值，也就是常说的round-robin算法。</li></ol><h3 id="kafka的数据可靠性怎么保证"><a href="#kafka的数据可靠性怎么保证" class="headerlink" title="kafka的数据可靠性怎么保证"></a>kafka的数据可靠性怎么保证</h3><p>为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。所以引出ack机制。</p><p><strong>ack应答机制（可问：造成数据重复和丢失的相关问题）</strong></p><p>Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。acks参数配置：</p><ul><li>0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据。</li><li>1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据。</li></ul><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220210112241.png"></p><ul><li>-1（all）：producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成数据重复。</li></ul><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220210112315.png"></p><h3 id="副本数据同步策略"><a href="#副本数据同步策略" class="headerlink" title="副本数据同步策略"></a>副本数据同步策略</h3><table><thead><tr><th align="left">方案</th><th align="center">优点</th><th align="right">缺点</th></tr></thead><tbody><tr><td align="left">半数以上完成同步，就发送ack</td><td align="center">延迟低</td><td align="right">选举新的leader时，容忍n台节点的故障，需要2n+1个副本</td></tr><tr><td align="left">全部完成同步，才发送ack</td><td align="center">选举新的leader时，容忍n台节点的故障，需要n+1个副本</td><td align="right">延迟高</td></tr></tbody></table><p>选择最后一个的原因：</p><ol><li>同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。</li><li>虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小。</li></ol><h3 id="ISR"><a href="#ISR" class="headerlink" title="ISR"></a>ISR</h3><p>如果采用全部完成同步，才发送ack的副本的同步策略的话：提出问题：leader收到数据，所有follower都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？</p><p>Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给follower发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定。Leader发生故障之后，就会从ISR中选举新的leader。</p><h3 id="故障处理-LEO与HW"><a href="#故障处理-LEO与HW" class="headerlink" title="故障处理(LEO与HW)"></a>故障处理(LEO与HW)</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220210112418.png"></p><p>LEO：指的是每个副本最大的offset。</p><p>HW：指的是消费者能见到的最大的offset，ISR队列中最小的LEO。</p><h6 id="follower故障"><a href="#follower故障" class="headerlink" title="follower故障"></a>follower故障</h6><p>follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了。</p><h6 id="leader故障"><a href="#leader故障" class="headerlink" title="leader故障"></a>leader故障</h6><p>leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。</p><p>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</p><p>问题纠正:</p><p> (1)ISR包含leader，不只是follower,所有的元数据都是Controller来维护的</p><p> (2)其实不是什么ack不ack的 Follower 发起 Fetcher请求 之后会返回 success ； 这就理解为  Follower向leader回复了ack，容易误解为ack是生产者和borker的关系。还有这句话应该是follow向leader反馈消息</p><h3 id="kafka的副本机制"><a href="#kafka的副本机制" class="headerlink" title="kafka的副本机制"></a>kafka的副本机制</h3><p>参考上一个问题(副本数据同步策略)。</p><h3 id="kafka的消费分区分配策略"><a href="#kafka的消费分区分配策略" class="headerlink" title="kafka的消费分区分配策略"></a>kafka的消费分区分配策略</h3><p>一个consumer group中有多个consumer，一个topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费 Kafka有三种分配策略，一是RoundRobin，一是Range。高版本还有一个StickyAssignor策略 将分区的所有权从一个消费者移到另一个消费者称为重新平衡（rebalance）。当以下事件发生时，Kafka 将会进行一次分区分配：</p><p>同一个 Consumer Group 内新增消费者。</p><p>消费者离开当前所属的Consumer Group，包括shuts down或crashes。</p><h6 id="Range分区分配策略"><a href="#Range分区分配策略" class="headerlink" title="Range分区分配策略"></a>Range分区分配策略</h6><p>Range是对每个Topic而言的（即一个Topic一个Topic分），首先对同一个Topic里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。然后用Partitions分区的个数除以消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。假设n=分区数/消费者数量，m=分区数%消费者数量，那么前m个消费者每个分配n+1个分区，后面的（消费者数量-m）个消费者每个分配n个分区。假如有10个分区，3个消费者线程，把分区按照序号排列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0，1，2，3，4，5，6，7，8，9</span><br></pre></td></tr></table></figure><p>消费者线程为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C1-0，C2-0，C2-1</span><br></pre></td></tr></table></figure><p>那么用partition数除以消费者线程的总数来决定每个消费者线程消费几个partition，如果除不尽，前面几个消费者将会多消费一个分区。在我们的例子里面，我们有10个分区，3个消费者线程，10/3 = 3，而且除除不尽，那么消费者线程C1-0将会多消费一个分区，所以最后分区分配的结果看起来是这样的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">C1-0：0，1，2，3</span><br><span class="line"></span><br><span class="line">C2-0：4，5，6</span><br><span class="line"></span><br><span class="line">C2-1：7，8，9</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如果有11个分区将会是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">C1-0：0，1，2，3</span><br><span class="line"></span><br><span class="line">C2-0：4，5，6，7</span><br><span class="line"></span><br><span class="line">C2-1：8，9，10</span><br></pre></td></tr></table></figure><p>假如我们有两个主题T1,T2，分别有10个分区，最后的分配结果将会是这样：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">C1-0：T1（0，1，2，3） T2（0，1，2，3）</span><br><span class="line"></span><br><span class="line">C2-0：T1（4，5，6） T2（4，5，6）</span><br><span class="line"></span><br><span class="line">C2-1：T1（7，8，9） T2（7，8，9）</span><br></pre></td></tr></table></figure><h6 id="RoundRobinAssignor分区分配策略"><a href="#RoundRobinAssignor分区分配策略" class="headerlink" title="RoundRobinAssignor分区分配策略"></a>RoundRobinAssignor分区分配策略</h6><p>RoundRobinAssignor策略的原理是将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序，然后通过轮询方式逐个将分区以此分配给每个消费者. 使用RoundRobin策略有两个前提条件必须满足：</p><p>同一个消费者组里面的所有消费者的num.streams（消费者消费线程数）必须相等；每个消费者订阅的主题必须相同。加入按照 hashCode 排序完的topic-partitions组依次为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">T1-5, T1-3, T1-0, T1-8, T1-2, T1-1, T1-4, T1-7, T1-6, T1-9</span><br></pre></td></tr></table></figure><p>我们的消费者线程排序为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C1-0, C1-1, C2-0, C2-1</span><br></pre></td></tr></table></figure><p>最后分区分配的结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">C1-0 将消费 T1-5, T1-2, T1-6 分区</span><br><span class="line"></span><br><span class="line">C1-1 将消费 T1-3, T1-1, T1-9 分区</span><br><span class="line"></span><br><span class="line">C2-0 将消费 T1-0, T1-4 分区</span><br><span class="line"></span><br><span class="line">C2-1 将消费 T1-8, T1-7 分区</span><br></pre></td></tr></table></figure><h6 id="StickyAssignor分区分配策略"><a href="#StickyAssignor分区分配策略" class="headerlink" title="StickyAssignor分区分配策略"></a>StickyAssignor分区分配策略</h6><p>Kafka从0.11.x版本开始引入这种分配策略，它主要有两个目的：</p><p>分区的分配要尽可能的均匀，分配给消费者者的主题分区数最多相差一个 分区的分配尽可能的与上次分配的保持相同。当两者发生冲突时，第一个目标优先于第二个目标。鉴于这两个目的，StickyAssignor策略的具体实现要比RangeAssignor和RoundRobinAssignor这两种分配策略要复杂很多。</p><p>假设消费组内有3个消费者</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C0、C1、C2</span><br></pre></td></tr></table></figure><p>它们都订阅了4个主题：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t0、t1、t2、t3</span><br></pre></td></tr></table></figure><p>并且每个主题有2个分区，也就是说整个消费组订阅了</p><p>t0p0、t0p1、t1p0、t1p1、t2p0、t2p1、t3p0、t3p1这8个分区</p><p>最终的分配结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">消费者C0：t0p0、t1p1、t3p0</span><br><span class="line"></span><br><span class="line">消费者C1：t0p1、t2p0、t3p1</span><br><span class="line"></span><br><span class="line">消费者C2：t1p0、t2p1</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这样初看上去似乎与采用RoundRobinAssignor策略所分配的结果相同</p><p>此时假设消费者C1脱离了消费组，那么消费组就会执行再平衡操作，进而消费分区会重新分配。如果采用RoundRobinAssignor策略，那么此时的分配结果如下：</p><p>消费者C0：t0p0、t1p0、t2p0、t3p0</p><p>消费者C2：t0p1、t1p1、t2p1、t3p1</p><p>如分配结果所示，RoundRobinAssignor策略会按照消费者C0和C2进行重新轮询分配。而如果此时使用的是StickyAssignor策略，那么分配结果为：</p><p>消费者C0：t0p0、t1p1、t3p0、t2p0</p><p>消费者C2：t1p0、t2p1、t0p1、t3p1</p><p>可以看到分配结果中保留了上一次分配中对于消费者C0和C2的所有分配结果，并将原来消费者C1的“负担”分配给了剩余的两个消费者C0和C2，最终C0和C2的分配还保持了均衡。</p><p>如果发生分区重分配，那么对于同一个分区而言有可能之前的消费者和新指派的消费者不是同一个，对于之前消费者进行到一半的处理还要在新指派的消费者中再次复现一遍，这显然很浪费系统资源。StickyAssignor策略如同其名称中的“sticky”一样，让分配策略具备一定的“粘性”，尽可能地让前后两次分配相同，进而减少系统资源的损耗以及其它异常情况的发生。</p><p>到目前为止所分析的都是消费者的订阅信息都是相同的情况，我们来看一下订阅信息不同的情况下的处理。</p><p>举例，同样消费组内有3个消费者：</p><p>C0、C1、C2</p><p>集群中有3个主题：</p><p>t0、t1、t2</p><p>这3个主题分别有</p><p>1、2、3个分区</p><p>也就是说集群中有</p><p>t0p0、t1p0、t1p1、t2p0、t2p1、t2p2这6个分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">消费者C0订阅了主题t0</span><br><span class="line"></span><br><span class="line">消费者C1订阅了主题t0和t1</span><br><span class="line"></span><br><span class="line">消费者C2订阅了主题t0、t1和t2</span><br></pre></td></tr></table></figure><p>如果此时采用RoundRobinAssignor策略：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">消费者C0：t0p0</span><br><span class="line"></span><br><span class="line">消费者C1：t1p0</span><br><span class="line"></span><br><span class="line">消费者C2：t1p1、t2p0、t2p1、t2p2</span><br></pre></td></tr></table></figure><p>如果此时采用的是StickyAssignor策略：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">消费者C0：t0p0</span><br><span class="line"></span><br><span class="line">消费者C1：t1p0、t1p1</span><br><span class="line"></span><br><span class="line">消费者C2：t2p0、t2p1、t2p2</span><br></pre></td></tr></table></figure><p>此时消费者C0脱离了消费组，那么RoundRobinAssignor策略的分配结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">消费者C1：t0p0、t1p1</span><br><span class="line"></span><br><span class="line">消费者C2：t1p0、t2p0、t2p1、t2p2</span><br></pre></td></tr></table></figure><p>StickyAssignor策略，那么分配结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">消费者C1：t1p0、t1p1、t0p0</span><br><span class="line"></span><br><span class="line">消费者C2：t2p0、t2p1、t2p2</span><br></pre></td></tr></table></figure><p>可以看到StickyAssignor策略保留了消费者C1和C2中原有的5个分区的分配：</p><p>t1p0、t1p1、t2p0、t2p1、t2p2。</p><p>从结果上看StickyAssignor策略比另外两者分配策略而言显得更加的优异，这个策略的代码实现也是异常复杂。</p><h3 id="kafka的offset怎么维护"><a href="#kafka的offset怎么维护" class="headerlink" title="kafka的offset怎么维护"></a>kafka的offset怎么维护</h3><p>Kafka 0.9版本之前，consumer默认将offset保存在Zookeeper中。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220210112614.png"></p><p>从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets。</p><p>额外补充：实际开发场景中在Spark和Flink中，可以自己手动提交kafka的offset，或者是flink两阶段提交自动提交offset。</p><h3 id="kafka为什么这么快"><a href="#kafka为什么这么快" class="headerlink" title="kafka为什么这么快"></a>kafka为什么这么快</h3><ol><li>Kafka本身是分布式集群，同时采用分区技术，并发度高。</li><li>顺序写磁盘</li></ol><p>Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到600M/s，而随机写只有100K/s。</p><ol><li>零拷贝技术</li></ol><p>零拷贝并不是不需要拷贝，而是减少不必要的拷贝次数。通常是说在IO读写过程中。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220210112638.png"></p><p>传统IO流程：</p><p>第一次：将磁盘文件，读取到操作系统内核缓冲区。</p><p>第二次：将内核缓冲区的数据，copy到application应用程序的buffer。</p><p>第三步：将application应用程序buffer中的数据，copy到socket网络发送缓冲区(属于操作系统内核的缓冲区)</p><p>第四次：将socket buffer的数据，copy到网卡，由网卡进行网络传输。</p><p>传统方式，读取磁盘文件并进行网络发送，经过的四次数据copy是非常繁琐的。实际IO读写，需要进行IO中断，需要CPU响应中断(带来上下文切换)，尽管后来引入DMA来接管CPU的中断请求，但四次copy是存在“不必要的拷贝”的。</p><p>重新思考传统IO方式，会注意到实际上并不需要第二个和第三个数据副本。应用程序除了缓存数据并将其传输回套接字缓冲区之外什么都不做。相反，数据可以直接从读缓冲区传输到套接字缓冲区。</p><p>显然，第二次和第三次数据copy 其实在这种场景下没有什么帮助反而带来开销，这也正是零拷贝出现的意义。</p><p>所以零拷贝是指读取磁盘文件后，不需要做其他处理，直接用网络发送出去。</p><h3 id="Kafka消费能力不足怎么处理"><a href="#Kafka消费能力不足怎么处理" class="headerlink" title="Kafka消费能力不足怎么处理"></a>Kafka消费能力不足怎么处理</h3><ol><li>如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数=分区数。（两者缺一不可）</li><li>如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间&lt;生产速度），使处理的数据小于生产的数据，也会造成数据积压。</li></ol><h3 id="kafka事务是怎么实现的"><a href="#kafka事务是怎么实现的" class="headerlink" title="kafka事务是怎么实现的"></a>kafka事务是怎么实现的</h3><p>Kafka从0.11版本开始引入了事务支持。事务可以保证Kafka在Exactly Once语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。</p><h6 id="Producer事务"><a href="#Producer事务" class="headerlink" title="Producer事务"></a>Producer事务</h6><p>为了实现跨分区跨会话的事务，需要引入一个全局唯一的Transaction ID，并将Producer获得的PID和Transaction ID绑定。这样当Producer重启后就可以通过正在进行的Transaction ID获得原来的PID。为了管理Transaction，Kafka引入了一个新的组件Transaction Coordinator。Producer就是通过和Transaction Coordinator交互获得Transaction ID对应的任务状态。Transaction Coordinator还负责将事务所有写入Kafka的一个内部Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。</p><h6 id="Consumer事务"><a href="#Consumer事务" class="headerlink" title="Consumer事务"></a>Consumer事务</h6><p>对于Consumer而言，事务的保证就会相对较弱，尤其时无法保证Commit的信息被精确消费。这是由于Consumer可以通过offset访问任意信息，而且不同的Segment File生命周期不同，同一事务的消息可能会出现重启后被删除的情况。</p><h3 id="Kafka中的数据是有序的吗"><a href="#Kafka中的数据是有序的吗" class="headerlink" title="Kafka中的数据是有序的吗"></a>Kafka中的数据是有序的吗</h3><p>单分区内有序。</p><p>多分区，分区与分区间无序。</p><h3 id="Kafka可以按照时间消费数据吗"><a href="#Kafka可以按照时间消费数据吗" class="headerlink" title="Kafka可以按照时间消费数据吗"></a>Kafka可以按照时间消费数据吗</h3><p>可以，提供的API方法：</p><p>KafkaUtil.fetchOffsetsWithTimestamp(topic, sTime, kafkaProp)</p><h3 id="Kafka单条日志传输大小"><a href="#Kafka单条日志传输大小" class="headerlink" title="Kafka单条日志传输大小"></a>Kafka单条日志传输大小</h3><p>kafka对于消息体的大小默认为单条最大值是1M但是在我们应用场景中, 常常会出现一条消息大于1M，如果不对kafka进行配置。则会出现生产者无法将消息推送到kafka或消费者无法去消费kafka里面的数据, 这时我们就要对kafka进行以下配置：server.properties</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">replica.fetch.max.bytes: 1048576  broker可复制的消息的最大字节数, 默认为1M</span><br><span class="line">message.max.bytes: 1000012   kafka 会接收单个消息size的最大限制， 默认为1M左右</span><br><span class="line"></span><br><span class="line">message.max.bytes必须小于等于replica.fetch.max.bytes，否则就会导致replica之间数据同步失败</span><br></pre></td></tr></table></figure><h3 id="Kafka参数优化"><a href="#Kafka参数优化" class="headerlink" title="Kafka参数优化"></a>Kafka参数优化</h3><h6 id="Broker参数配置（server-properties）"><a href="#Broker参数配置（server-properties）" class="headerlink" title="Broker参数配置（server.properties）"></a>Broker参数配置（server.properties）</h6><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1、日志保留策略配置</span><br><span class="line"># 保留三天，也可以更短 （log.cleaner.delete.retention.ms）</span><br><span class="line">log.retention.hours=72</span><br><span class="line"></span><br><span class="line">2、Replica相关配置</span><br><span class="line">default.replication.factor:1 默认副本1个</span><br><span class="line"></span><br><span class="line">3、网络通信延时</span><br><span class="line">replica.socket.timeout.ms:30000 #当集群之间网络不稳定时,调大该参数</span><br><span class="line">replica.lag.time.max.ms= 600000# 如果网络不好,或者kafka集群压力较大,会出现副本丢失,然后会频繁复制副本,导致集群压力更大,此时可以调大该参数。</span><br></pre></td></tr></table></figure><h6 id="Producer优化（producer-properties）"><a href="#Producer优化（producer-properties）" class="headerlink" title="Producer优化（producer.properties）"></a>Producer优化（producer.properties）</h6><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">compression.type:none                 gzip  snappy  lz4  </span><br><span class="line">#默认发送不进行压缩，推荐配置一种适合的压缩算法，可以大幅度的减缓网络压力和Broker的存储压力。</span><br></pre></td></tr></table></figure><h6 id="Kafka内存调整（kafka-server-start-sh）"><a href="#Kafka内存调整（kafka-server-start-sh）" class="headerlink" title="Kafka内存调整（kafka-server-start.sh）"></a>Kafka内存调整（kafka-server-start.sh）</h6><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">默认内存1个G，生产环境尽量不要超过6个G。</span><br><span class="line">export KAFKA_HEAP_OPTS=&quot;-Xms4g -Xmx4g&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Kafka适合以下应用场景"><a href="#Kafka适合以下应用场景" class="headerlink" title="Kafka适合以下应用场景"></a>Kafka适合以下应用场景</h3><ol><li>日志收集：一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer。</li><li>消息系统：解耦生产者和消费者、缓存消息等。</li><li>用户活动跟踪：kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后消费者通过订阅这些topic来做实时的监控分析，亦可保存到数据库。</li><li>运营指标：kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告；</li><li>流式处理：比如spark和flink。</li></ol><h3 id="Exactly-Once语义"><a href="#Exactly-Once语义" class="headerlink" title="Exactly Once语义"></a>Exactly Once语义</h3><p>将服务器的ACK级别设置为-1，可以保证Producer到Server之间不会丢失数据，即At Least Once语义。相对的，将服务器ACK级别设置为0，可以保证生产者每条消息只会被发送一次，即At Most Once语义。</p><p>At Least Once可以保证数据不丢失，但是不能保证数据不重复；</p><p>相对的，At Least Once可以保证数据不重复，但是不能保证数据不丢失。</p><p>但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即Exactly Once语义。在0.11版本以前的Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。</p><p>0.11版本的Kafka，引入了一项重大特性：幂等性。</p><p>开启幂等性enable.idempotence=true。</p><p>所谓的幂等性就是指Producer不论向Server发送多少次重复数据，Server端都只会持久化一条。幂等性结合At Least Once语义，就构成了Kafka的Exactly Once语义。即：</p><p>At Least Once + 幂等性 = Exactly Once</p><p>Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带Sequence Number。而Broker端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。</p><p>但是PID重启就会变化，同时不同的Partition也具有不同主键，所以幂等性无法保证跨分区跨会话的Exactly Once。</p><h3 id="补充，在流式计算中怎么Exactly-Once语义？以flink为例"><a href="#补充，在流式计算中怎么Exactly-Once语义？以flink为例" class="headerlink" title="补充，在流式计算中怎么Exactly Once语义？以flink为例"></a>补充，在流式计算中怎么Exactly Once语义？以flink为例</h3><h4 id="souce"><a href="#souce" class="headerlink" title="souce"></a>souce</h4><p>souce使用执行ExactlyOnce的数据源，比如kafka等</p><p>内部使用FlinkKafakConsumer，并开启CheckPoint，偏移量会保存到StateBackend中，并且默认会将偏移量写入到topic中去，即_consumer_offsets Flink设置CheckepointingModel.EXACTLY_ONCE</p><h4 id="sink"><a href="#sink" class="headerlink" title="sink"></a>sink</h4><p>存储系统支持覆盖也即幂等性：如Redis,Hbase,ES等 存储系统不支持覆：需要支持事务(预写式日志或者两阶段提交),两阶段提交可参考Flink集成的kafka sink的实现。</p><p>知识源于积累,登峰造极源于自律!</p><p>好文章就得收藏慢慢品, 文章转载于: <a href="https://mp.weixin.qq.com/s/_bZvPeAgC64ENzhIzydjSA">https://mp.weixin.qq.com/s/_bZvPeAgC64ENzhIzydjSA</a></p>]]></content>
    
    
    <summary type="html">&lt;ol&gt;
&lt;li&gt;为什么要用消息队列？为什么选择了kafka?&lt;/li&gt;
&lt;li&gt;kafka的组件与作用(架构)？&lt;/li&gt;
&lt;li&gt;kafka为什么要分区？&lt;/li&gt;
&lt;li&gt;Kafka生产者分区策略？&lt;/li&gt;
&lt;li&gt;kafka的数据可靠性怎么保证？(丢，重)&lt;/li&gt;
&lt;li&gt;kafka的副本机制？&lt;/li&gt;
&lt;li&gt;kafka的消费分区分配策略？&lt;/li&gt;
&lt;li&gt;kafka的offset怎么维护？&lt;/li&gt;
&lt;li&gt;kafka为什么这么快？(高效读写数据)&lt;/li&gt;
&lt;li&gt;Kafka消息数据积压，Kafka消费能力不足怎么处理？&lt;/li&gt;
&lt;li&gt;kafka事务是怎么实现的？&lt;/li&gt;
&lt;li&gt;Kafka中的数据是有序的吗？&lt;/li&gt;
&lt;li&gt;Kafka可以按照时间消费数据？&lt;/li&gt;
&lt;li&gt;Kafka单条日志传输大小？&lt;/li&gt;
&lt;li&gt;Kafka参数优化？&lt;/li&gt;
&lt;li&gt;Kafka适合以下应用场景？&lt;/li&gt;
&lt;li&gt;Exactly Once语义？在流式计算中怎么保持？&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="转载文章收录" scheme="http://xubatian.cn/tags/%E8%BD%AC%E8%BD%BD%E6%96%87%E7%AB%A0%E6%94%B6%E5%BD%95/"/>
    
    <category term="kafka" scheme="http://xubatian.cn/tags/kafka/"/>
    
    <category term="kafka面试题" scheme="http://xubatian.cn/tags/kafka%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>为什么要知道Hadoop机架感知？</title>
    <link href="http://xubatian.cn/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%9F%A5%E9%81%93Hadoop%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5%EF%BC%9F/"/>
    <id>http://xubatian.cn/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%9F%A5%E9%81%93Hadoop%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5%EF%BC%9F/</id>
    <published>2022-02-10T02:59:13.000Z</published>
    <updated>2022-02-10T03:14:50.998Z</updated>
    
    <content type="html"><![CDATA[<h4 id="一、首先，我为什么聊机架感知"><a href="#一、首先，我为什么聊机架感知" class="headerlink" title="一、首先，我为什么聊机架感知"></a><strong>一、首先，我为什么聊机架感知</strong></h4><p> 在了解hdfs<a href="https://cloud.tencent.com/product/clb?from=10680">负载均衡</a>时，需要获取DataNode情况，包括每个DataNode磁盘使用情况，获取到数据不均衡，就要做负载均衡处理。做负载均衡就要考虑热点数据发送到哪里去，集群服务器配置是否相同，机架使用情况等。</p><p> 机架感知在这里面有3个很重要的原因：</p><p>1、数据扩容，扩容的服务器在新机架上，导致数据不均衡</p><p>2、机架上的服务器磁盘配置不同（至于为什么，先不细聊）</p><p>通过感知机架，方便系统管理员手动操作，从而实现负载均衡</p><p>3、副本策略三副本，同节点、同机架、不同机架（同机房），可以实现保证有效存储时同时最大化安全策略</p><p>​                                                                                                                        </p><span id="more"></span><p><strong>机架图</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220210110107.png"></p><h4 id="二、关于机架感知"><a href="#二、关于机架感知" class="headerlink" title="二、关于机架感知"></a><strong>二、关于机架感知</strong></h4><ol><li>Hadoop不能自动获取节点是否分布在多机架上</li><li>Hadoop大规模集群才会存在跨机架</li><li>不同节点之间通信尽量发生在同一个机架（可用性）</li><li>数据块副本策略会跨机架（容错性）</li></ol><h4 id="三、机架感知配置"><a href="#三、机架感知配置" class="headerlink" title="三、机架感知配置"></a><strong>三、机架感知配置</strong></h4><p>1、自定义类实现 DNSToSwitchMapping，重写 resolve() 方法；打为 jar 包，并复制到 NameNode 节点的 /soft/hadoop/shared/hadoop/common/lib 目录下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * 机架感知类</span><br><span class="line"> */</span><br><span class="line">public class MyRackAware implements DNSToSwitchMapping &#123;</span><br><span class="line">    /**</span><br><span class="line">     * 根据需求，将不同的主机划分到不同的机架上</span><br><span class="line">     * @param names 数据节点主机的集合</span><br><span class="line">     * @return 机架感知的集合</span><br><span class="line">     */</span><br><span class="line">    public List&lt;String&gt; resolve(List&lt;String&gt; names) &#123;</span><br><span class="line">        List&lt;String&gt; list = new ArrayList&lt;String&gt;();</span><br><span class="line">        try &#123;</span><br><span class="line">            //将原始信息输出到目录，方便查看</span><br><span class="line">            FileWriter fw = new FileWriter(&quot;/home/centos/rackaware.txt&quot;);</span><br><span class="line">            for (String host : names) &#123;</span><br><span class="line">                //将输入的原始host写入文件</span><br><span class="line">                fw.append(host+&quot;/r/n&quot;);</span><br><span class="line"> </span><br><span class="line">                //进行原始的host进行分机架</span><br><span class="line">                // IP形式</span><br><span class="line">                if (host.startsWith(&quot;192&quot;)) &#123;</span><br><span class="line">                    String ipEnd = host.substring(host.lastIndexOf(&quot;.&quot;) + 1);</span><br><span class="line">                    if (Integer.parseInt(ipEnd) &lt;= 103) &#123; //s102,s103 在一个机架</span><br><span class="line">                        list.add(&quot;/rack1/&quot; + ipEnd);</span><br><span class="line">                    &#125; else &#123;                              //s104 在一个机架</span><br><span class="line">                        list.add(&quot;/rack2/&quot; + ipEnd);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                //主机名形式</span><br><span class="line">                else if (host.startsWith(&quot;s&quot;)) &#123;</span><br><span class="line">                    String ipEnd = host.substring(1);</span><br><span class="line">                    if (Integer.parseInt(ipEnd) &lt;= 103) &#123; //s102,s103 在一个机架</span><br><span class="line">                        list.add(&quot;/rack1/&quot; + ipEnd);</span><br><span class="line">                    &#125; else &#123;                              //s104 在一个机架</span><br><span class="line">                        list.add(&quot;/rack2/&quot; + ipEnd);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            fw.close();</span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        return list;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    public void reloadCachedMappings() &#123;</span><br><span class="line"> </span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    public void reloadCachedMappings(List&lt;String&gt; names) &#123;</span><br><span class="line"> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2、配置core-site.xml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;net.topology.node.switch.mapping.impl&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;com.fresher.hdfs.rackaware.MyRackAware&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>3、重启集群</p><h4 id="四、机架感知（来自官网）"><a href="#四、机架感知（来自官网）" class="headerlink" title="四、机架感知（来自官网）"></a><strong>四、机架感知（来自官网）</strong></h4><p> Hadoop 组件是机架感知的。例如，HDFS 块放置将通过将一个块副本放置在不同的机架上来使用机架感知来实现容错。这在网络交换机故障或集群内分区的情况下提供数据可用性。</p><p> Hadoop 主守护进程通过调用配置文件指定的外部脚本或 java 类来获取集群工作线程的机架 ID。使用 java 类或外部脚本进行拓扑，输出必须遵循 java <strong>org.apache.hadoop.net.DNSToSwitchMapping</strong>接口。接口期望保持一一对应，拓扑信息格式为’/myrack/myhost’，其中’/‘为拓扑分隔符，’myrack’为机架标识，’myhost’为个人主机。假设每个机架有一个 /24 子网，可以使用“/192.168.100.0/192.168.100.5”格式作为唯一的机架-主机拓扑映射。</p><p> 要使用java 类进行拓扑映射，类名由配置文件中的<strong>net.topology.node.switch.mapping.impl</strong>参数指定。一个示例 NetworkTopology.java 包含在 hadoop 发行版中，可由 Hadoop 管理员自定义。使用 Java 类而不是外部脚本具有性能优势，因为当新的工作节点注册自己时，Hadoop 不需要分叉外部进程。</p><p> 如果实现外部脚本，它将在配置文件中使用<strong>net.topology.script.file.name</strong>参数指定。与 java 类不同，外部拓扑脚本不包含在 Hadoop 发行版中，而是由管理员提供。Hadoop 在 fork 拓扑脚本时会向 ARGV 发送多个 IP 地址。发送到拓扑脚本的 IP 地址数由<strong>net.topology.script.number.args</strong>控制，默认为 100。如果将<strong>net.topology.script.number.args</strong>更改为 1，则拓扑脚本将为每个由 DataNodes 和/或 NodeManagers 提交的 IP。</p><p> 如果<strong>net.topology.script.file.name</strong>或<strong>net.topology.node.switch.mapping.impl</strong>未设置，则为任何传递的 IP 地址返回机架 ID ‘/default-rack’。虽然这种行为看起来很可取，但它可能会导致 HDFS 块复制问题，因为默认行为是将一个复制块写到机架外，并且无法这样做，因为只有一个名为“/default-rack”的机架。</p><p><strong>python Example</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"># this script makes assumptions about the physical environment.</span><br><span class="line">#  1) each rack is its own layer 3 network with a /24 subnet, which</span><br><span class="line"># could be typical where each rack has its own</span><br><span class="line">#     switch with uplinks to a central core router.</span><br><span class="line">#</span><br><span class="line">#             +-----------+</span><br><span class="line">#             |core router|</span><br><span class="line">#             +-----------+</span><br><span class="line">#            /             \</span><br><span class="line">#   +-----------+        +-----------+</span><br><span class="line">#   |rack switch|        |rack switch|</span><br><span class="line">#   +-----------+        +-----------+</span><br><span class="line">#   | data node |        | data node |</span><br><span class="line">#   +-----------+        +-----------+</span><br><span class="line">#   | data node |        | data node |</span><br><span class="line">#   +-----------+        +-----------+</span><br><span class="line">#</span><br><span class="line"># 2) topology script gets list of IP&#x27;s as input, calculates network address, and prints &#x27;/network_address/ip&#x27;.</span><br><span class="line"></span><br><span class="line">import netaddr</span><br><span class="line">import sys</span><br><span class="line">sys.argv.pop(0)                                                  # discard name of topology script from argv list as we just want IP addresses</span><br><span class="line"></span><br><span class="line">netmask = &#x27;255.255.255.0&#x27;                                        # set netmask to what&#x27;s being used in your environment.  The example uses a /24</span><br><span class="line"></span><br><span class="line">for ip in sys.argv:                                              # loop over list of datanode IP&#x27;s</span><br><span class="line">    address = &#x27;&#123;0&#125;/&#123;1&#125;&#x27;.format(ip, netmask)                      # format address string so it looks like &#x27;ip/netmask&#x27; to make netaddr work</span><br><span class="line">    try:</span><br><span class="line">        network_address = netaddr.IPNetwork(address).network     # calculate and print network address</span><br><span class="line">        print(&quot;/&#123;0&#125;&quot;.format(network_address))</span><br><span class="line">    except:</span><br><span class="line">        print(&quot;/rack-unknown&quot;)                                   # print catch-all value if unable to calculate network address</span><br></pre></td></tr></table></figure><p><strong>bash Example</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line"># Here&#x27;s a bash example to show just how simple these scripts can be</span><br><span class="line"># Assuming we have flat network with everything on a single switch, we can fake a rack topology.</span><br><span class="line"># This could occur in a lab environment where we have limited nodes,like 2-8 physical machines on a unmanaged switch.</span><br><span class="line"># This may also apply to multiple virtual machines running on the same physical hardware.</span><br><span class="line"># The number of machines isn&#x27;t important, but that we are trying to fake a network topology when there isn&#x27;t one.</span><br><span class="line">#</span><br><span class="line">#       +----------+    +--------+</span><br><span class="line">#       |jobtracker|    |datanode|</span><br><span class="line">#       +----------+    +--------+</span><br><span class="line">#              \        /</span><br><span class="line">#  +--------+  +--------+  +--------+</span><br><span class="line">#  |datanode|--| switch |--|datanode|</span><br><span class="line">#  +--------+  +--------+  +--------+</span><br><span class="line">#              /        \</span><br><span class="line">#       +--------+    +--------+</span><br><span class="line">#       |datanode|    |namenode|</span><br><span class="line">#       +--------+    +--------+</span><br><span class="line">#</span><br><span class="line"># With this network topology, we are treating each host as a rack.  This is being done by taking the last octet</span><br><span class="line"># in the datanode&#x27;s IP and prepending it with the word &#x27;/rack-&#x27;.  The advantage for doing this is so HDFS</span><br><span class="line"># can create its &#x27;off-rack&#x27; block copy.</span><br><span class="line"># 1) &#x27;echo $@&#x27; will echo all ARGV values to xargs.</span><br><span class="line"># 2) &#x27;xargs&#x27; will enforce that we print a single argv value per line</span><br><span class="line"># 3) &#x27;awk&#x27; will split fields on dots and append the last field to the string &#x27;/rack-&#x27;. If awk</span><br><span class="line">#    fails to split on four dots, it will still print &#x27;/rack-&#x27; last field value</span><br><span class="line"></span><br><span class="line">echo $@ | xargs -n 1 | awk -F &#x27;.&#x27; &#x27;&#123;print &quot;/rack-&quot;$NF&#125;&#x27;</span><br></pre></td></tr></table></figure><h4 id="五、Hadoop集群网络拓扑描述"><a href="#五、Hadoop集群网络拓扑描述" class="headerlink" title="五、Hadoop集群网络拓扑描述"></a><strong>五、Hadoop集群网络拓扑描述</strong></h4><p>Hadoop集群架构通常包含两级网络拓扑，一般来说，各级机架装配30~40个服务器。</p><blockquote><p>一个机架配置一个交换机，一个交换机实际的连接能力取决于交换机的端口数量，交换机的端口数量最多是48个</p></blockquote><p>为了达到Hadoop的最佳性能，配置Hadoop系统以让其了解网络拓扑状况就极为关键。</p><p>如果集群只包含一个机架，无需做什么，就是默认配置。对于多机架的集群来说，描述清楚节点-机架的映射关系，使得Hadoop将MapReduce任务分配到各个节点时，会倾向于执行机架内的数据传输，而非跨机架数据传输。HDFS还能更加智能地防止副本，以uqde性能和弹性的平衡。</p><p>重点！！！</p><p>节点和机架等网络位置以树的形式来表示，从而能够体现出各个位置之间的网络距离。namenode使用网络位置来确定在哪里防止块的副本。MapReduce的调度器根据网络位置来查找最近的副本，将它作为map任务的输入。</p><blockquote><p>比如spark中提到的移动数据不如移动计算也是同理。又比如yarn任务提交流程中，启动多个task，在哪启动的，现在是不是很清楚了。</p></blockquote><p>综上，回头文章开头，为什么要做负载均衡，为什么要了解机架感知，数据和计算是互相影响的。</p><p>文章转载于”大数据最后一公里公众号”, 原址: <a href="https://cloud.tencent.com/developer/article/1856190">https://cloud.tencent.com/developer/article/1856190</a></p><p>知识源于积累,登峰造极源于自律.</p>]]></content>
    
    
    <summary type="html">&lt;h4 id=&quot;一、首先，我为什么聊机架感知&quot;&gt;&lt;a href=&quot;#一、首先，我为什么聊机架感知&quot; class=&quot;headerlink&quot; title=&quot;一、首先，我为什么聊机架感知&quot;&gt;&lt;/a&gt;&lt;strong&gt;一、首先，我为什么聊机架感知&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt; 在了解hdfs&lt;a href=&quot;https://cloud.tencent.com/product/clb?from=10680&quot;&gt;负载均衡&lt;/a&gt;时，需要获取DataNode情况，包括每个DataNode磁盘使用情况，获取到数据不均衡，就要做负载均衡处理。做负载均衡就要考虑热点数据发送到哪里去，集群服务器配置是否相同，机架使用情况等。&lt;/p&gt;
&lt;p&gt; 机架感知在这里面有3个很重要的原因：&lt;/p&gt;
&lt;p&gt;1、数据扩容，扩容的服务器在新机架上，导致数据不均衡&lt;/p&gt;
&lt;p&gt;2、机架上的服务器磁盘配置不同（至于为什么，先不细聊）&lt;/p&gt;
&lt;p&gt;通过感知机架，方便系统管理员手动操作，从而实现负载均衡&lt;/p&gt;
&lt;p&gt;3、副本策略三副本，同节点、同机架、不同机架（同机房），可以实现保证有效存储时同时最大化安全策略&lt;/p&gt;
&lt;p&gt;​                                                                                                                        &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="hadoop" scheme="http://xubatian.cn/tags/hadoop/"/>
    
    <category term="机架感知" scheme="http://xubatian.cn/tags/%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5/"/>
    
    <category term="转载文章收录" scheme="http://xubatian.cn/tags/%E8%BD%AC%E8%BD%BD%E6%96%87%E7%AB%A0%E6%94%B6%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>Flink读取无界流数据计算过程演示</title>
    <link href="http://xubatian.cn/Flink%E8%AF%BB%E5%8F%96%E6%97%A0%E7%95%8C%E6%B5%81%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E6%BC%94%E7%A4%BA/"/>
    <id>http://xubatian.cn/Flink%E8%AF%BB%E5%8F%96%E6%97%A0%E7%95%8C%E6%B5%81%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E6%BC%94%E7%A4%BA/</id>
    <published>2022-02-10T02:07:21.000Z</published>
    <updated>2022-02-10T02:25:25.113Z</updated>
    
    <content type="html"><![CDATA[<p>Flink有界流式读取文本数据计算过程演示</p><span id="more"></span><div style="position: relative; padding: 30% 45%;"><iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/2022-02-10_10-12-09.mp4" frameborder="no" scrolling="no"></iframe></div> ]]></content>
    
    
    <summary type="html">&lt;p&gt;Flink有界流式读取文本数据计算过程演示&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink有界流式读取文本数据计算过程演示</title>
    <link href="http://xubatian.cn/Flink%E6%B5%81%E5%BC%8F%E8%AF%BB%E5%8F%96%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E6%BC%94%E7%A4%BA/"/>
    <id>http://xubatian.cn/Flink%E6%B5%81%E5%BC%8F%E8%AF%BB%E5%8F%96%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E6%BC%94%E7%A4%BA/</id>
    <published>2022-02-07T16:22:27.000Z</published>
    <updated>2022-02-10T02:17:20.844Z</updated>
    
    <content type="html"><![CDATA[<p>Flink流式读取文本数据计算过程演示. 文本数据总有一刻能读的完,所以他是有界的. 无界流读的是kafka的数据.</p><p>Flink是懒加载的,第一遍会检测整体代码.</p><p>并行度设置为1,就是单线程执行,所以Flink是一行一行读取文本数据的, 读一行计算一行. </p><p>sum算子是有状态的. 所以他的历史数据是保存在sum算子里面.sum算子做聚合计算的,他是一个有状态的算子.</p><p>批处理最终是输出一次,而流处理来一条计算一条.所以他保留了状态.方便后面来一条和前面对比进行计算.</p><span id="more"></span><p>源码地址: <a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.14.3-Demo/src/main/java/www/xubatian/cn/FlinkDemo01/Flink_WordCount_Bounded.java">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.14.3-Demo/src/main/java/www/xubatian/cn/FlinkDemo01/Flink_WordCount_Bounded.java</a></p><p>结果演示:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220210091224.png"></p><div style="position: relative; padding: 30% 45%;"><iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/2022-02-08_00-18-10.mp4" frameborder="no" scrolling="no"></iframe></div> ]]></content>
    
    
    <summary type="html">&lt;p&gt;Flink流式读取文本数据计算过程演示. 文本数据总有一刻能读的完,所以他是有界的. 无界流读的是kafka的数据.&lt;/p&gt;
&lt;p&gt;Flink是懒加载的,第一遍会检测整体代码.&lt;/p&gt;
&lt;p&gt;并行度设置为1,就是单线程执行,所以Flink是一行一行读取文本数据的, 读一行计算一行. &lt;/p&gt;
&lt;p&gt;sum算子是有状态的. 所以他的历史数据是保存在sum算子里面.sum算子做聚合计算的,他是一个有状态的算子.&lt;/p&gt;
&lt;p&gt;批处理最终是输出一次,而流处理来一条计算一条.所以他保留了状态.方便后面来一条和前面对比进行计算.&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Mybatis介绍之缓存</title>
    <link href="http://xubatian.cn/Mybatis%E4%BB%8B%E7%BB%8D%E4%B9%8B%E7%BC%93%E5%AD%98/"/>
    <id>http://xubatian.cn/Mybatis%E4%BB%8B%E7%BB%8D%E4%B9%8B%E7%BC%93%E5%AD%98/</id>
    <published>2022-02-07T14:25:10.000Z</published>
    <updated>2022-02-07T15:39:35.907Z</updated>
    
    <content type="html"><![CDATA[<p>Mybatis中有一级缓存和二级缓存，默认情况下一级缓存是开启的，而且是不能关闭的。一级缓存是指SqlSession级别的缓存，当在同一个SqlSession中进行相同的SQL语句查询时，第二次以后的查询不会从数据库查询，而是直接从缓存中获取，一级缓存最多缓存1024条SQL。二级缓存是指可以跨SqlSession的缓存。 </p><p>​    Mybatis中进行SQL查询是通过org.apache.ibatis.executor.Executor接口进行的，总体来讲，它一共有两类实现，一类是BaseExecutor，一类是CachingExecutor。前者是非启用二级缓存时使用的，而后者是采用的装饰器模式，在启用了二级缓存时使用，当二级缓存没有命中时，底层还是通过BaseExecutor来实现的。</p> <span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207233751.png"></p><h1 id="Mybatis介绍之缓存"><a href="#Mybatis介绍之缓存" class="headerlink" title="Mybatis介绍之缓存"></a>Mybatis介绍之缓存</h1><h2 id="一级缓存"><a href="#一级缓存" class="headerlink" title="一级缓存"></a>一级缓存</h2><p> 一级缓存是默认启用的，在BaseExecutor的query()方法中实现，底层默认使用的是PerpetualCache实现，PerpetualCache采用HashMap存储数据。一级缓存会在进行增、删、改操作时进行清除。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;E&gt; <span class="function">List&lt;E&gt; <span class="title">query</span><span class="params">(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql)</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">    ErrorContext.instance().resource(ms.getResource()).activity(<span class="string">&quot;executing a query&quot;</span>).object(ms.getId());</span><br><span class="line">    <span class="keyword">if</span> (closed) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> ExecutorException(<span class="string">&quot;Executor was closed.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (queryStack == <span class="number">0</span> &amp;&amp; ms.isFlushCacheRequired()) &#123;</span><br><span class="line">      clearLocalCache();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    List&lt;E&gt; list;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      queryStack++;</span><br><span class="line">      list = resultHandler == <span class="keyword">null</span> ? (List&lt;E&gt;) localCache.getObject(key) : <span class="keyword">null</span>;</span><br><span class="line">      <span class="keyword">if</span> (list != <span class="keyword">null</span>) &#123;</span><br><span class="line">        handleLocallyCachedOutputParameters(ms, key, parameter, boundSql);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      queryStack--;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (queryStack == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">for</span> (DeferredLoad deferredLoad : deferredLoads) &#123;</span><br><span class="line">        deferredLoad.load();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// issue #601</span></span><br><span class="line">      deferredLoads.clear();</span><br><span class="line">      <span class="keyword">if</span> (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) &#123;</span><br><span class="line">        <span class="comment">// issue #482</span></span><br><span class="line">        clearLocalCache();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> list;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>一级缓存的范围有SESSION和STATEMENT两种，默认是SESSION，如果我们不需要使用一级缓存，那么我们可以把一级缓存的范围指定为STATEMENT，这样每次执行完一个Mapper语句后都会将一级缓存清除。如果只是需要对某一条select语句禁用一级缓存，则可以在对应的select元素上加上flushCache=”true”。如果需要更改一级缓存的范围，请在Mybatis的配置文件中，在<settings>下通过localCacheScope指定。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;setting name=&quot;localCacheScope&quot; value=&quot;SESSION&quot;/&gt;</span><br></pre></td></tr></table></figure><p>为了验证一级缓存，我们进行如下测试，在testCache1中，我们通过同一个SqlSession查询了两次一样的SQL，第二次不会发送SQL。在testCache2中，我们也是查询了两次一样的SQL，但是它们是不同的SqlSession，结果会发送两次SQL请求。需要注意的是当Mybatis整合Spring后，直接通过Spring注入Mapper的形式，如果不是在同一个事务中每个Mapper的每次查询操作都对应一个全新的SqlSession实例，这个时候就不会有一级缓存的命中，如有需要可以启用二级缓存。而在同一个事务中时共用的就是同一个SqlSession。这块有兴趣的朋友可以去查看MapperFactoryBean的源码，其父类SqlSessionDaoSupport在设置SqlSessionFactory或设置SqlSessionTemplate时的逻辑。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 默认是有一级缓存的，一级缓存只针对于使用同一个SqlSession的情况。&lt;br/&gt;</span></span><br><span class="line"><span class="comment">  * 注意：当使用Spring整合后的Mybatis，不在同一个事务中的Mapper接口对应的操作也是没有一级缓存的，因为它们是对应不同的SqlSession。在本示例中如需要下面的第二个语句可使用一级缓存，需要testCache()方法在一个事务中，使用<span class="doctag">@Transactional</span>标注。</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="meta">@Test</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCache</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    PersonMapper mapper = session.getMapper(PersonMapper.class);</span><br><span class="line">    mapper.findById(<span class="number">5L</span>);</span><br><span class="line">    mapper.findById(<span class="number">5L</span>);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="meta">@Test</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCache2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    SqlSession session1 = <span class="keyword">this</span>.sessionFactory.openSession();</span><br><span class="line">    SqlSession session2 = <span class="keyword">this</span>.sessionFactory.openSession();</span><br><span class="line">    session1.getMapper(PersonMapper.class).findById(<span class="number">5L</span>);</span><br><span class="line">    session2.getMapper(PersonMapper.class).findById(<span class="number">5L</span>);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h2 id="二级缓存"><a href="#二级缓存" class="headerlink" title="二级缓存"></a>二级缓存</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>二级缓存是默认启用的，如想取消，则可以通过Mybatis配置文件中的<settings>元素下的子元素<setting>来指定cacheEnabled为false。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;settings&gt;</span><br><span class="line">    &lt;setting name=<span class="string">&quot;cacheEnabled&quot;</span> value=<span class="string">&quot;false&quot;</span> /&gt;</span><br><span class="line"> &lt;/settings&gt;</span><br></pre></td></tr></table></figure><p>cacheEnabled默认是启用的，只有在该值为true的时候，底层使用的Executor才是支持二级缓存的CachingExecutor。具体可参考Mybatis的核心配置类org.apache.ibatis.session.Configuration的newExecutor方法实现。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Executor <span class="title">newExecutor</span><span class="params">(Transaction transaction, ExecutorType executorType)</span> </span>&#123;</span><br><span class="line">   executorType = executorType == <span class="keyword">null</span> ? defaultExecutorType : executorType;</span><br><span class="line">   executorType = executorType == <span class="keyword">null</span> ? ExecutorType.SIMPLE : executorType;</span><br><span class="line">   Executor executor;</span><br><span class="line">   <span class="keyword">if</span> (ExecutorType.BATCH == executorType) &#123;</span><br><span class="line">     executor = <span class="keyword">new</span> BatchExecutor(<span class="keyword">this</span>, transaction);</span><br><span class="line">   &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ExecutorType.REUSE == executorType) &#123;</span><br><span class="line">     executor = <span class="keyword">new</span> ReuseExecutor(<span class="keyword">this</span>, transaction);</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     executor = <span class="keyword">new</span> SimpleExecutor(<span class="keyword">this</span>, transaction);</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">if</span> (cacheEnabled) &#123;</span><br><span class="line">     executor = <span class="keyword">new</span> CachingExecutor(executor);</span><br><span class="line">   &#125;</span><br><span class="line">   executor = (Executor) interceptorChain.pluginAll(executor);</span><br><span class="line">   <span class="keyword">return</span> executor;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>要使用二级缓存除了上面一个配置外，我们还需要在我们对应的Mapper.xml文件中定义需要使用的cache，具体可以参考CachingExecutor的以下实现，其中使用的cache就是我们在对应的Mapper.xml中定义的cache。还有一个条件就是需要当前的查询语句是配置了使用cache的，即下面源码的useCache()是返回true的，默认情况下所有select语句的useCache都是true，如果我们在启用了二级缓存后，有某个查询语句是我们不想缓存的，则可以通过指定其useCache为false来达到对应的效果。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">  public &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql)</span><br><span class="line">      throws SQLException &#123;</span><br><span class="line">    Cache cache = ms.getCache();</span><br><span class="line">    if (cache != null) &#123;</span><br><span class="line">      flushCacheIfRequired(ms);</span><br><span class="line">      if (ms.isUseCache() &amp;&amp; resultHandler == null) &#123;</span><br><span class="line">        ensureNoOutParams(ms, parameterObject, boundSql);</span><br><span class="line">        @SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line">        List&lt;E&gt; list = (List&lt;E&gt;) tcm.getObject(cache, key);</span><br><span class="line">        if (list == null) &#123;</span><br><span class="line">          list = delegate.&lt;E&gt; query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);</span><br><span class="line">          tcm.putObject(cache, key, list); // issue #578 and #116</span><br><span class="line">        &#125;</span><br><span class="line">        return list;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return delegate.&lt;E&gt; query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="cache定义"><a href="#cache定义" class="headerlink" title="cache定义"></a>cache定义</h3><p> 刚刚说了我们要想使用二级缓存，是需要在对应的Mapper.xml文件中定义其中的查询语句需要使用哪个cache来缓存数据的。这有两种方式可以定义，一种是通过cache元素定义，一种是通过cache-ref元素来定义。但是需要注意的是对于同一个Mapper来讲，它只能使用一个Cache，当同时使用了<cache>和<cache-ref>时使用<cache>定义的优先级更高。Mapper使用的Cache是与我们的Mapper对应的namespace绑定的，一个namespace最多只会有一个Cache与其绑定。</p><h3 id="cache元素定义"><a href="#cache元素定义" class="headerlink" title="cache元素定义"></a>cache元素定义</h3><p> 使用cache元素来定义使用的Cache时，最简单的做法是直接在对应的Mapper.xml文件中指定一个空的<cache/>元素，这个时候Mybatis会按照默认配置创建一个Cache对象，准备的说是PerpetualCache对象，更准确的说是LruCache对象（底层用了装饰器模式）。具体可以参考XMLMapperBuilder中的cacheElement()方法中解析cache元素的逻辑。空cache元素定义会生成一个采用最近最少使用算法最多只能存储1024个元素的缓存，而且是可读写的缓存，即该缓存是全局共享的，任何一个线程在拿到缓存结果后对数据的修改都将影响其它线程获取的缓存结果，因为它们是共享的，同一个对象。</p><p>​     cache元素可指定如下属性，每种属性的指定都是针对都是针对底层Cache的一种装饰，采用的是装饰器的模式。</p><p>Ø <strong>blocking</strong>：默认为false，当指定为true时将采用BlockingCache进行封装，blocking，阻塞的意思，使用BlockingCache会在查询缓存时锁住对应的Key，如果缓存命中了则会释放对应的锁，否则会在查询数据库以后再释放锁，这样可以阻止并发情况下多个线程同时查询数据，详情可参考BlockingCache的源码。</p><p>Ø <strong>eviction</strong>：eviction，驱逐的意思。也就是元素驱逐算法，默认是LRU，对应的就是LruCache，其默认只保存1024个Key，超出时按照最近最少使用算法进行驱逐，详情请参考LruCache的源码。如果想使用自己的算法，则可以将该值指定为自己的驱逐算法实现类，只需要自己的类实现Mybatis的Cache接口即可。除了LRU以外，系统还提供了FIFO（先进先出，对应FifoCache）、SOFT（采用软引用存储Value，便于垃圾回收，对应SoftCache）和WEAK（采用弱引用存储Value，便于垃圾回收，对应WeakCache）这三种策略。</p><p>Ø <strong>flushInterval</strong>：清空缓存的时间间隔，单位是毫秒，默认是不会清空的。当指定了该值时会再用ScheduleCache包装一次，其会在每次对缓存进行操作时判断距离最近一次清空缓存的时间是否超过了flushInterval指定的时间，如果超出了，则清空当前的缓存，详情可参考ScheduleCache的实现。</p><p>Ø <strong>readOnly</strong>：是否只读，默认为false。当指定为false时，底层会用SerializedCache包装一次，其会在写缓存的时候将缓存对象进行序列化，然后在读缓存的时候进行反序列化，这样每次读到的都将是一个新的对象，即使你更改了读取到的结果，也不会影响原来缓存的对象，即非只读，你每次拿到这个缓存结果都可以进行修改，而不会影响原来的缓存结果；当指定为true时那就是每次获取的都是同一个引用，对其修改会影响后续的缓存数据获取，这种情况下是不建议对获取到的缓存结果进行更改，意为只读。这是Mybatis二级缓存读写和只读的定义，可能与我们通常情况下的只读和读写意义有点不同。每次都进行序列化和反序列化无疑会影响性能，但是这样的缓存结果更安全，不会被随意更改，具体可根据实际情况进行选择。详情可参考SerializedCache的源码。</p><p>Ø <strong>size</strong>：用来指定缓存中最多保存的Key的数量。其是针对LruCache而言的，LruCache默认只存储最多1024个Key，可通过该属性来改变默认值，当然，如果你通过eviction指定了自己的驱逐算法，同时自己的实现里面也有setSize方法，那么也可以通过cache的size属性给自定义的驱逐算法里面的size赋值。</p><p>Ø <strong>type</strong>：type属性用来指定当前底层缓存实现类，默认是PerpetualCache，如果我们想使用自定义的Cache，则可以通过该属性来指定，对应的值是我们自定义的Cache的全路径名称。</p><h3 id="cache-ref元素定义"><a href="#cache-ref元素定义" class="headerlink" title="cache-ref元素定义"></a>cache-ref元素定义</h3><p>cache-ref元素可以用来指定其它Mapper.xml中定义的Cache，有的时候可能我们多个不同的Mapper需要共享同一个缓存的，是希望在MapperA中缓存的内容在MapperB中可以直接命中的，这个时候我们就可以考虑使用cache-ref，这种场景只需要保证它们的缓存的Key是一致的即可命中，二级缓存的Key是通过Executor接口的createCacheKey()方法生成的，其实现基本都是BaseExecutor，源码如下。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> CacheKey <span class="title">createCacheKey</span><span class="params">(MappedStatement ms, Object parameterObject, RowBounds rowBounds, BoundSql boundSql)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">if</span> (closed) &#123;</span><br><span class="line">     <span class="keyword">throw</span> <span class="keyword">new</span> ExecutorException(<span class="string">&quot;Executor was closed.&quot;</span>);</span><br><span class="line">   &#125;</span><br><span class="line">   CacheKey cacheKey = <span class="keyword">new</span> CacheKey();</span><br><span class="line">   cacheKey.update(ms.getId());</span><br><span class="line">   cacheKey.update(Integer.valueOf(rowBounds.getOffset()));</span><br><span class="line">   cacheKey.update(Integer.valueOf(rowBounds.getLimit()));</span><br><span class="line">   cacheKey.update(boundSql.getSql());</span><br><span class="line"></span><br><span class="line">   List&lt;ParameterMapping&gt; parameterMappings = boundSql.getParameterMappings();</span><br><span class="line"></span><br><span class="line">   TypeHandlerRegistry typeHandlerRegistry = ms.getConfiguration().getTypeHandlerRegistry();</span><br><span class="line">   <span class="comment">// mimic DefaultParameterHandler logic</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; parameterMappings.size(); i++) &#123;</span><br><span class="line">     ParameterMapping parameterMapping = parameterMappings.get(i);</span><br><span class="line">     <span class="keyword">if</span> (parameterMapping.getMode() != ParameterMode.OUT) &#123;</span><br><span class="line">       Object value;</span><br><span class="line">       String propertyName = parameterMapping.getProperty();</span><br><span class="line">       <span class="keyword">if</span> (boundSql.hasAdditionalParameter(propertyName)) &#123;</span><br><span class="line">         value = boundSql.getAdditionalParameter(propertyName);</span><br><span class="line">       &#125; <span class="keyword">else</span> <span class="keyword">if</span> (parameterObject == <span class="keyword">null</span>) &#123;</span><br><span class="line">         value = <span class="keyword">null</span>;</span><br><span class="line">       &#125; <span class="keyword">else</span> <span class="keyword">if</span>(typeHandlerRegistry.hasTypeHandler(parameterObject.getClass())) &#123;</span><br><span class="line">         value = parameterObject;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         MetaObject metaObject = configuration.newMetaObject(parameterObject);</span><br><span class="line">         value = metaObject.getValue(propertyName);</span><br><span class="line">       &#125;</span><br><span class="line">       cacheKey.update(value);</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">if</span> (configuration.getEnvironment() != <span class="keyword">null</span>) &#123;</span><br><span class="line">     <span class="comment">// issue #176</span></span><br><span class="line">     cacheKey.update(configuration.getEnvironment().getId());</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">return</span> cacheKey;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>打个比方我想在PersonMapper.xml中的查询都使用在UserMapper.xml中定义的Cache，则可以通过cache-ref元素的namespace属性指定需要引用的Cache所在的namespace，即UserMapper.xml中的定义的namespace，假设在UserMapper.xml中定义的namespace是com.elim.learn.mybatis.dao.UserMapper，则在PersonMapper.xml的cache-ref应该定义如下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;cache-ref namespace=&quot;com.elim.learn.mybatis.dao.UserMapper&quot;/&gt;</span><br></pre></td></tr></table></figure><h3 id="自定义cache"><a href="#自定义cache" class="headerlink" title="自定义cache"></a>自定义cache</h3><p> 前面提到Mybatis的Cache默认会使用PerpetualCache存储数据，如果我们不想按照它的逻辑实现，或者我们想使用其它缓存框架来实现，比如使用Ehcache、Redis等，这个时候我们就可以使用自己的Cache实现，Mybatis是给我们留有对应的接口，允许我们进行自定义的。要想实现自定义的Cache我们必须定义一个自己的类来实现Mybatis提供的Cache接口，实现对应的接口方法。注意，自定义的Cache必须包含一个接收一个String参数的构造方法，这个参数就是Cache的ID，详情请参考Mybatis初始化Cache的过程，对应XMLMapperBuilder的cacheElement()方法。以下是一个简单的MyCache的实现。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">publicclass MyCache implements Cache &#123;</span><br><span class="line">   <span class="keyword">private</span> String id;</span><br><span class="line">   <span class="keyword">private</span> String name;<span class="comment">//Name，故意加这么一个属性，以方便演示给自定义Cache的属性设值</span></span><br><span class="line">   <span class="keyword">private</span> Map&lt;Object, Object&gt; cache = <span class="keyword">new</span> HashMap&lt;Object, Object&gt;();</span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 构造方法。自定义的Cache实现一定要有一个id参数</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@param</span> id</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="title">MyCache</span><span class="params">(String id)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.id = id;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">this</span>.id;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">putObject</span><span class="params">(Object key, Object value)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.cache.put(key, value);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> Object <span class="title">getObject</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">this</span>.cache.get(key);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> Object <span class="title">removeObject</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">this</span>.cache.remove(key);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.cache.clear();</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getSize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">this</span>.cache.size();</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> ReadWriteLock <span class="title">getReadWriteLock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@return</span> the name</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> name;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@param</span> name the name to set</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.name = name;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 定义了自己的Cache实现类后我们就可以在需要使用它的Mapper.xml文件中通过<cache>标签的type属性来指定我们需要使用的Cache。如果我们的自定义Cache是需要指定参数的，则可以通过<cache>标签的子标签<property>来指定对应的参数，Mybatis在解析的时候会调用指定属性对应的set方法。针对于上面的自定义Cache，我们的配置如下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;cache type=&quot;com.elim.learn.mybatis.cache.MyCache&quot;&gt;</span><br><span class="line">      &lt;property name=&quot;name&quot; value=&quot;调用setName()方法需要传递的参数值&quot;/&gt;</span><br><span class="line">   &lt;/cache&gt;</span><br></pre></td></tr></table></figure><p>圆角矩形：注意：如果我们使用了自定义的Cache，那么cache标签的其它属性，如size、eviction等都不会对自定义的Cache起作用，也就是说不会自动对自定义的Cache进行包装，如果需要使用自定义的Cache，同时又希望使用Mybatis自带的那些Cache包装类，则可以在自定义的Cache中自己进行包装。</p><h3 id="缓存的清除"><a href="#缓存的清除" class="headerlink" title="缓存的清除"></a>缓存的清除</h3><p>二级缓存默认是会在执行update、insert和delete语句时进行清空的，具体可以参考CachingExecutor的update()实现。如果我们不希望在执行某一条更新语句时清空对应的二级缓存，那么我们可以在对应的语句上指定flushCache属性等于false。如果只是某一条select语句不希望使用二级缓存和一级缓存，则也可以在对应的select元素上加上flushCache=”true”。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;insert id=&quot;delete&quot; parameterType=&quot;java.lang.Long&quot;flushCache=&quot;false&quot;&gt;</span><br><span class="line">    delete t_person where id=#&#123;id&#125;</span><br><span class="line"> &lt;/insert&gt;</span><br></pre></td></tr></table></figure><h3 id="自己操作Cache"><a href="#自己操作Cache" class="headerlink" title="自己操作Cache"></a>自己操作Cache</h3><p>Mybatis中创建的二级缓存都会交给Configuration进行管理，Configuration类是Mybatis的核心类，里面包含了各种Mybatis资源的管理，其可以很方便的通过SqlSession、SqlSessionFactory获取，如有需要我们可以直接通过它来操作我们的Cache。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  @Test</span><br><span class="line">   public void testGetCache() &#123;</span><br><span class="line">      Configuration configuration = this.session.getConfiguration();</span><br><span class="line">//    this.sessionFactory.getConfiguration();</span><br><span class="line">      Collection&lt;Cache&gt; caches = configuration.getCaches();</span><br><span class="line">      System.out.println(caches);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>针对二级缓存进行了以下测试，获取两个不同的SqlSession执行两条相同的SQL，在未指定Cache时Mybatis将查询两次数据库，在指定了Cache时Mybatis只查询了一次数据库，第二次是从缓存中拿的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">  public void testCache2() &#123;</span><br><span class="line">     SqlSession session1 = this.sessionFactory.openSession();</span><br><span class="line">     SqlSession session2 = this.sessionFactory.openSession();</span><br><span class="line">     session1.getMapper(PersonMapper.class).findById(5L);</span><br><span class="line">     session1.commit();</span><br><span class="line">     session2.getMapper(PersonMapper.class).findById(5L);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p> 注意在上面的代码中，我在session1执行完对应的SQL后调用了session1的commit()方法，即提交了它的事务，这样我们在第二次查询的时候才会缓存命中，才不会查询数据库，否则就会连着查询两次数据库。这是因为在CachingExecutor中Mybatis在查询的过程中又在原来Cache的基础上包装了TransactionalCache，这个Cache只会在事务提交后才真正的写入缓存，所以在上面的示例中，如果session1执行完SQL后没有马上commit就紧接着用session2执行SQL，虽然session1查询时没有缓存命中，但是此时写入缓存操作还没有进行，session2再查询的时候也就不会缓存命中了。</p><p><strong>参考文档</strong></p><p><a href="http://www.mybatis.org/mybatis-3/zh/sqlmap-xml.html#cache">http://www.mybatis.org/mybatis-3/zh/sqlmap-xml.html#cache</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Mybatis中有一级缓存和二级缓存，默认情况下一级缓存是开启的，而且是不能关闭的。一级缓存是指SqlSession级别的缓存，当在同一个SqlSession中进行相同的SQL语句查询时，第二次以后的查询不会从数据库查询，而是直接从缓存中获取，一级缓存最多缓存1024条SQL。二级缓存是指可以跨SqlSession的缓存。 &lt;/p&gt;
&lt;p&gt;​    Mybatis中进行SQL查询是通过org.apache.ibatis.executor.Executor接口进行的，总体来讲，它一共有两类实现，一类是BaseExecutor，一类是CachingExecutor。前者是非启用二级缓存时使用的，而后者是采用的装饰器模式，在启用了二级缓存时使用，当二级缓存没有命中时，底层还是通过BaseExecutor来实现的。&lt;/p&gt;</summary>
    
    
    
    <category term="Java" scheme="http://xubatian.cn/categories/Java/"/>
    
    
    <category term="Java" scheme="http://xubatian.cn/tags/Java/"/>
    
    <category term="Mybatis" scheme="http://xubatian.cn/tags/Mybatis/"/>
    
  </entry>
  
  <entry>
    <title>推荐一款Mybatis分页插件</title>
    <link href="http://xubatian.cn/%E6%8E%A8%E8%8D%90%E4%B8%80%E6%AC%BEMybatis%E5%88%86%E9%A1%B5%E6%8F%92%E4%BB%B6/"/>
    <id>http://xubatian.cn/%E6%8E%A8%E8%8D%90%E4%B8%80%E6%AC%BEMybatis%E5%88%86%E9%A1%B5%E6%8F%92%E4%BB%B6/</id>
    <published>2022-02-07T13:52:43.000Z</published>
    <updated>2022-02-07T14:17:20.428Z</updated>
    
    <content type="html"><![CDATA[<p>介绍Mybatis的插件，以及如何通过Mybatis的插件功能实现一个自定义的分页插件。前段时间遇到了一款开源的Mybatis分页插件，叫<code>PageHelper</code>，github地址是 <a href="https://github.com/pagehelper/Mybatis-PageHelper">https://github.com/pagehelper/Mybatis-PageHelper</a>   其原理是通过<code>ThreadLocal</code>来存放分页信息，从而可以做到在Service层实现无侵入性的Mybatis分页。笔者感觉还不错，所以特意发博文记录一下，并推荐给大家。</p><span id="more"></span><h1 id="简单示例"><a href="#简单示例" class="headerlink" title="简单示例"></a>简单示例</h1><p>以下是使用<code>PageHelper</code>进行分页的一个简单的示例，更多详细的内容，请大家参数上面提供的<a href="https://github.com/pagehelper/Mybatis-PageHelper">github地址</a>。</p><h2 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h2><p>笔者使用的是Maven，添加依赖如下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;pagehelper&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;4.1.6&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h2 id="注册Mybatis-Plugin"><a href="#注册Mybatis-Plugin" class="headerlink" title="注册Mybatis Plugin"></a>注册Mybatis Plugin</h2><p>跟其它Mybatis Plugin一样，我们需要在Mybatis的配置文件中注册需要使用的Plugin，<code>PageHelper</code>中对应的Plugin实现类就是<code>com.github.pagehelper.PageHelper</code>自身。顺便说一句，Mybatis的Plugin我们说是Plugin，实际上对应的却是<code>org.apache.ibatis.plugin.Interceptor</code>接口，因为<code>Interceptor</code>的核心是其中的<code>plugin(Object target)</code>方法，而对于<code>plugin(Object target)</code>方法的实现，我们在需要对对应的对象进行拦截时会通过<code>org.apache.ibatis.plugin.Plugin</code>的静态方法<code>wrap(Object target, Interceptor interceptor)</code>返回一个代理对象，而方法入参就是当前的<code>Interceptor</code>实现类。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;plugins&gt;  </span><br><span class="line">   &lt;plugin interceptor=&quot;com.github.pagehelper.PageHelper&quot;/&gt;  </span><br><span class="line">&lt;/plugins&gt;</span><br></pre></td></tr></table></figure><h2 id="使用PageHelper"><a href="#使用PageHelper" class="headerlink" title="使用PageHelper"></a>使用PageHelper</h2><p><code>PageHelper</code>拦截的是<code>org.apache.ibatis.executor.Executor</code>的<code>query</code>方法，其传参的核心原理是通过<code>ThreadLocal</code>进行的。当我们需要对某个查询进行分页查询时，我们可以在调用Mapper进行查询前调用一次<code>PageHelper.startPage(..)</code>，这样<code>PageHelper</code>会把分页信息存入一个<code>ThreadLocal</code>变量中。在拦截到<code>Executor</code>的<code>query</code>方法执行时会从对应的<code>ThreadLocal</code>中获取分页信息，获取到了，则进行分页处理，处理完了后又会把<code>ThreadLocal</code>中的分页信息清理掉，以便不影响下一次的查询操作。所以当我们使用了<code>PageHelper.startPage(..)</code>后，每次将对最近一次的查询进行分页查询，如果下一次查询还需要进行分页查询，需要重新进行一次<code>PageHelper.startPage(..)</code>。这样就做到了在引入了分页后可以对原来的查询代码没有任何的侵入性。此外，在进行分页查询时，我们的返回结果一般是一个<code>java.util.List</code>，<code>PageHelper</code>分页查询后的结果会变成<code>com.github.pagehelper.Page</code>类型，其继承了<code>java.util.ArrayList</code>，所以不会对我们的方法声明造成影响。<code>com.github.pagehelper.Page</code>中包含有返回结果的分页信息，包括总记录数，总的分页数等信息，所以一般我们需要把返回结果强转为<code>com.github.pagehelper.Page</code>类型。以下是一个简单的使用<code>PageHelper</code>进行分页查询的示例代码。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">public class PageHelperTest &#123;</span><br><span class="line"></span><br><span class="line">private static SqlSessionFactory sqlSessionFactory;</span><br><span class="line">private SqlSession session;</span><br><span class="line"></span><br><span class="line">@BeforeClass</span><br><span class="line">public static void beforeClass() throws IOException &#123;</span><br><span class="line">InputStream is = Resources.getResourceAsStream(&quot;mybatis-config-single.xml&quot;);</span><br><span class="line">sqlSessionFactory = new SqlSessionFactoryBuilder().build(is);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Before</span><br><span class="line">public void before() &#123;</span><br><span class="line">this.session = sqlSessionFactory.openSession();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@After</span><br><span class="line">public void after() &#123;</span><br><span class="line">this.session.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Test</span><br><span class="line">public void test() &#123;</span><br><span class="line">int pageNum = 2;//页码，从1开始</span><br><span class="line">int pageSize = 10;//每页记录数</span><br><span class="line">PageHelper.startPage(pageNum, pageSize);//指定开始分页</span><br><span class="line">UserMapper userMapper = this.session.getMapper(UserMapper.class);</span><br><span class="line">List&lt;User&gt; all = userMapper.findAll();</span><br><span class="line">Page&lt;User&gt; page = (Page&lt;User&gt;) all;</span><br><span class="line">System.out.println(page.getPages());</span><br><span class="line">System.out.println(page);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上是通过<code>PageHelper.startPage(..)</code>传递分页信息的示例，其实<code>PageHelper</code>还支持Mapper参数传递分页信息等其它用法。关于<code>PageHelper</code>的更多用法和配置信息等请参考该项目的GitHub<a href="https://github.com/pagehelper/Mybatis-PageHelper">官方文档</a>。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;介绍Mybatis的插件，以及如何通过Mybatis的插件功能实现一个自定义的分页插件。前段时间遇到了一款开源的Mybatis分页插件，叫&lt;code&gt;PageHelper&lt;/code&gt;，github地址是 &lt;a href=&quot;https://github.com/pagehelper/Mybatis-PageHelper&quot;&gt;https://github.com/pagehelper/Mybatis-PageHelper&lt;/a&gt;   其原理是通过&lt;code&gt;ThreadLocal&lt;/code&gt;来存放分页信息，从而可以做到在Service层实现无侵入性的Mybatis分页。笔者感觉还不错，所以特意发博文记录一下，并推荐给大家。&lt;/p&gt;</summary>
    
    
    
    <category term="Java" scheme="http://xubatian.cn/categories/Java/"/>
    
    
    <category term="Java" scheme="http://xubatian.cn/tags/Java/"/>
    
    <category term="Mybatis" scheme="http://xubatian.cn/tags/Mybatis/"/>
    
  </entry>
  
  <entry>
    <title>mybatis简介</title>
    <link href="http://xubatian.cn/mybatis%E7%AE%80%E4%BB%8B/"/>
    <id>http://xubatian.cn/mybatis%E7%AE%80%E4%BB%8B/</id>
    <published>2022-02-07T13:41:56.000Z</published>
    <updated>2022-02-07T13:51:26.032Z</updated>
    
    <content type="html"><![CDATA[<p>Mybatis框架简介</p><span id="more"></span><h1 id="MyBatis介绍"><a href="#MyBatis介绍" class="headerlink" title="MyBatis介绍"></a>MyBatis介绍</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>原是apache的一个开源项目iBatis，2010年6月这个项目由apache software foundation 迁移到了google code，随着开发团队转投Google Code旗下，ibatis3.x正式更名为Mybatis ，代码于2013年11月迁移到Github。<br>相对Hibernate和ApacheOJB等“一站式”ORM（Object Mapping）解决方案而言，ibatis 是一种“半自动化”的ORM实现。Relational<br>无论 Hibernate还是Apache OJB，都对数据库结构提供了较为完整的封装，提供了从POJO到数据库表的全套映射机制。程序员往往只需定义好了POJO 到数据库表的映射关系，即可通过 Hibernate或者OJB 提供的方法完成持久层操作。程序员甚至不需要对 SQL 的熟练掌握，Hibernate/OJB 会根据制定的存储逻辑，自动生成对应的 SQL 并调用 JDBC 接口加以执行。</p><h2 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a>官方文档</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207214544.png" alt="博客:www.xubatian.cn"></p><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p>1）支持自定义SQL、存储过程、及高级映射<br>2）实现自动对SQL的参数设置<br>3）实现自动对结果集进行解析和封装<br>4）通过XML或者注解进行配置和映射，大大减少代码量<br>5）数据源的连接信息通过配置文件进行配置</p><p>可以发现，MyBatis是对JDBC进行了简单的封装，帮助用户进行SQL参数的自动设置，以及结果集与Java对象的自动映射。与Hibernate相比，配置更加简单、灵活、执行效率高。但是正因为此，所以没有实现完全自动化，需要手写SQL，这是优点也是缺点。</p><p>因此，对性能要求较高的电商类项目，一般会使用MyBatis，而对与业务逻辑复杂，不太在乎执行效率的传统行业，一般会使用Hibernate</p><h2 id="Mybaits整体架构"><a href="#Mybaits整体架构" class="headerlink" title="Mybaits整体架构"></a>Mybaits整体架构</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207214710.png" alt="博客:www.xubatian.cn"></p><p>1、配置文件<br>全局配置文件：mybatis-config.xmlhibernate.cfg.xml，作用：配置数据源，引入映射文件<br>映射文件：XxMapper.xmlxx.hbm.xml，作用：配置sql语句、参数、结果集封装类型等</p><p>2、SqlSessionFactory<br>相当于Hibernate的SessionFactory，作用：获取SqlSession<br>通过newSqlSessionFactoryBuilder().build(inputStream)来构建，inputStream：读取配置文件的IO流</p><p>3、SqlSession<br>相当于Hibernate的Session，作用：执行CRUD操作</p><p>4、Executor<br>执行器，SqlSession通过调用它来完成具体的CRUD<br>它是一个接口，提供了两种实现：缓存的实现、数据库的实现</p><p>5、Mapped Statement<br>在映射文件里面配置，包含3部分内容：<br>具体的sql，sql执行所需的参数类型，sql执行结果的封装类型<br>参数类型和结果集封装类型包括3种：<br>HashMap，基本数据类型，pojo</p><p>…….未完待续</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Mybatis框架简介&lt;/p&gt;</summary>
    
    
    
    <category term="Java" scheme="http://xubatian.cn/categories/Java/"/>
    
    
    <category term="Java" scheme="http://xubatian.cn/tags/Java/"/>
    
    <category term="Mybatis" scheme="http://xubatian.cn/tags/Mybatis/"/>
    
  </entry>
  
  <entry>
    <title>Flink使用idea将匿名内部类替换为lambda表达会擦除泛型</title>
    <link href="http://xubatian.cn/Flink%E4%BD%BF%E7%94%A8idea%E5%B0%86%E5%8C%BF%E5%90%8D%E5%86%85%E9%83%A8%E7%B1%BB%E6%9B%BF%E6%8D%A2%E4%B8%BAlambda%E8%A1%A8%E8%BE%BE%E4%BC%9A%E6%93%A6%E9%99%A4%E6%B3%9B%E5%9E%8B/"/>
    <id>http://xubatian.cn/Flink%E4%BD%BF%E7%94%A8idea%E5%B0%86%E5%8C%BF%E5%90%8D%E5%86%85%E9%83%A8%E7%B1%BB%E6%9B%BF%E6%8D%A2%E4%B8%BAlambda%E8%A1%A8%E8%BE%BE%E4%BC%9A%E6%93%A6%E9%99%A4%E6%B3%9B%E5%9E%8B/</id>
    <published>2022-02-07T06:12:57.000Z</published>
    <updated>2022-02-07T09:16:00.327Z</updated>
    
    <content type="html"><![CDATA[<p>Flink代码使用IDEA将new匿名内部类替换为lambda表达式运行会报错,因为替换后会擦除泛型</p><p>测试Flink1.12.0没有出现返回值类型报错问题. 不知道是否是Flink高版本结局了此问题还是idea高版本解决了此问题.</p><span id="more"></span><h2 id="源码地址-™"><a href="#源码地址-™" class="headerlink" title="源码地址:™"></a>源码地址:™</h2><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo01/Flink_WordCount_Bounded.java">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo01/Flink_WordCount_Bounded.java</a></p><h2 id="报错类型示例"><a href="#报错类型示例" class="headerlink" title="报错类型示例"></a>报错类型示例</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207141902.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207141937.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207142024.png" alt="博客:www.xubatian.cn"></p><h2 id="返回值类型报错"><a href="#返回值类型报错" class="headerlink" title="返回值类型报错"></a>返回值类型报错</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207142213.png" alt="博客:www.xubatian.cn"></p><h2 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式:"></a>解决方式:</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207142525.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207144407.png" alt="博客:www.xubatian.cn"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Flink代码使用IDEA将new匿名内部类替换为lambda表达式运行会报错,因为替换后会擦除泛型&lt;/p&gt;
&lt;p&gt;测试Flink1.12.0没有出现返回值类型报错问题. 不知道是否是Flink高版本结局了此问题还是idea高版本解决了此问题.&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>2022年,二月,你好!</title>
    <link href="http://xubatian.cn/%E4%BA%8C%E6%9C%88-%E4%BD%A0%E5%A5%BD/"/>
    <id>http://xubatian.cn/%E4%BA%8C%E6%9C%88-%E4%BD%A0%E5%A5%BD/</id>
    <published>2022-01-31T23:30:30.000Z</published>
    <updated>2022-02-10T02:54:12.479Z</updated>
    
    <content type="html"><![CDATA[<p>二月了呀! 要加油了…..</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220210105354.png" alt="博客:www.xubatian.cn"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;二月了呀! 要加油了…..&lt;/p&gt;</summary>
    
    
    
    <category term="动态" scheme="http://xubatian.cn/categories/%E5%8A%A8%E6%80%81/"/>
    
    
    <category term="动态" scheme="http://xubatian.cn/tags/%E5%8A%A8%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>Flink流处理案例</title>
    <link href="http://xubatian.cn/Flink%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/"/>
    <id>http://xubatian.cn/Flink%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/</id>
    <published>2022-01-31T09:02:26.000Z</published>
    <updated>2022-02-07T06:01:54.853Z</updated>
    
    <content type="html"><![CDATA[<p>简单Flink流处理案例</p><p>①自定义flatmap操作</p><p>② new 匿名内部类接口 操作</p><p>③简单的算子运用: FlatMap压平, Map转换,Tuple2二元组.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在Java中,二元组Tuple就是<span class="number">2</span>,即Tuple2;三元组就是<span class="number">3</span>,即Tuple3</span><br><span class="line">元组索引从<span class="number">0</span>开始</span><br></pre></td></tr></table></figure><span id="more"></span><h2 id="源代码地址"><a href="#源代码地址" class="headerlink" title="源代码地址"></a>源代码地址</h2><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/tree/main/flink-1.12.0-Demo">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/tree/main/flink-1.12.0-Demo</a></p><h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220131173631.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207125512.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220131173834.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220131174208.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220131180631.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207130501.png" alt="博客:www.xubatian.cn"></p><h2 id="补充另一种写法-new-匿名内部类接口"><a href="#补充另一种写法-new-匿名内部类接口" class="headerlink" title="补充另一种写法: new 匿名内部类接口"></a>补充另一种写法: new 匿名内部类接口</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207131308.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207135330.png" alt="博客:www.xubatian.cn"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;简单Flink流处理案例&lt;/p&gt;
&lt;p&gt;①自定义flatmap操作&lt;/p&gt;
&lt;p&gt;② new 匿名内部类接口 操作&lt;/p&gt;
&lt;p&gt;③简单的算子运用: FlatMap压平, Map转换,Tuple2二元组.&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;在Java中,二元组Tuple就是&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,即Tuple2;三元组就是&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,即Tuple3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;元组索引从&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;开始&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>《说书人》朱广权+尼格买提版</title>
    <link href="http://xubatian.cn/%E8%AF%B4%E4%B9%A6%E4%BA%BA/"/>
    <id>http://xubatian.cn/%E8%AF%B4%E4%B9%A6%E4%BA%BA/</id>
    <published>2022-01-28T14:33:41.000Z</published>
    <updated>2022-02-07T16:23:15.288Z</updated>
    
    <content type="html"><![CDATA[<p>侠义多是平凡辈 , 无须仗剑走天涯。——朱广权</p><span id="more"></span><div style="position: relative; padding: 30% 45%;"><iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/%E3%80%90%E5%89%8D%E6%96%B9%E9%AB%98%E7%87%83%E3%80%91%E5%BD%93%E5%94%A2%E5%91%90%E9%81%87%E8%A7%81%E7%94%B5%E9%9F%B3%EF%BC%81%E6%9C%B1%E5%B9%BF%E6%9D%83-%E5%B0%BC%E6%A0%BC%E4%B9%B0%E6%8F%90%E7%89%88%E3%80%8A%E8%AF%B4%E4%B9%A6%E4%BA%BA%E3%80%8B.mp4" frameborder="no" scrolling="no"></iframe></div> ]]></content>
    
    
    <summary type="html">&lt;p&gt;侠义多是平凡辈 , 无须仗剑走天涯。——朱广权&lt;/p&gt;</summary>
    
    
    
    <category term="轻松一刻" scheme="http://xubatian.cn/categories/%E8%BD%BB%E6%9D%BE%E4%B8%80%E5%88%BB/"/>
    
    
    <category term="轻松一刻" scheme="http://xubatian.cn/tags/%E8%BD%BB%E6%9D%BE%E4%B8%80%E5%88%BB/"/>
    
  </entry>
  
  <entry>
    <title>到底是什么使我心态不稳?</title>
    <link href="http://xubatian.cn/%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%E6%98%AF%E6%88%91%E5%BF%83%E6%80%81%E4%B8%8D%E7%A8%B3/"/>
    <id>http://xubatian.cn/%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%E6%98%AF%E6%88%91%E5%BF%83%E6%80%81%E4%B8%8D%E7%A8%B3/</id>
    <published>2022-01-28T05:47:56.000Z</published>
    <updated>2022-01-28T07:46:01.670Z</updated>
    
    <content type="html"><![CDATA[<p>编译spark源码的三天时间,我到底经历了什么?</p><span id="more"></span><p>原本我是打算使用本地进行编译的…结果撑了一天,不行了.一大波bug正在赶来….</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128135223.png" alt="博客: www.xubatian.cn"></p><p>这种bug,我解决了不下二十个. 最关键的是,这种报错都是些jar包下载不下来….</p><div align="center">    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128135425.png" alt="博客: www.xubatian.cn"></img></div><p>想想还是算了吧,换服务器编译吧… 在准备了一堆maven,scala等一堆环境变量之后,终于走上了编译之路.</p><p>打死我都没想,这条路黑暗了我两天美好的人生….</p><p>再哭一下…..</p><div align="center">    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128135700.png" alt="博客: www.xubatian.cn"></img></div><p>—————————————————————– 以下是我最常遇到的bug  , 经常光顾我 ,也是老熟人了  ———————————————————–</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128135009.png" alt="博客: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128135024.png" alt="博客: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128140050.png" alt="博客: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128144435.png" alt="博客: www.xubatian.cn"></p><p>阳光总在风雨后…</p><p>历经三天的折磨,终于编译出了合适我hadoop版本的spark…..</p><p>此处咧嘴大笑….</p><div align="center">    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128141051.png" alt="博客: www.xubatian.cn"></img></div><div align="center">    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/tempImage1643355914082.gif" alt="博客: www.xubatian.cn"></img></div><p>炫耀版的展示一下….嘻嘻…</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/www.xubatian.cn_400.png" alt="博客: www.xubatian.cn"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;编译spark源码的三天时间,我到底经历了什么?&lt;/p&gt;</summary>
    
    
    
    <category term="动态" scheme="http://xubatian.cn/categories/%E5%8A%A8%E6%80%81/"/>
    
    
    <category term="动态" scheme="http://xubatian.cn/tags/%E5%8A%A8%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>streamx源码编译及安装部署-本地编译(推荐)</title>
    <link href="http://xubatian.cn/streamx%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-%E6%9C%AC%E5%9C%B0%E7%BC%96%E8%AF%91(%E6%8E%A8%E8%8D%90)/"/>
    <id>http://xubatian.cn/streamx%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-%E6%9C%AC%E5%9C%B0%E7%BC%96%E8%AF%91(%E6%8E%A8%E8%8D%90)/</id>
    <published>2022-01-22T12:21:54.000Z</published>
    <updated>2022-01-23T03:05:51.541Z</updated>
    
    <content type="html"><![CDATA[<p>锚定既定奋斗目标，意气风发走向未来。——人民日报</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_394.jpg" alt="blog: www.xubatian.cn"></p><p>在服务器端进行了streamx编译的那文章也说了,我没有flink,hadoop 的配置,所以我重新进行了streamx的源码编译,版本依旧是streamx-1.2.0 稳定版本</p><p><strong>现在编译的是Flink版本为1.14.3 , hadoop版本为3.1.3.</strong></p><h1 id="源码编译的前提条件"><a href="#源码编译的前提条件" class="headerlink" title="源码编译的前提条件"></a>源码编译的前提条件</h1><p>我使用的是maven 3.8.3版本. node js , jdk1.8.3 ,npm. </p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/20220122203333.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/20220122203432.png" alt="blog: www.xubatian.cn"></p><h1 id="streamx源码编译"><a href="#streamx源码编译" class="headerlink" title="streamx源码编译"></a>streamx源码编译</h1><h2 id="从官网下载streamx稳定版本"><a href="#从官网下载streamx稳定版本" class="headerlink" title="从官网下载streamx稳定版本"></a>从官网下载streamx稳定版本</h2><p>官网地址: <a href="https://www.streamxhub.com/#">https://www.streamxhub.com/#</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_391.png" alt="blog: www.xubatian.cn"></p><p>github地址: <a href="https://github.com/streamxhub/streamx">https://github.com/streamxhub/streamx</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_390.png" alt="blog: www.xubatian.cn"></p><h2 id="修改streamx的相关版本"><a href="#修改streamx的相关版本" class="headerlink" title="修改streamx的相关版本"></a>修改streamx的相关版本</h2><p>修改为公司hadoop,flink,spark等相符合的大数据组件版本. </p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122201007.png" alt="blog: www.xubatian.cn"></p><h2 id="编译streamx源码"><a href="#编译streamx源码" class="headerlink" title="编译streamx源码"></a>编译streamx源码</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/20220122203910.png" alt="blog: www.xubatian.cn"></p><h2 id="报错及解决方式"><a href="#报错及解决方式" class="headerlink" title="报错及解决方式"></a>报错及解决方式</h2><p>常常报错的问题就是 jar包下载不下来. 或者maven镜像无法来取jar包. </p><p>解决方式:</p><p>① 看看是那个jar包下载不下来. </p><p>② 复制该jar名称,去maven中央仓库直接下载版本相同的jar</p><p>maven中央仓库地址: <a href="https://mvnrepository.com/">https://mvnrepository.com/</a> </p><p>注: 也可以直接放到谷歌浏览器上直接搜索.</p><p>③ 删除之前编译残留的文件,将下载好的jar包拷贝到本地maven仓库(注意:一定要放到指定的文件夹下)</p><p>④ 然后重新编译</p><h3 id="如下是示例"><a href="#如下是示例" class="headerlink" title="如下是示例:"></a>如下是示例:</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122194838.png" alt="blog: www.xubatian.cn"></p><h3 id="查看本地maven仓库-删除全部残余文件"><a href="#查看本地maven仓库-删除全部残余文件" class="headerlink" title="查看本地maven仓库,删除全部残余文件"></a>查看本地maven仓库,删除全部残余文件</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122195038.png" alt="blog: www.xubatian.cn"></p><h3 id="下载版本一样的jar包"><a href="#下载版本一样的jar包" class="headerlink" title="下载版本一样的jar包"></a>下载版本一样的jar包</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122195126.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122195208.png" alt="blog: www.xubatian.cn"></p><p>将下载好的jar拷贝至本地maven仓库</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122195342.png" alt="blog: www.xubatian.cn"></p><h3 id="重新编译"><a href="#重新编译" class="headerlink" title="重新编译"></a>重新编译</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/20220122203910.png" alt="blog: www.xubatian.cn"></p><h2 id="反复经过N次上述行为后-恭喜你-成功了"><a href="#反复经过N次上述行为后-恭喜你-成功了" class="headerlink" title="反复经过N次上述行为后,恭喜你,成功了"></a>反复经过N次上述行为后,恭喜你,成功了</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122200810.png" alt="blog: www.xubatian.cn"></p><h2 id="在此目录下的压缩包拷贝至服务器上解压"><a href="#在此目录下的压缩包拷贝至服务器上解压" class="headerlink" title="在此目录下的压缩包拷贝至服务器上解压"></a>在此目录下的压缩包拷贝至服务器上解压</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122201341.png" alt="blog: www.xubatian.cn"></p><p>后缀是macOS的是我在本地进行编译的. 后缀是Linux的是我在服务器端编译的.</p><p>二者的区别就是 macOS端的我修改了hadoop版本为3.1.3 ,Flink版本为1.14.3</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/20220122205512.png" alt="blog: www.xubatian.cn"></p><h2 id="streamx部署"><a href="#streamx部署" class="headerlink" title="streamx部署"></a>streamx部署</h2><p>请看我的另一篇文章: <a href="https://www.xubatian.cn/streamx%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E7%BC%96%E8%AF%91/">streamx源码编译及安装部署-服务器端编译</a></p><h2 id="启动streamx"><a href="#启动streamx" class="headerlink" title="启动streamx"></a>启动streamx</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122201928.png" alt="blog: www.xubatian.cn"></p><h2 id="访问Web页面"><a href="#访问Web页面" class="headerlink" title="访问Web页面"></a>访问Web页面</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122202012.png" alt="blog: www.xubatian.cn"></p><h2 id="编译好的streamx存放地址"><a href="#编译好的streamx存放地址" class="headerlink" title="编译好的streamx存放地址"></a>编译好的streamx存放地址</h2><p>streamx1.12.0 源码默认配置 直接编译 编译后的压缩包为: streamx-console-service-1.2.0-Linux-bin.tar.gz<br>streamx1.12.0 源码,修改hadoop版本为3.1.3, flink版本为1.14.3 编译后的压缩包为: streamx-console-service-1.2.0-macOS-bin.tar.gz<br>两个压缩包都方式这里,链接: <a href="https://pan.baidu.com/s/1M4R0K3rOzNZOdilnJiduFw">https://pan.baidu.com/s/1M4R0K3rOzNZOdilnJiduFw</a> 提取码: 2olc</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;锚定既定奋斗目标，意气风发走向未来。——人民日报&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="streamx" scheme="http://xubatian.cn/tags/streamx/"/>
    
  </entry>
  
  <entry>
    <title>streamx源码编译及安装部署-服务器端编译</title>
    <link href="http://xubatian.cn/streamx%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E7%BC%96%E8%AF%91/"/>
    <id>http://xubatian.cn/streamx%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E7%BC%96%E8%AF%91/</id>
    <published>2022-01-22T06:38:59.000Z</published>
    <updated>2022-01-23T03:05:56.394Z</updated>
    
    <content type="html"><![CDATA[<p>遇到问题，改变苛求别人的惯性，重新塑造思考问题的方式。换个角度看世界，换个方向看问题，就会豁然开朗。——人民日报</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_387.jpg" alt="blog: www.xubatian.cn"></p><h1 id="什么是streamx"><a href="#什么是streamx" class="headerlink" title="什么是streamx"></a>什么是streamx</h1><p> 大数据技术如今发展的如火如荼，已经呈现百花齐放欣欣向荣的景象，实时处理流域 Apache Spark 和 Apache Flink 更是一个伟大的进步，尤其是 Apache Flink 被普遍认为是下一代大数据流计算引擎， 我们在使用 Flink 时发现从编程模型， 启动配置到运维管理都有很多可以抽象共用的地方， 我们将一些好的经验固化下来并结合业内的最佳实践， 通过不断努力终于诞生了今天的框架 —— StreamX， 项目的初衷是 —— 让 Flink 开发更简单， 使用 StreamX 开发，可以极大降低学习成本和开发门槛， 让开发者只用关心最核心的业务， StreamX 规范了项目的配置，鼓励函数式编程，定义了最佳的编程方式，提供了一系列开箱即用的 Connectors ，标准化了配置、开发、测试、部署、监控、运维的整个过程， 提供 Scala 和 Java 两套api， 其最终目的是打造一个一站式大数据平台，流批一体，湖仓一体的解决方案.</p><h1 id="源码编译的前提条件"><a href="#源码编译的前提条件" class="headerlink" title="源码编译的前提条件"></a>源码编译的前提条件</h1><p>我使用的是CentOS Linux release 7.5.1804 (Core).  mysql5.7. 以及maven 3.8.3版本. node js 和 jdk1.8.3 .最少2个多G的磁盘空间</p><p>   <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122150459.png" alt="blog: www.xubatian.cn"></p><h2 id="安装node-js"><a href="#安装node-js" class="headerlink" title="安装node-js"></a>安装node-js</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ wget https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz</span><br></pre></td></tr></table></figure><h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ tar xf node-v16.13.1-linux-x64.tar.xz </span><br></pre></td></tr></table></figure><h3 id="进入解压目录"><a href="#进入解压目录" class="headerlink" title="进入解压目录"></a>进入解压目录</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ cd node-v16.13.1-linux-x64</span><br></pre></td></tr></table></figure><h3 id="修改Linux环境变量"><a href="#修改Linux环境变量" class="headerlink" title="修改Linux环境变量"></a>修改Linux环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ vim /etc/profile.d/shangbaishuyao_configurationfile.sh</span><br><span class="line"><span class="meta">#</span><span class="bash">JAVA_HOME 加<span class="built_in">export</span>是对全局有效,相当于对外暴露一个接口</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">HADOOP_HOME</span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">KAFKA_HOME</span></span><br><span class="line">export KAFKA_HOME=/opt/module/kafka_2.11-2.4.1</span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">HIVE_HOME</span></span><br><span class="line">export HIVE_HOME=/opt/module/hive-3.1.2</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">MAVEN_HOME</span></span><br><span class="line">export MAVEN_HOME=/opt/module/maven-3.8.3</span><br><span class="line">export MAVEN_HOME</span><br><span class="line">export PATH=$PATH:$MAVEN_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> SPARK_HOME</span></span><br><span class="line">export SPARK_HOME=/opt/module/spark-3.0.0-hadoop3.2</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> NODE_JS</span></span><br><span class="line">export NODE_HOME=/opt/module/node-v16.13.1-linux-x64</span><br><span class="line">export PATH=$PATH:$NODE_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">HBASE_HOME</span></span><br><span class="line">export HBASE_HOME=/opt/module/hbase-2.0.5</span><br><span class="line">export PATH=$PATH:$HBASE_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">FLINK_HOME</span></span><br><span class="line">export FLINK_HOME=/opt/module/flink-1.14.3</span><br><span class="line">export PATH=$PATH:$FLINK_HOME/bin</span><br><span class="line"></span><br><span class="line">export HADOOP_CLASSPATH=`hadoop classpath`</span><br><span class="line">[shangbaishuyao@hadoop102 module]$</span><br></pre></td></tr></table></figure><h3 id="刷新环境变量"><a href="#刷新环境变量" class="headerlink" title="刷新环境变量"></a>刷新环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ source /etc/profile.d/shangbaishuyao_configurationfile.sh</span><br></pre></td></tr></table></figure><h3 id="查看是否安装成功"><a href="#查看是否安装成功" class="headerlink" title="查看是否安装成功"></a>查看是否安装成功</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ node -v</span><br><span class="line">v16.13.1</span><br></pre></td></tr></table></figure><h2 id="安装maven"><a href="#安装maven" class="headerlink" title="安装maven"></a>安装maven</h2><p>网上例子很多,此处略.</p><p>参考文章: <a href="https://www.cnblogs.com/freeweb/p/5241013.html">https://www.cnblogs.com/freeweb/p/5241013.html</a></p><h2 id="安装npm"><a href="#安装npm" class="headerlink" title="安装npm"></a>安装npm</h2><p>直接安装好node.js就有npm命令了,此处略. 有个问题是. 我在编译streamx的时候因为npm版本过低所以失败三次. 所以我升级了npm命令为: npm install -g npm</p><h2 id="安装JDK"><a href="#安装JDK" class="headerlink" title="安装JDK"></a>安装JDK</h2><p>网上例子很多,此处略.</p><h2 id="安装mysql"><a href="#安装mysql" class="headerlink" title="安装mysql"></a>安装mysql</h2><p>网上例子很多,此处略.</p><h3 id="进入mysql修改配置"><a href="#进入mysql修改配置" class="headerlink" title="进入mysql修改配置"></a>进入mysql修改配置</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 streamx-release-1.2.0]$ vim /etc/my.cnf</span><br><span class="line"></span><br><span class="line">#streamx</span><br><span class="line">port=3306</span><br><span class="line">bind-address=0.0.0.0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122153152.png" alt="blog: www.xubatian.cn"></p><h3 id="重新启动mysql"><a href="#重新启动mysql" class="headerlink" title="重新启动mysql"></a>重新启动mysql</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql restart</span><br></pre></td></tr></table></figure><h1 id="streamx源码编译"><a href="#streamx源码编译" class="headerlink" title="streamx源码编译"></a>streamx源码编译</h1><h2 id="从官网下载streamx稳定版本"><a href="#从官网下载streamx稳定版本" class="headerlink" title="从官网下载streamx稳定版本"></a>从官网下载streamx稳定版本</h2><p>官网地址: <a href="https://www.streamxhub.com/#">https://www.streamxhub.com/#</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_391.png" alt="blog: www.xubatian.cn"></p><p>github地址: <a href="https://github.com/streamxhub/streamx">https://github.com/streamxhub/streamx</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_390.png" alt="blog: www.xubatian.cn"></p><h2 id="修改streamx的相关版本"><a href="#修改streamx的相关版本" class="headerlink" title="修改streamx的相关版本"></a>修改streamx的相关版本</h2><p>修改为公司hadoop,flink,spark等相符合的大数据组件版本. </p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_392.png" alt="blog: www.xubatian.cn"></p><p>然后将压缩包上传到服务器上并解压.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ unzip streamx-release-1.2.0.zip -d /opt/module/</span><br></pre></td></tr></table></figure><p>进入解压目录编译源码,1.2.0默认flink版本为1.4,如需更改修改pom.xml再进行编译。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 streamx-release-1.2.0]$ mvn clean install -DskipTests -Denv=prod</span><br></pre></td></tr></table></figure><p>等待……</p><p>最后成功!</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_376.png" alt="blog: www.xubatian.cn"></p><h2 id="进行解压编译成功后的压缩包"><a href="#进行解压编译成功后的压缩包" class="headerlink" title="进行解压编译成功后的压缩包"></a>进行解压编译成功后的压缩包</h2><p>编译后在/opt/module/streamx-release-1.2.0/streamx-console/streamx-console-service/target目录会有对应tar包</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_380.png" alt="blog: www.xubatian.cn"></p><h2 id="进入解压后的目录"><a href="#进入解压后的目录" class="headerlink" title="进入解压后的目录"></a>进入解压后的目录</h2><p>进入到对应目录，修改配置文件，需要使用mysql地址来存储数据。</p><p>注意：数据库不会自动创建，需要手动创建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ cd streamx-console-service-1.2.0/</span><br><span class="line">[shangbaishuyao@hadoop102 streamx-console-service-1.2.0]$ vim conf/application.yml</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_381.png" alt="blog: www.xubatian.cn"></p><p>手动创建streamx的数据库</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122153945.png" alt="blog: www.xubatian.cn"></p><h2 id="启动streamx"><a href="#启动streamx" class="headerlink" title="启动streamx"></a>启动streamx</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 streamx-release-1.2.0]$ bin/startup.sh </span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_382.png" alt="blog: www.xubatian.cn"></p><h2 id="查看是否启动成功"><a href="#查看是否启动成功" class="headerlink" title="查看是否启动成功"></a>查看是否启动成功</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_384.png" alt="blog: www.xubatian.cn"></p><p>如果没有streamXconsole说明出现错误. 去logs里面查看具体错误.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_383.png" alt="blog: www.xubatian.cn"></p><h2 id="使用浏览器访问streamx"><a href="#使用浏览器访问streamx" class="headerlink" title="使用浏览器访问streamx"></a>使用浏览器访问streamx</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_385.png" alt="blog: www.xubatian.cn"></p><p>账号为: admin 密码为: streamx</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_386.png" alt="blog: www.xubatian.cn"></p><h2 id="编译好的streamx存放地址"><a href="#编译好的streamx存放地址" class="headerlink" title="编译好的streamx存放地址"></a>编译好的streamx存放地址</h2><p>streamx1.12.0 源码默认配置 直接编译 编译后的压缩包为: streamx-console-service-1.2.0-Linux-bin.tar.gz<br>streamx1.12.0 源码,修改hadoop版本为3.1.3, flink版本为1.14.3 编译后的压缩包为: streamx-console-service-1.2.0-macOS-bin.tar.gz<br>两个压缩包都方式这里,链接: <a href="https://pan.baidu.com/s/1M4R0K3rOzNZOdilnJiduFw">https://pan.baidu.com/s/1M4R0K3rOzNZOdilnJiduFw</a> 提取码: 2olc</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;遇到问题，改变苛求别人的惯性，重新塑造思考问题的方式。换个角度看世界，换个方向看问题，就会豁然开朗。——人民日报&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="streamx" scheme="http://xubatian.cn/tags/streamx/"/>
    
  </entry>
  
  <entry>
    <title>Flink部署</title>
    <link href="http://xubatian.cn/Flink%E9%83%A8%E7%BD%B2/"/>
    <id>http://xubatian.cn/Flink%E9%83%A8%E7%BD%B2/</id>
    <published>2022-01-20T21:39:16.000Z</published>
    <updated>2022-01-23T02:58:21.611Z</updated>
    
    <content type="html"><![CDATA[<p>所处的位置不同，看到的风景和思考的问题也有所不同。——人民日报</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_360.jpg" alt="blog: www.xubatian.cn"></p><h1 id="Standalone模式Flink自带的"><a href="#Standalone模式Flink自带的" class="headerlink" title="Standalone模式Flink自带的"></a>Standalone模式Flink自带的</h1><p>首先运行我们standalone的环境    </p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_361.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_362.png" alt="blog: www.xubatian.cn"></p><p>解压缩  <strong>flink-1.7.2-bin-hadoop27-scala_2.11.tgz</strong>(如果你两个模式都想试一下就这个压缩包,实际上你要真正搭建独立模式只需要flink-1.7.2后面不需要接hadoop27-scala_2.11的压缩包)，进入conf目录中。</p><p>1）修改 flink/conf/flink-conf.yaml 文件：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_363.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_364.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_365.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">为了更好地看清图片内容,我复制出来这两行:</span><br><span class="line"><span class="meta">#</span><span class="bash"> The heap size <span class="keyword">for</span> the TaskManager JVM</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#整个TaskManager的内存大小</span></span></span><br><span class="line">taskmanager.heap.size: 1024m    </span><br><span class="line"></span><br><span class="line">下面是,这一个G的内存可以同时允许你同时并行运行多少个Task,每个task会占用一个插槽</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#插槽即可理解为流水槽,你的水从流水槽上流出去,这个slot就好比是两个木板上的流水槽,这个slot是TaskManager的</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#TaskManager说白了就暂时是我们的worker节点,实际上来说你也可以看成是executor节点也可以.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#但是这里和saprk的standalone去类比的话这就矛盾了,因为flink的TaskManager在这里有相当于executor,也相当</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#于worker.   spark的standalone模式中,一个worker下面可以有多个executor,每一个executor并行可以运行两个</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#以上的任务吗?可以的. 所以我们spark独立模式理解起来有点难度.他是一个worker下面有一个executor,executor下面</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#又可能会运行多个task,也可能只运行一个task.这对初学者不好理解.而这里我们的flink将他简化了,他把worker这</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 一层去掉了,worker这一层就是taskmanager,就还好比是executor一样.这个executor上到底运行了几个task是由</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># slot来决定的.即由插槽来决定的. 这里的插槽设置为1,表示每一个TaskManager上有多少个slot.这里是有1个slot.这就意味着</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#我这个taskmanager上只能同时运行一个任务.你如果想要增加我们的并行度,就必须修改插槽数量.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> taskmanager.numberOfTaskSlots: 1</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改为3个插槽,表示每一个TaskManager里面有三个插槽,这三个插槽可以同时允许运行三个并行度,这三个并行度可以是不一样的广告</span></span><br><span class="line">taskmanager.numberOfTaskSlots: 3</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The parallelism used <span class="keyword">for</span> programs that did not specify and other parallelism.</span></span><br></pre></td></tr></table></figure><p>就只需要配置下面两个文件就可以了</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_366.png" alt="blog: www.xubatian.cn"></p><p>2）修改 /conf/slave文件：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_367.png" alt="blog: www.xubatian.cn"></p><p>3）分发给另外两台机子：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_368.png" alt="blog: www.xubatian.cn"></p><p>4）启动：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_369.png" alt="blog: www.xubatian.cn"></p><p>访问<a href="http://localhost:8081可以对flink集群和任务进行监控管理">http://localhost:8081可以对flink集群和任务进行监控管理</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_370.png" alt="blog: www.xubatian.cn"></p><h2 id="提交任务"><a href="#提交任务" class="headerlink" title="提交任务"></a>提交任务</h2>]]></content>
    
    
    <summary type="html">&lt;p&gt;所处的位置不同，看到的风景和思考的问题也有所不同。——人民日报&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink简介</title>
    <link href="http://xubatian.cn/Flink%E7%AE%80%E4%BB%8B/"/>
    <id>http://xubatian.cn/Flink%E7%AE%80%E4%BB%8B/</id>
    <published>2022-01-19T07:57:22.000Z</published>
    <updated>2022-01-23T02:58:21.611Z</updated>
    
    <content type="html"><![CDATA[<p>一旦时机到来，我们要能迅速地发现时机、把握时机，不犹豫，不踌躇，乘风而起，破万里浪。——人民日报</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_347.jpg" alt="blog: www.xubatian.cn"></p><h1 id="Flink简介"><a href="#Flink简介" class="headerlink" title="Flink简介"></a>Flink简介</h1><p>Flink其中一半是java语言开发的,另一半是scala语言开发的;spark的源码是scala语言开发的.但是scala是基于jvm的,所有的语法直接照着jvm调就可以了    </p><p>大数据中比较重要的框架,Hadoop(mapreduce),Spark,Flink,只有这三个才叫计算框架.<br>Flink,不管你是开发流式计算也好,还是做离线计算也好,还是做批量计算也好,他就是那一套代码,他不像Spark,Spark开发流式计算用的是SparkStreaming,用批量计算使用RDD,sparkCore<br>Flink有一套代码,但是他有很多套API<br>Flink有中文版,你发现Apache顶级项目有中文版的有那几个?他们的特点是什么?<br>因为他们50%以上的代码都是由中国人贡献的,Flink也一样,50%以上的代码是由中国国内贡献的</p><p>Flink和spark类似,他们的数据都是在内存中直接计算的,甚至他的状态都是存在内存中的 </p><p>Flink是默认就是有状态的计算,Flink中没有无状态这个说法但是spark中有无状态这种说法</p><p>flink的kappa架构怎么实现: <a href="https://www.jianshu.com/p/5f5736656bd5">https://www.jianshu.com/p/5f5736656bd5</a><br>Flink数据倾斜问题: <a href="https://www.cnblogs.com/qiu-hua/p/14056747.html">https://www.cnblogs.com/qiu-hua/p/14056747.html</a><br>                  <a href="https://www.cnblogs.com/Christbao/p/13569616.html">https://www.cnblogs.com/Christbao/p/13569616.html</a></p><p>但是实际上大数据量经常出现，一个 Flink 作业包含 200 个 Task 节点，其中有 199 个节点可以在很短的时间内完成计算。但是有一个节点执行时间远超其他结果，并且随着数据量的持续增加，导致该计算节点挂掉，从而整个任务失败重启。<br>我们可以在 Flink 的管理界面中看到任务的某一个 Task 数据量远超其他节点。<br>Flink 任务出现数据倾斜的直观表现是任务节点频繁出现反压，但是增加并行度后并不能解决问题；部分节点出现 OOM 异常，是因为大量的数据集中在某个节点上，导致该节点内存被爆，任务失败重启。</p><h2 id="初识Flink"><a href="#初识Flink" class="headerlink" title="初识Flink"></a>初识Flink</h2><p>Flink起源于Stratosphere项目，Stratosphere是在2010~2014年由3所地处柏林的大学和欧洲的一些其他的大学共同进行的研究项目，2014年4月Stratosphere的代码被复制并捐赠给了Apache软件基金会，参加这个孵化项目的初始成员是Stratosphere系统的核心开发人员，2014年12月，Flink一跃成为Apache软件基金会的顶级项目。<br>在德语中，Flink一词表示快速和灵巧，项目采用一只松鼠的彩色图案作为logo，这不仅是因为松鼠具有快速和灵巧的特点，还因为柏林的松鼠有一种迷人的红棕色，而Flink的松鼠logo拥有可爱的尾巴，尾巴的颜色与Apache软件基金会的logo颜色相呼应，也就是说，这是一只Apache风格的松鼠。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_348.png" alt="blog: www.xubatian.cn">Flink项目的理念是：“Apache Flink是为分布式、高性能、随时可用以及准确的流处理应用程序打造的开源流处理框架”。</p><p>Apache Flink是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。Flink被设计在所有常见的集群环境中运行，以内存执行速度和任意规模来执行计算。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_349.png" alt="blog: www.xubatian.cn"></p><p>博主解析图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_350.png" alt="blog: www.xubatian.cn"></p><h2 id="Flink的重要特点"><a href="#Flink的重要特点" class="headerlink" title="Flink的重要特点"></a>Flink的重要特点</h2><h3 id="事件驱动型-Event-driven"><a href="#事件驱动型-Event-driven" class="headerlink" title="事件驱动型(Event-driven)"></a>事件驱动型(Event-driven)</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_351.png" alt="blog: www.xubatian.cn"></p><p>事件驱动型应用是一类具有状态的应用，它从一个或多个事件流提取数据(其实就是他可以从多个源中读取数据)，并根据到来的事件触发计算(就是来一条数据立马计算不等待(计算是根据业务来的,可以做聚合计算等))、状态更新或其他外部动作。比较典型的就是以kafka为代表的消息队列几乎都是事件驱动型应用。</p><p>与之不同的就是SparkStreaming微批次，如图：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_352.png" alt="blog: www.xubatian.cn"></p><p>事件驱动型：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_353.png" alt="blog: www.xubatian.cn"></p><p>博主解析图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_354.png" alt="blog: www.xubatian.cn"></p><h3 id="事件驱动型应用的优势？"><a href="#事件驱动型应用的优势？" class="headerlink" title="事件驱动型应用的优势？"></a>事件驱动型应用的优势？</h3><p>事件驱动型应用无须查询远程数据库，本地数据访问使得它具有更高的吞吐和更低的延迟。而由于定期向远程持久化存储的 checkpoint 工作可以异步、增量式完成，因此对于正常事件处理的影响甚微。事件驱动型应用的优势不仅限于本地数据访问。传统分层架构下，通常多个应用会共享同一个数据库，因而任何对数据库自身的更改（例如：由应用更新或服务扩容导致数据布局发生改变）都需要谨慎协调。反观事件驱动型应用，由于只需考虑自身数据，因此在更改数据表示或服务扩容时所需的协调工作将大大减少</p><h3 id="流与批的世界观"><a href="#流与批的世界观" class="headerlink" title="流与批的世界观"></a>流与批的世界观</h3><p>有界和无界分别对应的就是批处理和流处理<br><strong>批处理</strong>(就是所谓的有界数据)的特点是有界、持久、大量，非常适合需要访问全套记录才能完成的计算工作，一般用于离线统计。<br><strong>流处理</strong>(就是所谓的无界数据)的特点是无界、实时,  无需针对整个数据集执行操作，而是对通过系统传输的每个数据项执行操作，一般用于实时统计。<br>在spark的世界观中，一切都是由批次组成的，离线数据是一个大批次，而实时数据是由一个一个无限的小批次组成的。<br>而在flink的世界观中，一切都是由流组成的，离线数据是有界限的流，实时数据是一个没有界限的流，这就是所谓的有界流和无界流。</p><p><strong>无界数据流</strong>：无界数据流有一个开始但是没有结束，它们不会在生成时终止并提供数据，必须连续处理无界流，也就是说必须在获取后立即处理event。对于无界数据流我们无法等待所有数据都到达，因为输入是无界的，并且在任何时间点都不会完成。处理无界数据通常要求以特定顺序（例如事件发生的顺序）获取event，以便能够推断结果完整性。</p><p><strong>有界数据流</strong>：有界数据流有明确定义的开始和结束，可以在执行任何计算之前通过获取所有数据来处理有界流，处理有界流不需要有序获取，因为可以始终对有界数据集进行排序，有界流的处理也称为批处理。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_355.png" alt="blog: www.xubatian.cn"></p><p>这种以流为世界观的架构，获得的最大好处就是具有极低的延迟。</p><p>博主解析图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_356.png" alt="blog: www.xubatian.cn"></p><h3 id="分层API"><a href="#分层API" class="headerlink" title="分层API"></a>分层API</h3><p>(对于我们学flink来说,我们三层都必须得会,经常用到的是中间那层DataStream API)<br>Flink他本质上把批量的数据和流数据都看成是流了,所以他本质上是流处理,所以他也可以做批处理,他和saprk相反,spark把所有的数据都看成是批处理了,但是spark也是可以做流处理的<br>记住: DataStream API做流处理,流处理是无界的  DataSetAPI是做批处理是有界的    </p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_357.png" alt="blog: www.xubatian.cn"></p><p>分层API是Flink根据抽象程度,提供的三种不同的API,所谓抽象程度就是看你封装的程度,如果你不怎么封装,那就是底层的API,稍微封装一下就叫中间的API,封装的很厉害就叫高级API.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_358.png" alt="blog: www.xubatian.cn"></p><p>最底层级的抽象仅仅提供了有状态流，它将通过过程函数（Process Function）被嵌入到DataStream API中。底层过程函数（Process Function） 与 DataStream API 相集成，使其可以对某些特定的操作进行底层的抽象，它允许用户可以自由地处理来自一个或多个数据流的事件，并使用一致的容错的状态。除此之外，用户可以注册事件时间并处理时间回调，从而使程序可以处理复杂的计算。</p><p>实际上，大多数应用并不需要上述的底层抽象，而是针对核心API（Core APIs） 进行编程，比如DataStream API（有界或无界流数据）以及DataSet API（有界数据集）。这些API为数据处理提供了通用的构建模块，比如由用户定义的多种形式的转换（transformations），连接（joins），聚合（aggregations），窗口操作（windows）等等。DataSet API 为有界数据集提供了额外的支持，例如循环与迭代。这些API处理的数据类型以类（classes）的形式由各自的编程语言所表示。</p><p>Table API 是以表为中心的声明式编程，其中表可能会动态变化（在表达流数据时）。Table API遵循（扩展的）关系模型：表有二维数据结构（schema）（类似于关系数据库中的表），同时API提供可比较的操作，例如select、project、join、group-by、aggregate等。Table API程序声明式地定义了什么逻辑操作应该执行，而不是准确地确定这些操作代码的看上去如何。</p><p>尽管Table API可以通过多种类型的用户自定义函数（UDF）进行扩展，其仍不如核心API更具表达能力，但是使用起来却更加简洁（代码量更少）。除此之外，Table API程序在执行之前会经过内置优化器进行优化。<br>你可以在表与 DataStream/DataSet 之间无缝切换，以允许程序将 Table API 与 DataStream 以及 DataSet 混合使用。</p><p>Flink提供的最高层级的抽象是 SQL 。这一层抽象在语法与表达能力上与 Table API 类似，但是是以SQL查询表达式的形式表现程序。SQL抽象与Table API交互密切，同时SQL查询可以直接在Table API定义的表上执行。</p><p>目前Flink作为批处理还不是主流，不如Spark成熟，所以DataSet使用的并不是很多。Flink Table API和Flink SQL也并不完善，大多都由各大厂商自己定制。所以我们主要学习DataStream API的使用。实际上Flink作为最接近Google DataFlow模型的实现，是流批统一的观点，所以基本上使用DataStream就可以了。<br>Flink几大模块<br>Flink Table &amp; SQL(还没开发完)<br>Flink Gelly(图计算)<br>Flink CEP(复杂事件处理)</p><p>Flink官方给你提供的连接器,虽然都是flink的,但是他来自两个不同的库,一个来自flink的,一个来自Bahir的</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_359.png" alt="blog: www.xubatian.cn"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;一旦时机到来，我们要能迅速地发现时机、把握时机，不犹豫，不踌躇，乘风而起，破万里浪。——人民日报&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>SparkCore之RDD编程的编程模型</title>
    <link href="http://xubatian.cn/SparkCore%E4%B9%8BRDD%E7%BC%96%E7%A8%8B%E7%9A%84%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/"/>
    <id>http://xubatian.cn/SparkCore%E4%B9%8BRDD%E7%BC%96%E7%A8%8B%E7%9A%84%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/</id>
    <published>2022-01-19T06:50:36.000Z</published>
    <updated>2022-01-23T02:58:21.755Z</updated>
    
    <content type="html"><![CDATA[<p>闲适因为忙碌才获得意义。如果摸鱼成为常态，放松就失去了意义；如果划水占据人生，幸福就会失去方向。               ——人民日报    </p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_340.jpg" alt="blog: www.xubatian.cn"></p><h1 id="RDD编程"><a href="#RDD编程" class="headerlink" title="RDD编程"></a>RDD编程</h1><p>创建RDD ,RDD的转换, RDD的输出</p><h2 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h2><p>在spark中无论是Transformations方法还是Actions方法,我们都要把他们称作算子</p><p>在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的Transformations(转换)定义RDD之后，就可以调用Actions(行动)触发RDD的计算，Action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到Action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。 </p><p>要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行。Driver中定义了一个或多个RDD，并调用RDD上的Action，Executor则执行RDD分区计算任务。</p><p>Actions(行动)算子会真正的去触发job去执行<br>Transformation(转换)算子懒执行<br>所以返回值是RDD类型的是Transformation算子,返回值非RDD类型就是Action算子</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_341.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_342.png" alt="blog: www.xubatian.cn"></p><h2 id="RDD的创建"><a href="#RDD的创建" class="headerlink" title="RDD的创建"></a>RDD的创建</h2><p>在Spark中创建RDD的创建方式可以分为三种：<br>从scala集合中创建RDD；<br>从外部存储创建RDD；<br>从其他RDD创建(这个其实讲的就是转换)。</p><h3 id="从集合中创建"><a href="#从集合中创建" class="headerlink" title="从集合中创建"></a>从集合中创建</h3><p>从集合中创建RDD，Spark主要提供了两种函数：parallelize(并行化)和makeRDD(创建RDD)</p><p>1）使用parallelize()从集合创建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(Array(1,2,3,4,5,6,7,8))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br></pre></td></tr></table></figure><p>2）使用makeRDD()从集合创建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd1 = sc.makeRDD(Array(1,2,3,4,5,6,7,8))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at &lt;console&gt;:24</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_343.png" alt="blog: www.xubatian.cn"></p><h3 id="由外部存储系统的数据集创建"><a href="#由外部存储系统的数据集创建" class="headerlink" title="由外部存储系统的数据集创建"></a>由外部存储系统的数据集创建</h3><p>包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等，</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd2= sc.textFile(&quot;hdfs://hadoop102:9000/RELEASE&quot;)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] = hdfs://hadoop102:9000/RELEASE MapPartitionsRDD[4] at textFile at &lt;console&gt;:24</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="从其他RDD创建"><a href="#从其他RDD创建" class="headerlink" title="从其他RDD创建"></a>从其他RDD创建</h3><p>下面都是…此处略</p><h2 id="RDD的转换"><a href="#RDD的转换" class="headerlink" title="RDD的转换"></a>RDD的转换</h2><p>RDD整体上分为Value类型和Key-Value类型,  K-V形式其实也是value类型<br>eg:   (k,(k,v))是value, 是不是kv? 是,只不过value类型是二元组.<br>如果是单个值, 是value, 如果是形如二元组是K,V.  k,v形式是value类型, 我把整个k,v元组当成整体来看,他就是value类型.  他们两是包含的关系. 所有的RDD都可以看做是value类型, 只过不特殊的我们拎出来,如k-v等等</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_344.png" alt="blog: www.xubatian.cn"></p><h3 id="Value类型"><a href="#Value类型" class="headerlink" title="Value类型"></a>Value类型</h3><p>什么叫做value类型呢?<br>因为他里面传的函数都是操作当前这个RDD里面的元素. 可能是单个元素, 可能是一个分区里面的元素.但是他操作的是里面的数据.<br>什么叫双value类型呢?<br>双value类型它里面传的参数是任意一个RDD.<br>Eg : RDD1.调用一个算子(RDD2)<br>以为之前提过, scala也好,spark也好,他是面向数据处理的. 那这个双value类型就是数学里面的,集合之间的关系. 集合里面有哪些关系呢? 并集, 交叉 ,笛卡尔集</p><h4 id="map-func-案例"><a href="#map-func-案例" class="headerlink" title="map(func)案例"></a>map(func)案例</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. 作用：返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成</span><br><span class="line">2. 需求：创建一个1-10数组的RDD，将所有元素*2形成新的RDD</span><br></pre></td></tr></table></figure><p>  (1)   创建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var source  = sc.parallelize(1 to 10)</span><br><span class="line"></span><br><span class="line">source: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at &lt;console&gt;:24</span><br></pre></td></tr></table></figure><p>（2）打印</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; source.collect()</span><br><span class="line"></span><br><span class="line">res7: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</span><br></pre></td></tr></table></figure><p>（3）将所有元素*2</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val mapadd = source.map(_ * 2)</span><br><span class="line">这个map是算子</span><br><span class="line"></span><br><span class="line">mapadd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at map at &lt;console&gt;:26</span><br></pre></td></tr></table></figure><p>（4）打印最终结果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; mapadd.collect()</span><br><span class="line"></span><br><span class="line">res8: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_345.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_346.png" alt="blog: www.xubatian.cn"></p><h3 id="双Value类型交互"><a href="#双Value类型交互" class="headerlink" title="双Value类型交互"></a>双Value类型交互</h3><p>以后慢慢写……</p><h3 id="Key-Value类型"><a href="#Key-Value类型" class="headerlink" title="Key-Value类型"></a>Key-Value类型</h3><p>以后慢慢写……</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;闲适因为忙碌才获得意义。如果摸鱼成为常态，放松就失去了意义；如果划水占据人生，幸福就会失去方向。               ——人民日报    &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="spark" scheme="http://xubatian.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>SparkCore之RDD概述</title>
    <link href="http://xubatian.cn/SparkCore%E4%B9%8BRDD%E6%A6%82%E8%BF%B0/"/>
    <id>http://xubatian.cn/SparkCore%E4%B9%8BRDD%E6%A6%82%E8%BF%B0/</id>
    <published>2022-01-19T06:22:21.000Z</published>
    <updated>2022-01-23T02:58:21.728Z</updated>
    
    <content type="html"><![CDATA[<p>仰观天宇，时间更加深邃；俯身耕耘，未来无限可能                  ——人民日报    </p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_331.jpg" alt="blog: www.xubatian.cn"></p><p>我们Spark中的stage是按照shuffle来切的.</p><h1 id="RDD概述"><a href="#RDD概述" class="headerlink" title="RDD概述"></a>RDD概述</h1><h2 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h2><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象。代码中是一个抽象类，它代表一个弹性的(分区)、不可变(元素)、可分区、里面的元素可并行计算的集合。</p><p>RDD是抽象类</p><h2 id="RDD的属性"><a href="#RDD的属性" class="headerlink" title="RDD的属性"></a>RDD的属性</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_332.png" alt="blog: www.xubatian.cn"></p><p>（1）一组分区（Partition）,切片和分区是一样的，即数据集的基本组成单位；<br>（2）一个计算每个分区的函数；<br>（3）RDD之间的依赖关系；<br>（4）一个Partitioner，即RDD的分片函数；<br>（5）一个列表，存储存取每个Partition的优先位置（preferred location）。</p><h2 id="RDD特点"><a href="#RDD特点" class="headerlink" title="RDD特点"></a>RDD特点</h2><p>RDD表示只读(不可变性)的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。</p><h2 id="弹性"><a href="#弹性" class="headerlink" title="弹性"></a>弹性</h2><p>存储的弹性：内存与磁盘的自动切换；<br>容错的弹性：数据丢失可以自动恢复；<br>计算的弹性：计算出错重试机制；<br>分片的弹性：可根据需要重新分片。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_333.png" alt="blog: www.xubatian.cn"></p><h2 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h2><p>RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute(计算)函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute(计算)函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute(计算)函数是执行转换逻辑将其他RDD的数据进行转换</p><h2 id="只读"><a href="#只读" class="headerlink" title="只读"></a>只读</h2><p>RDD是只读的（元素不可变），要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。<br>由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了。</p><p>RDD的操作算子包括两类，<br>一类叫做Transformations(转换)，它是用来将RDD进行转化，构建RDD的血缘关系；<br>另一类叫做Actions(行动)，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。</p><h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p>RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，</p><p>依赖包括两种，<br>一种是窄依赖，RDDs之间分区是一一对应的，<br>另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。(一对一,多对多指的是分区) </p><p>这里的一对一,一对多指的是分区</p><p>注意: 依赖和shuffle有关系</p><p>原图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_334.png" alt="blog: www.xubatian.cn"></p><p>博主解读:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_336.png" alt="blog: www.xubatian.cn"></p><p>父RDD中的全部数据被某一个子RDD的某个分区全部拥有我们叫窄依赖</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_337.png" alt="blog: www.xubatian.cn"></p><h2 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h2><p>如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_338.png" alt="blog: www.xubatian.cn"></p><h2 id="CheckPoint"><a href="#CheckPoint" class="headerlink" title="CheckPoint"></a>CheckPoint</h2><p>切断血缘关系后可以从CheckPoint中拿数据</p><p>虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。</p><p>checkPoint将数据持久化了之后他就会切断血缘关系, 因为他认为你持久化到文件当中后这个数据就不会丢了,这个依赖关系就不需要了,就切断了. 因为你是文件嘛, 而且默认一般存在hdfs中,hdfs又默认有三个副本. 所以他觉得你数据不会丢了. 既然不会丢了,依赖关系就不要了, 因为依赖关系就是防止你数据丢了重新计算做数据恢复的.</p><p>但是你缓存到内存当中,你不能将依赖切断, 因为内存当中数据可能会掉的.他可能还要用这个依赖关系重新做计算的.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_339.png" alt="blog: www.xubatian.cn"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;仰观天宇，时间更加深邃；俯身耕耘，未来无限可能                  ——人民日报    &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="spark" scheme="http://xubatian.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark基础解析</title>
    <link href="http://xubatian.cn/spark%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90/"/>
    <id>http://xubatian.cn/spark%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90/</id>
    <published>2022-01-19T03:09:24.000Z</published>
    <updated>2022-02-07T13:27:08.253Z</updated>
    
    <content type="html"><![CDATA[<p> 征途漫漫，惟有奋斗；梦想成真，惟有实干。                     ——人民日报</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_308.jpg" alt="blog: www.xubatian.cn"></p><h1 id="Spark概述"><a href="#Spark概述" class="headerlink" title="Spark概述"></a>Spark概述</h1><h2 id="什么是Spark"><a href="#什么是Spark" class="headerlink" title="什么是Spark"></a>什么是Spark</h2><p>1、定义<br>Spark是一种基于内存的快速、通用、可扩展的大数据分析引擎。<br>2、历史<br>2009年诞生于加州大学伯克利分校AMPLab, 项目采用Scala编写;<br>2010年开源;<br>2013年6月成为Apache孵化项目；<br>2014年2月成为Apache顶级项目。</p><p><strong>xubatian解析:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在Scala当中的map ，reduce这些方法在spark当中同样也有这些方法。</span><br><span class="line"></span><br><span class="line">要知道的是Scala的这些方法是面向的是集合当中做处理的，面向的是数据集合，数组等等这些操作。而spark是面对的是海量数据处理的，他面向的数据分析的什么东西呢？叫分布式数据集。Scala处理的数据在一个集合当中，而spark处理的数据可能跨了很多台机器。因为他是用hdfs来存储的。而hdfs存储的时候不是把所有的数据都放在一台机器上的。而是很多台机器上都有。而spark就是同时处理很多台机器上的事情。所以Scala和spark都有map方法，可能功能上都是一样的，都是把里面每一个元素做一个转变。但是他们面向的数据集不一样，spark面向的数据集时RDD。</span><br><span class="line"></span><br><span class="line">SparkStream和kafka做对接, 你kafka过来的还是一行一行的数据.虽然封装成了Dstream,但是他还是一行一行的数据. 你要做分析转换输出等</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Spark内置模块"><a href="#Spark内置模块" class="headerlink" title="Spark内置模块"></a>Spark内置模块</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_309.png" alt="blog: www.xubatian.cn"></p><p><strong>Spark Core</strong>：实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core中还包含了对弹性分布式数据集(Resilient Distributed DataSet，简称RDD)的API定义；</p><p><strong>Spark SQL</strong>：是Spark用来操作结构化数据的程序包。通过Spark SQL，我们可以使用 SQL或者Apache Hive版本的SQL方言(HQL)来查询数据。Spark SQL支持多种数据源，比如Hive表、Parquet以及JSON等；</p><p><strong>Spark Streaming</strong>：是Spark提供的对实时数据进行流式计算的组件。提供了用来操作数据流的API，并且与Spark Core中的 RDD API高度对应；</p><p><strong>Spark MLlib</strong>：提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能；</p><p>集群管理器：Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计 算。为了实现这样的要求，同时获得最大灵活性，Spark支持在各种集群管理器(Cluster Manager)上运行，包括Hadoop YARN、Apache Mesos，以及Spark自带的一个调度器，叫作独立调度器。</p><p> Spark得到了众多大数据公司的支持，这些公司包括Hortonworks、IBM、Intel、Cloudera、MapR、Pivotal、百度、腾讯、京东、携程、优酷土豆。当前百度的Spark已应用于大搜索、直达号、百度大数据等业务；阿里利用GraphX构建了大规模的图计算和图挖掘系统，实现了很多生产系统的推荐算法；腾讯Spark集群达到8000台的规模，是当前已知的世界上最大的Spark集群。</p><h2 id="Spark特点-DAG"><a href="#Spark特点-DAG" class="headerlink" title="Spark特点(DAG)"></a>Spark特点(DAG)</h2><ol><li><p><strong>快</strong><br>与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。计算的中间结果是存在于内存中的。</p></li><li><p><strong>易用</strong><br>Spark支持Java、Python和Scala的API, 还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的Shell,可以非常方便地在这些Shell中使用Spark集群来验证解决问题的方法。</p></li></ol><ol start="3"><li><strong>通用</strong></li></ol><p>  Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理(Spark Streaming) 、机器学习 (Spark MLlib)和图计算(GraphX).这些不同类型的处理都可以在同一个应用中无缝使用。减少了开发和维护的人力成本和部署平台的物力成本。</p><ol start="4"><li><strong>兼容性</strong><br>Spark可以非常方便地与其他的开源产品进行融合。比如, Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。</li></ol><h2 id="博主补充"><a href="#博主补充" class="headerlink" title="博主补充"></a>博主补充</h2><p>Spark实现了高效的DAG执行引擎。DAG是有向无环图即多个任务之间通过内存来做交互。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_310.png" alt="blog: www.xubatian.cn"></p><p>多个任务之间直接通过内存来做交互。它可以直接将他们串联起来。而我们之前可能需要用到MR1，MR2，MR3进行落盘操作。</p><p>另一个spark快的原因是：<br>                对于MR来说，你整个Map任务和Reduce任务是计算的核心。而map任务和reduce任务你用jps能看到进程吗？不能。也看不到spark当中的maptask和reducetask。这也是spark比mr快的一个比较核心的一个点。一个呢，对于hadoop来说他是使用进程来调度的。我启动一个单独的task都是一个单独的进程。你能jps看到的是进程号。而在spark当中启动一个任务他是线程。你说是调用进程快呢还是线程快呢？我线程我可以事先启动好一个线程池。我要的时候去取一下就完了。嘿嘿~~阴险。这也是spark比mr快的一个很重要的一个点。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_311.png" alt="blog: www.xubatian.cn"></p><h1 id="Spark运行模式"><a href="#Spark运行模式" class="headerlink" title="Spark运行模式"></a>Spark运行模式</h1><p>重点local模式,和yarn模式,为什么不掌握spark自己的呢?<br>第一重点:<br>本地模式主要是用于教学和测试.公司当中的一些demo级别的测试也是用本地模式.因为他相对来说,资源消耗等等都比较简单一点.local模式相对来说简单一点.不需要启动很多进程去占用额外的资源<br>第二重点:<br>Yarn模式,这个是应用于公司的生产环境.为什么公司的生产环境会用到yarn模式呢?稍微结合standlone模式思考一下.standlone是spark他自己来管理这一套资源.而公司当中其实并不愿意采用standlone模式,而是采用yarn模式比较多.为什么公司当中不用呢?既然spark自己有一套独立的调度资源系统,那你说他和standlone模式兼容性更好还是yarn模式兼容性更好呢?肯定是standlone。因为这是他自己的。那为什么兼容性更好却不用呢？说明他两又有区别，而且区别在公司当中standlone模式比yarn模式更严重一点。我们的mapreduce是yarn分配资源的，我们学过的tez也是yarn分配资源的。Storm也是yarn分配资源的。如果说我们spark也用yarn分配调度资源有什么好处呢？是不是统一的资源调度呢呀！<br>如果说我们spark当中使用独立的一套呢？会产生资源争抢。因为yarn认为这块资源是我独有的，而spark的standlone也认为这块资源是我独有的，那我分配的时候有可能两个任务就冲突了。但是我交给某一个人统一的安排这个资源，不行就等待，就不会产生资源争抢的问题。这个就是公司当中用yarn模式做的一个点。<br>Yarn模式在生产环境中用的比较多。主要体现在中小型公司。他整个集群资源规模不大，他整个MR任务，spark任务，或者其他任务都是运行在同一套资源上的。如果告诉你你公司比较有钱，你的spark集群是独立的spark集群。那么我们就用spark的standlone模式。<br>但是绝大多数公司他的整个集群都是资源混布的。这就比较依赖与统一的资源管理了。这样就不至于产生资源争抢。<br>我们所讲的几种模式都是在Liunx环境当中开一个shell窗口。类似于之前写的hive，在里面写sql操作。但是实际生产当中，他更多的对于spark来说还是要写代码，打jar包来运行。所以最后是我们写的一个wordcount程序，打jar包来提交到集群上去运行。</p><h2 id="Spark安装地址"><a href="#Spark安装地址" class="headerlink" title="Spark安装地址"></a>Spark安装地址</h2><p>1．官网地址<br><a href="http://spark.apache.org/">http://spark.apache.org</a><br>2．文档查看地址<br><a href="https://spark.apache.org/docs">https://spark.apache.org/docs</a><br>3．下载地址<br><a href="https://spark.apache.org/downloads.html">https://spark.apache.org/downloads.html</a></p><h2 id="集群角色"><a href="#集群角色" class="headerlink" title="集群角色"></a>集群角色</h2><p>注意: 不是standlone模式就没有Master和Worker</p><h3 id="Master和Worker"><a href="#Master和Worker" class="headerlink" title="Master和Worker"></a>Master和Worker</h3><p>Master和Worker:   负责资源的,具体运行,哪个的代码他不管.用户客户端提交代码后,你告诉我分配3G内存,2核CPU我给你分配就完了.    Master和Worker:   是standlone模式所独有的. yarn模式没有. Applicationmaster提交任务前,master和work一定是启动状态.</p><p>1）Master</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Spark特有资源调度系统的Leader。掌管着整个集群的资源信息，类似于Yarn框架中的ResourceManager，主要功能：</span><br><span class="line">（1）监听Worker，看Worker是否正常工作；</span><br><span class="line">（2）Master对Worker、Application等的管理(接收Worker的注册并管理所有的Worker，接收Client提交的application，调度等待的Application并向Worker提交)。</span><br></pre></td></tr></table></figure><p>2）Worker</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Spark特有资源调度系统的Slave(奴隶,随从)，有多个。每个Slave掌管着所在节点的资源信息，类似于Yarn框架中的NodeManager，主要功能：</span><br><span class="line">（1）通过RegisterWorker注册到Master；</span><br><span class="line">（2）定时发送心跳给Master；</span><br><span class="line">（3）根据Master发送的Application配置进程环境，并启动ExecutorBackend(执行Task所需的临时进程)</span><br></pre></td></tr></table></figure><h3 id="Driver和Executor"><a href="#Driver和Executor" class="headerlink" title="Driver和Executor"></a>Driver和Executor</h3><p>Driver和Executor:  负责具体执行的任务.他和具体提执行的任务相关.驱动器和执行器.驱动器是主,执行器是从.M任务的resourcemanager和nodemanager是负责管理资源, 资源申请下来之后他先启动的是Applicationmaster,是当前这个任务的小组长. Driver类似于MR的Applicationmaster . Applicationmaster来了之后,他去执行执行相应的具体的任务.就是mapTask,ReduceTask等.这些task就executer中去运行. Driver和Executer是线程级别的任务.</p><p>1）Driver（驱动器）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Spark的驱动器是执行开发程序中的main方法的线程。它负责开发人员编写的用来创建SparkContext (sc)、创建RDD，以及进行RDD的转化操作和行动操作代码的执行。如果你是用Spark Shell，那么当你启动Spark shell的时候，系统后台自启了一个Spark驱动器程序，就是在Spark shell中预加载的一个叫作 sc的SparkContext对象。如果驱动器程序终止，那么Spark应用也就结束了。</span><br><span class="line">Driver(驱动器)主要负责：</span><br><span class="line">（1）将用户程序代码转化为作业（Job）；</span><br><span class="line">（2）在Executor之间调度任务（Task）；</span><br><span class="line">（3）跟踪Executor的执行情况；</span><br><span class="line">（4）通过UI展示查询运行情况。</span><br></pre></td></tr></table></figure><p>2）Executor（执行器）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Spark Executor是一个工作节点，负责在 Spark 作业(Job)中运行任务(Task)，任务间相互独立。Spark 应用启动时，Executor节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有Executor节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他Executor节点上继续运行。</span><br><span class="line">Executor（执行器）主要负责：</span><br><span class="line">（1）负责运行组成 Spark 应用的任务，并将状态信息返回给驱动器(Driver)程序；</span><br><span class="line">（2）通过自身的块管理器（Block Manager）为用户程序中要求缓存的RDD提供内存式存储。RDD是直接缓存在Executor内的，因此任务可以在运行时充分利用缓存数据加速运算。</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Master和Worker是Spark的守护进程(什么叫守护进程?即一直都在的)，即Spark在特定模式下正常运行所必须的进程。</p><p>Driver和Executor是临时程序，当有具体任务提交到Spark集群才会开启的程序。其实Driver和Executor是线程.  而Master和Worker是进程.</p><h3 id="博主补充-1"><a href="#博主补充-1" class="headerlink" title="博主补充"></a>博主补充</h3><p>Driver(驱动器) 和 Executer(执行器) 有主从关系, Driver是主,Executer是从.<br>Driver可以这样理解,MR中资源准备好了之后,要启一个ApplicationMaster,即当前这个任务的守护者,相当于Driver(驱动器).  ApplicationMaster启动好了之后,启动相应的任务, 如mapTask,ReduceTask等. 具体的一个个Task去运行,这就相当于Executer里面运行的内容</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_312.png" alt="blog: www.xubatian.cn"></p><p>  1.只要你用的是StandLone模式,master和worker将一直都有.如果你是yarn模式就不需要. </p><ol start="2"><li>对于Driver和Executer只有等任务来了才有. 而Driver和Executer,无论本地模式和yarn模式都有, 他和模式没有关系 </li></ol><p>以下是Spark的几个模式, 它运行的位置可能不一样. standlone的Drive和Executor是由master和Worker来决定位置的.<br>如果是yarn模式就有ResourceManager来决定位置.</p><h2 id="Local模式"><a href="#Local模式" class="headerlink" title="Local模式"></a>Local模式</h2><p>本地模式：解压完了就等于安装好了。就和Hadoop一样，解压完了，什么都没改，直接就可以运行jar包了。这个也一样的，直接可以运行jar包</p><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Local模式就是运行在一台计算机上的模式，通常就是用于在本机上练手和测试。它可以通过以下集中方式设置Master。</p><p>local: 所有计算都运行在一个Core当中，没有任何并行计算，通常我们在本机执行些测试代码, 或者练手, 就用这种模式;</p><p>local[K]: 指定使用K个Core来运行计算，比如local[4]就是运行4个Core来执行;</p><p>local[*]:  这种模式直接使用最大Core数。</p><p>master叫资源管理器. 我们统称为master.<br>我们将standlone里面的master, yarn里面的ResourceManager. 以及这里上图的资源管理器都成为master.</p><h3 id="安装使用"><a href="#安装使用" class="headerlink" title="安装使用"></a>安装使用</h3><p>1）上传并解压spark安装包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 sorfware]$ tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/</span><br><span class="line">[shangbaishuyao@hadoop102 module]$ mv spark-2.1.1-bin-hadoop2.7 spark</span><br></pre></td></tr></table></figure><p>2）官方求PI案例(类似java jar)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>（1）基本语法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class &lt;main-class&gt;</span><br><span class="line">--master &lt;master-url&gt; \</span><br><span class="line">--deploy-mode &lt;deploy-mode&gt; \</span><br><span class="line">--conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">... # other options</span><br><span class="line">&lt;application-jar&gt; \        -- jar 包所在路径</span><br><span class="line">[application-arguments]    --大括号表示可选的,有些main方法不需要参数</span><br><span class="line"></span><br><span class="line">===============上面是模板,下面是实例,对比====================</span><br><span class="line"></span><br><span class="line"> bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>（2）参数说明</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">--master 指定Master的地址；</span><br><span class="line">--class: 你的应用的启动类 (如 org.apache.spark.examples.SparkPi)；</span><br><span class="line">--deploy-mode: 是否发布你的驱动到worker节点(cluster) 或者作为一个本地客户端 (client) (default: client)；</span><br><span class="line">--conf: 任意的Spark配置属性， 格式key=value. 如果值包含空格，可以加引号“key=value” ；</span><br><span class="line">application-jar: 打包好的应用jar,包含依赖. 这个URL在集群中全局可见。 比如hdfs:// 共享存储系统， 如果是 file:// path， 那么所有的节点的path都包含同样的jar</span><br><span class="line">application-arguments: 传给main()方法的参数；</span><br><span class="line">--executor-memory 1G 指定每个executor可用内存为1G；</span><br><span class="line">--total-executor-cores 2 指定每个executor使用的cup核数为2个。</span><br></pre></td></tr></table></figure><p>3）结果</p><p>该算法是利用蒙特·卡罗算法求PI</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_313.png" alt="blog: www.xubatian.cn"></p><p>4）准备文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ mkdir input</span><br><span class="line">在input下创建3个文件1.txt和2.txt，并输入以下内容</span><br><span class="line">hello shangbaishuyao</span><br><span class="line">hello spark</span><br></pre></td></tr></table></figure><p>5）启动spark-shell</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-shell</span><br><span class="line">Using Spark&#x27;s default log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line">Setting default log level to &quot;WARN&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">18/09/29 08:50:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">18/09/29 08:50:58 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</span><br><span class="line">Spark context Web UI available at http://192.168.9.102:4040</span><br><span class="line">Spark context available as &#x27;sc&#x27; (master = local[*], app id = local-1538182253312).</span><br><span class="line">Spark session available as &#x27;spark&#x27;.</span><br><span class="line">Welcome to</span><br><span class="line"></span><br><span class="line">      / __/__  ___ _____/ /__</span><br><span class="line">     _\ \/ _ \/ _ `/ __/  &#x27;_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1</span><br><span class="line">       /_/</span><br><span class="line">          </span><br><span class="line">Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)</span><br><span class="line">Type in expressions to have them evaluated.</span><br><span class="line">Type :help for more information.</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span></span><br></pre></td></tr></table></figure><p>6）结果图示</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_314.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_315.png" alt="blog: www.xubatian.cn"></p><p>7）运行WordCount程序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;sc.textFile(&quot;input&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">res0: Array[(String, Int)] = Array((hadoop,6), (oozie,3), (spark,3), (hive,3), (shangbaishuyao,3), (hbase,6))</span><br><span class="line">scala&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可登录hadoop102:4040查看程序运行</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_316.png" alt="blog: www.xubatian.cn"></p><h3 id="提交流程"><a href="#提交流程" class="headerlink" title="提交流程"></a>提交流程</h3><p>1）提交任务分析</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_317.png" alt="blog: www.xubatian.cn"></p><h3 id="博主补充解析"><a href="#博主补充解析" class="headerlink" title="博主补充解析"></a>博主补充解析</h3><p>因为是Local模式,没有master和worker. 我们在提交任务之前,没有启动任何程序. 所以资源管理者就是本身,就是spark-submit,即他自己管理计算,自己管理资源.  正常提交,提交之后就会运行一个Driver. 其实你在起动spark-shell的时候就已经有了这个Driver了. Driver去资源管理者里注册应用程序,然后启动Executor. 至此,这一套就在起动saprk-shell的时候就已经搞好了. 接下来过程就是我们自己写代码了.  就是Executor反向注册到Driver中产生通信. 然后写代码, 如初始化sparkContext, 任务划分,任务调度等. 调度完后给Executor中去运行.</p><h3 id="数据流程"><a href="#数据流程" class="headerlink" title="数据流程"></a>数据流程</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_319.png" alt=" blog: www.xubatian.cn"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">textFile(&quot;input&quot;)：  读取本地文件input文件夹数据；</span><br><span class="line">flatMap(_.spl it(&quot; &quot;))：压平操作，按照空格分割符将一行数据映射成一个个单词；</span><br><span class="line">map((_,1))：对每一个元素操作，将单词映射为元组；</span><br><span class="line">reduceByKey(_+_)：按照key将值进行聚合，相加；</span><br><span class="line">collect：将数据收集到Driver端展示。</span><br></pre></td></tr></table></figure><p><strong>案例分析</strong></p><p> <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_320.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_321.png" alt="blog: www.xubatian.cn"></p><h2 id="Standalone模式"><a href="#Standalone模式" class="headerlink" title="Standalone模式"></a>Standalone模式</h2><p>Standalone模式有一组进程叫master和worker</p><p>单机模式，这里指的是spark自己来管理整个的计算资源，交给spark来管理了，他也是一个分布式的。但是这个计算资源不跟其他的mapreduce呀或者storm等程序所共用的，他自己来管理的。意思就是说，spark他自己玩自己的。他有一套独立的资源管理系统在里面此模式中有一组进程叫master和worker</p><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_323.png" alt="blog: www.xubatian.cn"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">构建一个由Master+Slave构成的Spark集群，使Spark程序运行在集群中，且有Cluster与Client模式(默认是这种)两种。主要区别在于：Driver程序的运行节点不一样。</span><br><span class="line">Driver是一个线程,是执行我写的程序的main方法,就是执行的spark-submit --class里面的main方法.</span><br><span class="line"></span><br><span class="line">Client模式指什么意思呢? 我们需要执行Spark-submit来提交一个任务. 如果我们采用的是client模式. 那么我们的Driver程序就在当前提交的机器的线程. 这个spark-submit是不是一个进程,这个进程的名字叫spark-submit. 这个线程就运行在进程spark-submit里面. 这是client模式.</span><br><span class="line">而Cluster模式,他这个Driver运行在哪? 他是由master来决定的一个位置. 所以cluster模式和Client模式他们两个区别就在这. </span><br><span class="line">如果生产环境中要用的话, 用的最多的是Cluster模式. 因为 Driver在整个运行过程中,他会和其他节点Executor做通信. 这样就对内存用的比较大了. 这样的话,我们让集群自己去做选择是更好一些. 因为client模式, 你在哪提交的,你的Driver就运行在哪. 很有可能,你提交的地方的这台机器本身资源不足等问题,所以用cluster模式更好一些</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">7077 standlone模式下master的服务端口</span><br><span class="line">8080 standlone模式下master的web端口</span><br><span class="line">4040 Driver的web端口</span><br><span class="line">18080 历史服务端口</span><br><span class="line">8088 ResourceManager的web端口</span><br><span class="line">19888 是MapReduce里面yarn的历史服务端口</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_322.png" alt="blog: www.xubatian.cn"></p><h3 id="安装使用-1"><a href="#安装使用-1" class="headerlink" title="安装使用"></a>安装使用</h3><p>1）进入spark安装目录下的conf文件夹</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ cd spark/conf/</span><br></pre></td></tr></table></figure><p>2）修改配置文件名称</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ mv slaves.template slaves</span><br><span class="line">[shangbaishuyao@hadoop102 conf]$ mv spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure><p>3）修改slave文件，添加work节点</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vim slaves</span><br><span class="line"></span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p>4）修改spark-env.sh文件，添加如下配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vim spark-env.sh</span><br><span class="line"></span><br><span class="line">SPARK_MASTER_HOST=hadoop102</span><br><span class="line">SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure><p>5）分发spark包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ xsync spark/</span><br></pre></td></tr></table></figure><p>6）启动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ sbin/start-all.sh</span><br><span class="line">[shangbaishuyao@hadoop102 spark]$ util.sh </span><br><span class="line">================shangbaishuyao@hadoop102================</span><br><span class="line">3330 Jps</span><br><span class="line">3238 Worker</span><br><span class="line">3163 Master</span><br><span class="line">================shangbaishuyao@hadoop103================</span><br><span class="line">2966 Jps</span><br><span class="line">2908 Worker</span><br><span class="line">================shangbaishuyao@hadoop104================</span><br><span class="line">2978 Worker</span><br><span class="line">3036 Jps</span><br></pre></td></tr></table></figure><p>网页查看：hadoop102:8080<br>注意：如果遇到 “JAVA_HOME not set” 异常，可以在sbin目录下的spark-config.sh 文件中加入如下配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure><p>7）官方求PI案例</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077 \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_324.png" alt="blog: www.xubatian.cn"></p><p>8）启动spark shell</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/spark/bin/spark-shell \</span><br><span class="line">--master spark://hadoop102:7077 \                     </span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--total-executor-cores 2</span><br><span class="line">参数：--master spark://hadoop102:7077指定要连接的集群的master</span><br><span class="line">执行WordCount程序</span><br><span class="line">scala&gt;sc.textFile(&quot;/opt/module/spark/input&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">res0: Array[(String, Int)] = Array((hadoop,6), (oozie,3), (spark,3), (hive,3), (shangbaishuyao,3), (hbase,6))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><h3 id="JobHistoryServer配置-查看历史用的-历史服务器"><a href="#JobHistoryServer配置-查看历史用的-历史服务器" class="headerlink" title="JobHistoryServer配置  (查看历史用的,历史服务器)"></a>JobHistoryServer配置  (查看历史用的,历史服务器)</h3><p>1）修改spark-default.conf.template名称</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ mv spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure><p>2）修改spark-default.conf文件，开启Log  (配置的是写)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vi spark-defaults.conf</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               hdfs://hadoop102:9000/directory</span><br></pre></td></tr></table></figure><p>注意：HDFS上的目录需要提前存在。<br>3）修改spark-env.sh文件，添加如下配置 (配置的是读取)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vi spark-env.sh</span><br><span class="line"></span><br><span class="line">export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=18080</span><br><span class="line">-Dspark.history.retainedApplications=30 </span><br><span class="line">-Dspark.history.fs.logDirectory=hdfs://hadoop102:9000/directory&quot;</span><br></pre></td></tr></table></figure><p>参数描述：<br>spark.eventLog.dir：Application在运行过程中所有的信息均记录在该属性指定的路径下<br>spark.history.ui.port=18080  WEBUI访问的端口号为18080<br>spark.history.fs.logDirectory=hdfs://hadoop102:9000/directory配置了该属性后，在start-history-server.sh时就无需再显式的指定路径，Spark History Server页面只展示该指定路径下的信息<br>spark.history.retainedApplications=30指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除。注意：这个是内存中的应用数，而不是页面上显示的应用数。<br>4）分发配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ xsync spark-defaults.conf</span><br><span class="line">[shangbaishuyao@hadoop102 conf]$ xsync spark-env.sh</span><br></pre></td></tr></table></figure><p>5）启动历史服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ sbin/start-history-server.sh</span><br></pre></td></tr></table></figure><p>6）再次执行任务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077 \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>7）查看历史服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop102:18080</span><br></pre></td></tr></table></figure><h3 id="HA配置"><a href="#HA配置" class="headerlink" title="HA配置"></a>HA配置</h3><p>我们worker有三个宕机一个还能用,但是master只有一个,我发高可用,所以我们要将master依赖zookeeper,由zookeeper来选举master,不能让我们直接指定</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_325.png" alt="blog: www.xubatian.cn"></p><p>1）zookeeper正常安装并启动<br>2）修改spark-env.sh文件，添加如下配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vi spark-env.sh</span><br><span class="line">注释掉如下内容：因为我们的master由zookeeper来选举,不能由我们自己指定了,故注释掉</span><br><span class="line">#SPARK_MASTER_HOST=hadoop102</span><br><span class="line">#SPARK_MASTER_PORT=7077</span><br><span class="line">添加上如下内容：</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS=&quot;</span><br><span class="line">-Dspark.deploy.recoveryMode=ZOOKEEPER </span><br><span class="line">-Dspark.deploy.zookeeper.url=hadoop102,hadoop103,hadoop104 </span><br><span class="line">-Dspark.deploy.zookeeper.dir=/spark&quot;</span><br></pre></td></tr></table></figure><p>3）分发配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ xsync spark-env.sh</span><br></pre></td></tr></table></figure><p>4）在hadoop102上启动全部节点</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ sbin/start-all.sh</span><br></pre></td></tr></table></figure><p>5）在hadoop103上单独启动master节点</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop103 spark]$ sbin/start-master.sh</span><br></pre></td></tr></table></figure><p>6）spark HA集群访问,一般先连接前面的master,前面挂掉了再连接后面的 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/spark/bin/spark-shell \</span><br><span class="line">--master spark://hadoop102:7077,hadoop103:7077 \</span><br><span class="line">--executor-memory 2g \</span><br><span class="line">--total-executor-cores 2</span><br></pre></td></tr></table></figure><h2 id="Yarn模式"><a href="#Yarn模式" class="headerlink" title="Yarn模式"></a>Yarn模式</h2><p>他不需要部署spark集群,我只需要部署yarn集群,因为我所解压的spark只是作为本地客户端,只是提交用,当然你也可以分发, 分发后的目的也就是hadoop102,hadoop103,hadoop104都是可以提交任务而已. 因为yarn模式,我们解压的spark仅仅作为客户端来用的<br>生产环境当中用的最多的一种模式，就是说spark他有一个计算任务。任务呢，我来执行，但是运行任务的CPU,还有内存这些东西交给yarn来管理。交给yarn来管理其实就是交给resourcemanager和nodemanager来管理。</p><h3 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h3><p>Spark客户端直接连接Yarn，不需要额外构建Spark集群。有yarn-client和yarn-cluster两种模式，主要区别在于：Driver程序的运行节点。<br>yarn-client：Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出<br>yarn-cluster：Driver程序运行在由RM（ResourceManager）启动的AM（APPMaster）适用于生产环境。</p><p>Yarn-cluster提交流程图:<br><a href="https://www.cnblogs.com/shi-qi/articles/12174206.html">https://www.cnblogs.com/shi-qi/articles/12174206.html</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_326.png" alt="blog: www.xubatian.cn"></p><h3 id="博主解析"><a href="#博主解析" class="headerlink" title="博主解析"></a>博主解析</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_327.png" alt="blog: www.xubatian.cn"></p><h3 id="安装使用-2"><a href="#安装使用-2" class="headerlink" title="安装使用"></a>安装使用</h3><p>1）修改hadoop配置文件yarn-site.xml,添加如下内容<br>[shangbaishuyao@hadoop102 hadoop]$ vi yarn-site.xml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>2）修改spark-env.sh，添加如下配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vi spark-env.sh</span><br><span class="line"></span><br><span class="line">YARN_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop</span><br></pre></td></tr></table></figure><p>3）分发配置文件,只是分发这个配置文件,我spark-yarn要分发吗?不需要.因为我们不需要额外去构建spark集群,yarn是分布式的,而本地的spark-yarn仅仅是做提交任务的客户端,所以<br>Spark-yarn不许要分发</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ xsync /opt/module/hadoop-2.7.2/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure><p>4）执行一个程序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>注意：在提交任务之前需启动HDFS以及YARN集群。</p><p>Yarn 模式读取的文件是HDFS里面的</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_328.png" alt="blog: www.xubatian.cn"></p><h3 id="日志查看"><a href="#日志查看" class="headerlink" title="日志查看"></a>日志查看</h3><p>1）修改配置文件spark-defaults.conf，添加如下内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.yarn.historyServer.address=hadoop102:18080</span><br><span class="line">spark.history.ui.port=18080</span><br></pre></td></tr></table></figure><p>2）重启Spark历史服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ sbin/stop-history-server.sh </span><br><span class="line">stopping org.apache.spark.deploy.history.HistoryServer</span><br><span class="line">[shangbaishuyao@hadoop102 spark]$ sbin/start-history-server.sh </span><br><span class="line">starting org.apache.spark.deploy.history.HistoryServer, logging to /opt/module/spark/logs/spark-shangbaishuyao-org.apache.spark.deploy.history.HistoryServer-1-hadoop102.out</span><br></pre></td></tr></table></figure><p>3）提交任务到Yarn执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>4）Web页面查看日志</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_329.png" alt="blog: www.xubatian.cn"></p><h2 id="Mesos模式"><a href="#Mesos模式" class="headerlink" title="Mesos模式"></a>Mesos模式</h2><p>（这种很少用，几乎没有公司在用。Mesos也是apache的一个资源调度框架，就和yarn是类似的东西）</p><p>Spark客户端直接连接Mesos；不需要额外构建Spark集群。国内应用比较少，更多的是运用yarn调度。</p><h2 id="几种模式对比"><a href="#几种模式对比" class="headerlink" title="几种模式对比"></a>几种模式对比</h2><table><thead><tr><th>模式</th><th>Spark安装机器数</th><th>需启动的进程(提交任务前)</th><th>所属者</th></tr></thead><tbody><tr><td>Local</td><td>1</td><td>无</td><td>Spark</td></tr><tr><td>Standalone</td><td>3</td><td>Master及Worker</td><td>Spark</td></tr><tr><td>Yarn</td><td>1</td><td>Yarn及HDFS</td><td>Hadoop</td></tr></tbody></table><h1 id="案例代码"><a href="#案例代码" class="headerlink" title="案例代码"></a>案例代码</h1><p>WordCount案例代码:</p><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/sparkCore/src/main/scala/com/shangbaishuyao/wordCount">https://github.com/ShangBaiShuYao/bigdata/blob/master/sparkCore/src/main/scala/com/shangbaishuyao/wordCount</a></p><p>整个Spark学习案例代码:</p><p><a href="https://github.com/ShangBaiShuYao/bigdata">https://github.com/ShangBaiShuYao/bigdata</a></p><p>调试补充:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_330.png" alt="https://www.xubatian.cn/"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt; 征途漫漫，惟有奋斗；梦想成真，惟有实干。                     ——人民日报&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="spark" scheme="http://xubatian.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>hive常用函数收录</title>
    <link href="http://xubatian.cn/hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E6%94%B6%E5%BD%95/"/>
    <id>http://xubatian.cn/hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E6%94%B6%E5%BD%95/</id>
    <published>2022-01-18T14:43:32.000Z</published>
    <updated>2022-01-23T02:58:21.757Z</updated>
    
    <content type="html"><![CDATA[<p>山再高，往上攀，总能登顶；路再长，走下去，定能到达。       ——人民日报                  </p><p>​                                              </p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_307.jpg" alt="blog: www.xubatian.cn"></p><h1 id="常用日期函数"><a href="#常用日期函数" class="headerlink" title="常用日期函数"></a>常用日期函数</h1><p>unix_timestamp:返回当前或指定时间的时间戳    </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select unix_timestamp();</span><br><span class="line">select unix_timestamp(&quot;2020-10-28&quot;,&#x27;yyyy-MM-dd&#x27;);</span><br></pre></td></tr></table></figure><p>from_unixtime：将时间戳转为日期格式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select from_unixtime(1603843200);</span><br></pre></td></tr></table></figure><p>current_date：当前日期</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select current_date;</span><br></pre></td></tr></table></figure><p>current_timestamp：当前的日期加时间</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select current_timestamp;</span><br></pre></td></tr></table></figure><p>to_date：抽取日期部分</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select to_date(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>year：获取年</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select year(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>month：获取月</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select month(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>day：获取日</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select day(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>hour：获取时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select hour(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>minute：获取分</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select minute(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>second：获取秒</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select second(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>weekofyear：当前时间是一年中的第几周</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select weekofyear(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>dayofmonth：当前时间是一个月中的第几天</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select dayofmonth(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>months_between： 两个日期间的月份</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select months_between(&#x27;2020-04-01&#x27;,&#x27;2020-10-28&#x27;);</span><br></pre></td></tr></table></figure><p>add_months：日期加减月</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select add_months(&#x27;2020-10-28&#x27;,-3);</span><br></pre></td></tr></table></figure><p>datediff：两个日期相差的天数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select datediff(&#x27;2020-11-04&#x27;,&#x27;2020-10-28&#x27;);</span><br></pre></td></tr></table></figure><p>date_add：日期加天数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select date_add(&#x27;2020-10-28&#x27;,4);</span><br></pre></td></tr></table></figure><p>date_sub：日期减天数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select date_sub(&#x27;2020-10-28&#x27;,-4);</span><br></pre></td></tr></table></figure><p>last_day：日期的当月的最后一天</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select last_day(&#x27;2020-02-30&#x27;);</span><br></pre></td></tr></table></figure><p>date_format(): 格式化日期</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select date_format(&#x27;2020-10-28 12:12:12&#x27;,&#x27;yyyy/MM/dd HH:mm:ss&#x27;);</span><br></pre></td></tr></table></figure><p>常用取整函数<br>round： 四舍五入</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select round(3.14);</span><br><span class="line">select round(3.54);</span><br></pre></td></tr></table></figure><p>ceil：  向上取整</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select ceil(3.14);</span><br><span class="line">select ceil(3.54);</span><br></pre></td></tr></table></figure><p>floor： 向下取整</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select floor(3.14);</span><br><span class="line">select floor(3.54);</span><br></pre></td></tr></table></figure><h1 id="常用字符串操作函数"><a href="#常用字符串操作函数" class="headerlink" title="常用字符串操作函数"></a>常用字符串操作函数</h1><p>upper： 转大写</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select upper(&#x27;low&#x27;);</span><br></pre></td></tr></table></figure><p>lower： 转小写</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select lower(&#x27;low&#x27;);</span><br></pre></td></tr></table></figure><p>length： 长度</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select length(&quot;shangbaishuyao&quot;);</span><br></pre></td></tr></table></figure><p>trim：  前后去空格</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select trim(&quot; shangbaishuyao &quot;);</span><br></pre></td></tr></table></figure><p>lpad： 向左补齐，到指定长度</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select lpad(&#x27;shangbaishuyao&#x27;,9,&#x27;g&#x27;);</span><br></pre></td></tr></table></figure><p>rpad：  向右补齐，到指定长度</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select rpad(&#x27;shangbaishuyao&#x27;,9,&#x27;g&#x27;);</span><br></pre></td></tr></table></figure><p>regexp_replace：使用正则表达式匹配目标字符串，匹配成功后替换！</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT regexp_replace(&#x27;2020/10/25&#x27;, &#x27;/&#x27;, &#x27;-&#x27;);</span><br></pre></td></tr></table></figure><h1 id="集合操作"><a href="#集合操作" class="headerlink" title="集合操作"></a>集合操作</h1><p>size： 集合中元素的个数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select size(friends) from test3;</span><br></pre></td></tr></table></figure><p>map_keys： 返回map中的key</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select map_keys(children) from test3;</span><br></pre></td></tr></table></figure><p>map_values: 返回map中的value</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select map_values(children) from test3;</span><br></pre></td></tr></table></figure><p>array_contains: 判断array中是否包含某个元素</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select array_contains(friends,&#x27;bingbing&#x27;) from test3;</span><br></pre></td></tr></table></figure><p>sort_array： 将array中的元素排序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select sort_array(friends) from test3;</span><br></pre></td></tr></table></figure><p>grouping_set:多维分析</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;山再高，往上攀，总能登顶；路再长，走下去，定能到达。       ——人民日报                  &lt;/p&gt;
&lt;p&gt;​                                              &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="hive" scheme="http://xubatian.cn/tags/hive/"/>
    
  </entry>
  
</feed>
