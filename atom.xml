<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>我的梦想是星辰大海</title>
  
  <subtitle>知识源于积累,登峰造极源于自律</subtitle>
  <link href="http://xubatian.cn/atom.xml" rel="self"/>
  
  <link href="http://xubatian.cn/"/>
  <updated>2022-02-28T17:06:43.765Z</updated>
  <id>http://xubatian.cn/</id>
  
  <author>
    <name>xubatian</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大数据公羊说之Flink每日一题收录</title>
    <link href="http://xubatian.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%AC%E7%BE%8A%E8%AF%B4%E4%B9%8BFlink%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E6%94%B6%E5%BD%95/"/>
    <id>http://xubatian.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%AC%E7%BE%8A%E8%AF%B4%E4%B9%8BFlink%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E6%94%B6%E5%BD%95/</id>
    <published>2022-02-28T16:12:12.000Z</published>
    <updated>2022-02-28T17:06:43.765Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">文章收录的是公众号: 大数据公羊说 的 面试题总结.</span><br></pre></td></tr></table></figure><p>[toc]</p><span id="more"></span><h4 id="面试题：你在生产环境中碰到哪些问题会导致反压？"><a href="#面试题：你在生产环境中碰到哪些问题会导致反压？" class="headerlink" title="面试题：你在生产环境中碰到哪些问题会导致反压？"></a>面试题：你在生产环境中碰到哪些问题会导致反压？</h4><ul><li>数据倾斜：比如当前算子的每个并发只能处理 1w qps 的数据，而由于数据倾斜，这个算子平均 1s 需要处理 2w 条数据，因此倾斜的算子处理压力大，从而反压</li><li>算子性能问题：比如下游整个整个算子的处理性能差，上游是 1w qps，当前整个算子算下来平均只能处理 1k qps，因此就有反压的情况</li></ul><h4 id="面试题：反压有哪些危害？"><a href="#面试题：反压有哪些危害？" class="headerlink" title="面试题：反压有哪些危害？"></a>面试题：反压有哪些危害？</h4><ul><li>CK 时间长或者失败。反压导致 barrier 需要花很长时间才能对齐。</li><li>整个任务完全卡住。比如在 TUMBLE 窗口算子的任务中，反压后可能会导致下游算子的 inputpool 和上游算子的 outputpool 满了，这时候如果下游窗口的 watermark 一直对不齐，窗口触发不了的话，下游算子就永远无法触发窗口计算了。整个任务卡住。</li></ul><h4 id="面试题-状态、状态后端、Checkpoint-三者之间的区别及关系？"><a href="#面试题-状态、状态后端、Checkpoint-三者之间的区别及关系？" class="headerlink" title="面试题:   状态、状态后端、Checkpoint 三者之间的区别及关系？"></a>面试题:   状态、状态后端、Checkpoint 三者之间的区别及关系？</h4><p><strong>结论：拿五个字做比喻：”铁锅炖大鹅”，铁锅是状态后端，大鹅是状态，Checkpoint 是炖的动作。</strong></p><ol><li> <strong>状态</strong>：本质来说就是数据，在 Flink 中，其实就是 Flink 提供给用户的状态编程接口。比如 flink 中的 MapState，ValueState，ListState。</li><li><strong>状态后端</strong>：Flink 提供的用于管理状态的组件，状态后端决定了以什么样数据结构，什么样的存储方式去存储和管理我们的状态。Flink 目前官方提供了 memory、filesystem，rocksdb 三种状态后端来存储我们的状态。</li><li><strong>Checkpoint（状态管理）</strong>：Flink 提供的用于定时将状态后端中存储的状态同步到远程的存储系统的组件或者能力。为了防止 long run 的 Flink 任务挂了导致状态丢失，产生数据质量问题，Flink 提供了状态管理（Checkpoint，Savepoint）的能力把我们使用的状态给管理起来，定时的保存到远程。然后可以在 Flink 任务 failover 时，从远程把状态数据恢复到 Flink 任务中，保障数据质量。</li></ol><h4 id="面试题-把状态后端从-FileSystem-变为-RocksDB-后，Flink-任务状态存储会发生那些变化？"><a href="#面试题-把状态后端从-FileSystem-变为-RocksDB-后，Flink-任务状态存储会发生那些变化？" class="headerlink" title="面试题 : 把状态后端从 FileSystem 变为 RocksDB 后，Flink 任务状态存储会发生那些变化？"></a>面试题 : 把状态后端从 FileSystem 变为 RocksDB 后，Flink 任务状态存储会发生那些变化？</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">结论：是否使用 RocksDB 只会影响 Flink 任务中 keyed<span class="operator">-</span>state 存储的方式和地方，Flink 任务中的 operator<span class="operator">-</span>state 不会受到影响。</span><br></pre></td></tr></table></figure><p>首先我们来看看，Flink 中的状态只会分为两类：</p><ol><li> keyed-state：键值状态，如其名字，此类状态是以 k-v 的形式存储，状态值和 key 绑定。Flink 中的 keyby 之后紧跟的算子的 state 就是键值状态；</li><li> operator-state：算子状态，非 keyed-state 的 state 都是算子状态，非 k-v 结构，状态值和算子绑定，不和 key 绑定。Flink 中的 kafka source 算子中用于存储 kafka offset 的 state 就是算子状态。</li></ol><p>如下图所示是 3 种状态后端和 2 种 State 的对应存储关系：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220301005248.png"></p><ol><li> 横向（行）来看，即 Flink 的状态分类。分为 Operator state-backend、Keyed state-backend；</li><li> 纵向（列）来看，即 Flink 的状态后端分类。用户可以配置 memory，filesystem，rocksdb 3 中状态后端，在 Flink 任务中生成 MemoryStateBackend，FsStateBackend，RocksdbStateBackend，其声明了整个任务的状态管理后端类型；</li><li>每个格子中的内容就是用户在配置 xx 状态后端（列）时，给用户使用的状态（行）生成的状态后端实例，生成的这个实例就是在 Flink 中实际用于管理用户使用的状态的组件。</li></ol><p>因此对应的结论就是：</p><ol><li> Flink 任务中的 operator-state。无论用户配置哪种状态后端（无论是 memory，filesystem，rocksdb），都是使用 DefaultOperatorStateBackend 来管理的，状态数据都存储在内存中，做 Checkpoint 时同步到远程文件存储中（比如 HDFS）。</li><li>Flink 任务中的 keyed-state。<strong>用户在配置 rocksdb 时，会使用 RocksdbKeyedStateBackend 去管理状态；用户在配置 memory，filesystem 时，会使用 HeapKeyedStateBackend 去管理状态。因此就有了这个问题的结论，配置 rocksdb 只会影响 keyed-state 存储的方式和地方，operator-state 不会受到影响。</strong></li></ol><h4 id="面试题-什么样的业务场景你会选择-filesystem，什么样的业务场景你会选-rocksdb-状态后端？"><a href="#面试题-什么样的业务场景你会选择-filesystem，什么样的业务场景你会选-rocksdb-状态后端？" class="headerlink" title="面试题:   什么样的业务场景你会选择 filesystem，什么样的业务场景你会选 rocksdb 状态后端？"></a>面试题:   什么样的业务场景你会选择 filesystem，什么样的业务场景你会选 rocksdb 状态后端？</h4><p>在回答这个问题前，我们先看看每种状态后端的特性：</p><ol><li><strong>MemoryStateBackend</strong></li></ol><ul><li><p>原理：运行时所需的 State 数据全部保存在 TaskManager JVM 堆上内存中，执行 Checkpoint 的时候，会把 State 的快照数据保存到 JobManager 进程 的内存中。执行 Savepoint 时，可以把 State 存储到文件系统中。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220301005308.png"></p></li><li><p>适用场景：</p></li><li><ul><li>基于内存的 StateBackend 在生产环境下不建议使用，因为 State 大小超过 JobManager 内存就 OOM 了，<strong>此种状态后端适合在本地开发调试测试，生产环境基本不用。</strong></li><li>State 存储在 JobManager 的内存中。受限于 JobManager 的内存大小。</li><li>每个 State 默认 5MB,可通过 MemoryStateBackend 构造函数调整。d.每个 Stale 不能超过 Akka Frame 大小。</li></ul></li></ul><ol start="2"><li><strong>FSStateBackend</strong></li></ol><ul><li><p>原理：运行时所需的 State 数据全部保存在 TaskManager 的内存中，执行 Checkpoint 的时候，会把 State 的快照数据保存到配置的文件系统中。TM 是异步将 State 数据写入外部存储。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220301005325.png"></p></li><li><p>适用场景：</p></li><li><ul><li>a.适用于<strong>处理小状态、短窗口、或者小键值状态的有状态处理任务</strong>，<strong>不建议在大状态的任务下使用 FSStateBackend</strong>。比如 ETL 任务，小时间间隔的 TUMBLE 窗口 b.State 大小不能超过 TM 内存。</li></ul></li></ul><ol start="3"><li><strong>RocksDBStateBackend</strong></li></ol><ul><li><p>原理：使用嵌入式的本地数据库 RocksDB 将流计算数据状态存储在本地磁盘中。在执行 Checkpoint 的时候，会将整个 RocksDB 中保存的 State 数据全量或者增量持久化到配置的文件系统中。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220301005339.png"></p></li><li><p>适用场景：</p></li><li><ul><li>a.最适合用于处理<strong>大状态、长窗口，或大键值状态的有状态处理任务。</strong></li><li>b.RocksDBStateBackend 是目前<strong>唯一支持增量检查点</strong>的后端。</li><li>c.增量检查点非常适用于超大状态的场景。比如计算 DAU 这种大数据量去重，<strong>大状态的任务都建议直接使用 RocksDB 状态后端</strong>。</li></ul></li></ul><p>到生产环境中：</p><ol><li> 如果状态很大，使用 Rocksdb；如果状态不大，使用 Filesystem。</li><li> Rocksdb 使用磁盘存储 State，所以会涉及到访问 State 磁盘序列化、反序列化，性能会收到影响，而 Filesystem 直接访问内存，单纯从访问状态的性能来说 Filesystem 远远好于 Rocksdb。<strong>生产环境中实测，相同任务使用 Filesystem 性能为 Rocksdb 的 n 倍，因此需要根据具体场景评估选择。</strong></li></ol><h4 id="面试题-Flink-SQL-API-State-TTL-的过期机制是-onCreateAndUpdate-还是onReadAndWrite"><a href="#面试题-Flink-SQL-API-State-TTL-的过期机制是-onCreateAndUpdate-还是onReadAndWrite" class="headerlink" title="面试题:   Flink SQL API State TTL 的过期机制是 onCreateAndUpdate 还是onReadAndWrite?"></a>面试题:   Flink SQL API State TTL 的过期机制是 onCreateAndUpdate 还是onReadAndWrite?</h4><ol><li><strong>结论：Flink SQL API State TTL 的过期机制目前只支持 onCreateAndUpdate，DataStream API 两个都支持</strong></li></ol><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220301005404.png"></p><ol><li>剖析：</li></ol><ul><li>onCreateAndUpdate：是在创建 State 和更新 State 时【更新 State TTL】</li><li>onReadAndWrite：是在访问 State 和写入 State 时【更新 State TTL】</li></ul><ol><li> 实际踩坑场景：Flink SQL Deduplicate 写法，row_number partition by user_id order by proctime asc，此 SQL 最后生成的算子只会在第一条数据来的时候更新 state，后续访问不会更新 state TTL，因此 state 会在用户设置的 state TTL 时间之后过期。</li></ol><h4 id="面试题-watermark-到底是干啥的？应用场景？"><a href="#面试题-watermark-到底是干啥的？应用场景？" class="headerlink" title="面试题: watermark 到底是干啥的？应用场景？"></a>面试题: watermark 到底是干啥的？应用场景？</h4><p>大部分同学都只能回答出：watermark 是用于缓解时间时间的乱序问题的。</p><p>没错，这个观点是正确的。但是博主认为这只是 watermark 第二重要的作用，其更重要的作用在于可以标识一个 Flink 任务的事件 <strong>时间进度</strong>。</p><p>怎么理解 <strong>时间进度</strong>？</p><p>我们可以现象一下，一个事件时间窗口的任务，如果没有一个 <strong>东西</strong> 去标识其事件时间的进度，那么这个事件时间的窗口也就是不知道什么时候能够触发了，也就是说这个窗口永远不会触发并且输出结果。</p><p>所以要有一个 <strong>东西</strong> 去标识其事件时间的进度，从而让这个事件时间窗口知道，这个事件时间窗口已经结束了，可以触发计算了。在 Flink 中，这个 <strong>东西</strong> 就是 <strong>watermark</strong>。</p><p>总结一下，博主认为 <strong>watermark</strong> 为 Flink 解决了两个问题：</p><ol><li><strong>标识 Flink 任务的事件时间进度，从而能够推动事件时间窗口的触发、计算。</strong></li><li><strong>解决事件时间窗口的乱序问题。</strong></li></ol><h4 id="面试题-一个-Flink-任务中可以既有事件时间窗口，又有处理时间窗口吗？"><a href="#面试题-一个-Flink-任务中可以既有事件时间窗口，又有处理时间窗口吗？" class="headerlink" title="面试题: 一个 Flink 任务中可以既有事件时间窗口，又有处理时间窗口吗？"></a>面试题: 一个 Flink 任务中可以既有事件时间窗口，又有处理时间窗口吗？</h4><p><strong>结论：一个 Flink 任务可以同时有事件时间窗口，又有处理时间窗口。</strong></p><p>那么有些小伙伴们问了，为什么我们常见的 Flink 任务要么设置为事件时间语义，要么设置为处理时间语义？</p><p>确实，在生产环境中，我们的 Flink 任务一般不会同时拥有两种时间语义的窗口。</p><p>那么怎么解释开头博主所说的结论呢？</p><p>博主这里从两个角度进行说明：</p><ol><li> 我们其实没有必要把一个 Flink 任务和某种特定的时间语义进行绑定。对于事件时间窗口来说，我们只要给它 watermark，能让 watermark 一直往前推进，让事件时间窗口能够持续触发计算就行。对于处理时间来说更简单，只要窗口算子按照本地时间按照固定的时间间隔进行触发就行。无论哪种时间窗口，主要满足时间窗口的触发条件就行。</li><li> Flink 的实现上来说也是支持的。Flink 是使用一个叫做 TimerService 的组件来管理 timer 的，我们可以同时注册事件时间和处理时间的 timer，Flink 会自行判断 timer 是否满足触发条件，如果是，则回调窗口处理函数进行计算。</li></ol><h4 id="面试题-window-后面跟-aggregate-和-process-的两个窗口计算的区别是什么？"><a href="#面试题-window-后面跟-aggregate-和-process-的两个窗口计算的区别是什么？" class="headerlink" title="面试题: window 后面跟 aggregate 和 process 的两个窗口计算的区别是什么？"></a>面试题: window 后面跟 aggregate 和 process 的两个窗口计算的区别是什么？</h4><ol><li> aggregate：是增量聚合，来一条数据计算完了存储在累加器中，不需要等到窗口触发时计算，<strong>性能较好</strong>；</li><li> process：全量函数，缓存全部窗口内的数据，满足窗口触发条件再触发计算，同时还提供定时触发，窗口信息等上下文信息；</li><li> 应用场景：aggregate 一个一个处理的聚合结果向后传递一般来说都是有信息损失的，而 <strong>process 则可以更加定制化的处理</strong>。</li></ol><h4 id="面试题-为什么-Flink-DataStream-API-在函数入参或者出参有泛型时，不能使用-lambda-表达式？"><a href="#面试题-为什么-Flink-DataStream-API-在函数入参或者出参有泛型时，不能使用-lambda-表达式？" class="headerlink" title="面试题: 为什么 Flink DataStream API 在函数入参或者出参有泛型时，不能使用 lambda 表达式？"></a>面试题: 为什么 Flink DataStream API 在函数入参或者出参有泛型时，不能使用 lambda 表达式？</h4><p>Flink 类型信息系统是通过反射获取到 Java class 的方法签名去获取类型信息的。</p><p>以 FlatMap 为例，Flink 在通过反射时会检查及获取 FlatMap collector 的出参类型信息。</p><p>但是 lambda 表达式写的 FlatMap 逻辑，会导致反射方法获取类型信息时【直接获取不到】collector 的出参类型参数，所以才会报错。</p><h4 id="面试题-Flink-为什么强调-function-实现时，实例化的变量要实现-serializable-接口？"><a href="#面试题-Flink-为什么强调-function-实现时，实例化的变量要实现-serializable-接口？" class="headerlink" title="面试题: Flink 为什么强调 function 实现时，实例化的变量要实现 serializable 接口？"></a>面试题: Flink 为什么强调 function 实现时，实例化的变量要实现 serializable 接口？</h4><p><strong>其实这个问题可以延伸成 3 个问题：</strong></p><ol><li> 为什么 Flink 要用到 Java 序列化机制。和 Flink 类型系统的数据序列化机制的用途有啥区别？</li><li> 非实例化的变量没有实现 Serializable 为啥就不报错，实例化就报错？</li><li> 为啥加 transient 就不报错？</li></ol><p><strong>上面 3 个问题的答案如下：</strong></p><ol><li> Flink 写的函数式编程代码或者说闭包，需要 Java 序列化从 JobManager 分发到 TaskManager，而 Flink 类型系统的数据序列化机制是为了分发数据，不是分发代码，可以用非Java的序列化机制，比如 Kyro。</li><li> 编译期不做序列化，所以不实现 Serializable 不会报错，但是运行期会执行序列化动作，没实现 Serializable 接口的就报错了</li><li> Flink DataStream API 的 Function 作为闭包在网络传输，必须采用 Java 序列化，所以要通过 Serializable 接口标记，根据 Java 序列化的规定，内部成员变量要么都可序列化，要么通过 transient 关键字跳过序列化，否则 Java 序列化的时候会报错。静态变量不参与序列化，所以不用加 transient。</li></ol><h4 id="面试题-Flink-的并行度可以通过哪几种方式设置，优先级关系是什么？"><a href="#面试题-Flink-的并行度可以通过哪几种方式设置，优先级关系是什么？" class="headerlink" title="面试题: Flink 的并行度可以通过哪几种方式设置，优先级关系是什么？"></a>面试题: Flink 的并行度可以通过哪几种方式设置，优先级关系是什么？</h4><ol><li> 代码中算子单独设置</li><li> 代码中Env全局设置</li><li> 提交参数</li><li> 默认配置信息</li></ol><p>上面的 Flink 并行度优先级从上往下由大变小。</p><h4 id="面试题-你是怎么合理的评估-Flink-任务的并行度？"><a href="#面试题-你是怎么合理的评估-Flink-任务的并行度？" class="headerlink" title="面试题: 你是怎么合理的评估 Flink 任务的并行度？"></a>面试题: 你是怎么合理的评估 Flink 任务的并行度？</h4><p>Flink 任务并行度合理行一般根据<strong>峰值流量进行压测评估</strong>，并且根据集群负载情况<strong>留一定量的 buffer 资源</strong>。</p><ol><li>如果数据源已经存在，则可以直接消费进行测试</li><li>如果数据源不存在，需要自行造压测数据进行测试</li></ol><p>对于一个 Flink 任务来说，一般可以按照以下方式进行细粒度设置并行度：</p><ol><li><p>source 并行度配置：以 kafka 为例，source 的并行度一般设置为 kafka 对应的 topic 的分区数</p></li><li><p>transform（比如 flatmap、map、filter 等算子）并行度的配置：这些算子一般不会做太重的操作，并行度可以和 source 保持一致，使得算子之间可以做到 forward 传输数据，不经过网络传输</p></li><li><p>keyby 之后的处理算子：<strong>建议最大并行度为此算子并行度的整数倍</strong>，这样可以使每个算子上的 keyGroup 是相同的，从而使得数据相对均匀 shuffle 到下游算子，如下图为 shuffle 策略</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220301005428.png"></p></li><li><p>sink 并行度的配置：sink 是数据流向下游的地方，可以根据 sink 的数据量及下游的服务抗压能力进行评估。如果 sink 是 kafka，可以设为 kafka 对应 topic 的分区数。注意 sink 并行度最好和 kafka partition 成倍数关系，否则可能会出现如到 kafka partition 数据不均匀的情况。但是大多数情况下 sink 算子并行度不需要特别设置，只需要和整个任务的并行度相同就行。</p></li></ol><h4 id="面试题-你是怎么合理评估任务最大并行度？"><a href="#面试题-你是怎么合理评估任务最大并行度？" class="headerlink" title="面试题: 你是怎么合理评估任务最大并行度？"></a>面试题: 你是怎么合理评估任务最大并行度？</h4><ol><li> 前提：并行度必须 &lt;= 最大并行度</li><li> 最大并行度的作用：合理设置最大并行度可以缓解数据倾斜的问题</li><li> 根据具体场景的不同，最大并行度大小设置也有不同的方式：</li></ol><ul><li><strong>在 key 非常多的情况下，最大并行度适合设置比较大（几千）</strong>，不容易出现数据倾斜，以 Flink SQL 场景举例：row_number = 1 partition key user_id 的 Deduplicate 场景（user_id 一般都非常多）</li><li><strong>在 key 不是很多的情况下，最大并行度适合设置不是很大</strong>，不然会加重数据倾斜，以 Flink SQL 场景举例：group by dim1,dim2 聚合并且维度值不多的 group agg 场景（dim1，dim2 可以枚举），如果依然有数据倾斜的问题，需要自己先打散数据，缓解数据倾斜</li></ul><ol><li><p> 最大并行度的使用限制：<strong>最大并行度一旦设置，是不能随意变更的</strong>，否则会导致检查点或保存点失效；最大并行度设置会影响 MapState 状态划分的 KeyGroup 数，并行度修改后再从保存点启动时，KeyGroup 会根据并行度的设定进行重新分布。</p></li><li><p>最大并行度的设置：最大并行度可以自己设置，也可以框架默认生成；默认的算法是取当前算子并行度的 1.5 倍和 2 的 7 次方比较，取两者之间的最大值，然后用上面的结果和 2 的 15 次方比较，取其中的最小值为默认的最大并行度，<strong>非常不建议自动生成，建议用户自己设置</strong>。</p></li></ol><p><strong>收录文章地址</strong>:</p><p><a href="https://mp.weixin.qq.com/s/meKL9R0zlkLh-OAWyfLZ1g">https://mp.weixin.qq.com/s/meKL9R0zlkLh-OAWyfLZ1g</a></p>]]></content>
    
    
    <summary type="html">&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;文章收录的是公众号: 大数据公羊说 的 面试题总结.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;p&gt;[toc]&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="flink" scheme="http://xubatian.cn/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>hive常用函数之分区表和分桶表</title>
    <link href="http://xubatian.cn/hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E4%B9%8B%E5%88%86%E5%8C%BA%E8%A1%A8%E5%92%8C%E5%88%86%E6%A1%B6%E8%A1%A8/"/>
    <id>http://xubatian.cn/hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E4%B9%8B%E5%88%86%E5%8C%BA%E8%A1%A8%E5%92%8C%E5%88%86%E6%A1%B6%E8%A1%A8/</id>
    <published>2022-02-28T15:19:57.000Z</published>
    <updated>2022-02-28T15:57:34.254Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220228232524.png"></p><p>一般生产当中会有一级分区和二级分区. 如果数据量比较大,会有二级分区. 比如按照每天的每个小时进行分区.    分区的原因就是数据量太大了. 分区指的是reduce里面. 而map里面叫分片.  </p><p><strong>map里面的切片和reduce里面的分区有本质意义上的区别吗?</strong></p><p>没有区别, 你map端分片.其实也是提高并行度. 其实都是将数据分开,然后大家一起去计算.</p><p>hive里面有张表叫做分区表. 表里面放数据. 这张表每天一个分区. 就是每天一个目录. 以后查数据的时候,直接用where筛选目录.即,查询的sql中写上分区信息. 这样我就能够避免全表扫描.</p><span id="more"></span><h3 id="分区表基本操作"><a href="#分区表基本操作" class="headerlink" title="分区表基本操作"></a><strong>分区表基本操作</strong></h3><h4 id="1）引入分区表（需要根据日期对日志进行管理-通过部门信息模拟）"><a href="#1）引入分区表（需要根据日期对日志进行管理-通过部门信息模拟）" class="headerlink" title="1）引入分区表（需要根据日期对日志进行管理, 通过部门信息模拟）"></a><strong>1）引入分区表（需要根据日期对日志进行管理, 通过部门信息模拟）</strong></h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dept_20200401.log</span><br><span class="line">dept_20200402.log</span><br><span class="line">dept_20200403.log</span><br></pre></td></tr></table></figure><h4 id="2）创建分区表语法"><a href="#2）创建分区表语法" class="headerlink" title="2）创建分区表语法"></a><strong>2）创建分区表语法</strong></h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> dept_partition(</span><br><span class="line">deptno <span class="type">int</span>, </span><br><span class="line">dname string, </span><br><span class="line">loc string</span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">day</span> string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>注意：分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。</p><h4 id="3）加载数据到分区表中"><a href="#3）加载数据到分区表中" class="headerlink" title="3）加载数据到分区表中"></a><strong>3）加载数据到分区表中</strong></h4><p>（1）数据准备</p><p>dept_20200401.log</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span>ACCOUNTING<span class="number">1700</span></span><br><span class="line"><span class="number">20</span>RESEARCH<span class="number">1800</span></span><br></pre></td></tr></table></figure><p>dept_20200402.log</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">30</span>SALES<span class="number">1900</span></span><br><span class="line"><span class="number">40</span>OPERATIONS<span class="number">1700</span></span><br></pre></td></tr></table></figure><p>dept_20200403.log</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">50</span>TEST<span class="number">2000</span></span><br><span class="line"><span class="number">60</span>DEV<span class="number">1900</span></span><br></pre></td></tr></table></figure><p>（2）加载数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/hive/datas/dept_20200401.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> dept_partition <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span>);</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/hive/datas/dept_20200402.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> dept_partition <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200402&#x27;</span>);</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/hive/datas/dept_20200403.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> dept_partition <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200403&#x27;</span>);</span><br></pre></td></tr></table></figure><p>注意：分区表加载数据时，必须指定分区</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220228234117.png"></p><h4 id="4）查询分区表中数据"><a href="#4）查询分区表中数据" class="headerlink" title="4）查询分区表中数据"></a><strong>4）查询分区表中数据</strong></h4><p>单分区查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span>;</span><br></pre></td></tr></table></figure><p>多分区联合查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span></span><br><span class="line">              <span class="keyword">union</span></span><br><span class="line">              <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200402&#x27;</span></span><br><span class="line">              <span class="keyword">union</span></span><br><span class="line">              <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200403&#x27;</span>;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span> <span class="keyword">or</span></span><br><span class="line">                <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200402&#x27;</span> <span class="keyword">or</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200403&#x27;</span> ;</span><br></pre></td></tr></table></figure><h4 id="5）增加分区"><a href="#5）增加分区" class="headerlink" title="5）增加分区"></a>5）增加分区</h4><p>创建单个分区</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200404&#x27;</span>) ;</span><br></pre></td></tr></table></figure><p>同时创建多个分区</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200405&#x27;</span>) <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200406&#x27;</span>);</span><br></pre></td></tr></table></figure><h4 id="6）删除分区"><a href="#6）删除分区" class="headerlink" title="6）删除分区"></a>6）删除分区</h4><p>删除单个分区</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span> (<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200406&#x27;</span>);</span><br></pre></td></tr></table></figure><p>同时删除多个分区</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span> (<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200404&#x27;</span>), <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200405&#x27;</span>);</span><br></pre></td></tr></table></figure><h4 id="7-查看分区表有多少分区"><a href="#7-查看分区表有多少分区" class="headerlink" title="7) 查看分区表有多少分区"></a>7) 查看分区表有多少分区</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">show</span> partitions dept_partition;</span><br></pre></td></tr></table></figure><h4 id="8）查看分区表结构"><a href="#8）查看分区表结构" class="headerlink" title="8）查看分区表结构"></a>8）查看分区表结构</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">desc</span> formatted dept_partition;</span><br><span class="line"></span><br><span class="line"># <span class="keyword">Partition</span> Information          </span><br><span class="line"># col_name              data_type               comment             </span><br><span class="line"><span class="keyword">month</span>                   string    </span><br></pre></td></tr></table></figure><h3 id="二级分区"><a href="#二级分区" class="headerlink" title="二级分区"></a>二级分区</h3><p>思考: 如何一天的日志数据量也很大，如何再将数据拆分?</p><h4 id="1）创建二级分区表"><a href="#1）创建二级分区表" class="headerlink" title="1）创建二级分区表"></a>1）创建二级分区表</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> dept_partition2(</span><br><span class="line">               deptno <span class="type">int</span>, dname string, loc string</span><br><span class="line">               )</span><br><span class="line">               partitioned <span class="keyword">by</span> (<span class="keyword">day</span> string, <span class="keyword">hour</span> string)</span><br><span class="line">               <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><h4 id="2）正常的加载数据"><a href="#2）正常的加载数据" class="headerlink" title="2）正常的加载数据"></a>2）正常的加载数据</h4><h5 id="（1）加载数据到二级分区表中"><a href="#（1）加载数据到二级分区表中" class="headerlink" title="（1）加载数据到二级分区表中"></a>（1）加载数据到二级分区表中</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/hive/datas/dept_20200401.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span></span><br><span class="line">dept_partition2 <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span>, <span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;12&#x27;</span>);</span><br></pre></td></tr></table></figure><h5 id="（2）查询分区数据"><a href="#（2）查询分区数据" class="headerlink" title="（2）查询分区数据"></a>（2）查询分区数据</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition2 <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span> <span class="keyword">and</span> <span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;12&#x27;</span>;</span><br></pre></td></tr></table></figure><h4 id="3）把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式"><a href="#3）把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式" class="headerlink" title="3）把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式"></a>3）把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式</h4><h5 id="（1）方式一：上传数据后修复"><a href="#（1）方式一：上传数据后修复" class="headerlink" title="（1）方式一：上传数据后修复"></a>（1）方式一：上传数据后修复</h5><p>上传数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p</span><br><span class="line"> <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mydb.db<span class="operator">/</span>dept_partition2<span class="operator">/</span><span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span><span class="operator">/</span><span class="keyword">hour</span><span class="operator">=</span><span class="number">13</span>;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>put <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>datas<span class="operator">/</span>dept_20200401.log  <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mydb.db<span class="operator">/</span>dept_partition2<span class="operator">/</span><span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span><span class="operator">/</span><span class="keyword">hour</span><span class="operator">=</span><span class="number">13</span>;</span><br></pre></td></tr></table></figure><p>查询数据（查询不到刚上传的数据）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition2 <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span> <span class="keyword">and</span> <span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;13&#x27;</span>;</span><br></pre></td></tr></table></figure><p>执行修复命令</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> msck repair <span class="keyword">table</span> dept_partition2;</span><br></pre></td></tr></table></figure><p>再次查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition2 <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span> <span class="keyword">and</span> <span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;13&#x27;</span>;</span><br></pre></td></tr></table></figure><h5 id="（2）方式二：上传数据后添加分区"><a href="#（2）方式二：上传数据后添加分区" class="headerlink" title="（2）方式二：上传数据后添加分区"></a>（2）方式二：上传数据后添加分区</h5><p>上传数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p</span><br><span class="line"> <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mydb.db<span class="operator">/</span>dept_partition2<span class="operator">/</span><span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span><span class="operator">/</span><span class="keyword">hour</span><span class="operator">=</span><span class="number">14</span>;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>put <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>hive<span class="operator">/</span>datas<span class="operator">/</span>dept_20200401.log  <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mydb.db<span class="operator">/</span>dept_partition2<span class="operator">/</span><span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span><span class="operator">/</span><span class="keyword">hour</span><span class="operator">=</span><span class="number">14</span>;</span><br></pre></td></tr></table></figure><p>执行添加分区</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept_partition2 <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span>,<span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;14&#x27;</span>);</span><br></pre></td></tr></table></figure><p>查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition2 <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span> <span class="keyword">and</span> <span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;14&#x27;</span>;</span><br></pre></td></tr></table></figure><h5 id="（3）方式三：创建文件夹后load数据到分区"><a href="#（3）方式三：创建文件夹后load数据到分区" class="headerlink" title="（3）方式三：创建文件夹后load数据到分区"></a>（3）方式三：创建文件夹后load数据到分区</h5><p>创建目录</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p</span><br><span class="line"> <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mydb.db<span class="operator">/</span>dept_partition2<span class="operator">/</span><span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span><span class="operator">/</span><span class="keyword">hour</span><span class="operator">=</span><span class="number">15</span>;</span><br></pre></td></tr></table></figure><p>上传数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/hive/datas/dept_20200401.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span></span><br><span class="line"> dept_partition2 <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span>,<span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;15&#x27;</span>);</span><br></pre></td></tr></table></figure><p>查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition2 <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span> <span class="keyword">and</span> <span class="keyword">hour</span><span class="operator">=</span><span class="string">&#x27;15&#x27;</span>;</span><br></pre></td></tr></table></figure><h3 id="动态分区调整"><a href="#动态分区调整" class="headerlink" title="动态分区调整"></a>动态分区调整</h3><p>​        关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。</p><h4 id="1）开启动态分区参数设置"><a href="#1）开启动态分区参数设置" class="headerlink" title="1）开启动态分区参数设置"></a>1）开启动态分区参数设置</h4><h5 id="（1）开启动态分区功能（默认true，开启）"><a href="#（1）开启动态分区功能（默认true，开启）" class="headerlink" title="（1）开启动态分区功能（默认true，开启）"></a>（1）开启动态分区功能（默认true，开启）</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.dynamic.partition<span class="operator">=</span><span class="literal">true</span></span><br></pre></td></tr></table></figure><h5 id="（2）设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）"><a href="#（2）设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）" class="headerlink" title="（2）设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）"></a>（2）设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.dynamic.partition.mode<span class="operator">=</span>nonstrict</span><br></pre></td></tr></table></figure><h5 id="（3）在所有执行MR的节点上，最大一共可以创建多少个动态分区。默认1000"><a href="#（3）在所有执行MR的节点上，最大一共可以创建多少个动态分区。默认1000" class="headerlink" title="（3）在所有执行MR的节点上，最大一共可以创建多少个动态分区。默认1000"></a>（3）在所有执行MR的节点上，最大一共可以创建多少个动态分区。默认1000</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions<span class="operator">=</span><span class="number">1000</span></span><br></pre></td></tr></table></figure><h5 id="（4）在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。"><a href="#（4）在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。" class="headerlink" title="（4）在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。"></a>（4）在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions.pernode<span class="operator">=</span><span class="number">100</span></span><br></pre></td></tr></table></figure><h5 id="（5）整个MR-Job中，最大可以创建多少个HDFS文件。默认100000"><a href="#（5）整个MR-Job中，最大可以创建多少个HDFS文件。默认100000" class="headerlink" title="（5）整个MR Job中，最大可以创建多少个HDFS文件。默认100000"></a>（5）整个MR Job中，最大可以创建多少个HDFS文件。默认100000</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.created.files<span class="operator">=</span><span class="number">100000</span></span><br></pre></td></tr></table></figure><h5 id="（6）当有空分区生成时，是否抛出异常。一般不需要设置。默认false"><a href="#（6）当有空分区生成时，是否抛出异常。一般不需要设置。默认false" class="headerlink" title="（6）当有空分区生成时，是否抛出异常。一般不需要设置。默认false"></a>（6）当有空分区生成时，是否抛出异常。一般不需要设置。默认false</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.error.on.empty.partition<span class="operator">=</span><span class="literal">false</span></span><br></pre></td></tr></table></figure><h4 id="2）案例实操"><a href="#2）案例实操" class="headerlink" title="2）案例实操"></a>2）案例实操</h4><p>需求：将dept表中的数据按照地区（loc字段），插入到目标表dept_partition的相应分区中。</p><h5 id="（1）创建目标分区表"><a href="#（1）创建目标分区表" class="headerlink" title="（1）创建目标分区表"></a>（1）创建目标分区表</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> dept_partition_dy(id <span class="type">int</span>, name string) partitioned <span class="keyword">by</span> (loc <span class="type">int</span>) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><h5 id="（2）设置动态分区"><a href="#（2）设置动态分区" class="headerlink" title="（2）设置动态分区"></a>（2）设置动态分区</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode <span class="operator">=</span> nonstrict;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> dept_partition_dy <span class="keyword">partition</span>(loc) <span class="keyword">select</span> deptno, dname, loc <span class="keyword">from</span> dept;</span><br></pre></td></tr></table></figure><h5 id="（3）查看目标分区表的分区情况"><a href="#（3）查看目标分区表的分区情况" class="headerlink" title="（3）查看目标分区表的分区情况"></a>（3）查看目标分区表的分区情况</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">show</span> partitions dept_partition;</span><br></pre></td></tr></table></figure><p>思考：目标分区表是如何匹配到分区字段的？</p><h1 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h1><p>​        分区提供一个隔离数据和优化查询的便利方式。不过，<strong>并非所有的数据集都可形成合理的分区</strong>。<strong>对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。</strong></p><p>​        <strong>分桶是将数据集分解成更容易管理的若干部分的另一个技术。</strong></p><p>​        <strong>分区针对的是数据的存储路径；分桶针对的是数据文件。</strong></p><h2 id="1）先创建分桶表"><a href="#1）先创建分桶表" class="headerlink" title="1）先创建分桶表"></a>1）先创建分桶表</h2><h3 id="（1）数据准备"><a href="#（1）数据准备" class="headerlink" title="（1）数据准备"></a>（1）数据准备</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1001</span>ss1</span><br><span class="line"><span class="number">1002</span>ss2</span><br><span class="line"><span class="number">1003</span>ss3</span><br><span class="line"><span class="number">1004</span>ss4</span><br><span class="line"><span class="number">1005</span>ss5</span><br><span class="line"><span class="number">1006</span>ss6</span><br><span class="line"><span class="number">1007</span>ss7</span><br><span class="line"><span class="number">1008</span>ss8</span><br><span class="line"><span class="number">1009</span>ss9</span><br><span class="line"><span class="number">1010</span>ss10</span><br><span class="line"><span class="number">1011</span>ss11</span><br><span class="line"><span class="number">1012</span>ss12</span><br><span class="line"><span class="number">1013</span>ss13</span><br><span class="line"><span class="number">1014</span>ss14</span><br><span class="line"><span class="number">1015</span>ss15</span><br><span class="line"><span class="number">1016</span>ss16</span><br></pre></td></tr></table></figure><h3 id="（2）创建分桶表"><a href="#（2）创建分桶表" class="headerlink" title="（2）创建分桶表"></a>（2）创建分桶表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck(id <span class="type">int</span>, name string)</span><br><span class="line">clustered <span class="keyword">by</span>(id) </span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><h3 id="（3）查看表结构"><a href="#（3）查看表结构" class="headerlink" title="（3）查看表结构"></a>（3）查看表结构</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">desc</span> formatted stu_buck;</span><br><span class="line">Num Buckets:            <span class="number">4</span>     </span><br></pre></td></tr></table></figure><h3 id="（4）导入数据到分桶表中，load的方式"><a href="#（4）导入数据到分桶表中，load的方式" class="headerlink" title="（4）导入数据到分桶表中，load的方式"></a>（4）导入数据到分桶表中，load的方式</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data inpath   <span class="string">&#x27;/student.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck;</span><br></pre></td></tr></table></figure><h3 id="（5）查看创建的分桶表中是否分成4个桶"><a href="#（5）查看创建的分桶表中是否分成4个桶" class="headerlink" title="（5）查看创建的分桶表中是否分成4个桶"></a>（5）查看创建的分桶表中是否分成4个桶</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220228235253.png"></p><h3 id="（6）查询分桶的数据"><a href="#（6）查询分桶的数据" class="headerlink" title="（6）查询分桶的数据"></a>（6）查询分桶的数据</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> stu_buck;</span><br></pre></td></tr></table></figure><h3 id="（7）分桶规则："><a href="#（7）分桶规则：" class="headerlink" title="（7）分桶规则："></a>（7）分桶规则：</h3><p>根据结果可知：Hive的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方 式决定该条记录存放在哪个桶当中</p><h2 id="2）分桶表操作需要注意的事项"><a href="#2）分桶表操作需要注意的事项" class="headerlink" title="2）分桶表操作需要注意的事项:"></a>2）分桶表操作需要注意的事项:</h2><p><strong>（1）reduce的个数设置为-1,让Job自行决定需要用多少个reduce或者将reduce的个数设置为大于等于分桶表的桶数</strong><br><strong>（2）从hdfs中load数据到分桶表中，避免本地文件找不到问题</strong><br><strong>（3）不要使用本地模式</strong></p><h2 id="3）insert方式将数据导入分桶表"><a href="#3）insert方式将数据导入分桶表" class="headerlink" title="3）insert方式将数据导入分桶表"></a>3）insert方式将数据导入分桶表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student_insert ;</span><br></pre></td></tr></table></figure><h2 id="抽样查询"><a href="#抽样查询" class="headerlink" title="抽样查询"></a>抽样查询</h2><p>​    对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。<br>语法: TABLESAMPLE(BUCKET x OUT OF y)<br>查询表stu_buck中的数据。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> stu_buck <span class="keyword">tablesample</span>(bucket <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span> <span class="keyword">on</span> id);</span><br></pre></td></tr></table></figure><p>注意：x的值必须小于等于y的值，否则</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FAILED: SemanticException [Error <span class="number">10061</span>]: Numerator should <span class="keyword">not</span> be bigger than denominator <span class="keyword">in</span> sample clause <span class="keyword">for</span> <span class="keyword">table</span> stu_buck</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;分区表&quot;&gt;&lt;a href=&quot;#分区表&quot; class=&quot;headerlink&quot; title=&quot;分区表&quot;&gt;&lt;/a&gt;分区表&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220228232524.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;一般生产当中会有一级分区和二级分区. 如果数据量比较大,会有二级分区. 比如按照每天的每个小时进行分区.    分区的原因就是数据量太大了. 分区指的是reduce里面. 而map里面叫分片.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;map里面的切片和reduce里面的分区有本质意义上的区别吗?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;没有区别, 你map端分片.其实也是提高并行度. 其实都是将数据分开,然后大家一起去计算.&lt;/p&gt;
&lt;p&gt;hive里面有张表叫做分区表. 表里面放数据. 这张表每天一个分区. 就是每天一个目录. 以后查数据的时候,直接用where筛选目录.即,查询的sql中写上分区信息. 这样我就能够避免全表扫描.&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="hive" scheme="http://xubatian.cn/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>hive常用函数之排序</title>
    <link href="http://xubatian.cn/hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E4%B9%8B%E6%8E%92%E5%BA%8F/"/>
    <id>http://xubatian.cn/hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E4%B9%8B%E6%8E%92%E5%BA%8F/</id>
    <published>2022-02-28T14:16:21.000Z</published>
    <updated>2022-02-28T16:04:31.357Z</updated>
    
    <content type="html"><![CDATA[<p>​    hive 里面其实我们可以设置reduce的个数. 类似于在mapreduce里面,最后封装一个Driver一样. 可以最后写我们需要运行多少个reducer.  set mapreduce.job.reduces=3; 设置reducer个数为3. 但是在orderBy 里面,就算你最后设置了属性也不会生效. </p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220228232138.png"></p><span id="more"></span><h1 id="全局排序（Order-By）"><a href="#全局排序（Order-By）" class="headerlink" title="全局排序（Order By）"></a>全局排序（Order By）</h1><p>Order By：全局排序，会进入到一个Reducer里面,最终会生成一个结果文件</p><p><strong>1．使用 ORDER BY 子句排序</strong><br>        ASC（ascend）: 升序（默认）<br>        DESC（descend）: 降序<br><strong>2．ORDER BY 子句在SELECT语句的结尾</strong></p><p><strong>3．案例实操</strong><br>    （1）查询员工信息按工资升序排列</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal;</span><br></pre></td></tr></table></figure><p>​    （2）查询员工信息按工资降序排列</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><h2 id="按照别名排序"><a href="#按照别名排序" class="headerlink" title="按照别名排序"></a><strong>按照别名排序</strong></h2><p><strong>按照员工薪水的2倍排序</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> ename, sal<span class="operator">*</span><span class="number">2</span> twosal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> twosal;</span><br></pre></td></tr></table></figure><h2 id="多个列排序"><a href="#多个列排序" class="headerlink" title="多个列排序"></a>多个列排序</h2><p><strong>按照部门和工资升序排序</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> ename, deptno, sal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> deptno, sal ;</span><br></pre></td></tr></table></figure><h1 id="每个MapReduce内部排序（Sort-By）"><a href="#每个MapReduce内部排序（Sort-By）" class="headerlink" title="每个MapReduce内部排序（Sort By）"></a>每个MapReduce内部排序（Sort By）</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220228223331.png"></p><p>什么情况下map进入多个reduce呢?  分区.  </p><p>Sort By：对每个Reducer里面的数据进行排序，对全局结果集来说不是排序。<br>Order by : 是对整个数据进行排序</p><p>1．设置reduce个数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">3</span>;</span><br></pre></td></tr></table></figure><p>2．查看设置reduce个数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.job.reduces;</span><br></pre></td></tr></table></figure><p>3．根据部门编号降序查看员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp sort <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><p>4．将查询结果导入到文件中（按照部门编号降序排序）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/opt/module/datas/sortby-result&#x27;</span></span><br><span class="line"> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp sort <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><p><strong>MapReduce里面是按照hash来进行分区的,所以我们无法指定分区. 所以他是没有规律的. 所以我们使用sort by的话 一般会和后面的字段连用,执行分区规则. 因为当我们不指定分区规则的时候,他就是随机的. 因为他害怕数据倾斜</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220228223654.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_287.png" alt="blog: www.xubatian.cn"></p><p><strong>可以看到,这是随机分配的,是没有规律的,我们一般排序肯定会先指定分区然后在排序,分区的话,一般我们会指定按照什么来分区,然而这里没有,说明这种排序方式不提供指定分区,从而出现了随机分配的现象</strong></p><p>hive选择语法:</p><p>官网</p><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220228224310.png"></p><h1 id="分区排序（Distribute-By）和sort-by-连用-因为sort-by没有分区规则"><a href="#分区排序（Distribute-By）和sort-by-连用-因为sort-by没有分区规则" class="headerlink" title="分区排序（Distribute By）和sort by 连用,因为sort by没有分区规则"></a>分区排序（Distribute By）和sort by 连用,因为sort by没有分区规则</h1><p>先使用distribute by 然后再使用 sort by.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">比如: 我可以按照部门编号进行分区,然后再部门内部做一个按照薪水的排序</span><br><span class="line">这种方式用<span class="keyword">order</span> <span class="keyword">by</span> 全局排序是这么写的:</span><br><span class="line"><span class="keyword">select</span> deptno,ename,sal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> deptno,sal;</span><br><span class="line">但是 这种我们设置的reduce个数只能是<span class="number">1</span>个.即使我们设置为<span class="number">3</span>个reduce.但是他不生效.</span><br><span class="line">但是用 先分区(distribute <span class="keyword">by</span>) 再 分区内排序(sort <span class="keyword">by</span>)的话是这么写的:</span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">4</span>;</span><br><span class="line"><span class="keyword">select</span> deptno,ename,sal <span class="keyword">from</span> emp distribute <span class="keyword">by</span> deptno sort <span class="keyword">by</span> sal;</span><br><span class="line">但是这样我们也看不出来他的分区,因为他还是按照hash的方式进行分区的. </span><br></pre></td></tr></table></figure><p>所以,如果我们想通过区内排序的规则来达到我们order by的规则的话, 我们就需要自定义分区.就是要把我们不同的部门放到不同的区里面,然后拼接起来就到达了order by的效果. 而不是使用hash的方式分区.    </p><p>这种方式确确实实分区了,分区之后你在哪个区就用sort by了</p><p>Distribute By：类似MR中partition，进行分区,就是先分好区,分好之后在每个partition里面进行sort by一下，结合sort by使用。</p><p>注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。<br>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><p>Distribute指定正常的分区字段,指定之后就可以正常的分区操作了</p><h1 id="Cluster-By"><a href="#Cluster-By" class="headerlink" title="Cluster By"></a>Cluster By</h1><p><strong>当distribute by和sort by字段相同时，可以使用cluster by方式。</strong><br>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。<br>Sort by可以指定是升序还是降序,但是用cluster by之后就不能指定了,只能是升序<br>1）以下两种写法等价</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp distribute <span class="keyword">by</span> deptno sort <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure><p>注意：按照部门编号分区，不一定就是固定死的数值，可以是30号和60号部门分到一个分区里面去. 因为他是按照hash规则将hash相同的放到同一个分区里面的.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;​    hive 里面其实我们可以设置reduce的个数. 类似于在mapreduce里面,最后封装一个Driver一样. 可以最后写我们需要运行多少个reducer.  set mapreduce.job.reduces=3; 设置reducer个数为3. 但是在orderBy 里面,就算你最后设置了属性也不会生效. &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220228232138.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="hive" scheme="http://xubatian.cn/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink流程小总结</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E6%B5%81%E7%A8%8B%E5%B0%8F%E6%80%BB%E7%BB%93/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E6%B5%81%E7%A8%8B%E5%B0%8F%E6%80%BB%E7%BB%93/</id>
    <published>2022-02-20T15:28:43.000Z</published>
    <updated>2022-02-20T15:38:39.097Z</updated>
    
    <content type="html"><![CDATA[<h1 id="整个Flink-Job运行过程图"><a href="#整个Flink-Job运行过程图" class="headerlink" title="整个Flink Job运行过程图"></a>整个Flink Job运行过程图</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220220144839.png"></p><span id="more"></span><h1 id="Flink-Job提交流程到背压到TaskManager里面的slot-任务链等详情"><a href="#Flink-Job提交流程到背压到TaskManager里面的slot-任务链等详情" class="headerlink" title="Flink Job提交流程到背压到TaskManager里面的slot,任务链等详情"></a>Flink Job提交流程到背压到TaskManager里面的slot,任务链等详情</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220220155717.png"></p><h1 id="Flink-窗口总结"><a href="#Flink-窗口总结" class="headerlink" title="Flink 窗口总结"></a>Flink 窗口总结</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220220181733.png"></p><h1 id="Flink-时间语义及watermark机制小总结"><a href="#Flink-时间语义及watermark机制小总结" class="headerlink" title="Flink 时间语义及watermark机制小总结"></a>Flink 时间语义及watermark机制小总结</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220220195829.png"></p><h1 id="Flink-状态一致性及保证状态的机制-checkpoint检查点算法"><a href="#Flink-状态一致性及保证状态的机制-checkpoint检查点算法" class="headerlink" title="Flink 状态一致性及保证状态的机制(checkpoint检查点算法)"></a>Flink 状态一致性及保证状态的机制(checkpoint检查点算法)</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220220232454.png"></p><h1 id="Flink和kafka是如何实现端到端的exactly-once的呢"><a href="#Flink和kafka是如何实现端到端的exactly-once的呢" class="headerlink" title="Flink和kafka是如何实现端到端的exactly-once的呢?"></a>Flink和kafka是如何实现端到端的exactly-once的呢?</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214231316.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214164852.png"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;整个Flink-Job运行过程图&quot;&gt;&lt;a href=&quot;#整个Flink-Job运行过程图&quot; class=&quot;headerlink&quot; title=&quot;整个Flink Job运行过程图&quot;&gt;&lt;/a&gt;整个Flink Job运行过程图&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220220144839.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink内存管理: 网络传输中的内存管理</title>
    <link href="http://xubatian.cn/Flink%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86-%E7%BD%91%E7%BB%9C%E4%BC%A0%E8%BE%93%E4%B8%AD%E7%9A%84%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    <id>http://xubatian.cn/Flink%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86-%E7%BD%91%E7%BB%9C%E4%BC%A0%E8%BE%93%E4%B8%AD%E7%9A%84%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</id>
    <published>2022-02-19T13:31:41.000Z</published>
    <updated>2022-02-20T07:58:38.238Z</updated>
    
    <content type="html"><![CDATA[<h3 id="网络传输中的内存管理"><a href="#网络传输中的内存管理" class="headerlink" title="网络传输中的内存管理"></a>网络传输中的内存管理</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220220135443.png"></p><p>​    网络上传输的数据会写到 Task 的 InputGate（IG） 中，经过 Task 的处理后，再由 Task 写到 ResultPartition（RS） 中。每个 Task 都包括了输入和输入，输入和输出的数据存在 Buffer 中（都是字节数据）。Buffer 是 MemorySegment 的包装类。</p><span id="more"></span><p>​        1）TaskManager（TM）在启动时，会先初始化NetworkEnvironment对象，TM 中所有与网络相关的东西都由该类来管理（如 Netty 连接），其中就包括NetworkBufferPool。根据配置，Flink 会在 NetworkBufferPool 中生成一定数量（默认2048）的内存块 MemorySegment（关于 Flink 的内存管理，后续文章会详细谈到），内存块的总数量就代表了网络传输中所有可用的内存。NetworkEnvironment 和 NetworkBufferPool 是 Task 之间共享的，每个 TM 只会实例化一个。</p><p>​        2）Task 线程启动时，会向 NetworkEnvironment 注册，NetworkEnvironment 会为 Task 的 InputGate（IG）和 ResultPartition（RP） 分别创建一个 LocalBufferPool（缓冲池）并设置可申请的 MemorySegment（内存块）数量。IG 对应的缓冲池初始的内存块数量与 IG 中 InputChannel 数量一致，RP 对应的缓冲池初始的内存块数量与 RP 中的 ResultSubpartition 数量一致。不过，每当创建或销毁缓冲池时，NetworkBufferPool 会计算剩余空闲的内存块数量，并平均分配给已创建的缓冲池。注意，这个过程只是指定了缓冲池所能使用的内存块数量，并没有真正分配内存块，只有当需要时才分配。为什么要动态地为缓冲池扩容呢？因为内存越多，意味着系统可以更轻松地应对瞬时压力（如GC），不会频繁地进入反压状态，所以我们要利用起那部分闲置的内存块。</p><p>​        3）在 Task 线程执行过程中，当 Netty 接收端收到数据时，为了将 Netty 中的数据拷贝到 Task 中，InputChannel（实际是 RemoteInputChannel）会向其对应的缓冲池申请内存块（上图中的①）。如果缓冲池中也没有可用的内存块且已申请的数量还没到池子上限，则会向 NetworkBufferPool 申请内存块（上图中的②）并交给 InputChannel 填上数据（上图中的③和④）。如果缓冲池已申请的数量达到上限了呢？或者 NetworkBufferPool 也没有可用内存块了呢？这时候，Task 的 Netty Channel 会暂停读取，上游的发送端会立即响应停止发送，拓扑会进入反压状态。当 Task 线程写数据到 ResultPartition 时，也会向缓冲池请求内存块，如果没有可用内存块时，会阻塞在请求内存块的地方，达到暂停写入的目的。</p><p>​        4）当一个内存块被消费完成之后（在输入端是指内存块中的字节被反序列化成对象了，在输出端是指内存块中的字节写入到 Netty Channel 了），会调用 Buffer.recycle() 方法，会将内存块还给 LocalBufferPool （上图中的⑤）。如果LocalBufferPool中当前申请的数量超过了池子容量（由于上文提到的动态容量，由于新注册的 Task 导致该池子容量变小），则LocalBufferPool会将该内存块回收给 NetworkBufferPool（上图中的⑥）。如果没超过池子容量，则会继续留在池子中，减少反复申请的开销。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219214650.png"></p><p>​    </p><h3 id="反压的过程"><a href="#反压的过程" class="headerlink" title="反压的过程"></a>反压的过程</h3><p>spark里面也有背压机制.背压机制需要参数明确的开启.Flink是不需要的.</p><p>Flink是如何实现反压的呢?</p><p>​    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219215719.png"></p><p>1）记录“A”进入了 Flink 并且被 Task 1 处理。（这里省略了 Netty 接收、反序列化等过程）</p><p>2）记录被序列化到 buffer 中。</p><p>3）该 buffer 被发送到 Task 2，然后 Task 2 从这个 buffer 中读出记录。</p><p>记录能被 Flink 处理的前提是：必须有空闲可用的 Buffer。</p><p>结合上面两张图看：Task 1 在输出端有一个相关联的 LocalBufferPool（称缓冲池1），Task 2 在输入端也有一个相关联的 LocalBufferPool（称缓冲池2）。如果缓冲池1中有空闲可用的 buffer 来序列化记录 “A”，我们就序列化并发送该 buffer。</p><p>注意两个场景：</p><p>1）本地传输：如果 Task 1 和 Task 2 运行在同一个 worker 节点（TaskManager），该 buffer 可以直接交给下一个 Task。一旦 Task 2 消费了该 buffer，则该 buffer 会被缓冲池1回收。如果 Task 2 的速度比 1 慢，那么 buffer 回收的速度就会赶不上 Task 1 取 buffer 的速度，导致缓冲池1无可用的 buffer，Task 1 等待在可用的 buffer 上。最终形成 Task 1 的降速。</p><p>2）远程传输：如果 Task 1 和 Task 2 运行在不同的 worker 节点上，那么 buffer 会在发送到网络（TCP Channel）后被回收。在接收端，会从 LocalBufferPool 中申请 buffer，然后拷贝网络中的数据到 buffer 中。如果没有可用的 buffer，会停止从 TCP 连接中读取数据。在输出端，通过 Netty 的水位值机制来保证不往网络中写入太多数据（后面会说）。如果网络中的数据（Netty输出缓冲中的字节数）超过了高水位值，我们会等到其降到低水位值以下才继续写入数据。这保证了网络中不会有太多的数据。如果接收端停止消费网络中的数据（由于接收端缓冲池没有可用 buffer），网络中的缓冲数据就会堆积，那么发送端也会暂停发送。另外，这会使得发送端的缓冲池得不到回收，writer 阻塞在向 LocalBufferPool 请求 buffer，阻塞了 writer 往 ResultSubPartition 写数据。</p><p>这种固定大小缓冲池就像阻塞队列一样，保证了 Flink 有一套健壮的反压机制，使得 Task 生产数据的速度不会快于消费的速度。我们上面描述的这个方案可以从两个 Task 之间的数据传输自然地扩展到更复杂的 pipeline 中，保证反压机制可以扩散到整个 pipeline。</p><h3 id="Flink-Job提交流程到背压到TaskManager里面的slot-任务链等详情"><a href="#Flink-Job提交流程到背压到TaskManager里面的slot-任务链等详情" class="headerlink" title="Flink Job提交流程到背压到TaskManager里面的slot,任务链等详情"></a>Flink Job提交流程到背压到TaskManager里面的slot,任务链等详情</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220220155717.png"></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;网络传输中的内存管理&quot;&gt;&lt;a href=&quot;#网络传输中的内存管理&quot; class=&quot;headerlink&quot; title=&quot;网络传输中的内存管理&quot;&gt;&lt;/a&gt;网络传输中的内存管理&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220220135443.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;​    网络上传输的数据会写到 Task 的 InputGate（IG） 中，经过 Task 的处理后，再由 Task 写到 ResultPartition（RS） 中。每个 Task 都包括了输入和输入，输入和输出的数据存在 Buffer 中（都是字节数据）。Buffer 是 MemorySegment 的包装类。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink内存管理: 内存管理器</title>
    <link href="http://xubatian.cn/Flink%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%99%A8/"/>
    <id>http://xubatian.cn/Flink%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%99%A8/</id>
    <published>2022-02-19T13:04:52.000Z</published>
    <updated>2022-02-19T13:18:49.920Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TaskManager内存模型"><a href="#TaskManager内存模型" class="headerlink" title="TaskManager内存模型"></a>TaskManager内存模型</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219184944.png"></p><h2 id="内存管理器作用"><a href="#内存管理器作用" class="headerlink" title="内存管理器作用"></a>内存管理器作用</h2><p>​    我们有一块内存叫做管理内存. 内存管理器就是用来管理这块内存的.你设计的这块内存是不是得有人来操作维护的呀? 就是内存管理器维护管理内存的.</p><p>​        <span id="more"></span></p><p>​        Flink 1.10 对TaskManager的内存模型和Flink应用程序的配置选项进行了重大更改，让用户能够更加严格地控制其内存开销。</p><pre><code>    1.10之前版本，负责TaskManager所有内存。    1.10版本开始，管理范围是Slot级别。</code></pre><h3 id="堆外内存资源申请"><a href="#堆外内存资源申请" class="headerlink" title="堆外内存资源申请"></a>堆外内存资源申请</h3><p>MemoryManager.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">allocatePages</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">Object owner,</span></span></span><br><span class="line"><span class="params"><span class="function">Collection&lt;MemorySegment&gt; target,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">int</span> numberOfPages)</span> <span class="keyword">throws</span> MemoryAllocationException </span>&#123;</span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">allocatedSegments.compute(owner, (o, currentSegmentsForOwner) -&gt; &#123;</span><br><span class="line">Set&lt;MemorySegment&gt; segmentsForOwner = currentSegmentsForOwner == <span class="keyword">null</span> ?</span><br><span class="line"><span class="keyword">new</span> HashSet&lt;&gt;(numberOfPages) : currentSegmentsForOwner;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">long</span> i = numberOfPages; i &gt; <span class="number">0</span>; i--) &#123;</span><br><span class="line">MemorySegment segment = allocateOffHeapUnsafeMemory(getPageSize(), owner, pageCleanup);</span><br><span class="line">target.add(segment);</span><br><span class="line">segmentsForOwner.add(segment);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> segmentsForOwner;</span><br><span class="line">&#125;);</span><br><span class="line">... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>MemorySegmentFactory.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> MemorySegment <span class="title">allocateOffHeapUnsafeMemory</span><span class="params">(<span class="keyword">int</span> size, Object owner, Runnable customCleanupAction)</span> </span>&#123;</span><br><span class="line"><span class="keyword">long</span> address = MemoryUtils.allocateUnsafe(size);</span><br><span class="line">ByteBuffer offHeapBuffer = MemoryUtils.wrapUnsafeMemoryWithByteBuffer(address, size);</span><br><span class="line">MemoryUtils.createMemoryGcCleaner(offHeapBuffer, address, customCleanupAction);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> HybridMemorySegment(offHeapBuffer, owner);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="RocksDB自己负责内存申请和释放"><a href="#RocksDB自己负责内存申请和释放" class="headerlink" title="RocksDB自己负责内存申请和释放"></a>RocksDB自己负责内存申请和释放</h3><p>RocksDBOperationUtils.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> OpaqueMemoryResource&lt;RocksDBSharedResources&gt; <span class="title">allocateSharedCachesIfConfigured</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">RocksDBMemoryConfiguration memoryConfig,</span></span></span><br><span class="line"><span class="params"><span class="function">MemoryManager memoryManager,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">double</span> memoryFraction,</span></span></span><br><span class="line"><span class="params"><span class="function">Logger logger)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (memoryConfig.isUsingFixedMemoryPerSlot()) &#123;</span><br><span class="line"><span class="keyword">assert</span> memoryConfig.getFixedMemoryPerSlot() != <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">logger.info(<span class="string">&quot;Getting fixed-size shared cache for RocksDB.&quot;</span>);</span><br><span class="line"><span class="keyword">return</span> memoryManager.getExternalSharedMemoryResource(</span><br><span class="line">FIXED_SLOT_MEMORY_RESOURCE_ID, allocator, memoryConfig.getFixedMemoryPerSlot().getBytes());</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">logger.info(<span class="string">&quot;Getting managed memory shared cache for RocksDB.&quot;</span>);</span><br><span class="line"><span class="keyword">return</span> memoryManager.getSharedMemoryResourceForManagedMemory(MANAGED_MEMORY_RESOURCE_ID, allocator, memoryFraction);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>MemoryManager.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;T extends AutoCloseable&gt; <span class="function">OpaqueMemoryResource&lt;T&gt; <span class="title">getExternalSharedMemoryResource</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">String type,</span></span></span><br><span class="line"><span class="params"><span class="function">LongFunctionWithException&lt;T, Exception&gt; initializer,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">long</span> numBytes)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// This object identifies the lease in this request. It is used only to identify the release operation.</span></span><br><span class="line"><span class="comment">// Using the object to represent the lease is a bit nicer safer than just using a reference counter.</span></span><br><span class="line"><span class="keyword">final</span> Object leaseHolder = <span class="keyword">new</span> Object();</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> SharedResources.ResourceAndSize&lt;T&gt; resource =</span><br><span class="line"><span class="comment">//获取内存资源后,get 或者 分配</span></span><br><span class="line">sharedResources.getOrAllocateSharedResource(type, leaseHolder, initializer, numBytes);</span><br><span class="line"><span class="comment">// 创建资源释放函数  : 这表示管理内存器管理的RocksDB.但是RocksDB内存的申请和释放是由RocksDB自己来的.只不过RocksDB受这个管理器的管理</span></span><br><span class="line"><span class="keyword">final</span> ThrowingRunnable&lt;Exception&gt; disposer = () -&gt; sharedResources.release(type, leaseHolder);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> OpaqueMemoryResource&lt;&gt;(resource.resourceHandle(), resource.size(), disposer);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;TaskManager内存模型&quot;&gt;&lt;a href=&quot;#TaskManager内存模型&quot; class=&quot;headerlink&quot; title=&quot;TaskManager内存模型&quot;&gt;&lt;/a&gt;TaskManager内存模型&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219184944.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;内存管理器作用&quot;&gt;&lt;a href=&quot;#内存管理器作用&quot; class=&quot;headerlink&quot; title=&quot;内存管理器作用&quot;&gt;&lt;/a&gt;内存管理器作用&lt;/h2&gt;&lt;p&gt;​    我们有一块内存叫做管理内存. 内存管理器就是用来管理这块内存的.你设计的这块内存是不是得有人来操作维护的呀? 就是内存管理器维护管理内存的.&lt;/p&gt;
&lt;p&gt;​</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink内存管理: 内存数据结构</title>
    <link href="http://xubatian.cn/Flink%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86-%E5%86%85%E5%AD%98%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    <id>http://xubatian.cn/Flink%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86-%E5%86%85%E5%AD%98%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</id>
    <published>2022-02-19T11:45:46.000Z</published>
    <updated>2022-02-19T13:17:21.490Z</updated>
    
    <content type="html"><![CDATA[<h3 id="内存段"><a href="#内存段" class="headerlink" title="内存段"></a>内存段</h3><p>​        <strong>内存段在 Flink 内部叫 MemorySegment，是 Flink 中最小的内存分配单元，默认大小32KB。Flink内存是区分堆内和堆外的,但是MemorySegment是最小单元,不区分堆内核堆外的</strong>.它即可以是堆上内存（Java的byte数组），也可以是堆外内存（基于Netty的DirectByteBuffer），同时提供了对二进制数据进行读取和写入的方法。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219194503.png"></p><p><strong>HeapMemorySegment：用来分配堆上内存</strong><br><strong>HybridMemorySegment：用来分配堆外内存和堆上内存，2017年以后的版本实际上只使用了HybridMemo`rySegment。</strong></p><span id="more"></span><p>他有什么有点? 他为什么要设置这个抽象?</p><p>假设我们有一个Tuple3&lt;Integer,Double,Person&gt; 的数据.</p><p>如下图展示一个内嵌型的Tuple3&lt;Integer,Double,Person&gt; 对象的序列化过程：</p><p>Tuple3&lt;Integer,Double,Person&gt; 对象如果用java的JVM自己来管理就会出现密度不紧凑,散列的等情况.而Flink的内存段MemorySegment 他不是散列的,他是紧密连在一起的.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219195326.png"></p><p>​        可以看出这种序列化方式存储密度是相当紧凑的。其中 int 占4字节，double 占8字节，POJO多个一个字节的header，PojoSerializer只负责将header序列化进去，并委托每个字段对应的serializer对字段进行序列化.</p><h3 id="内存页"><a href="#内存页" class="headerlink" title="内存页"></a>内存页</h3><p>​        内存页是内存段更高一层的封装.</p><p>​        内存页是MemorySegment之上的数据访问视图，数据<strong>读取</strong>抽象为DataInputView，数据<strong>写入</strong>抽象为DataOutputView。使用时就无需关心MemorySegment的细节，会自动处理跨MemorySegment的读取和写入。</p><p>​        有了内存页之后,我们就不需要关注,这一段是第几段了.只需要关注是第几页就ok了.</p><h3 id="Buffer"><a href="#Buffer" class="headerlink" title="Buffer"></a>Buffer</h3><p>​        <strong>一个Buffer对应一个内存段(MemorySegment)</strong></p><p>​        Buffer是网络缓冲内存.是数据传输的时候会用到的.</p><p>​        Task算子之间在网络层面上传输数据，使用的是Buffer，申请和释放由Flink<strong>自行管理</strong>,并不是由我们代码指定，实现类为NetworkBuffer。<strong>1个NetworkBuffer包装了1个MemorySegment</strong>。同时继承了AbstractReferenceCountedByteBuf，是Netty中的抽象类。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NetworkBuffer</span> <span class="keyword">extends</span> <span class="title">AbstractReferenceCountedByteBuf</span> <span class="keyword">implements</span> <span class="title">Buffer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** The backing &#123;<span class="doctag">@link</span> MemorySegment&#125; instance. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> MemorySegment memorySegment;</span><br><span class="line"></span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Buffer资源池"><a href="#Buffer资源池" class="headerlink" title="Buffer资源池"></a>Buffer资源池</h3><p>​        Buffer资源池类似我们的连接池,比如说我们连接mysql.创建一次连接是一个连接对象,一个线程.但是我们通常会用一个链接池来管理.Buffer资源池和连接池其实是一样的.</p><p>​        BufferPool 用来管理Buffer，包含Buffer的申请、释放、销毁、可用Buffer通知等，实现类是LocalBufferPool，<strong>每个Task拥有自己的LocalBufferPool</strong>。</p><p>​        Buffer资源池是如何创建的呢?</p><p>​        他用了一个工厂. 设计模式的工厂模式创建的.</p><p>​        BufferPoolFactory 用来提供 BufferPool 的创建和销毁，唯一的实现类是NetworkBufferPool，每个TaskManager只有一个NetworkBufferPool。同一个TaskManager上的Task共享NetworkBufferPool，在TaskManager启动的时候创建并分配内存。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;内存段&quot;&gt;&lt;a href=&quot;#内存段&quot; class=&quot;headerlink&quot; title=&quot;内存段&quot;&gt;&lt;/a&gt;内存段&lt;/h3&gt;&lt;p&gt;​        &lt;strong&gt;内存段在 Flink 内部叫 MemorySegment，是 Flink 中最小的内存分配单元，默认大小32KB。Flink内存是区分堆内和堆外的,但是MemorySegment是最小单元,不区分堆内核堆外的&lt;/strong&gt;.它即可以是堆上内存（Java的byte数组），也可以是堆外内存（基于Netty的DirectByteBuffer），同时提供了对二进制数据进行读取和写入的方法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219194503.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HeapMemorySegment：用来分配堆上内存&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;HybridMemorySegment：用来分配堆外内存和堆上内存，2017年以后的版本实际上只使用了HybridMemo`rySegment。&lt;/strong&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink内存管理: 内存模型</title>
    <link href="http://xubatian.cn/Flink%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86-%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"/>
    <id>http://xubatian.cn/Flink%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86-%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/</id>
    <published>2022-02-19T10:42:16.000Z</published>
    <updated>2022-02-19T13:31:03.981Z</updated>
    
    <content type="html"><![CDATA[<p>​        目前，大数据计算引擎主要用 Java 或是基于 JVM 的编程语言实现的，例如 Apache Hadoop、Apache Spark、Apache Drill、Apache Flink等。Java语言的好处在于程序员不需要太关注底层内存资源的管理，但同样会面临一个问题，就是如何在内存中存储大量的数据（包括缓存和高效处理）。Flink使用自主的内存管理，来避免这个问题。</p><p><strong>JVM内存管理的不足:</strong></p><p>​            1）<strong>Java 对象存储密度低</strong>。Java的对象在内存中存储包含3个主要部分：对象头、实例数据、对齐填充部分。例如，一个只包含 boolean 属性的对象占16byte：对象头占8byte，boolean 属性占1byte，为了对齐达到8的倍数额外占7byte。而实际上只需要一个bit（1/8字节）就够了。</p><span id="more"></span><p>​            2）<strong>Full GC 会极大地影响性能</strong>。尤其是为了处理更大数据而开了很大内存空间的JVM来说，GC 会达到秒级甚至分钟级。</p><p>​            3）<strong>OOM 问题影响稳定性</strong>。OutOfMemoryError是分布式计算框架经常会遇到的问题，当JVM中所有对象大小超过分配给JVM的内存大小时，就会发生OutOfMemoryError错误，导致JVM崩溃，分布式框架的健壮性和性能都会受到影响。</p><p>​            4）<strong>缓存未命中问题</strong>。CPU进行计算的时候，是从CPU缓存中获取数据。现代体系的CPU会有多级缓存，而加载的时候是以Cache Line为单位加载。如果能够将对象连续存储，这样就会大大降低Cache Miss。使得CPU集中处理业务，而不是空转。（Java对象在堆上存储的时候并不是连续的，所以从内存中读取Java对象时，缓存的邻近的内存区域的数据往往不是CPU下一步计算所需要的，这就是缓存未命中。此时CPU需要空转等待从内存中重新读取数据。）</p><p>​            Flink 并不是将大量对象存在堆内存上，而是将对象都序列化到一个预分配的内存块上，这个内存块叫做 MemorySegment，它代表了一段固定长度的内存（默认大小为 32KB），也是 Flink 中最小的内存分配单元，并且提供了非常高效的读写方法，很多运算可以直接操作二进制数据，不需要反序列化即可执行。每条记录都会以序列化的形式存储在一个或多个MemorySegment中。如果需要处理的数据多于可以保存在内存中的数据，Flink 的运算符会将部分数据溢出到磁盘。</p><!--more--><h1 id="内存模型"><a href="#内存模型" class="headerlink" title="内存模型"></a>内存模型</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219184452.png"></p><h2 id="JobManager内存模型"><a href="#JobManager内存模型" class="headerlink" title="JobManager内存模型"></a>JobManager内存模型</h2><p><strong>JobManagerFlinkMemory.java</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment"> * or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment"> * distributed with this work for additional information</span></span><br><span class="line"><span class="comment"> * regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment"> * to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment"> * &quot;License&quot;); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment"> * with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> org.apache.flink.runtime.util.config.memory.jobmanager;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.annotation.VisibleForTesting;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.MemorySize;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.util.config.memory.FlinkMemory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Objects;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.util.Preconditions.checkNotNull;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink internal memory components of Job Manager.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;A Job Manager&#x27;s internal Flink memory consists of the following components.</span></span><br><span class="line"><span class="comment"> * &lt;ul&gt;</span></span><br><span class="line"><span class="comment"> *     &lt;li&gt;JVM Heap Memory&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *     &lt;li&gt;Off-Heap Memory (also JVM Direct Memory)&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;/ul&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;The relationships of Job Manager Flink memory components are shown below.</span></span><br><span class="line"><span class="comment"> * &lt;pre&gt;</span></span><br><span class="line"><span class="comment"> *               ┌ ─ ─  Total Flink Memory - ─ ─ ┐</span></span><br><span class="line"><span class="comment"> *                 ┌───────────────────────────┐</span></span><br><span class="line"><span class="comment"> *               | │       JVM Heap Memory     │ |   TODO JVM 堆内存</span></span><br><span class="line"><span class="comment"> *                 └───────────────────────────┘</span></span><br><span class="line"><span class="comment"> *               │ ┌───────────────────────────┐ │</span></span><br><span class="line"><span class="comment"> *                 |    Off-heap Heap Memory   │   -─ JVM Direct Memory  TODO JVM 堆外内存</span></span><br><span class="line"><span class="comment"> *               │ └───────────────────────────┘ │</span></span><br><span class="line"><span class="comment"> *               └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘</span></span><br><span class="line"><span class="comment"> * &lt;/pre&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobManagerFlinkMemory</span> <span class="keyword">implements</span> <span class="title">FlinkMemory</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> MemorySize jvmHeap;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> MemorySize offHeapMemory;</span><br><span class="line"></span><br><span class="line"><span class="meta">@VisibleForTesting</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">JobManagerFlinkMemory</span><span class="params">(MemorySize jvmHeap, MemorySize offHeapMemory)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.jvmHeap = checkNotNull(jvmHeap);</span><br><span class="line"><span class="keyword">this</span>.offHeapMemory = checkNotNull(offHeapMemory);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemorySize <span class="title">getJvmHeapMemorySize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> jvmHeap;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemorySize <span class="title">getJvmDirectMemorySize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> offHeapMemory;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemorySize <span class="title">getTotalFlinkMemorySize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> jvmHeap.add(offHeapMemory);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (obj == <span class="keyword">this</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (obj <span class="keyword">instanceof</span> JobManagerFlinkMemory) &#123;</span><br><span class="line">JobManagerFlinkMemory that = (JobManagerFlinkMemory) obj;</span><br><span class="line"><span class="keyword">return</span> Objects.equals(<span class="keyword">this</span>.jvmHeap, that.jvmHeap) &amp;&amp;</span><br><span class="line">Objects.equals(<span class="keyword">this</span>.offHeapMemory, that.offHeapMemory);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> Objects.hash(</span><br><span class="line">jvmHeap,</span><br><span class="line">offHeapMemory);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219184654.png"></p><p>​        在1.10中，Flink 统一了 TM 端的内存管理和配置，相应的在1.11中，Flink 进一步对JM 端的内存配置进行了修改，使它的选项和配置方式与TM 端的配置方式保持一致。</p><p><strong>1.10版本</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># The heap size <span class="keyword">for</span> the JobManager JVM</span><br><span class="line"></span><br><span class="line">jobmanager.heap.size: 1024m</span><br></pre></td></tr></table></figure><p><strong>1.11版本及以后</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># The total process memory size <span class="keyword">for</span> the JobManager.</span><br><span class="line">#</span><br><span class="line"># Note <span class="keyword">this</span> accounts <span class="keyword">for</span> all memory usage within the JobManager process, including JVM metaspace and other overhead.</span><br><span class="line"></span><br><span class="line">jobmanager.memory.process.size: 1600m</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219191433.png"></p><h2 id="TaskManager内存模型"><a href="#TaskManager内存模型" class="headerlink" title="TaskManager内存模型"></a>TaskManager内存模型</h2><p>​        Flink 1.10 对TaskManager的内存模型和Flink应用程序的配置选项进行了重大更改，让用户能够更加严格地控制其内存开销。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219184944.png"></p><p><strong>TaskExecutorFlinkMemory.java</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment"> * or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment"> * distributed with this work for additional information</span></span><br><span class="line"><span class="comment"> * regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment"> * to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment"> * &quot;License&quot;); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment"> * with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> org.apache.flink.runtime.util.config.memory.taskmanager;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.MemorySize;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.util.config.memory.FlinkMemory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Objects;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.util.Preconditions.checkNotNull;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink internal memory components of Task Executor.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;A TaskExecutor&#x27;s internal Flink memory consists of the following components.</span></span><br><span class="line"><span class="comment"> * &lt;ul&gt;</span></span><br><span class="line"><span class="comment"> *     &lt;li&gt;Framework Heap Memory&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *     &lt;li&gt;Framework Off-Heap Memory&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *     &lt;li&gt;Task Heap Memory&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *     &lt;li&gt;Task Off-Heap Memory&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *     &lt;li&gt;Network Memory&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *     &lt;li&gt;Managed Memory&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;/ul&gt;</span></span><br><span class="line"><span class="comment"> *  TODO 这是Flink内存 不包含JVM本身的开销的</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;The relationships of TaskExecutor Flink memory components are shown below.</span></span><br><span class="line"><span class="comment"> * &lt;pre&gt;</span></span><br><span class="line"><span class="comment"> *               ┌ ─ ─  Total Flink Memory - ─ ─ ┐</span></span><br><span class="line"><span class="comment"> *               |┌ ─ ─ - - - On-Heap - - - ─ ─ ┐|</span></span><br><span class="line"><span class="comment"> *                 ┌───────────────────────────┐</span></span><br><span class="line"><span class="comment"> *               |││   Framework Heap Memory   ││|</span></span><br><span class="line"><span class="comment"> *                 └───────────────────────────┘</span></span><br><span class="line"><span class="comment"> *               │ ┌───────────────────────────┐ │</span></span><br><span class="line"><span class="comment"> *                ||      Task Heap Memory     ││</span></span><br><span class="line"><span class="comment"> *               │ └───────────────────────────┘ │</span></span><br><span class="line"><span class="comment"> *                └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘</span></span><br><span class="line"><span class="comment"> *               |┌ ─ ─ - - - Off-Heap  - - ─ ─ ┐|</span></span><br><span class="line"><span class="comment"> *                │┌───────────────────────────┐│</span></span><br><span class="line"><span class="comment"> *               │ │ Framework Off-Heap Memory │ │ ─┐</span></span><br><span class="line"><span class="comment"> *                │└───────────────────────────┘│   │</span></span><br><span class="line"><span class="comment"> *               │ ┌───────────────────────────┐ │  │</span></span><br><span class="line"><span class="comment"> *                ││   Task Off-Heap Memory    ││   ┼─ JVM Direct Memory</span></span><br><span class="line"><span class="comment"> *               │ └───────────────────────────┘ │  │</span></span><br><span class="line"><span class="comment"> *                │┌───────────────────────────┐│   │</span></span><br><span class="line"><span class="comment"> *               │ │      Network Memory       │ │ ─┘</span></span><br><span class="line"><span class="comment"> *                │└───────────────────────────┘│</span></span><br><span class="line"><span class="comment"> *               │ ┌───────────────────────────┐ │</span></span><br><span class="line"><span class="comment"> *                |│      Managed Memory       │|</span></span><br><span class="line"><span class="comment"> *               │ └───────────────────────────┘ │</span></span><br><span class="line"><span class="comment"> *                └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘</span></span><br><span class="line"><span class="comment"> *               └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘</span></span><br><span class="line"><span class="comment"> * &lt;/pre&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TaskExecutorFlinkMemory</span> <span class="keyword">implements</span> <span class="title">FlinkMemory</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> MemorySize frameworkHeap;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> MemorySize frameworkOffHeap;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> MemorySize taskHeap;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> MemorySize taskOffHeap;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> MemorySize network;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> MemorySize managed;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">TaskExecutorFlinkMemory</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">final</span> MemorySize frameworkHeap,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">final</span> MemorySize frameworkOffHeap,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">final</span> MemorySize taskHeap,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">final</span> MemorySize taskOffHeap,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">final</span> MemorySize network,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">final</span> MemorySize managed)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">this</span>.frameworkHeap = checkNotNull(frameworkHeap);</span><br><span class="line"><span class="keyword">this</span>.frameworkOffHeap = checkNotNull(frameworkOffHeap);</span><br><span class="line"><span class="keyword">this</span>.taskHeap = checkNotNull(taskHeap);</span><br><span class="line"><span class="keyword">this</span>.taskOffHeap = checkNotNull(taskOffHeap);</span><br><span class="line"><span class="keyword">this</span>.network = checkNotNull(network);</span><br><span class="line"><span class="keyword">this</span>.managed = checkNotNull(managed);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemorySize <span class="title">getFrameworkHeap</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> frameworkHeap;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemorySize <span class="title">getFrameworkOffHeap</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> frameworkOffHeap;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemorySize <span class="title">getTaskHeap</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> taskHeap;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemorySize <span class="title">getTaskOffHeap</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> taskOffHeap;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemorySize <span class="title">getNetwork</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> network;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemorySize <span class="title">getManaged</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> managed;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemorySize <span class="title">getJvmHeapMemorySize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> frameworkHeap.add(taskHeap);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemorySize <span class="title">getJvmDirectMemorySize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> frameworkOffHeap.add(taskOffHeap).add(network);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemorySize <span class="title">getTotalFlinkMemorySize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> frameworkHeap.add(frameworkOffHeap).add(taskHeap).add(taskOffHeap).add(network).add(managed);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (obj == <span class="keyword">this</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (obj <span class="keyword">instanceof</span> TaskExecutorFlinkMemory) &#123;</span><br><span class="line">TaskExecutorFlinkMemory that = (TaskExecutorFlinkMemory) obj;</span><br><span class="line"><span class="keyword">return</span> Objects.equals(<span class="keyword">this</span>.frameworkHeap, that.frameworkHeap) &amp;&amp;</span><br><span class="line">Objects.equals(<span class="keyword">this</span>.frameworkOffHeap, that.frameworkOffHeap) &amp;&amp;</span><br><span class="line">Objects.equals(<span class="keyword">this</span>.taskHeap, that.taskHeap) &amp;&amp;</span><br><span class="line">Objects.equals(<span class="keyword">this</span>.taskOffHeap, that.taskOffHeap) &amp;&amp;</span><br><span class="line">Objects.equals(<span class="keyword">this</span>.network, that.network) &amp;&amp;</span><br><span class="line">Objects.equals(<span class="keyword">this</span>.managed, that.managed);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> Objects.hash(</span><br><span class="line">frameworkHeap,</span><br><span class="line">frameworkOffHeap,</span><br><span class="line">taskHeap,</span><br><span class="line">taskOffHeap,</span><br><span class="line">network,</span><br><span class="line">managed);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219185043.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219185113.png"></p><h3 id="JVM-Heap：JVM堆上内存"><a href="#JVM-Heap：JVM堆上内存" class="headerlink" title="JVM Heap：JVM堆上内存"></a>JVM Heap：JVM堆上内存</h3><p>​        1、Framework Heap Memory：Flink框架本身使用的内存，即TaskManager本身所占用的堆上内存，不计入Slot的资源中。</p><p>配置参数：taskmanager.memory.framework.heap.size=128MB,默认128MB</p><p>​        2、Task Heap Memory：Task执行用户代码时所使用的堆上内存。</p><p>配置参数：taskmanager.memory.task.heap.size</p><h3 id="Off-Heap-Mempry：JVM堆外内存"><a href="#Off-Heap-Mempry：JVM堆外内存" class="headerlink" title="Off-Heap Mempry：JVM堆外内存"></a>Off-Heap Mempry：JVM堆外内存</h3><p>​        1、DirectMemory：JVM直接内存</p><p>​                1） Framework Off-Heap Memory：Flink框架本身所使用的内存，即TaskManager本身所占用的对外内存，不计入Slot资源。</p><p>​                        配置参数：taskmanager.memory.framework.off-heap.size=128MB,默认128MB</p><p>​                2）Task Off-Heap Memory：Task执行用户代码所使用的对外内存。</p><p>​                        配置参数：taskmanager.memory.task.off-heap.size=0,默认0</p><p>​                3）Network Memory：网络数据交换所使用的堆外内存大小，如网络数据交换缓冲区</p><p>​                        配置参数：</p><p>​                            taskmanager.memory.network.fraction: 0.1</p><p>​                            taskmanager.memory.network.min: 64mb</p><p>​                            taskmanager.memory.network.max: 1gb</p><p>​        2、Managed Memory：Flink管理的堆外内存，用于排序、哈希表、缓存中间结果及 RocksDB State Backend 的本地内存。</p><p>​              RocksDB就是存储RocksDB类型的状态后端的.状态后端关注两件事情①本地状态存在哪里. ②checkpoint存在哪里. 本地内存是存在RocksDB里面.RocksDB又用到了内存加磁盘.那RocksDB用到的内存就是这个Managed Memory.</p><p>​                        配置参数：</p><p>​                            taskmanager.memory.managed.fraction=0.4</p><p>​                            taskmanager.memory.managed.size</p><h3 id="JVM-specific-memory：JVM本身使用的内存"><a href="#JVM-specific-memory：JVM本身使用的内存" class="headerlink" title="JVM specific memory：JVM本身使用的内存"></a>JVM specific memory：JVM本身使用的内存</h3><p>​        1、JVM metaspace：JVM元空间</p><p>​        2、JVM over-head执行开销：JVM执行时自身所需要的内容，包括线程堆栈、IO、编译缓存等所使用的内存。</p><p>​                配置参数：</p><p>​                    taskmanager.memory.jvm-overhead.min=192mb</p><p>​                    taskmanager.memory.jvm-overhead.max=1gb</p><p>​                    taskmanager.memory.jvm-overhead.fraction=0.1</p><h3 id="总体内存"><a href="#总体内存" class="headerlink" title="总体内存"></a>总体内存</h3><p>​            1、总进程内存：Flink Java应用程序（包括用户代码）和JVM运行整个进程所消耗的总内存。</p><p>​                        <strong>总进程内存 = Flink使用内存 + JVM元空间 + JVM执行开销</strong></p><p>​                 配置项：taskmanager.memory.process.size: 1728m</p><p>​            2、Flink总内存：仅Flink Java应用程序消耗的内存，包括用户代码，但不包括JVM为其运行而分配的内存</p><p>​                        <strong>Flink使用内存：框架堆内外 + task堆内外 + network + manage</strong></p><p>​                    配置项：taskmanager.memory.flink.size: 1280m</p><p>​    <strong>说明：</strong>配置项详细信息查看如下链接</p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.14/deployment/config.html#memory-configuration">https://ci.apache.org/projects/flink/flink-docs-release-1.14/deployment/config.html#memory-configuration</a></p><h2 id="内存分配"><a href="#内存分配" class="headerlink" title="内存分配"></a>内存分配</h2><p>内存在配置文件内配置之后,在代码里面 是如何加载的呢?如何加载到配置项里面的.</p><h3 id="JobManager内存分配"><a href="#JobManager内存分配" class="headerlink" title="JobManager内存分配"></a>JobManager内存分配</h3><p><strong>YarnClusterDescriptor.java</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> ApplicationReport <span class="title">startAppMaster</span><span class="params">( 集群描述器启动Applicationmaster.启动的时候封装了一些配置项.就是在这里做的.</span></span></span><br><span class="line"><span class="params"><span class="function">Configuration configuration,</span></span></span><br><span class="line"><span class="params"><span class="function">String applicationName,</span></span></span><br><span class="line"><span class="params"><span class="function">String yarnClusterEntrypoint,</span></span></span><br><span class="line"><span class="params"><span class="function">JobGraph jobGraph,</span></span></span><br><span class="line"><span class="params"><span class="function">YarnClient yarnClient,</span></span></span><br><span class="line"><span class="params"><span class="function">YarnClientApplication yarnApplication,</span></span></span><br><span class="line"><span class="params"><span class="function">ClusterSpecification clusterSpecification)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">... ...</span><br><span class="line"><span class="keyword">final</span> JobManagerProcessSpec processSpec = JobManagerProcessUtils.processSpecFromConfigWithNewOptionToInterpretLegacyHeap(</span><br><span class="line">flinkConfiguration,</span><br><span class="line">JobManagerOptions.TOTAL_PROCESS_MEMORY);</span><br><span class="line"><span class="keyword">final</span> ContainerLaunchContext amContainer = setupApplicationMasterContainer(</span><br><span class="line">yarnClusterEntrypoint,</span><br><span class="line">hasKrb5,</span><br><span class="line">processSpec);</span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219190826.png"></p><p><strong>JobManagerProcessUtils.java</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> JobManagerProcessSpec <span class="title">processSpecFromConfigWithNewOptionToInterpretLegacyHeap</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">Configuration config,</span></span></span><br><span class="line"><span class="params"><span class="function">ConfigOption&lt;MemorySize&gt; newOptionToInterpretLegacyHeap)</span> </span>&#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line"><span class="keyword">return</span> processSpecFromConfig(</span><br><span class="line">getConfigurationWithLegacyHeapSizeMappedToNewConfigOption(</span><br><span class="line">config,</span><br><span class="line">newOptionToInterpretLegacyHeap));</span><br><span class="line">&#125; <span class="keyword">catch</span> (IllegalConfigurationException e) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IllegalConfigurationException(<span class="string">&quot;JobManager memory configuration failed: &quot;</span> + e.getMessage(), e);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> JobManagerProcessSpec <span class="title">processSpecFromConfig</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> createMemoryProcessSpec(PROCESS_MEMORY_UTILS.memoryProcessSpecFromConfig(config));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>ProcessMemoryUtils.java</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> CommonProcessMemorySpec&lt;FM&gt; <span class="title">memoryProcessSpecFromConfig</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (options.getRequiredFineGrainedOptions().stream().allMatch(config::contains)) &#123;</span><br><span class="line"><span class="comment">// all internal memory options are configured, use these to derive total Flink and process memory</span></span><br><span class="line"><span class="keyword">return</span> deriveProcessSpecWithExplicitInternalMemory(config);</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (config.contains(options.getTotalFlinkMemoryOption())) &#123;</span><br><span class="line"><span class="comment">// internal memory options are not configured, total Flink memory is configured,</span></span><br><span class="line"><span class="comment">// derive from total flink memory</span></span><br><span class="line"><span class="comment">// 如果只配置了JM的Flink总内存，调用下面方法</span></span><br><span class="line"><span class="keyword">return</span> deriveProcessSpecWithTotalFlinkMemory(config);</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (config.contains(options.getTotalProcessMemoryOption())) &#123;</span><br><span class="line"><span class="comment">// total Flink memory is not configured, total process memory is configured,</span></span><br><span class="line"><span class="comment">// derive from total process memory</span></span><br><span class="line"><span class="keyword">return</span> deriveProcessSpecWithTotalProcessMemory(config);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> failBecauseRequiredOptionsNotConfigured();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> CommonProcessMemorySpec&lt;FM&gt; <span class="title">deriveProcessSpecWithTotalFlinkMemory</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line">MemorySize totalFlinkMemorySize = getMemorySizeFromConfig(config, options.getTotalFlinkMemoryOption());</span><br><span class="line"><span class="comment">// 获取JM的Flink总内存</span></span><br><span class="line">FM flinkInternalMemory = flinkMemoryUtils.deriveFromTotalFlinkMemory(config, totalFlinkMemorySize);</span><br><span class="line"><span class="comment">// 获取JM的JVM元空间和执行开销</span></span><br><span class="line">JvmMetaspaceAndOverhead jvmMetaspaceAndOverhead = deriveJvmMetaspaceAndOverheadFromTotalFlinkMemory(config, totalFlinkMemorySize);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> CommonProcessMemorySpec&lt;&gt;(flinkInternalMemory, jvmMetaspaceAndOverhead);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>JobManagerFlinkMemoryUtils.java</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> JobManagerFlinkMemory <span class="title">deriveFromTotalFlinkMemory</span><span class="params">(Configuration config, MemorySize totalFlinkMemorySize)</span> </span>&#123;</span><br><span class="line">MemorySize offHeapMemorySize = ProcessMemoryUtils.getMemorySizeFromConfig(config, JobManagerOptions.OFF_HEAP_MEMORY);</span><br><span class="line"><span class="keyword">if</span> (totalFlinkMemorySize.compareTo(offHeapMemorySize) &lt; <span class="number">1</span>) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IllegalConfigurationException(</span><br><span class="line"><span class="string">&quot;The configured Total Flink Memory (%s) is less than the configured Off-heap Memory (%s).&quot;</span>,</span><br><span class="line">totalFlinkMemorySize.toHumanReadableString(),</span><br><span class="line">offHeapMemorySize.toHumanReadableString());</span><br><span class="line">&#125;</span><br><span class="line">MemorySize derivedJvmHeapMemorySize = totalFlinkMemorySize.subtract(offHeapMemorySize);</span><br><span class="line"><span class="keyword">return</span> createJobManagerFlinkMemory(derivedJvmHeapMemorySize, offHeapMemorySize);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> JobManagerFlinkMemory <span class="title">createJobManagerFlinkMemory</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">MemorySize jvmHeap,</span></span></span><br><span class="line"><span class="params"><span class="function">MemorySize offHeapMemory)</span> </span>&#123;</span><br><span class="line">verifyJvmHeapSize(jvmHeap);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> JobManagerFlinkMemory(jvmHeap, offHeapMemory);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="TaskManager内存分配"><a href="#TaskManager内存分配" class="headerlink" title="TaskManager内存分配"></a>TaskManager内存分配</h3><p><strong>ActiveResourceManager.java</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">requestNewWorker</span><span class="params">(WorkerResourceSpec workerResourceSpec)</span> </span>&#123;</span><br><span class="line"><span class="keyword">final</span> TaskExecutorProcessSpec taskExecutorProcessSpec =</span><br><span class="line">TaskExecutorProcessUtils.processSpecFromWorkerResourceSpec(flinkConfig, workerResourceSpec);</span><br><span class="line">... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>TaskExecutorProcessUtils.java</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> TaskExecutorProcessSpec <span class="title">processSpecFromWorkerResourceSpec</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">final</span> Configuration config, <span class="keyword">final</span> WorkerResourceSpec workerResourceSpec)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> MemorySize frameworkHeapMemorySize = TaskExecutorFlinkMemoryUtils.getFrameworkHeapMemorySize(config);</span><br><span class="line"><span class="keyword">final</span> MemorySize frameworkOffHeapMemorySize = TaskExecutorFlinkMemoryUtils.getFrameworkOffHeapMemorySize(config);</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> TaskExecutorFlinkMemory flinkMemory = <span class="keyword">new</span> TaskExecutorFlinkMemory(</span><br><span class="line">frameworkHeapMemorySize,</span><br><span class="line">frameworkOffHeapMemorySize,</span><br><span class="line">workerResourceSpec.getTaskHeapSize(),</span><br><span class="line">workerResourceSpec.getTaskOffHeapSize(),</span><br><span class="line">workerResourceSpec.getNetworkMemSize(),</span><br><span class="line">workerResourceSpec.getManagedMemSize());</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> JvmMetaspaceAndOverhead jvmMetaspaceAndOverhead =</span><br><span class="line">PROCESS_MEMORY_UTILS.deriveJvmMetaspaceAndOverheadFromTotalFlinkMemory(</span><br><span class="line">config, flinkMemory.getTotalFlinkMemorySize());</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> TaskExecutorProcessSpec(workerResourceSpec.getCpuCores(), flinkMemory, jvmMetaspaceAndOverhead);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;​        目前，大数据计算引擎主要用 Java 或是基于 JVM 的编程语言实现的，例如 Apache Hadoop、Apache Spark、Apache Drill、Apache Flink等。Java语言的好处在于程序员不需要太关注底层内存资源的管理，但同样会面临一个问题，就是如何在内存中存储大量的数据（包括缓存和高效处理）。Flink使用自主的内存管理，来避免这个问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;JVM内存管理的不足:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;​            1）&lt;strong&gt;Java 对象存储密度低&lt;/strong&gt;。Java的对象在内存中存储包含3个主要部分：对象头、实例数据、对齐填充部分。例如，一个只包含 boolean 属性的对象占16byte：对象头占8byte，boolean 属性占1byte，为了对齐达到8的倍数额外占7byte。而实际上只需要一个bit（1/8字节）就够了。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink源码分析: Flink on yarn 的 per-job-cluster模式及提交流程</title>
    <link href="http://xubatian.cn/Flink%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-Flink-on-yarn-%E7%9A%84-per-job-cluster%E6%A8%A1%E5%BC%8F%E5%8F%8A%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/"/>
    <id>http://xubatian.cn/Flink%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-Flink-on-yarn-%E7%9A%84-per-job-cluster%E6%A8%A1%E5%BC%8F%E5%8F%8A%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/</id>
    <published>2022-02-19T06:57:08.000Z</published>
    <updated>2022-02-19T07:36:46.363Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Flink-on-Yarn"><a href="#Flink-on-Yarn" class="headerlink" title="Flink on Yarn"></a>Flink on Yarn</h3><p>​        Flink提供了两种在yarn上运行的模式，分别为<strong>Session-Cluster和Per-Job-Cluster模式</strong>.</p><p>​        Application就是我们的JobManager.</p><p>什么是Session-Cluster呢?</p><p>​        就是多个job或者多个application共享一份集群资源,共享一份yarn session的进程或者共用一个进程中的资源,那个进程叫yarn session<br>什么是Per-Job-Cluster呢?<br>每一个job对应一个yarn session</p><span id="more"></span><p><strong>如图:</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219150143.png"></p><h4 id="Session-cluster-模式：（一个作业是失败很可能影响下一个作业的提交）"><a href="#Session-cluster-模式：（一个作业是失败很可能影响下一个作业的提交）" class="headerlink" title="Session-cluster 模式：（一个作业是失败很可能影响下一个作业的提交）"></a>Session-cluster 模式：（一个作业是失败很可能影响下一个作业的提交）</h4><p>yarn session模式<br>一个作业的失败很可能会影响下一个作业的正常提交，因为他一个作业的失败很可能他把所有的资源都占完了，占完了而且你一直都没有停止，没有停止那么其他的作业就无法提交了<br><strong>如图所示:</strong><br>五个Flink Job都是跑在一个Flink的yarn session当中的,这个Flink yarn session可以理解为就是一份资源,这个资源从哪里生成来的呢?是从我们的yarn的ResourceManager中生成来的,生成过来之后他启动一个进程,这个进程我们把它称之为叫yarn session的进程,这个yarn session中包含了一些资源,这些资源允许我们的多个flink job共享这个里面的资源去运行;但是有这种情况,什么情况呢?<br>假设我提交两个flink job提交给这一个yarn session之后,发现我yarn session中的共享资源被占完了,就是满了.什么叫满了呢?就是没有空余的slot(因为我们flink的任务是运行在slot上面的),如图所示:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219150251.png"></p><p>所以,如果我当前yarn session中运行了两个job之后已经没有剩余的slot了,请问后面五个flink job还能提交上去吗?不能,他们会在等待,等待有人释放了slot,然后他们在去抢这些资源======&gt;这就是Session-cluster 模式</p><h4 id="Per-Job-Cluster模式-（一个作业的失败不会影响下一个作业的提交）"><a href="#Per-Job-Cluster模式-（一个作业的失败不会影响下一个作业的提交）" class="headerlink" title="Per-Job-Cluster模式:（一个作业的失败不会影响下一个作业的提交）"></a>Per-Job-Cluster模式:（一个作业的失败不会影响下一个作业的提交）</h4><p>​        这种模式是每一个job会对应一个yarn session集群,所以这个job所对应的yarn session集群他不会和其他的job共享的,他是相互独立的,只要你这台集群总的资源是够大的,那么你就可以源源不断的一直提交新的job,所以两种模式,这一种用的最多,也是最方便的,如图所示:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219150435.png"></p><p><strong>为什么说Per-Job-Cluster模式最方便呢?</strong><br>        因为session-cluster模式在flink job没提交之前我得先把yarn session集群准备好,(多个flink共享一个yarn session集群)所以我在flink job集群提交之前就得把yarn session集群准备好,或者说他早就存在已经有了,我再提交    ;所以一般来说session-cluster这种模式我首先得启动yarn session这个集群<br>   <strong>什么是yarn session集群呢?</strong><br>        Yarn session集群就是由yarn管理的一个flink的包含了JobManager,TaskManager的这么一个集群,如图所示:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219150527.png"></p><p>我独立模式下,一个JobManager里面包含了三个TaskManager,那为什么是三个TaskManager呢?</p><p>因为我的每台机器上都只有一个TaskManager呀!<br>请问这些TaskManager跟Hadoop的yarn有关系吗?<br>没有关系.他被yarn管理吗?<br>不会.他的资源来自于yarn吗?<br>不是,是直接来自于操作系统<br>所以什么叫yarn session的集群?<br><strong>yarn session集群就是由yarn来管理的一个在hadoop内部的一个小的Flink的运行集群,包含了JobManager和多个TaskManager</strong></p><h3 id="简要记录源码阅读过程"><a href="#简要记录源码阅读过程" class="headerlink" title="简要记录源码阅读过程"></a>简要记录源码阅读过程</h3><div style="position: relative; padding: 30% 45%;"><iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/2022-02-19_15-25-23.mp4" frameborder="no" scrolling="no"></iframe></div> <h3 id="总结per-job-cluster模式及提交流程"><a href="#总结per-job-cluster模式及提交流程" class="headerlink" title="总结per-job-cluster模式及提交流程"></a>总结per-job-cluster模式及提交流程</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219150738.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220219150939.png"></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;Flink-on-Yarn&quot;&gt;&lt;a href=&quot;#Flink-on-Yarn&quot; class=&quot;headerlink&quot; title=&quot;Flink on Yarn&quot;&gt;&lt;/a&gt;Flink on Yarn&lt;/h3&gt;&lt;p&gt;​        Flink提供了两种在yarn上运行的模式，分别为&lt;strong&gt;Session-Cluster和Per-Job-Cluster模式&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;​        Application就是我们的JobManager.&lt;/p&gt;
&lt;p&gt;什么是Session-Cluster呢?&lt;/p&gt;
&lt;p&gt;​        就是多个job或者多个application共享一份集群资源,共享一份yarn session的进程或者共用一个进程中的资源,那个进程叫yarn session&lt;br&gt;什么是Per-Job-Cluster呢?&lt;br&gt;每一个job对应一个yarn session&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink SQL 案例代码: 使用连接器的方式读取Kafka的数据</title>
    <link href="http://xubatian.cn/Flink-SQL-%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81-%E4%BD%BF%E7%94%A8%E8%BF%9E%E6%8E%A5%E5%99%A8%E7%9A%84%E6%96%B9%E5%BC%8F%E8%AF%BB%E5%8F%96Kafka%E7%9A%84%E6%95%B0%E6%8D%AE/"/>
    <id>http://xubatian.cn/Flink-SQL-%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81-%E4%BD%BF%E7%94%A8%E8%BF%9E%E6%8E%A5%E5%99%A8%E7%9A%84%E6%96%B9%E5%BC%8F%E8%AF%BB%E5%8F%96Kafka%E7%9A%84%E6%95%B0%E6%8D%AE/</id>
    <published>2022-02-17T10:31:22.000Z</published>
    <updated>2022-02-19T13:19:16.455Z</updated>
    
    <content type="html"><![CDATA[<p>注意: 在将动态表转换为 DataStream 时，只支持 append 流和 retract 流。<br>         只有当我们对接Hbase,ES等这些外部系统的时候才会有upsert模式.</p><span id="more"></span><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shangbaishuyao.demo.FlinkDemo10;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.DataTypes;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.descriptors.Json;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.descriptors.Kafka;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.descriptors.Schema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.$;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 表到流的转换:</span></span><br><span class="line"><span class="comment"> * Append-only 流（追加流）</span></span><br><span class="line"><span class="comment"> * Retract 流（撤回流，使用聚合操作，count，sum等）</span></span><br><span class="line"><span class="comment"> * Upsert 流(更新流,直接更新)</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 注意: 在将动态表转换为 DataStream 时，只支持 append 流和 retract 流。</span></span><br><span class="line"><span class="comment"> * 只有当我们对接Hbase,ES等这些外部系统的时候才会有upsert模式.</span></span><br><span class="line"><span class="comment"> * Author: shangbaishuyao</span></span><br><span class="line"><span class="comment"> * Date: 13:29 2021/4/23</span></span><br><span class="line"><span class="comment"> * Desc:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkSQL04_Source_Kafka</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//1.获取执行环境</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line">        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line">        <span class="comment">//2.使用连接器的方式读取Kafka的数据</span></span><br><span class="line">        tableEnv.connect(<span class="keyword">new</span> Kafka()</span><br><span class="line">                .version(<span class="string">&quot;universal&quot;</span>)</span><br><span class="line">                .topic(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">                .startFromLatest()</span><br><span class="line">                .property(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>)</span><br><span class="line">                .property(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;shangbaishuyao&quot;</span>))</span><br><span class="line">                .withSchema(<span class="keyword">new</span> Schema()</span><br><span class="line">                        .field(<span class="string">&quot;id&quot;</span>, DataTypes.STRING())</span><br><span class="line">                        .field(<span class="string">&quot;ts&quot;</span>, DataTypes.BIGINT())</span><br><span class="line">                        .field(<span class="string">&quot;vc&quot;</span>, DataTypes.INT()))</span><br><span class="line">                .withFormat(<span class="keyword">new</span> Json())</span><br><span class="line"><span class="comment">//                .withFormat(new Csv())</span></span><br><span class="line">                .createTemporaryTable(<span class="string">&quot;sensor&quot;</span>);</span><br><span class="line">        <span class="comment">//3.使用连接器创建表</span></span><br><span class="line">        Table sensor = tableEnv.from(<span class="string">&quot;sensor&quot;</span>);</span><br><span class="line">        <span class="comment">//4.查询数据</span></span><br><span class="line">        Table resultTable = sensor.groupBy($(<span class="string">&quot;id&quot;</span>))</span><br><span class="line">                .select($(<span class="string">&quot;id&quot;</span>), $(<span class="string">&quot;id&quot;</span>).count());</span><br><span class="line">        <span class="comment">//6.将表转换为流进行输出</span></span><br><span class="line">        tableEnv.toRetractStream(resultTable, Row.class).print();</span><br><span class="line">        <span class="comment">//7.执行任务</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;注意: 在将动态表转换为 DataStream 时，只支持 append 流和 retract 流。&lt;br&gt;         只有当我们对接Hbase,ES等这些外部系统的时候才会有upsert模式.&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink SQL 案例代码: 使用Connect方式读取文本数据</title>
    <link href="http://xubatian.cn/Flink-SQL-%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81-%E4%BD%BF%E7%94%A8Connect%E6%96%B9%E5%BC%8F%E8%AF%BB%E5%8F%96%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE/"/>
    <id>http://xubatian.cn/Flink-SQL-%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81-%E4%BD%BF%E7%94%A8Connect%E6%96%B9%E5%BC%8F%E8%AF%BB%E5%8F%96%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE/</id>
    <published>2022-02-17T10:29:50.000Z</published>
    <updated>2022-02-19T13:19:01.448Z</updated>
    
    <content type="html"><![CDATA[<p>注意: 在将动态表转换为 DataStream 时，只支持 append 流和 retract 流。<br>     只有当我们对接Hbase,ES等这些外部系统的时候才会有upsert模式.</p><span id="more"></span><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shangbaishuyao.demo.FlinkDemo10;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.DataTypes;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.descriptors.Csv;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.descriptors.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.descriptors.Schema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.$;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 表到流的转换:</span></span><br><span class="line"><span class="comment"> * Append-only 流（追加流）</span></span><br><span class="line"><span class="comment"> * Retract 流（撤回流，使用聚合操作，count，sum等）</span></span><br><span class="line"><span class="comment"> * Upsert 流(更新流,直接更新)</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 注意: 在将动态表转换为 DataStream 时，只支持 append 流和 retract 流。</span></span><br><span class="line"><span class="comment"> * 只有当我们对接Hbase,ES等这些外部系统的时候才会有upsert模式.</span></span><br><span class="line"><span class="comment"> * Author: shangbaishuyao</span></span><br><span class="line"><span class="comment"> * Date: 13:28 2021/4/23</span></span><br><span class="line"><span class="comment"> * Desc:  使用Connect方式读取文本数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkSQL03_Source_File</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//1.获取执行环境</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">//1.创建表的执行环境</span></span><br><span class="line">        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line">        <span class="comment">//2.使用Connect方式读取文本数据</span></span><br><span class="line">        tableEnv.connect(<span class="keyword">new</span> FileSystem().path(<span class="string">&quot;/Users/shangbaishuyao/warehouse/IDEA_WorkSpace/Flink_WorkSpace/flink-learning-from-zhisheng/flink-1.12.0-Demo/input/sensor.txt&quot;</span>))</span><br><span class="line">                .withSchema(<span class="keyword">new</span> Schema()</span><br><span class="line">                        .field(<span class="string">&quot;id&quot;</span>, DataTypes.STRING())</span><br><span class="line">                        .field(<span class="string">&quot;ts&quot;</span>, DataTypes.BIGINT())</span><br><span class="line">                        .field(<span class="string">&quot;vc&quot;</span>, DataTypes.INT()))</span><br><span class="line">                .withFormat(<span class="keyword">new</span> Csv().fieldDelimiter(<span class="string">&#x27;,&#x27;</span>).lineDelimiter(<span class="string">&quot;\n&quot;</span>))</span><br><span class="line">                .createTemporaryTable(<span class="string">&quot;sensor&quot;</span>);</span><br><span class="line">        <span class="comment">//3.将连接器应用,转换为表</span></span><br><span class="line">        Table sensor = tableEnv.from(<span class="string">&quot;sensor&quot;</span>);</span><br><span class="line">        <span class="comment">//4.查询</span></span><br><span class="line">        Table resultTable = sensor.groupBy($(<span class="string">&quot;id&quot;</span>))</span><br><span class="line">                .select($(<span class="string">&quot;id&quot;</span>), $(<span class="string">&quot;id&quot;</span>).count().as(<span class="string">&quot;ct&quot;</span>));</span><br><span class="line">        <span class="comment">//5.转换为流进行输出</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; tuple2DataStream = tableEnv.toRetractStream(resultTable, Row.class);</span><br><span class="line">        <span class="comment">//6.打印数据</span></span><br><span class="line">        tuple2DataStream.print();</span><br><span class="line">        <span class="comment">//7.执行任务</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;注意: 在将动态表转换为 DataStream 时，只支持 append 流和 retract 流。&lt;br&gt;     只有当我们对接Hbase,ES等这些外部系统的时候才会有upsert模式.&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink SQL 案例代码: 将Stream流转化成Table聚合操作, 求总和,某个count或者sum将转为Stream打印</title>
    <link href="http://xubatian.cn/Flink-SQL-%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81-%E5%B0%86Stream%E6%B5%81%E8%BD%AC%E5%8C%96%E6%88%90Table%E8%81%9A%E5%90%88%E6%93%8D%E4%BD%9C-%E6%B1%82%E6%80%BB%E5%92%8C-%E6%9F%90%E4%B8%AAcount%E6%88%96%E8%80%85sum%E5%B0%86%E8%BD%AC%E4%B8%BAStream%E6%89%93%E5%8D%B0/"/>
    <id>http://xubatian.cn/Flink-SQL-%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81-%E5%B0%86Stream%E6%B5%81%E8%BD%AC%E5%8C%96%E6%88%90Table%E8%81%9A%E5%90%88%E6%93%8D%E4%BD%9C-%E6%B1%82%E6%80%BB%E5%92%8C-%E6%9F%90%E4%B8%AAcount%E6%88%96%E8%80%85sum%E5%B0%86%E8%BD%AC%E4%B8%BAStream%E6%89%93%E5%8D%B0/</id>
    <published>2022-02-17T10:24:06.000Z</published>
    <updated>2022-02-19T13:21:53.445Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">表到流的转换:</span><br><span class="line"> Append-only 流（追加流）</span><br><span class="line"> Retract 流（撤回流，使用聚合操作，count，sum等）</span><br><span class="line"> Upsert 流(更新流,直接更新)</span><br><span class="line">注意: 在将动态表转换为 DataStream 时，只支持 append 流和 retract 流。</span><br><span class="line">  只有当我们对接Hbase,ES等这些外部系统的时候才会有upsert模式.</span><br><span class="line">  注意: 聚合操作需要使用撤回流,不能使用追加流</span><br></pre></td></tr></table></figure><span id="more"></span><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shangbaishuyao.demo.FlinkDemo10;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.shangbaishuyao.demo.bean.WaterSensor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 表到流的转换:</span></span><br><span class="line"><span class="comment"> * Append-only 流（追加流）</span></span><br><span class="line"><span class="comment"> * Retract 流（撤回流，使用聚合操作，count，sum等）</span></span><br><span class="line"><span class="comment"> * Upsert 流(更新流,直接更新)</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 注意: 在将动态表转换为 DataStream 时，只支持 append 流和 retract 流。</span></span><br><span class="line"><span class="comment"> * 只有当我们对接Hbase,ES等这些外部系统的时候才会有upsert模式.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Author: shangbaishuyao</span></span><br><span class="line"><span class="comment"> * Date: 13:28 2021/4/23</span></span><br><span class="line"><span class="comment"> * Desc: 聚合操作, 求总和,某个count或者sum</span></span><br><span class="line"><span class="comment"> * 注意: 聚合操作需要使用撤回流,不能使用追加流</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkSQL02_StreamToTable_Agg</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//1.获取流执行环境</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">//2.读取端口数据创建流并装换为JavaBean</span></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorDS = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">                .map(data -&gt; &#123;</span><br><span class="line">                    String[] split = data.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>],</span><br><span class="line">                            Long.parseLong(split[<span class="number">1</span>]),</span><br><span class="line">                            Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">                &#125;);</span><br><span class="line">        <span class="comment">//3.创建表执行环境</span></span><br><span class="line">        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line">        <span class="comment">//4.将流转换为动态表</span></span><br><span class="line">        Table sensorTable = tableEnv.fromDataStream(waterSensorDS);</span><br><span class="line">        <span class="comment">//5.使用TableAPI 实现 : select id,sum(vc) from sensor where vc&gt;=20 group by id;</span></span><br><span class="line">        <span class="comment">//TODO Flink V1.12.0新版本的写法</span></span><br><span class="line"><span class="comment">//        Table selectTable = sensorTable</span></span><br><span class="line"><span class="comment">//                .where($(&quot;vc&quot;).isGreaterOrEqual(20)) //水位线大于等于20的</span></span><br><span class="line"><span class="comment">//                .groupBy($(&quot;id&quot;))                    //做聚合操作肯定是要分组</span></span><br><span class="line"><span class="comment">//                .aggregate($(&quot;vc&quot;).sum().as(&quot;sum_vc&quot;)) //聚合vc 按照sum聚合,区别名sum_vc</span></span><br><span class="line"><span class="comment">//                .select($(&quot;id&quot;), $(&quot;sum_vc&quot;));       //查询数据, id可以查,但是ts不能查询,因为我并没有按照ts进行聚合</span></span><br><span class="line">        <span class="comment">//TODO 老版本的写法</span></span><br><span class="line">        Table selectTable = sensorTable</span><br><span class="line">                .groupBy(<span class="string">&quot;id&quot;</span>)</span><br><span class="line">                .select(<span class="string">&quot;id,id.sum&quot;</span>);</span><br><span class="line">        <span class="comment">//6.将selectTable转换为流进行输出</span></span><br><span class="line">        <span class="comment">// toAppendStream: 追加. 来一条数据往动态表里面去追加,来一条追加一条. 但是涉及到一个问题, 第一条数据20则输出20,第二条数据20,应该输出40,但是以前的20怎么办呢?</span></span><br><span class="line">        <span class="comment">// 会报错. toAppendStream 追加流. 聚合操作本来是20, 但是追加后是40, 这个40能聚合进去吗?不能,不然我同一个id出现了多次.</span></span><br><span class="line">        <span class="comment">// 所以我们使用toRetractStream 撤回流. 这个Boolean是干什么用的呢? 是问你要不要撤回.</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; rowDataStream = tableEnv.toRetractStream(selectTable, Row.class);</span><br><span class="line">        rowDataStream.print();</span><br><span class="line">        <span class="comment">//7.执行任务</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>源码地址: <a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo10/FlinkSQL02_StreamToTable_Agg.java">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo10/FlinkSQL02_StreamToTable_Agg.java</a></p>]]></content>
    
    
    <summary type="html">&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;表到流的转换:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; Append-only 流（追加流）&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; Retract 流（撤回流，使用聚合操作，count，sum等）&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; Upsert 流(更新流,直接更新)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;注意: 在将动态表转换为 DataStream 时，只支持 append 流和 retract 流。&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; 		 只有当我们对接Hbase,ES等这些外部系统的时候才会有upsert模式.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; 		 注意: 聚合操作需要使用撤回流,不能使用追加流&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink SQL 案例代码: 将Stream流转化成Table</title>
    <link href="http://xubatian.cn/Flink-SQL-%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81-%E5%B0%86Stream%E6%B5%81%E8%BD%AC%E5%8C%96%E6%88%90Table/"/>
    <id>http://xubatian.cn/Flink-SQL-%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81-%E5%B0%86Stream%E6%B5%81%E8%BD%AC%E5%8C%96%E6%88%90Table/</id>
    <published>2022-02-17T10:13:46.000Z</published>
    <updated>2022-02-19T13:22:02.459Z</updated>
    
    <content type="html"><![CDATA[<p>将流转换成表,再将表转换成流打印</p><span id="more"></span><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shangbaishuyao.demo.FlinkDemo10;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.shangbaishuyao.demo.bean.WaterSensor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.$;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: shangbaishuyao</span></span><br><span class="line"><span class="comment"> * Date: 21:53 2021/4/23</span></span><br><span class="line"><span class="comment"> * Desc: 测试Flink SQL: 由Stream流转化成Table</span></span><br><span class="line"><span class="comment"> * 将流转换成表,再将表转换成流打印</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkSQL01_StreamToTable_Test</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//1.获取流执行环境</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">//设置并行度</span></span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">//2.读取端口数据创建流并装换为JavaBean</span></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorDS = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">                .map(data -&gt; &#123;</span><br><span class="line">                    String[] split = data.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>],</span><br><span class="line">                            Long.parseLong(split[<span class="number">1</span>]),</span><br><span class="line">                            Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.创建表执行环境</span></span><br><span class="line"><span class="comment">//        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span></span><br><span class="line">        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"><span class="comment">//        tableEnv.sqlQuery();</span></span><br><span class="line">        <span class="comment">//4.将流转换为动态表</span></span><br><span class="line"><span class="comment">//        Table sensorTable = tableEnv.fromDataStream(waterSensorDS);</span></span><br><span class="line">        Table sensorTable = tableEnv.fromDataStream(waterSensorDS);</span><br><span class="line">        <span class="comment">//5.使用TableAPI过滤出&quot;ws_001&quot;的数据,其他字段都保留</span></span><br><span class="line">        <span class="comment">//TODO Flink V1.12.0写法</span></span><br><span class="line">        Table selectTable = sensorTable</span><br><span class="line">                .where($(<span class="string">&quot;id&quot;</span>).isEqual(<span class="string">&quot;ws_001&quot;</span>)) <span class="comment">//过滤</span></span><br><span class="line">                .select($(<span class="string">&quot;id&quot;</span>), $(<span class="string">&quot;ts&quot;</span>), $(<span class="string">&quot;vc&quot;</span>)); <span class="comment">//过滤完后你要查什么东西</span></span><br><span class="line">        <span class="comment">//TODO 老版本写法.</span></span><br><span class="line"><span class="comment">//        Table selectTable = sensorTable</span></span><br><span class="line"><span class="comment">//                .where(&quot;id =&#x27;ws_001&#x27;&quot;)</span></span><br><span class="line"><span class="comment">//                .select(&quot;id,ts,vc&quot;);</span></span><br><span class="line">        <span class="comment">//6.将selectTable转换为流进行输出</span></span><br><span class="line"><span class="comment">//        DataStream&lt;Row&gt; rowDataStream = tableEnv.toAppendStream(selectTable, Row.class);</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; rowDataStream = tableEnv.toRetractStream(selectTable, Row.class);<span class="comment">//Row.class这是通用的类</span></span><br><span class="line">        rowDataStream.print();</span><br><span class="line">        <span class="comment">//7.执行任务</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>代码地址: <a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo10/FlinkSQL01_StreamToTable_Test.java">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo10/FlinkSQL01_StreamToTable_Test.java</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;将流转换成表,再将表转换成流打印&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>kafka3.0.0之kafka概述</title>
    <link href="http://xubatian.cn/kafka3-0-0%E4%B9%8Bkafka%E6%A6%82%E8%BF%B0/"/>
    <id>http://xubatian.cn/kafka3-0-0%E4%B9%8Bkafka%E6%A6%82%E8%BF%B0/</id>
    <published>2022-02-17T04:53:43.000Z</published>
    <updated>2022-02-17T06:43:43.999Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217124445.png"></p><span id="more"></span><h2 id="Kafka-概述"><a href="#Kafka-概述" class="headerlink" title="Kafka 概述"></a><strong>Kafka</strong> <strong>概述</strong></h2><h3 id="kafka定义"><a href="#kafka定义" class="headerlink" title="kafka定义"></a>kafka定义</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217125929.png"></p><h3 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h3><p>​            目 前企 业中比 较常 见的 消息 队列产 品主 要有 Kafka、ActiveMQ 、RabbitMQ 、RocketMQ 等。<br>​            在大数据场景主要采用 Kafka 作为消息队列。在 JavaEE 开发中主要采用 ActiveMQ、RabbitMQ、RocketMQ。</p><h3 id="传统消息队列的应用场景"><a href="#传统消息队列的应用场景" class="headerlink" title="传统消息队列的应用场景"></a>传统消息队列的应用场景</h3><p>​        传统的消息队列的主要应用场景包括：缓存/消峰、解耦和异步通信。</p><h4 id="消息队列的应用场景——缓冲-消峰"><a href="#消息队列的应用场景——缓冲-消峰" class="headerlink" title="消息队列的应用场景——缓冲/消峰"></a>消息队列的应用场景——缓冲/消峰</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217130109.png"></p><h4 id="消息队列的应用场景——解耦"><a href="#消息队列的应用场景——解耦" class="headerlink" title="消息队列的应用场景——解耦"></a>消息队列的应用场景——解耦</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217130150.png"></p><h4 id="消息队列的应用场景——异步通信"><a href="#消息队列的应用场景——异步通信" class="headerlink" title="消息队列的应用场景——异步通信"></a>消息队列的应用场景——异步通信</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217130231.png"></p><h3 id="消息队列的两种模式-Rabbit-MQ-Active-MQ-kafka"><a href="#消息队列的两种模式-Rabbit-MQ-Active-MQ-kafka" class="headerlink" title="消息队列的两种模式(Rabbit MQ,Active MQ,kafka)"></a>消息队列的两种模式(Rabbit MQ,Active MQ,kafka)</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217130446.png"></p><h3 id="Kafka-基础架构"><a href="#Kafka-基础架构" class="headerlink" title="Kafka 基础架构"></a>Kafka 基础架构</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217130607.png"></p><p>100T的数据存储到一台服务器上,一台服务器一般是8-90T. 所以不能存下. 所以,遇到海量数据分而治之. 把100T数据切分成几块来处理.</p><p>​    kafka也是采用这种思想. 把100T的数据切割成几块来处理. 就是把一个主题的数据分割成多个partition分区. 100T的数据存到一个主题里面,但是因为数据太大. 我们把这个主题又分为多个分区. 即,100T的数据发往topicA主题当中, 但是我每一台服务器都无法同时存储下这100T数据. 所以我们把这100T数据切割成n份分区.每一个服务器只存储30多T的数据.(broker0,broker1,broker2代表hadoop101,102,103对应的主题名称,即三台服务器)</p><p>​    既然生产切分了,消费要并行消费才会很快. 所以就有消费者组的概念, 消费者组里面有n个消费者,每一个人负责对应的分区. 消费者组就消费你这个topic主题.这样消费就很快. <strong>一个分区的数据只能由一个消费者来消费.</strong></p><h4 id="如果一个分区挂了怎么办呢"><a href="#如果一个分区挂了怎么办呢" class="headerlink" title="如果一个分区挂了怎么办呢?"></a>如果一个分区挂了怎么办呢?</h4><p>​        考虑到可靠性,kafka可以增加副本. hadoop里面的副本是一样的.但是kafka里面的副本分为 leader和 follower.</p><p>无论是生产者和消费者,生产消费的时候只针对leader这个副本进行生产和消费. follower的作用就是,当你的leader挂掉之后,follower有条件成为新的leader.</p><h4 id="zookeeper上存了kafka的那些信息"><a href="#zookeeper上存了kafka的那些信息" class="headerlink" title="zookeeper上存了kafka的那些信息?"></a>zookeeper上存了kafka的那些信息?</h4><p>​      kafka有一部分信息存在zookeeper里面的.他帮kafka来存储整个集群中那些服务器上线了.也就是记录服务器运行的节点状态.zookeeper还会帮你记录每一个分区谁是leader. 这样后续生产和消费的时候直接找leader. Kafka2.8.0之前,kafka必须要有zookeeper进行配合使用.2.8.0之后就不是必须的了.他是可选的.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217130719.png"></p><p>​    </p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217124445.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Kafka" scheme="http://xubatian.cn/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Flink写入clickhouse案例</title>
    <link href="http://xubatian.cn/Flink%E5%86%99%E5%85%A5clickhouse%E6%A1%88%E4%BE%8B/"/>
    <id>http://xubatian.cn/Flink%E5%86%99%E5%85%A5clickhouse%E6%A1%88%E4%BE%8B/</id>
    <published>2022-02-16T17:40:56.000Z</published>
    <updated>2022-02-19T13:25:11.070Z</updated>
    
    <content type="html"><![CDATA[<p>Flink写入clickhouse案例</p><p>源码地址: </p><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/Flink-shangbaishuyao-RTDW-gmall-realtime/src/main/java/com/shangbaishuyao/gmall/realtime/app/DWS/KeywordStats4ProductApp.java">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/Flink-shangbaishuyao-RTDW-gmall-realtime/src/main/java/com/shangbaishuyao/gmall/realtime/app/DWS/KeywordStats4ProductApp.java</a></p><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/Flink-shangbaishuyao-RTDW-gmall-realtime/src/main/java/com/shangbaishuyao/gmall/realtime/utils/ClickHouseUtil.java">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/Flink-shangbaishuyao-RTDW-gmall-realtime/src/main/java/com/shangbaishuyao/gmall/realtime/utils/ClickHouseUtil.java</a></p><span id="more"></span><p>主程序:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shangbaishuyao.gmall.realtime.app.DWS;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.shangbaishuyao.gmall.realtime.app.Function.KeywordUDTF;</span><br><span class="line"><span class="keyword">import</span> com.shangbaishuyao.gmall.realtime.bean.KeywordStats;</span><br><span class="line"><span class="keyword">import</span> com.shangbaishuyao.gmall.realtime.utils.ClickHouseUtil;</span><br><span class="line"><span class="keyword">import</span> com.shangbaishuyao.gmall.realtime.utils.MyKafkaUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: shangbaishuyao</span></span><br><span class="line"><span class="comment"> * Date: 2021/3/1</span></span><br><span class="line"><span class="comment"> * Desc:  从商品统计中获取关键词</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KeywordStats4ProductApp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//TODO 0.基本环境准备</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">//设置并行度</span></span><br><span class="line">        env.setParallelism(<span class="number">4</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        //CK相关设置</span></span><br><span class="line"><span class="comment">        env.enableCheckpointing(5000, CheckpointingMode.AT_LEAST_ONCE);</span></span><br><span class="line"><span class="comment">        env.getCheckpointConfig().setCheckpointTimeout(60000);</span></span><br><span class="line"><span class="comment">        StateBackend fsStateBackend = new FsStateBackend(</span></span><br><span class="line"><span class="comment">                &quot;hdfs://hadoop202:8020/gmall/flink/checkpoint/ProvinceStatsSqlApp&quot;);</span></span><br><span class="line"><span class="comment">        env.setStateBackend(fsStateBackend);</span></span><br><span class="line"><span class="comment">        System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;shangbaishuyao&quot;);</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="comment">//TODO 1.定义Table流环境</span></span><br><span class="line">        EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">            .newInstance()</span><br><span class="line">            .inStreamingMode()</span><br><span class="line">            .build();</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//TODO 2.注册自定义函数</span></span><br><span class="line">        tableEnv.createTemporarySystemFunction(<span class="string">&quot;ik_analyze&quot;</span>,  KeywordUDTF.class);</span><br><span class="line">        tableEnv.createTemporarySystemFunction(<span class="string">&quot;keywordProductC2R&quot;</span>,  KeywordProductC2RUDTF.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//TODO 3.将数据源定义为动态表</span></span><br><span class="line">        String groupId = <span class="string">&quot;keyword_stats_app&quot;</span>;</span><br><span class="line">        String productStatsSourceTopic =<span class="string">&quot;dws_product_stats&quot;</span>;</span><br><span class="line"></span><br><span class="line">        tableEnv.executeSql(<span class="string">&quot;CREATE TABLE product_stats (spu_name STRING, &quot;</span> +</span><br><span class="line">            <span class="string">&quot;click_ct BIGINT,&quot;</span> +</span><br><span class="line">            <span class="string">&quot;cart_ct BIGINT,&quot;</span> +</span><br><span class="line">            <span class="string">&quot;order_ct BIGINT ,&quot;</span> +</span><br><span class="line">            <span class="string">&quot;stt STRING,edt STRING ) &quot;</span> +</span><br><span class="line">            <span class="string">&quot;  WITH (&quot;</span>+ MyKafkaUtil.getKafkaDDL(productStatsSourceTopic,groupId)+<span class="string">&quot;)&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//TODO 6.聚合计数</span></span><br><span class="line">        Table keywordStatsProduct = tableEnv.sqlQuery(<span class="string">&quot;select keyword,ct,source, &quot;</span> +</span><br><span class="line">            <span class="string">&quot;DATE_FORMAT(stt,&#x27;yyyy-MM-dd HH:mm:ss&#x27;)  stt,&quot;</span> +</span><br><span class="line">            <span class="string">&quot;DATE_FORMAT(edt,&#x27;yyyy-MM-dd HH:mm:ss&#x27;) as edt, &quot;</span> +</span><br><span class="line">            <span class="string">&quot;UNIX_TIMESTAMP()*1000 ts from product_stats  , &quot;</span> +</span><br><span class="line">            <span class="string">&quot;LATERAL TABLE(ik_analyze(spu_name)) as T(keyword) ,&quot;</span> +</span><br><span class="line">            <span class="string">&quot;LATERAL TABLE(keywordProductC2R( click_ct ,cart_ct,order_ct)) as T2(ct,source)&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//TODO 7.转换为数据流</span></span><br><span class="line">        DataStream&lt;KeywordStats&gt; keywordStatsProductDataStream =</span><br><span class="line">            tableEnv.&lt;KeywordStats&gt;toAppendStream(keywordStatsProduct, KeywordStats.class);</span><br><span class="line"></span><br><span class="line">        keywordStatsProductDataStream.print();</span><br><span class="line">        <span class="comment">//TODO 8.写入到ClickHouse</span></span><br><span class="line">        keywordStatsProductDataStream.addSink(</span><br><span class="line">            ClickHouseUtil.&lt;KeywordStats&gt;getJdbcSink(</span><br><span class="line">                <span class="string">&quot;insert into keyword_stats(keyword,ct,source,stt,edt,ts) values(?,?,?,?,?,?)&quot;</span></span><br><span class="line">            )</span><br><span class="line">        );</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>clickhouse工具类:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shangbaishuyao.gmall.realtime.utils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.shangbaishuyao.gmall.realtime.bean.TransientSink;</span><br><span class="line"><span class="keyword">import</span> com.shangbaishuyao.gmall.realtime.common.GmallConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.connector.jdbc.JdbcConnectionOptions;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.connector.jdbc.JdbcExecutionOptions;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.connector.jdbc.JdbcSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.connector.jdbc.JdbcStatementBuilder;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.SinkFunction;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Field;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.sql.SQLException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: 上白书妖</span></span><br><span class="line"><span class="comment"> * Date: 2021/2/23</span></span><br><span class="line"><span class="comment"> * Desc: 操作ClickHouse的工具类</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 其中flink-connector-jdbc 是官方通用的jdbcSink包。</span></span><br><span class="line"><span class="comment"> * 只要引入对应的jdbc驱动，flink可以用它应对各种支持jdbc的数据库，</span></span><br><span class="line"><span class="comment"> * 比如phoenix也可以用它。但是这个jdbc-sink只支持数据流对应一张数据表。</span></span><br><span class="line"><span class="comment"> * 如果是一流对多表，就必须通过自定义的方式实现了，比如之前的维度数据。</span></span><br><span class="line"><span class="comment"> * 虽然这种jdbc-sink只能一流对一表，但是由于内部使用了预编译器，</span></span><br><span class="line"><span class="comment"> * 所以可以实现批量提交以优化写入速度。</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * A.JdbcSink.&lt;T&gt;sink( )的四个参数说明</span></span><br><span class="line"><span class="comment"> * 参数1： 传入Sql，格式如：insert into xxx values(?,?,?,?)</span></span><br><span class="line"><span class="comment"> * 参数2:  可以用lambda表达实现(jdbcPreparedStatement, t) -&gt; t为数据对象，要装配到语句预编译器的参数中。</span></span><br><span class="line"><span class="comment"> * 参数3：设定一些执行参数，比如重试次数，批次大小。</span></span><br><span class="line"><span class="comment"> * 参数4：设定连接参数，比如地址，端口，驱动名。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ClickHouseUtil</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取向Clickhouse中写入数据的SinkFunction</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> sql</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">SinkFunction <span class="title">getJdbcSink</span><span class="params">(String sql)</span> </span>&#123;</span><br><span class="line">        SinkFunction&lt;T&gt; sinkFunction = JdbcSink.&lt;T&gt;sink(</span><br><span class="line">            <span class="comment">//要执行的SQL语句</span></span><br><span class="line">            sql,</span><br><span class="line">            <span class="comment">//执行写入操作   就是将当前流中的对象属性赋值给SQL的占位符 insert into visitor_stats values(?,?,?,?,?,?,?,?,?,?,?,?)</span></span><br><span class="line">            <span class="keyword">new</span> JdbcStatementBuilder&lt;T&gt;() &#123;</span><br><span class="line">                <span class="comment">//obj  就是流中的一条数据对象</span></span><br><span class="line">                <span class="comment">//我要想实现通用型的话,这个当前流的类型就不能固定,只能使用泛型.T. 可以用反射拿到属性.</span></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(PreparedStatement ps, T obj)</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">                    <span class="comment">//insert into visitor_stats values(?,?,?,?,?,?,?,?,?,?,?,?)</span></span><br><span class="line">                    <span class="comment">//获取当前类中  所有的属性</span></span><br><span class="line">                    Field[] fields = obj.getClass().getDeclaredFields();<span class="comment">//这个可以获取私有的.</span></span><br><span class="line">                    <span class="comment">//跳过的属性计数    insert into visitor_stats values(?,?,?,?,跳过,?,?,?,?,?,?,?)</span></span><br><span class="line">                    <span class="keyword">int</span> skipOffset = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; fields.length; i++) &#123;</span><br><span class="line">                        Field field = fields[i];</span><br><span class="line">                        <span class="comment">//通过属性对象获取属性上是否有@TransientSink注解</span></span><br><span class="line">                        TransientSink transientSink = field.getAnnotation(TransientSink.class);</span><br><span class="line">                        <span class="comment">//如果transientSink不为空，说明属性上有@TransientSink标记，那么在给?占位符赋值的时候，应该跳过当前属性</span></span><br><span class="line">                        <span class="keyword">if</span> (transientSink != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            skipOffset++;</span><br><span class="line">                            <span class="keyword">continue</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="comment">//设置私有属性可访问</span></span><br><span class="line">                        field.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">                        <span class="keyword">try</span> &#123;</span><br><span class="line">                            <span class="comment">//获取属性值</span></span><br><span class="line">                            Object o = field.get(obj);</span><br><span class="line">                            ps.setObject(i + <span class="number">1</span> - skipOffset, o);</span><br><span class="line">                        &#125; <span class="keyword">catch</span> (IllegalAccessException e) &#123;</span><br><span class="line">                            e.printStackTrace();</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="comment">//构建者设计模式，创建JdbcExecutionOptions对象，给batchSize属性赋值，执行执行批次大小</span></span><br><span class="line">            <span class="keyword">new</span> JdbcExecutionOptions.Builder().withBatchSize(<span class="number">5</span>).build(),</span><br><span class="line">            <span class="comment">//构建者设计模式，JdbcConnectionOptions，给连接相关的属性进行赋值</span></span><br><span class="line">            <span class="keyword">new</span> JdbcConnectionOptions.JdbcConnectionOptionsBuilder()</span><br><span class="line">                .withUrl(GmallConfig.CLICKHOUSE_URL)</span><br><span class="line">                .withDriverName(<span class="string">&quot;ru.yandex.clickhouse.ClickHouseDriver&quot;</span>)</span><br><span class="line">                .build()</span><br><span class="line">        );</span><br><span class="line">        <span class="keyword">return</span> sinkFunction;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;Flink写入clickhouse案例&lt;/p&gt;
&lt;p&gt;源码地址: &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/Flink-shangbaishuyao-RTDW-gmall-realtime/src/main/java/com/shangbaishuyao/gmall/realtime/app/DWS/KeywordStats4ProductApp.java&quot;&gt;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/Flink-shangbaishuyao-RTDW-gmall-realtime/src/main/java/com/shangbaishuyao/gmall/realtime/app/DWS/KeywordStats4ProductApp.java&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/Flink-shangbaishuyao-RTDW-gmall-realtime/src/main/java/com/shangbaishuyao/gmall/realtime/utils/ClickHouseUtil.java&quot;&gt;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/Flink-shangbaishuyao-RTDW-gmall-realtime/src/main/java/com/shangbaishuyao/gmall/realtime/utils/ClickHouseUtil.java&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
    <category term="clickhouse" scheme="http://xubatian.cn/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse的副本和分片集群</title>
    <link href="http://xubatian.cn/ClickHouse%E7%9A%84%E5%89%AF%E6%9C%AC%E5%92%8C%E5%88%86%E7%89%87%E9%9B%86%E7%BE%A4/"/>
    <id>http://xubatian.cn/ClickHouse%E7%9A%84%E5%89%AF%E6%9C%AC%E5%92%8C%E5%88%86%E7%89%87%E9%9B%86%E7%BE%A4/</id>
    <published>2022-02-16T17:25:15.000Z</published>
    <updated>2022-02-16T17:32:26.811Z</updated>
    
    <content type="html"><![CDATA[<h3 id="副本"><a href="#副本" class="headerlink" title="副本"></a>副本</h3><p>​        副本的目的主要是保障数据的高可用性，即使一台ClickHouse节点宕机，那么也可以从其他服务器获得相同的数据。<br>高可用: 包括数据的高可用 , 服务的高可用(HA)</p><h4 id="副本写入流程"><a href="#副本写入流程" class="headerlink" title="副本写入流程"></a>副本写入流程</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217012620.png"></p><span id="more"></span><p><strong>副本只能同步数据，不能同步表结构，所以我们需要在每台机器上自己手动建表</strong></p><p>建表语句案例:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217012717.png"></p><p><strong>参数解释</strong><br>ReplicatedMergeTree 中，<br>第一个参数是分片的zk_path一般按照： /clickhouse/table/{shard}/{table_name} 的格式写，如果只有一个分片就写01即可。<br>第二个参数是副本名称，相同的分片副本名称不能相同。</p><h3 id="分片集群"><a href="#分片集群" class="headerlink" title="分片集群"></a>分片集群</h3><p>​        <strong>副本虽然能够提高数据的可用性，降低丢失风险，但是每台服务器实际上必须容纳全量数据，对数据的横向扩容没有解决。</strong><br>​        要解决数据水平切分的问题，需要引入分片的概念。通过分片把一份完整的数据进行切分，不同的分片分布到不同的节点上，再通过Distributed表引擎把数据拼接起来一同使用。<br>​        Distributed表引擎本身不存储数据，有点类似于MyCat之于MySql，成为一种中间件，通过分布式逻辑表来写入、分发、路由来操作多台节点不同分片的分布式数据。<br><strong>注意：ClickHouse的集群是表级别的，实际企业中，大部分做了高可用，但是没有用分片，避免降低查询性能以及操作集群的复杂性。</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217012912.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217012932.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217012940.png"></p><h4 id="集群写入流程（3分片2副本共6个节点）"><a href="#集群写入流程（3分片2副本共6个节点）" class="headerlink" title="集群写入流程（3分片2副本共6个节点）"></a>集群写入流程（3分片2副本共6个节点）</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217013014.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217013035.png"></p><h4 id="集群读取流程（3分片2副本共6个节点）"><a href="#集群读取流程（3分片2副本共6个节点）" class="headerlink" title="集群读取流程（3分片2副本共6个节点）"></a>集群读取流程（3分片2副本共6个节点）</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217013054.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217013117.png"></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;副本&quot;&gt;&lt;a href=&quot;#副本&quot; class=&quot;headerlink&quot; title=&quot;副本&quot;&gt;&lt;/a&gt;副本&lt;/h3&gt;&lt;p&gt;​        副本的目的主要是保障数据的高可用性，即使一台ClickHouse节点宕机，那么也可以从其他服务器获得相同的数据。&lt;br&gt;高可用: 包括数据的高可用 , 服务的高可用(HA)&lt;/p&gt;
&lt;h4 id=&quot;副本写入流程&quot;&gt;&lt;a href=&quot;#副本写入流程&quot; class=&quot;headerlink&quot; title=&quot;副本写入流程&quot;&gt;&lt;/a&gt;副本写入流程&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217012620.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="clickhouse" scheme="http://xubatian.cn/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse表引擎</title>
    <link href="http://xubatian.cn/ClickHouse%E8%A1%A8%E5%BC%95%E6%93%8E/"/>
    <id>http://xubatian.cn/ClickHouse%E8%A1%A8%E5%BC%95%E6%93%8E/</id>
    <published>2022-02-16T16:46:52.000Z</published>
    <updated>2022-02-16T17:20:05.097Z</updated>
    
    <content type="html"><![CDATA[<h3 id="表引擎的使用"><a href="#表引擎的使用" class="headerlink" title="表引擎的使用"></a>表引擎的使用</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217004854.png"></p><span id="more"></span><h3 id="TinyLog"><a href="#TinyLog" class="headerlink" title="TinyLog"></a>TinyLog</h3><p>​            以列文件的形式保存在磁盘上，不支持索引，没有并发控制。一般保存少量数据的小表，生产环境上作用有限。可以用于<strong>平时练习测试</strong>用。如：</p><pre><code>create table t_tinylog ( id String, name String) engine=TinyLog;</code></pre><h3 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h3><p>​            内存引擎，数据以未压缩的原始形式直接保存在内存当中，服务器重启数据就会消失。读写操作不会相互阻塞，不支持索引。简单查询下有非常非常高的性能表现（超过10G/s）。<br>​            一般用到它的地方不多，除了用来测试，就是在需要非常高的性能，同时数据量又不太大（上限大概 1 亿行）的场景。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据保存在内存中,但是内存数据不稳定,一旦服务器重启,数据消失.除非你对查询的性能有非常高的要求.但是这种情况很少.</span><br></pre></td></tr></table></figure><h3 id="MergeTree-最强大的表引擎"><a href="#MergeTree-最强大的表引擎" class="headerlink" title="MergeTree(最强大的表引擎)"></a>MergeTree(最强大的表引擎)</h3><p>​            <strong>ClickHouse中最强大的表引擎当属MergeTree（合并树）引擎及该系列（MergeTree）中的其他引擎，支持索引和分区， 地位可以相当于innodb之于Mysql</strong>。 而且基于MergeTree，还衍生除了很多小弟，也是非常有特色的引擎。</p><p><strong>建表语句</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table t_order_mt(</span><br><span class="line">    id UInt32,</span><br><span class="line">    sku_id String,</span><br><span class="line">    total_amount Decimal(16,2),</span><br><span class="line">    create_time  Datetime                      </span><br><span class="line"> ) engine =MergeTree</span><br><span class="line">   partition by toYYYYMMDD(create_time)      </span><br><span class="line">   primary key (id)</span><br><span class="line">   order by (id,sku_id);</span><br></pre></td></tr></table></figure><p><strong>Partition</strong>:  分区,假如我Datetime设置的是2020-12-12 12:12 但是我的分区是按照2020-12-12来分区的转换成年月日的形式.<br><strong>primary key</strong>:  主键,以前的主键是数据看看表中字段的唯一标记. 现在在clickhouse中主键没有唯一标记.它主要做的就是提升我们的查询效率.他其实就是为了建我们的查询索引的.<br><strong>order by</strong>:  排序,在clickhouse中,order by不仅仅是排序,去重等也是依靠order by来做的</p><p><strong>插入数据</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">insert into  t_order_mt values</span><br><span class="line">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;);</span><br></pre></td></tr></table></figure><p><strong>MergeTree其实还有很多参数(绝大多数用默认值即可)，但是三个参数是更加重要的，也涉及了关于MergeTree的很多概念。</strong></p><h4 id="partition-by-分区-（可选项-不填的话数据在一个分区）"><a href="#partition-by-分区-（可选项-不填的话数据在一个分区）" class="headerlink" title="partition by 分区 （可选项,不填的话数据在一个分区）"></a>partition by 分区 （可选项,不填的话数据在一个分区）</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005428.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005448.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005514.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimize table xxxx final;</span><br></pre></td></tr></table></figure><p>Ø 例如</p><p>再次执行上面的插入操作</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">insert into  t_order_mt values</span><br><span class="line">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;);</span><br></pre></td></tr></table></figure><p>查看数据并没有纳入任何分区</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005604.png"></p><p>看到分成了四块,按照create_time分区.</p><p>手动optimize之后</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop202 :) optimize table t_order_mt final;</span><br></pre></td></tr></table></figure><p>再次查询</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005655.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005736.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">插入数据不会立马合并,要不然就是你插入的数据量足够大,要不然就是你插入的数据时间足够长,要不然就是你手动合并.</span><br></pre></td></tr></table></figure><h4 id="primary-key主键-可选"><a href="#primary-key主键-可选" class="headerlink" title="primary key主键(可选)"></a>primary key主键(可选)</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005829.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005847.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005857.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005917.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005939.png"></p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010017.png" style="zoom:200%;" /><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010059.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">稀疏索引的好处就是可以用很少的索引数据，定位更多的数据，代价就是只能定位到索引粒度的第一行，然后再进行进行一点扫描。</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010125.png"></p><h4 id="order-by（必选-作用-分区内排序）"><a href="#order-by（必选-作用-分区内排序）" class="headerlink" title="order by（必选,作用:分区内排序）"></a>order by（必选,作用:分区内排序）</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010152.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010213.png"></p><h3 id="二级索引"><a href="#二级索引" class="headerlink" title="二级索引"></a>二级索引</h3><p>​        目前在ClickHouse的官网上二级索引的功能是被标注为实验性的。即不稳定,还有很大的发展空间.</p><h4 id="1-使用二级索引前需要增加设置"><a href="#1-使用二级索引前需要增加设置" class="headerlink" title="(1)使用二级索引前需要增加设置"></a>(1)使用二级索引前需要增加设置</h4><p>是否允许使用实验性的二级索引,开启二级索引:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set allow_experimental_data_skipping_indices=1;</span><br></pre></td></tr></table></figure><h4 id="2-创建测试表"><a href="#2-创建测试表" class="headerlink" title="(2)创建测试表"></a>(2)创建测试表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table t_order_mt2(</span><br><span class="line">    id UInt32,</span><br><span class="line">    sku_id String,</span><br><span class="line">    total_amount Decimal(16,2),</span><br><span class="line">    create_time  Datetime,</span><br><span class="line">INDEX a total_amount TYPE minmax GRANULARITY 5</span><br><span class="line"> ) engine =MergeTree</span><br><span class="line">   partition by toYYYYMMDD(create_time)</span><br><span class="line">   primary key (id)</span><br><span class="line">   order by (id, sku_id);</span><br></pre></td></tr></table></figure><p>其中<strong>GRANULARITY N</strong> 是设定二级索引对于一级索引粒度的粒度。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010335.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010442.png"><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010500.png"></p><h4 id="3-插入数据"><a href="#3-插入数据" class="headerlink" title="(3)插入数据"></a>(3)插入数据</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">insert into  t_order_mt2 values</span><br><span class="line">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;);</span><br></pre></td></tr></table></figure><h4 id="4-对比效果"><a href="#4-对比效果" class="headerlink" title="(4)对比效果"></a>(4)对比效果</h4><p>那么在使用下面语句进行测试，可以看出二级索引能够为非主键字段的查询发挥作用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop202 lib]$ clickhouse-client  --send_logs_level=trace &lt;&lt;&lt; &#x27;select * from t_order_mt2  where total_amount &gt; toDecimal32(900., 2)&#x27;;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010727.png"></p><h3 id="数据TTL"><a href="#数据TTL" class="headerlink" title="数据TTL"></a>数据TTL</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010904.png"></p><h4 id="1-列级别TTL（执行整个列的失效时间）"><a href="#1-列级别TTL（执行整个列的失效时间）" class="headerlink" title="(1)列级别TTL（执行整个列的失效时间）"></a>(1)列级别TTL（执行整个列的失效时间）</h4><p><strong>创建测试表</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table t_order_mt3(</span><br><span class="line">    id UInt32,</span><br><span class="line">    sku_id String,</span><br><span class="line">    total_amount Decimal(16,2)  TTL create_time+interval 10 SECOND, //表示当前这一列数值的存活时间.就是在你创建</span><br><span class="line">    create_time  Datetime                                           //时间过了十秒之后,这一列数据就失效了</span><br><span class="line"> ) engine =MergeTree</span><br><span class="line"> partition by toYYYYMMDD(create_time)</span><br><span class="line">   primary key (id)</span><br><span class="line">   order by (id, sku_id);</span><br></pre></td></tr></table></figure><p><strong>插入数据</strong>（注意：根据实际时间改变）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">insert into  t_order_mt3 values</span><br><span class="line">(106,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-12 22:52:30&#x27;),</span><br><span class="line">(107,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-12 22:52:30&#x27;),</span><br><span class="line">(110,&#x27;sku_003&#x27;,600.00,&#x27;2020-06-13 12:00:00&#x27;);</span><br></pre></td></tr></table></figure><p><strong>手动合并，查看效果  到期后，指定的字段数据归0</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011106.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010933.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">其实这个TTL这个手动合并，其实不是帮你解决业务上的合并的。 其实他是从本身优化的角度来考虑的。 比如： 加入我有些数据，数据量比较大， 但是这些数据我执行完成后就不需要了， 那我可以设置一个失效时间。 在长时间后数据失效。 设置完失效时间后，虽然不能到了指定时间后，马上将这个字段重置为0. 但是他会过一段时间后，后台看到这个标记，后台会自动帮你进行合并。这样就相当于把空间释放掉了。这是Clickhouse自己做的一个优化。</span><br></pre></td></tr></table></figure><h4 id="2-表级TTL（指定整个表的失效时间）"><a href="#2-表级TTL（指定整个表的失效时间）" class="headerlink" title="(2)表级TTL（指定整个表的失效时间）"></a>(2)表级TTL（指定整个表的失效时间）</h4><p>下面的这条语句是数据会在create_time 之后10秒丢失</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table t_order_mt3 MODIFY TTL create_time + INTERVAL 10 SECOND;</span><br></pre></td></tr></table></figure><p>涉及判断的字段必须是Date或者Datetime类型，推荐使用分区的日期字段。<br>能够使用的时间周期：</p><ul><li>SECOND</li><li>MINUTE</li><li>HOUR</li><li>DAY</li><li>WEEK</li><li>MONTH</li><li>QUARTER</li><li>YEAR </li></ul><h2 id="ReplacingMergeTree"><a href="#ReplacingMergeTree" class="headerlink" title="ReplacingMergeTree"></a>ReplacingMergeTree</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011322.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">去重不是业务上的去重,而是Clickhouse本身为了去优化做的去重.所以说,clickhouse是可以去重,但是什么时间去重,我们不确定. 所以说你给Clickhouse的数据应该是在你在前台已经去重的数据这更合适</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011346.png"></p><p><strong>案例演示</strong></p><p><strong>创建表</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table t_order_rmt(</span><br><span class="line">    id UInt32,</span><br><span class="line">    sku_id String,</span><br><span class="line">    total_amount Decimal(16,2) ,</span><br><span class="line">    create_time  Datetime </span><br><span class="line"> ) engine =ReplacingMergeTree(create_time)</span><br><span class="line">   partition by toYYYYMMDD(create_time)</span><br><span class="line">   primary key (id)</span><br><span class="line">   order by (id, sku_id);</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011431.png"></p><p><strong>向表中插入数据</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">insert into  t_order_rmt values</span><br><span class="line">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;);</span><br></pre></td></tr></table></figure><p><strong>执行第一次查询</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop202 :) select * from t_order_rmt;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011516.png"></p><p><strong>手动合并</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OPTIMIZE TABLE t_order_rmt FINAL;</span><br></pre></td></tr></table></figure><p><strong>再执行一次查询</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop202 :) select * from t_order_rmt;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011601.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011624.png"></p><h2 id="SummingMergeTree-提供预聚合的作用"><a href="#SummingMergeTree-提供预聚合的作用" class="headerlink" title="SummingMergeTree(提供预聚合的作用)"></a>SummingMergeTree(提供预聚合的作用)</h2><pre><code>    对于不查询明细，只关心以维度进行汇总聚合结果的场景。如果只使用普通的MergeTree的话，无论是存储空间的开销，还是查询时临时聚合的开销都比较大。    ClickHouse 为了这种场景，提供了一种能够“预聚合”的引擎SummingMergeTree</code></pre><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011728.png"></p><p><strong>案例演示</strong></p><p><strong>创建表</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table t_order_smt(</span><br><span class="line">    id UInt32,</span><br><span class="line">    sku_id String,</span><br><span class="line">    total_amount Decimal(16,2) ,</span><br><span class="line">    create_time  Datetime </span><br><span class="line"> ) engine =SummingMergeTree(total_amount)</span><br><span class="line">   partition by toYYYYMMDD(create_time)</span><br><span class="line">   primary key (id)</span><br><span class="line">   order by (id,sku_id );</span><br></pre></td></tr></table></figure><p><strong>插入数据</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">insert into  t_order_smt values</span><br><span class="line">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;);</span><br></pre></td></tr></table></figure><p><strong>执行第一次查询</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop202 :) select * from t_order_smt;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011833.png"></p><p><strong>手动合并</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OPTIMIZE TABLE t_order_smt FINAL;</span><br></pre></td></tr></table></figure><p><strong>再执行一次查询</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop202 :) select * from t_order_smt;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011907.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011935.png"></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;表引擎的使用&quot;&gt;&lt;a href=&quot;#表引擎的使用&quot; class=&quot;headerlink&quot; title=&quot;表引擎的使用&quot;&gt;&lt;/a&gt;表引擎的使用&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217004854.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="clickhouse" scheme="http://xubatian.cn/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse数据类型</title>
    <link href="http://xubatian.cn/ClickHouse%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
    <id>http://xubatian.cn/ClickHouse%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</id>
    <published>2022-02-16T16:34:28.000Z</published>
    <updated>2022-02-16T16:46:30.504Z</updated>
    
    <content type="html"><![CDATA[<p>官方文档：<a href="https://clickhouse.yandex/docs/zh/data_types/">https://clickhouse.yandex/docs/zh/data_types/</a></p><h3 id="整型-主要用到"><a href="#整型-主要用到" class="headerlink" title="整型(主要用到)"></a>整型(主要用到)</h3><p>固定长度的整型，包括有符号整型或无符号整型。</p><p>整型范围（-2n-1~2n-1-1）：</p><p>Int8 - [-128 : 127]</p><p>Int16 - [-32768 : 32767]</p><p>Int32 - [-2147483648 : 2147483647]</p><p>Int64 - [-9223372036854775808 : 9223372036854775807]</p><span id="more"></span><p>无符号整型范围（0~2n-1）：</p><p>UInt8 - [0 : 255]</p><p>UInt16 - [0 : 65535]</p><p>UInt32 - [0 : 4294967295]</p><p>UInt64 - [0 : 18446744073709551615]</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用场景： 个数、数量、也可以存储型id</span><br></pre></td></tr></table></figure><h3 id="浮点型"><a href="#浮点型" class="headerlink" title="浮点型"></a>浮点型</h3><p>Float32 - float</p><p>Float64 – double</p><p>建议尽可能以整数形式存储数据。例如，将固定精度的数字转换为整数值，如时间用毫秒为单位表示，因为浮点型进行计算时可能引起四舍五入的误差。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217003813.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用场景：一般数据值比较小，不涉及大量的统计计算，精度要求不高的时候。比如保存商品的重量。</span><br></pre></td></tr></table></figure><h3 id="布尔型"><a href="#布尔型" class="headerlink" title="布尔型"></a>布尔型</h3><p>​        没有单独的类型来存储布尔值。可以使用 UInt8 类型，取值限制为 0 或 1。</p><h3 id="Decimal-型-主要用到-对精度要求高用Decimal"><a href="#Decimal-型-主要用到-对精度要求高用Decimal" class="headerlink" title="Decimal 型((主要用到)对精度要求高用Decimal)"></a>Decimal 型((主要用到)对精度要求高用Decimal)</h3><p>​        有符号的浮点数，可在加、减和乘法运算过程中保持精度。对于除法，最低有效数字会被丢弃（不舍入）。        </p><p>有三种声明：(32,64,128表示分配的空间大小)(表示有效位数是9,s是小数位)</p><ul><li>​            Decimal32(s)，相当于Decimal(9-s,s)，有效位数为1~9</li><li>​            Decimal64(s)，相当于Decimal(18-s,s)，有效位数为1~18</li><li>​            Decimal128(s)，相当于Decimal(38-s,s)，有效位数为1~38</li></ul><p>s标识小数位</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用场景： 一般金额字段、汇率、利率等字段为了保证小数点精度，都使用Decimal进行存储。</span><br></pre></td></tr></table></figure><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><ul><li><p>​    String</p><p>​        字符串可以任意长度的。它可以包含任意的字节集，包含空字节。</p></li><li><p>​    FixedString(N)</p></li></ul><p>​                固定长度 N 的字符串，N 必须是严格的正自然数。当服务端读取长度小于 N 的字符串时候，通过在字符串末尾添加空字节来达到 N 字节长度。 当服务端读取长度大于 N 的字符串时候，将返回错误消息。<br>​          与String相比，极少会使用FixedString，因为使用起来不是很方便。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用场景：名称、文字描述、字符型编码。 固定长度的可以保存一些定长的内容，比如一些编码，性别等但是考虑到一定的变化风险，带来收益不够明显，所以定长字符串使用意义有限。</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217004107.png"></p><h3 id="枚举类型-枚举-一个类它创建对象的个数是固定的叫枚举-比如季节-星期"><a href="#枚举类型-枚举-一个类它创建对象的个数是固定的叫枚举-比如季节-星期" class="headerlink" title="枚举类型(枚举: 一个类它创建对象的个数是固定的叫枚举,比如季节,星期)"></a>枚举类型(枚举: 一个类它创建对象的个数是固定的叫枚举,比如季节,星期)</h3><p>包括 Enum8 和 Enum16 类型。Enum 保存 ‘string’= integer 的对应关系。</p><p>Enum8 用 ‘String’= Int8 对描述。</p><p>Enum16 用 ‘String’= Int16 对描述。</p><p><strong>用法演示</strong></p><p>创建一个带有一个枚举 Enum8(‘hello’ = 1, ‘world’ = 2) 类型的列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE t_enum</span><br><span class="line">(</span><br><span class="line">    x Enum8(&#x27;hello&#x27; = 1, &#x27;world&#x27; = 2)</span><br><span class="line">)</span><br><span class="line">ENGINE = TinyLog;  //这个是引擎</span><br></pre></td></tr></table></figure><p>这个 x 列只能存储类型定义中列出的值：’hello’或’world’</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop202 :) INSERT INTO t_enum VALUES (&#x27;hello&#x27;), (&#x27;world&#x27;), (&#x27;hello&#x27;);</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217004225.png"></p><p>如果尝试保存任何其他值，ClickHouse 抛出异常</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop202 :) insert into t_enum values(&#x27;a&#x27;)</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217004247.png"></p><p>如果需要看到对应行的数值，则必须将 Enum 值转换为整数类型</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop202 :) SELECT CAST(x, &#x27;Int8&#x27;) FROM t_enum;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217004318.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用场景：对一些状态、类型的字段算是一种空间优化，也算是一种数据约束。但是实际使用中往往因为一些数据内容的变化增加一定的维护成本，甚至是数据丢失问题。所以谨慎使用。</span><br></pre></td></tr></table></figure><h3 id="时间类型-主要用到"><a href="#时间类型-主要用到" class="headerlink" title="时间类型(主要用到)"></a>时间类型(主要用到)</h3><p><strong>目前ClickHouse 有三种时间类型</strong><br>    Date接受年-月-日的字符串比如 ‘2019-12-16’<br>    Datetime接受年-月-日 时:分:秒的字符串比如 ‘2019-12-16 20:50:10’<br>    Datetime64接受年-月-日 时:分:秒.亚秒的字符串比如‘2019-12-16 20:50:10.66’</p><p>日期类型，用两个字节存储，表示从 1970-01-01 (无符号) 到当前的日期值。</p><p>还有很多数据结构，可以参考官方文档：<a href="https://clickhouse.yandex/docs/zh/data_types/">https://clickhouse.yandex/docs/zh/data_types/</a></p><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><p>​        Array(T)：由 T 类型元素组成的数组。<br>​        T 可以是任意类型，包含数组类型。 但不推荐使用多维数组，ClickHouse 对多维数组的支持有限。例如，不能在MergeTree表中存储多维数组。<br><strong>创建数组方式1，使用array函数</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array(T)</span><br><span class="line">hadoop202 :) SELECT array(1, 2) AS x, toTypeName(x) ;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217004531.png"></p><p><strong>创建数组方式2：使用方括号</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[]</span><br><span class="line">hadoop202 :) SELECT [1, 2] AS x, toTypeName(x);</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217004600.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;官方文档：&lt;a href=&quot;https://clickhouse.yandex/docs/zh/data_types/&quot;&gt;https://clickhouse.yandex/docs/zh/data_types/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;整型-主要用到&quot;&gt;&lt;a href=&quot;#整型-主要用到&quot; class=&quot;headerlink&quot; title=&quot;整型(主要用到)&quot;&gt;&lt;/a&gt;整型(主要用到)&lt;/h3&gt;&lt;p&gt;固定长度的整型，包括有符号整型或无符号整型。&lt;/p&gt;
&lt;p&gt;整型范围（-2n-1~2n-1-1）：&lt;/p&gt;
&lt;p&gt;Int8 - [-128 : 127]&lt;/p&gt;
&lt;p&gt;Int16 - [-32768 : 32767]&lt;/p&gt;
&lt;p&gt;Int32 - [-2147483648 : 2147483647]&lt;/p&gt;
&lt;p&gt;Int64 - [-9223372036854775808 : 9223372036854775807]&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="clickhouse" scheme="http://xubatian.cn/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse入门</title>
    <link href="http://xubatian.cn/ClickHouse%E5%85%A5%E9%97%A8/"/>
    <id>http://xubatian.cn/ClickHouse%E5%85%A5%E9%97%A8/</id>
    <published>2022-02-16T16:19:36.000Z</published>
    <updated>2022-02-16T16:44:32.002Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ClickHouse入门"><a href="#ClickHouse入门" class="headerlink" title="ClickHouse入门"></a>ClickHouse入门</h1><p>​        ClickHouse 是俄罗斯的Yandex于2016年开源的列式存储数据库（DBMS），使用C++语言编写，主要用于<strong>在线分析处理查询（OLAP），能够使用SQL查询实时生成分析数据报告。</strong>  </p><p>官方文档：<a href="https://clickhouse.yandex/docs/zh/data_types/">https://clickhouse.yandex/docs/zh/data_types/</a></p><span id="more"></span><h2 id="ClickHouse的特点"><a href="#ClickHouse的特点" class="headerlink" title="ClickHouse的特点"></a>ClickHouse的特点</h2><h3 id="列式存储"><a href="#列式存储" class="headerlink" title="列式存储"></a>列式存储</h3><p>以下面的表为例：</p><table><thead><tr><th><strong>Id</strong></th><th><strong>Name</strong></th><th><strong>Age</strong></th></tr></thead><tbody><tr><td>1</td><td>张三</td><td>18</td></tr><tr><td>2</td><td>李四</td><td>22</td></tr><tr><td>3</td><td>王五</td><td>34</td></tr></tbody></table><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217002324.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217002355.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217002441.png"></p><p><strong>OALP: 可以做查询,插入,但是不擅长删除和修改操作. 其实这个也可以理解. 因为我大数据拿到数据,会有对数据进行删除和修改的场景吗? 基本没有.</strong></p><h3 id="DBMS的功能"><a href="#DBMS的功能" class="headerlink" title="DBMS的功能"></a>DBMS的功能</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">几乎覆盖了标准SQL的大部分语法，包括 DDL和 DML(crud)，以及配套的各种函数，用户管理及权限管理，数据的备份与恢复</span><br></pre></td></tr></table></figure><h3 id="多样化引擎"><a href="#多样化引擎" class="headerlink" title="多样化引擎"></a>多样化引擎</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ClickHouse和MySQL类似，把表级的存储引擎插件化，根据表的不同需求可以设定不同的存储引擎。目前包括合并树、日志、接口和其他四大类20多种引擎。</span><br></pre></td></tr></table></figure><p><strong>表级的存储引擎插件化:</strong><br>        就是,这张表使用则引擎, 那张表使用那个引擎.<br>        可以根据本表的需求使用不同的引擎.<br>        目前包括树的,日志的,接口的,和其他四大类20多种引擎.<br>        Mysql有InnoDB和myISAM,InnoDB支持事务,myISQM不支持事务.</p><h3 id="高吞吐写入能力"><a href="#高吞吐写入能力" class="headerlink" title="高吞吐写入能力"></a>高吞吐写入能力</h3><p>​        ClickHouse采用类LSM Tree的结构，数据写入后定期在后台Compaction。通过类LSM tree的结构，<strong>ClickHouse在数据导入时全部是顺序append写，写入后数据段不可更改，在后台compaction(合并)时也是多个段merge sort后顺序写回磁盘</strong>。顺序写的特性，充分利用了磁盘的吞吐能力，即便在HDD上也有着优异的写入性能。<br>​        官方公开benchmark测试显示能够达到50MB-200MB/s的写入吞吐能力，按照每行100Byte估算，大约相当于50W-200W条/s的写入速度。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217002815.png"></p><h3 id="数据分区与线程级并行"><a href="#数据分区与线程级并行" class="headerlink" title="数据分区与线程级并行"></a>数据分区与线程级并行</h3><p>​        ClickHouse将数据划分为多个partition，每个partition再进一步划分为多个index granularity(索引力度)，然后通过多个CPU核心分别处理其中的一部分来实现并行数据处理。在这种设计下，单条Query就能利用整机所有CPU。极致的并行处理能力，极大的降低了查询延时。<br>​        所以，ClickHouse即使对于大量数据的查询也能够化整为零平行处理。但是有一个弊端就是对于单条查询使用多cpu，就不利于同时并发多条查询。所以对于高qps的查询业务，ClickHouse并不是强项。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217002917.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217003005.png"></p><h3 id="性能对比"><a href="#性能对比" class="headerlink" title="性能对比"></a>性能对比</h3><p>某网站精华帖，中对几款数据库做了性能对比。</p><h4 id="单表查询"><a href="#单表查询" class="headerlink" title="单表查询"></a>单表查询</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217003054.png"></p><h4 id="关联查询"><a href="#关联查询" class="headerlink" title="关联查询"></a>关联查询</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217003115.png"></p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>​     ClickHouse像很多OLAP数据库一样，单表查询速度由于关联查询，而且ClickHouse的两者差距更为明显。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217003219.png"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;ClickHouse入门&quot;&gt;&lt;a href=&quot;#ClickHouse入门&quot; class=&quot;headerlink&quot; title=&quot;ClickHouse入门&quot;&gt;&lt;/a&gt;ClickHouse入门&lt;/h1&gt;&lt;p&gt;​        ClickHouse 是俄罗斯的Yandex于2016年开源的列式存储数据库（DBMS），使用C++语言编写，主要用于&lt;strong&gt;在线分析处理查询（OLAP），能够使用SQL查询实时生成分析数据报告。&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;官方文档：&lt;a href=&quot;https://clickhouse.yandex/docs/zh/data_types/&quot;&gt;https://clickhouse.yandex/docs/zh/data_types/&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="clickhouse" scheme="http://xubatian.cn/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>常用的shell脚本</title>
    <link href="http://xubatian.cn/%E5%B8%B8%E7%94%A8%E7%9A%84shell%E8%84%9A%E6%9C%AC/"/>
    <id>http://xubatian.cn/%E5%B8%B8%E7%94%A8%E7%9A%84shell%E8%84%9A%E6%9C%AC/</id>
    <published>2022-02-16T14:17:52.000Z</published>
    <updated>2022-02-16T14:58:13.787Z</updated>
    
    <content type="html"><![CDATA[<h1 id="myhadoop-sh"><a href="#myhadoop-sh" class="headerlink" title="myhadoop.sh"></a>myhadoop.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 hadoop集群 ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; --------------- 启动 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/sbin/start-dfs.sh&quot;</span><br><span class="line">        echo &quot; --------------- 启动 yarn ---------------&quot;</span><br><span class="line">        ssh hadoop103 &quot;/opt/module/hadoop-3.1.3/sbin/start-yarn.sh&quot;</span><br><span class="line">        echo &quot; --------------- 启动 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 hadoop集群 ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; --------------- 关闭 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 yarn ---------------&quot;</span><br><span class="line">        ssh hadoop103 &quot;/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><span id="more"></span><h1 id="myzookeeper-sh"><a href="#myzookeeper-sh" class="headerlink" title="myzookeeper.sh"></a>myzookeeper.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">for i in hadoop100 hadoop101 hadoop102 </span><br><span class="line">do</span><br><span class="line">        echo ---------- zookeeper $i 启动 ------------</span><br><span class="line">ssh $i &quot;/opt/module/zookeeper-3.5.7/bin/zkServer.sh start&quot;</span><br><span class="line">done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">for i in hadoop100 hadoop101 hadoop102 </span><br><span class="line">do</span><br><span class="line">        echo ---------- zookeeper $i 停止 ------------    </span><br><span class="line">ssh $i &quot;/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop&quot;</span><br><span class="line">done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;status&quot;)&#123;</span><br><span class="line">for i in hadoop100 hadoop101 hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">        echo ---------- zookeeper $i 状态 ------------    </span><br><span class="line">ssh $i &quot;/opt/module/zookeeper-3.5.7/bin/zkServer.sh status&quot;</span><br><span class="line">done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="mykafka-sh"><a href="#mykafka-sh" class="headerlink" title="mykafka.sh"></a>mykafka.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">    for i in hadoop100 hadoop101 hadoop102</span><br><span class="line">    do</span><br><span class="line">        echo &quot; --------启动 $i Kafka-------&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/kafka_2.11-2.4.1/bin/kafka-server-start.sh -daemon /opt/module/kafka_2.11-2.4.1/config/server.properties&quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">    for i in hadoop100 hadoop101 hadoop102</span><br><span class="line">    do</span><br><span class="line">        echo &quot; --------停止 $i Kafka-------&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/kafka_2.11-2.4.1/bin/kafka-server-stop.sh stop&quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="mykafka-producer-sh"><a href="#mykafka-producer-sh" class="headerlink" title="mykafka_producer.sh"></a>mykafka_producer.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">topic=$1</span><br><span class="line"></span><br><span class="line">for i in hadoop102</span><br><span class="line">do</span><br><span class="line">    echo &quot;------------ 启动 kafka 生产者 , 生产者主题为: $1 ---------------&quot;</span><br><span class="line">    ssh $i &quot;/opt/module/kafka_2.11-2.4.1/bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic $topic&quot;</span><br><span class="line">done</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="mykafka-listTopic-sh"><a href="#mykafka-listTopic-sh" class="headerlink" title="mykafka_listTopic.sh"></a>mykafka_listTopic.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">for i in hadoop102</span><br><span class="line">do</span><br><span class="line">    echo &quot; ----------查看 kafka 主题--------&quot;</span><br><span class="line">    ssh $i &quot;/opt/module/kafka_2.11-2.4.1/bin/kafka-topics.sh --zookeeper hadoop102:2181/kafka_2.11-2.4.1 --list&quot;</span><br><span class="line">done</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="mykafka-deleteTopic-sh"><a href="#mykafka-deleteTopic-sh" class="headerlink" title="mykafka_deleteTopic.sh"></a>mykafka_deleteTopic.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">delete_topic=$1</span><br><span class="line"></span><br><span class="line">for i in hadoop102</span><br><span class="line">do</span><br><span class="line">    echo &quot;------------ 删除 kafka $1 主题 ---------------&quot;</span><br><span class="line">    ssh $i &quot;/opt/module/kafka_2.11-2.4.1/bin/kafka-topics.sh --delete --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka_2.11-2.4.1 --topic $delete_topic&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h1 id="mykafka-createTopic-sh"><a href="#mykafka-createTopic-sh" class="headerlink" title="mykafka_createTopic.sh"></a>mykafka_createTopic.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">#$1: 脚本的第一个参数 做非空判断</span><br><span class="line">if [ -n &quot;$1&quot; ] ;then</span><br><span class="line">   one=$1</span><br><span class="line">else </span><br><span class="line">   echo &quot;请传入副本数&quot;</span><br><span class="line">   exit</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#$2: 脚本的第二个参数 做非空判断</span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">   two=$2</span><br><span class="line">else </span><br><span class="line">   echo &quot;请传入分区数&quot;</span><br><span class="line">   exit</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#$3: 脚本的第三个参数 做非空判断</span><br><span class="line">if [ -n &quot;$3&quot; ] ;then</span><br><span class="line">   topic=$3</span><br><span class="line">else </span><br><span class="line">   echo &quot;请传入需要创建的主题名称&quot;</span><br><span class="line">   exit</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for i in hadoop102</span><br><span class="line">do</span><br><span class="line">    echo &quot;------------ 创建 kafka 主题 , 主题为: $topic ---------------&quot;</span><br><span class="line">    ssh $i &quot;/opt/module/kafka_2.11-2.4.1/bin/kafka-topics.sh --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka_2.11-2.4.1  --create --replication-factor $one --partitions $two --topic $topic&quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># bin/kafka-topics.sh --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka_2.11-2.4.1  --create --replication-factor 1 --partitions 1 --topic topic_log</span><br></pre></td></tr></table></figure><h1 id="mykafka-consumer-sh"><a href="#mykafka-consumer-sh" class="headerlink" title="mykafka_consumer.sh"></a>mykafka_consumer.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">topic=$1</span><br><span class="line"></span><br><span class="line">for i in hadoop102</span><br><span class="line">do</span><br><span class="line">    echo &quot;------------ 启动 kafka 消费者 , 消费者主题为: $1 ---------------&quot;</span><br><span class="line">    ssh $i &quot;/opt/module/kafka_2.11-2.4.1/bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic $topic&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h1 id="mykylin-sh"><a href="#mykylin-sh" class="headerlink" title="mykylin.sh"></a>mykylin.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">    </span><br><span class="line">    for i in hadoop102</span><br><span class="line">    do</span><br><span class="line">        echo &quot; =================== 启动 kylin ===================&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/kylin-3.0.2/bin/kylin.sh start&quot;</span><br><span class="line">    done</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        for i in hadoop102</span><br><span class="line">    do</span><br><span class="line">        echo &quot; =================== 关闭 kylin ===================&quot;</span><br><span class="line"></span><br><span class="line">        ssh $i &quot;/opt/module/kylin-3.0.2/bin/kylin.sh stop&quot;</span><br><span class="line">    done</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="kylin-build-cube-sh"><a href="#kylin-build-cube-sh" class="headerlink" title="kylin_build_cube.sh"></a>kylin_build_cube.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">cube_name=order_cube</span><br><span class="line">do_date=`date -d &#x27;-1 day&#x27; +%F`</span><br><span class="line"></span><br><span class="line">#获取00:00时间戳</span><br><span class="line">start_date_unix=`date -d &quot;$do_date 08:00:00&quot; +%s`</span><br><span class="line">start_date=$(($start_date_unix*1000))</span><br><span class="line"></span><br><span class="line">#获取24:00的时间戳</span><br><span class="line">stop_date=$(($start_date+86400000))</span><br><span class="line"></span><br><span class="line">curl -X PUT -H &quot;Authorization: Basic QURNSU46S1lMSU4=&quot; -H &#x27;Content-Type: application/json&#x27; -d &#x27;&#123;&quot;startTime&quot;:&#x27;$start_date&#x27;, &quot;endTime&quot;:&#x27;$stop_date&#x27;, &quot;buildType&quot;:&quot;BUILD&quot;&#125;&#x27; http://hadoop102:7070/kylin/api/cubes/$cube_name/build</span><br></pre></td></tr></table></figure><h1 id="mysolr-sh"><a href="#mysolr-sh" class="headerlink" title="mysolr.sh"></a>mysolr.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">    for i in hadoop100 hadoop101 hadoop102 hadoop103 hadoop104</span><br><span class="line">    do</span><br><span class="line">        echo &quot; --------启动 $i solr-------&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/solr-5.2.1/bin/solr start&quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">    for i in hadoop100 hadoop101 hadoop102 hadoop103 hadoop104</span><br><span class="line">    do</span><br><span class="line">        echo &quot; --------停止 $i solr-------&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/solr-5.2.1/bin/solr stop&quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="myzabbix-sh"><a href="#myzabbix-sh" class="headerlink" title="myzabbix.sh"></a>myzabbix.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 Zabbix ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo ===hadoop102==</span><br><span class="line"></span><br><span class="line">        ssh hadoop102 &quot;sudo systemctl start zabbix-server zabbix-agent httpd rh-php72-php-fpm&quot;</span><br><span class="line"></span><br><span class="line">        # sleep 5</span><br><span class="line"></span><br><span class="line">        # ssh hadoop102 &quot;sudo systemctl enable zabbix-server zabbix-agent httpd rh-php72-php-fpm&quot;</span><br><span class="line"></span><br><span class="line">        echo ===hadoop100==</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop100 &quot;sudo systemctl start zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop100 &quot;sudo systemctl enable zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        echo ===hadoop101==</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop101 &quot;sudo systemctl start zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop101 &quot;sudo systemctl enable zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        echo ===hadoop103==</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop103 &quot;sudo systemctl start zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop103 &quot;sudo systemctl enable zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        echo ===hadoop104==</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop104 &quot;sudo systemctl start zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop104 &quot;sudo systemctl enable zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 Zabbix ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo ===hadoop102==</span><br><span class="line"></span><br><span class="line">        ssh hadoop102 &quot;sudo systemctl stop zabbix-server zabbix-agent httpd rh-php72-php-fpm&quot;</span><br><span class="line"></span><br><span class="line">        # sleep 5</span><br><span class="line"></span><br><span class="line">        # ssh hadoop102 &quot;sudo systemctl disable zabbix-server zabbix-agent httpd rh-php72-php-fpm&quot;</span><br><span class="line"></span><br><span class="line">        echo ===hadoop100==</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop100 &quot;sudo systemctl stop zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop100 &quot;sudo systemctl disable zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        echo ===hadoop101==</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop101 &quot;sudo systemctl stop zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop101 &quot;sudo systemctl disable zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        echo ===hadoop103==</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop103 &quot;sudo systemctl stop zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop103 &quot;sudo systemctl disable zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        echo ===hadoop104==</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop104 &quot;sudo systemctl stop zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop104 &quot;sudo systemctl disable zabbix-agent&quot;</span><br><span class="line">       </span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="ps-ef-sh"><a href="#ps-ef-sh" class="headerlink" title="ps-ef.sh"></a>ps-ef.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">echo ================== 查看本机所有进程,为zabbix监控所需: ps -ef | grep $1==================</span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...select All : ps -ef&quot;</span><br><span class="line">    ps -ef </span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line">ps -ef | grep $1</span><br></pre></td></tr></table></figure><h1 id="rpm-qa-sh"><a href="#rpm-qa-sh" class="headerlink" title="rpm-qa.sh"></a>rpm-qa.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">echo ================== 查询系统当中是否有这个安装包:&quot;sudo rpm qa | grep&quot; $1 ==================</span><br><span class="line">sudo rpm -qa | grep $1</span><br></pre></td></tr></table></figure><h1 id="superset-sh"><a href="#superset-sh" class="headerlink" title="superset.sh"></a>superset.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">echo === 启动superset命令: gunicorn --workers 5 --timeout 120 --bind 0.0.0.0:8787  &#x27;&quot;superset.app:create_app()&quot;&#x27; --daemon  ===</span><br><span class="line">echo === 停止superset命令: &quot;ps -ef | awk &#x27;/superset/ &amp;&amp; !/awk/&#123;print $2&#125;&#x27; | xargs kill -9&quot; ====</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">superset_status()&#123;</span><br><span class="line">    result=`ps -ef | awk &#x27;/gunicorn/ &amp;&amp; !/awk/&#123;print $2&#125;&#x27; | wc -l`</span><br><span class="line">    if [[ $result -eq 0 ]]; then</span><br><span class="line">        return 0</span><br><span class="line">    else</span><br><span class="line">        return 1</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line">superset_start()&#123;</span><br><span class="line">        source ~/.bashrc</span><br><span class="line">        superset_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">        if [[ $? -eq 0 ]]; then</span><br><span class="line">            conda activate superset ; gunicorn --workers 5 --timeout 120 --bind hadoop102:8787 --daemon &#x27;superset.app:create_app()&#x27;</span><br><span class="line">        else</span><br><span class="line">            echo &quot;superset正在运行&quot;</span><br><span class="line">        fi</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">superset_stop()&#123;</span><br><span class="line">    source ~/.bashrc</span><br><span class="line">    superset_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">    if [[ $? -eq 0 ]]; then</span><br><span class="line">        echo &quot;superset未在运行&quot;</span><br><span class="line">    else</span><br><span class="line">        ps -ef | awk &#x27;/gunicorn/ &amp;&amp; !/awk/&#123;print $2&#125;&#x27; | xargs kill -9 ; conda deactivate</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    start )</span><br><span class="line">        echo &quot;启动Superset&quot;</span><br><span class="line">        superset_start</span><br><span class="line">    ;;</span><br><span class="line">    stop )</span><br><span class="line">        echo &quot;停止Superset&quot;</span><br><span class="line">        superset_stop</span><br><span class="line">    ;;</span><br><span class="line">    restart )</span><br><span class="line">        echo &quot;重启Superset&quot;</span><br><span class="line">        superset_stop</span><br><span class="line">        superset_start</span><br><span class="line">    ;;</span><br><span class="line">    status )</span><br><span class="line">        superset_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">        if [[ $? -eq 0 ]]; then</span><br><span class="line">            echo &quot;superset未在运行&quot;</span><br><span class="line">        else</span><br><span class="line">            echo &quot;superset正在运行&quot;</span><br><span class="line">        fi</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="conda-superset-sh"><a href="#conda-superset-sh" class="headerlink" title="conda-superset.sh"></a>conda-superset.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">echo === 使用conda的python环境管理更改节点的python环境为: Python 3.7.11  进入命令是:conda activate superset 退出命令是:conda deactivate ,因为superset仅支持Python 3.7.11及以上的版本 ===</span><br><span class="line"></span><br><span class="line"># if [ $# -lt 1 ]</span><br><span class="line"># then</span><br><span class="line">#     echo &quot;No Args Input...&quot;</span><br><span class="line">#     exit ;</span><br><span class="line"># fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># conda_start()&#123;</span><br><span class="line">#         source ~/.bashrc</span><br><span class="line">#         # superset_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">#         if [[ $? -eq 0 ]]; then</span><br><span class="line">#             conda activate superset</span><br><span class="line">#         else</span><br><span class="line">#             echo &quot;conda_superset正在运行&quot;</span><br><span class="line">#         fi</span><br><span class="line"></span><br><span class="line"># &#125;</span><br><span class="line"></span><br><span class="line"># conda_stop()&#123;</span><br><span class="line">#     # superset_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">#     if [[ $? -eq 0 ]]; then</span><br><span class="line">#         conda deactivate</span><br><span class="line">#     else</span><br><span class="line">#         conda deactivate</span><br><span class="line">#     fi</span><br><span class="line"># &#125;</span><br><span class="line"></span><br><span class="line"># case $1 in</span><br><span class="line">#     start )</span><br><span class="line">#         echo &quot;启动conda-superset&quot;</span><br><span class="line">#         conda_start</span><br><span class="line">#     ;;</span><br><span class="line">#     stop )</span><br><span class="line">#         echo &quot;停止conda-superset&quot;</span><br><span class="line">#         conda_stop</span><br><span class="line">#     ;;</span><br><span class="line"># esac</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="datax-import-config-py"><a href="#datax-import-config-py" class="headerlink" title="datax_import_config.py"></a>datax_import_config.py</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"># coding=utf-8</span><br><span class="line">import json</span><br><span class="line">import getopt</span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">import MySQLdb</span><br><span class="line"></span><br><span class="line">#MySQL相关配置，需根据实际情况作出修改</span><br><span class="line">mysql_host = &quot;hadoop102&quot;</span><br><span class="line">mysql_port = &quot;3306&quot;</span><br><span class="line">mysql_user = &quot;root&quot;</span><br><span class="line">mysql_passwd = &quot;shangbaishuyao&quot;</span><br><span class="line"></span><br><span class="line">#HDFS NameNode相关配置，需根据实际情况作出修改</span><br><span class="line">hdfs_nn_host = &quot;hadoop102&quot;</span><br><span class="line">hdfs_nn_port = &quot;8020&quot;</span><br><span class="line"></span><br><span class="line">#生成配置文件的目标路径，可根据实际情况作出修改</span><br><span class="line">output_path = &quot;/opt/module/datax/job/mysql_import_hdfs_table_json_files&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_connection():</span><br><span class="line">    return MySQLdb.connect(host=mysql_host, port=int(mysql_port), user=mysql_user, passwd=mysql_passwd)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_mysql_meta(database, table):</span><br><span class="line">    connection = get_connection()</span><br><span class="line">    cursor = connection.cursor()</span><br><span class="line">    sql = &quot;SELECT COLUMN_NAME,DATA_TYPE from information_schema.COLUMNS WHERE TABLE_SCHEMA=%s AND TABLE_NAME=%s ORDER BY ORDINAL_POSITION&quot;</span><br><span class="line">    cursor.execute(sql, [database, table])</span><br><span class="line">    fetchall = cursor.fetchall()</span><br><span class="line">    cursor.close()</span><br><span class="line">    connection.close()</span><br><span class="line">    return fetchall</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_mysql_columns(database, table):</span><br><span class="line">    return map(lambda x: x[0], get_mysql_meta(database, table))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_hive_columns(database, table):</span><br><span class="line">    def type_mapping(mysql_type):</span><br><span class="line">        mappings = &#123;</span><br><span class="line">            &quot;bigint&quot;: &quot;bigint&quot;,</span><br><span class="line">            &quot;int&quot;: &quot;bigint&quot;,</span><br><span class="line">            &quot;smallint&quot;: &quot;bigint&quot;,</span><br><span class="line">            &quot;tinyint&quot;: &quot;bigint&quot;,</span><br><span class="line">            &quot;decimal&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;double&quot;: &quot;double&quot;,</span><br><span class="line">            &quot;float&quot;: &quot;float&quot;,</span><br><span class="line">            &quot;binary&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;char&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;varchar&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;datetime&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;time&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;timestamp&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;date&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;text&quot;: &quot;string&quot;</span><br><span class="line">        &#125;</span><br><span class="line">        return mappings[mysql_type]</span><br><span class="line"></span><br><span class="line">    meta = get_mysql_meta(database, table)</span><br><span class="line">    return map(lambda x: &#123;&quot;name&quot;: x[0], &quot;type&quot;: type_mapping(x[1].lower())&#125;, meta)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generate_json(source_database, source_table):</span><br><span class="line">    job = &#123;</span><br><span class="line">        &quot;job&quot;: &#123;</span><br><span class="line">            &quot;setting&quot;: &#123;</span><br><span class="line">                &quot;speed&quot;: &#123;</span><br><span class="line">                    &quot;channel&quot;: 3</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;errorLimit&quot;: &#123;</span><br><span class="line">                    &quot;record&quot;: 0,</span><br><span class="line">                    &quot;percentage&quot;: 0.02</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;content&quot;: [&#123;</span><br><span class="line">                &quot;reader&quot;: &#123;</span><br><span class="line">                    &quot;name&quot;: &quot;mysqlreader&quot;,</span><br><span class="line">                    &quot;parameter&quot;: &#123;</span><br><span class="line">                        &quot;username&quot;: mysql_user,</span><br><span class="line">                        &quot;password&quot;: mysql_passwd,</span><br><span class="line">                        &quot;column&quot;: get_mysql_columns(source_database, source_table),</span><br><span class="line">                        &quot;splitPk&quot;: &quot;&quot;,</span><br><span class="line">                        &quot;connection&quot;: [&#123;</span><br><span class="line">                            &quot;table&quot;: [source_table],</span><br><span class="line">                            &quot;jdbcUrl&quot;: [&quot;jdbc:mysql://&quot; + mysql_host + &quot;:&quot; + mysql_port + &quot;/&quot; + source_database]</span><br><span class="line">                        &#125;]</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;writer&quot;: &#123;</span><br><span class="line">                    &quot;name&quot;: &quot;hdfswriter&quot;,</span><br><span class="line">                    &quot;parameter&quot;: &#123;</span><br><span class="line">                        &quot;defaultFS&quot;: &quot;hdfs://&quot; + hdfs_nn_host + &quot;:&quot; + hdfs_nn_port,</span><br><span class="line">                        &quot;fileType&quot;: &quot;text&quot;,</span><br><span class="line">                        &quot;path&quot;: &quot;$&#123;targetdir&#125;&quot;,</span><br><span class="line">                        &quot;fileName&quot;: source_table,</span><br><span class="line">                        &quot;column&quot;: get_hive_columns(source_database, source_table),</span><br><span class="line">                        &quot;writeMode&quot;: &quot;append&quot;,</span><br><span class="line">                        &quot;fieldDelimiter&quot;: &quot;\t&quot;,</span><br><span class="line">                        &quot;compress&quot;: &quot;gzip&quot;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    if not os.path.exists(output_path):</span><br><span class="line">        os.makedirs(output_path)</span><br><span class="line">    with open(os.path.join(output_path, &quot;.&quot;.join([source_database, source_table, &quot;json&quot;])), &quot;w&quot;) as f:</span><br><span class="line">        json.dump(job, f)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main(args):</span><br><span class="line">    source_database = &quot;&quot;</span><br><span class="line">    source_table = &quot;&quot;</span><br><span class="line"></span><br><span class="line">    options, arguments = getopt.getopt(args, &#x27;-d:-t:&#x27;, [&#x27;sourcedb=&#x27;, &#x27;sourcetbl=&#x27;])</span><br><span class="line">    for opt_name, opt_value in options:</span><br><span class="line">        if opt_name in (&#x27;-d&#x27;, &#x27;--sourcedb&#x27;):</span><br><span class="line">            source_database = opt_value</span><br><span class="line">        if opt_name in (&#x27;-t&#x27;, &#x27;--sourcetbl&#x27;):</span><br><span class="line">            source_table = opt_value</span><br><span class="line"></span><br><span class="line">    generate_json(source_database, source_table)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    main(sys.argv[1:])</span><br></pre></td></tr></table></figure><h1 id="datax-import-config-sh"><a href="#datax-import-config-sh" class="headerlink" title="datax_import_config.sh"></a>datax_import_config.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t activity_info</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t activity_rule</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t base_category1</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t base_category2</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t base_category3</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t base_dic</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t base_province</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t base_region</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t base_trademark</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t cart_info</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t coupon_info</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t sku_attr_value</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t sku_info</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t sku_sale_attr_value</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t spu_info</span><br></pre></td></tr></table></figure><h1 id="datax-import-config-files-sh"><a href="#datax-import-config-files-sh" class="headerlink" title="datax_import_config_files.sh"></a>datax_import_config_files.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">#全量表数据同步脚本</span><br><span class="line"></span><br><span class="line">DATAX_HOME=/opt/module/datax</span><br><span class="line"></span><br><span class="line"># 如果传入日期则do_date等于传入的日期，否则等于前一天日期</span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else</span><br><span class="line">    do_date=`date -d &quot;-1 day&quot; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#处理目标路径，此处的处理逻辑是，如果目标路径不存在，则创建；若存在，则清空，目的是保证同步任务可重复执行</span><br><span class="line">handle_targetdir() &#123;</span><br><span class="line">  hadoop fs -test -e $1</span><br><span class="line">  if [[ $? -eq 1 ]]; then</span><br><span class="line">    echo &quot;路径$1不存在，正在创建......&quot;</span><br><span class="line">    hadoop fs -mkdir -p $1</span><br><span class="line">  else</span><br><span class="line">    echo &quot;路径$1已经存在&quot;</span><br><span class="line">    fs_count=$(hadoop fs -count $1)</span><br><span class="line">    content_size=$(echo $fs_count | awk &#x27;&#123;print $3&#125;&#x27;)</span><br><span class="line">    if [[ $content_size -eq 0 ]]; then</span><br><span class="line">      echo &quot;路径$1为空&quot;</span><br><span class="line">    else</span><br><span class="line">      echo &quot;路径$1不为空，正在清空......&quot;</span><br><span class="line">      hadoop fs -rm -r -f $1/*</span><br><span class="line">    fi</span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#数据同步</span><br><span class="line">import_data() &#123;</span><br><span class="line">  datax_config=$1</span><br><span class="line">  target_dir=$2</span><br><span class="line"></span><br><span class="line">  handle_targetdir $target_dir</span><br><span class="line">  python $DATAX_HOME/bin/datax.py -p&quot;-Dtargetdir=$target_dir&quot; $datax_config</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;activity_info&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.activity_info.json /origin_data/gmall/db/ods_activity_info/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;activity_rule&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.activity_rule.json /origin_data/gmall/db/ods_activity_rule/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;base_category1&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_category1.json /origin_data/gmall/db/ods_base_category1/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;base_category2&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_category2.json /origin_data/gmall/db/ods_base_category2/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;base_category3&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_category3.json /origin_data/gmall/db/ods_base_category3/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;base_dic&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_dic.json /origin_data/gmall/db/ods_base_dic/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;base_province&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_province.json /origin_data/gmall/db/ods_base_province/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;base_region&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_region.json /origin_data/gmall/db/ods_base_region/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;base_trademark&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_trademark.json /origin_data/gmall/db/ods_base_trademark/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;cart_info&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.cart_info.json /origin_data/gmall/db/ods_cart_info/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;coupon_info&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.coupon_info.json /origin_data/gmall/db/ods_coupon_info/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;sku_attr_value&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.sku_attr_value.json /origin_data/gmall/db/ods_sku_attr_value/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;sku_info&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.sku_info.json /origin_data/gmall/db/ods_sku_info/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;sku_sale_attr_value&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.sku_sale_attr_value.json /origin_data/gmall/db/ods_sku_sale_attr_value/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;spu_info&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.spu_info.json /origin_data/gmall/db/ods_spu_info/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;all&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.activity_info.json /origin_data/gmall/db/ods_activity_info/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.activity_rule.json /origin_data/gmall/db/ods_activity_rule/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_category1.json /origin_data/gmall/db/ods_base_category1/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_category2.json /origin_data/gmall/db/ods_base_category2/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_category3.json /origin_data/gmall/db/ods_base_category3/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_dic.json /origin_data/gmall/db/ods_base_dic/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_province.json /origin_data/gmall/db/ods_base_province/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_region.json /origin_data/gmall/db/ods_base_region/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_trademark.json /origin_data/gmall/db/ods_base_trademark/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.cart_info.json /origin_data/gmall/db/ods_cart_info/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.coupon_info.json /origin_data/gmall/db/ods_coupon_info/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.sku_attr_value.json /origin_data/gmall/db/ods_sku_attr_value/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.sku_info.json /origin_data/gmall/db/ods_sku_info/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.sku_sale_attr_value.json /origin_data/gmall/db/ods_sku_sale_attr_value/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.spu_info.json /origin_data/gmall/db/ods_spu_info/$do_date</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="db-maxwell-kafka-flume-hdfs-sh"><a href="#db-maxwell-kafka-flume-hdfs-sh" class="headerlink" title="db_maxwell_kafka_flume_hdfs.sh"></a>db_maxwell_kafka_flume_hdfs.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; --------启动 hadoop101上的flume采集maxwell传到kafka上的业务数据-------&quot;</span><br><span class="line">        ssh hadoop100 &quot;nohup /opt/module/flume-1.9.0/bin/flume-ng agent -n a1 -c /opt/module/flume-1.9.0/conf -f /opt/module/flume-1.9.0/db_maxwell_kafka_topic/db_maxwell_kafka_topic.conf &gt;/dev/null 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line"></span><br><span class="line">        echo &quot; --------停止 hadoop101上的flume采集maxwell传到kafka上的业务数据--------&quot;</span><br><span class="line">        ssh hadoop100 &quot;ps -ef | grep db_maxwell_kafka | grep -v grep |awk &#x27;&#123;print \$2&#125;&#x27; | xargs -n1 kill&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#说明1：nohup，该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。nohup就是不挂起的意思，不挂断地运行命令。</span><br><span class="line">#说明2：awk 默认分隔符为空格</span><br><span class="line">#说明3：$2是在“”双引号内部会被解析为脚本的第二个参数，但是这里面想表达的含义是awk的第二个值，所以需要将他转义，用\$2表示。</span><br><span class="line">#说明4：xargs 表示取出前面命令运行的结果，作为后面命令的输入参数。</span><br></pre></td></tr></table></figure><h1 id="hdfs-to-ods-db-every-day-sh"><a href="#hdfs-to-ods-db-every-day-sh" class="headerlink" title="hdfs_to_ods_db_every_day.sh"></a>hdfs_to_ods_db_every_day.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">APP=gmall</span><br><span class="line"></span><br><span class="line"># 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天</span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else </span><br><span class="line">    do_date=`date -d &quot;-1 day&quot; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">ods_order_info=&quot; </span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_info partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_order_detail=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_detail/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_detail partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_sku_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/sku_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_sku_info partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_user_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/user_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_user_info partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_payment_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/payment_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_payment_info partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_base_category1=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_category1/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_category1 partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_base_category2=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_category2/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_category2 partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_base_category3=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_category3/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_category3 partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_base_trademark=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_trademark/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_trademark partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_activity_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/activity_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_activity_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_cart_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/cart_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_cart_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_comment_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/comment_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_comment_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_coupon_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/coupon_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_coupon_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_coupon_use=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/coupon_use/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_coupon_use partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_favor_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/favor_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_favor_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_order_refund_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_refund_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_refund_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_order_status_log=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_status_log/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_status_log partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_spu_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/spu_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_spu_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_activity_rule=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/activity_rule/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_activity_rule partition(dt=&#x27;$do_date&#x27;);&quot; </span><br><span class="line"></span><br><span class="line">ods_base_dic=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_dic/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_dic partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_order_detail_activity=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_detail_activity/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_detail_activity partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_order_detail_coupon=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_detail_coupon/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_detail_coupon partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_refund_payment=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/refund_payment/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_refund_payment partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_sku_attr_value=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/sku_attr_value/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_sku_attr_value partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_sku_sale_attr_value=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/sku_sale_attr_value/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_sku_sale_attr_value partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_base_province=&quot; </span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_province/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_province;&quot;</span><br><span class="line"></span><br><span class="line">ods_base_region=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_region/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_region;&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    &quot;ods_order_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_detail&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_detail&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_sku_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_sku_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_user_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_user_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_payment_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_payment_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_category1&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_category1&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_category2&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_category2&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_category3&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_category3&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_trademark&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_trademark&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_activity_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_activity_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_cart_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_cart_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_comment_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_comment_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_coupon_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_coupon_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_coupon_use&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_coupon_use&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_favor_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_favor_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_refund_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_refund_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_status_log&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_status_log&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_spu_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_spu_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_activity_rule&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_activity_rule&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_dic&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_dic&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_detail_activity&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_detail_activity&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_detail_coupon&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_detail_coupon&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_refund_payment&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_refund_payment&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_sku_attr_value&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_sku_attr_value&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_sku_sale_attr_value&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_sku_sale_attr_value&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;all&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_info$ods_order_detail$ods_sku_info$ods_user_info$ods_payment_info$ods_base_category1$ods_base_category2$ods_base_category3$ods_base_trademark$ods_activity_info$ods_cart_info$ods_comment_info$ods_coupon_info$ods_coupon_use$ods_favor_info$ods_order_refund_info$ods_order_status_log$ods_spu_info$ods_activity_rule$ods_base_dic$ods_order_detail_activity$ods_order_detail_coupon$ods_refund_payment$ods_sku_attr_value$ods_sku_sale_attr_value&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="hdfs-to-ods-db-first-day-sh"><a href="#hdfs-to-ods-db-first-day-sh" class="headerlink" title="hdfs_to_ods_db_first_day.sh"></a>hdfs_to_ods_db_first_day.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">APP=gmall</span><br><span class="line"></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">   do_date=$2</span><br><span class="line">else </span><br><span class="line">   echo &quot;请传入日期参数&quot;</span><br><span class="line">   exit</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line">ods_order_info=&quot; </span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_info partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_order_detail=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_detail/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_detail partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_sku_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/sku_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_sku_info partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_user_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/user_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_user_info partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_payment_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/payment_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_payment_info partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_base_category1=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_category1/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_category1 partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_base_category2=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_category2/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_category2 partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_base_category3=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_category3/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_category3 partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_base_trademark=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_trademark/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_trademark partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_activity_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/activity_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_activity_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_cart_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/cart_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_cart_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_comment_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/comment_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_comment_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_coupon_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/coupon_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_coupon_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_coupon_use=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/coupon_use/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_coupon_use partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_favor_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/favor_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_favor_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_order_refund_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_refund_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_refund_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_order_status_log=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_status_log/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_status_log partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_spu_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/spu_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_spu_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_activity_rule=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/activity_rule/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_activity_rule partition(dt=&#x27;$do_date&#x27;);&quot; </span><br><span class="line"></span><br><span class="line">ods_base_dic=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_dic/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_dic partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_order_detail_activity=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_detail_activity/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_detail_activity partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_order_detail_coupon=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_detail_coupon/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_detail_coupon partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_refund_payment=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/refund_payment/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_refund_payment partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_sku_attr_value=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/sku_attr_value/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_sku_attr_value partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_sku_sale_attr_value=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/sku_sale_attr_value/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_sku_sale_attr_value partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_base_province=&quot; </span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_province/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_province;&quot;</span><br><span class="line"></span><br><span class="line">ods_base_region=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_region/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_region;&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    &quot;ods_order_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_detail&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_detail&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_sku_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_sku_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_user_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_user_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_payment_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_payment_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_category1&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_category1&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_category2&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_category2&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_category3&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_category3&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_trademark&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_trademark&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_activity_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_activity_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_cart_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_cart_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_comment_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_comment_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_coupon_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_coupon_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_coupon_use&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_coupon_use&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_favor_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_favor_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_refund_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_refund_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_status_log&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_status_log&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_spu_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_spu_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_activity_rule&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_activity_rule&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_dic&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_dic&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_detail_activity&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_detail_activity&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_detail_coupon&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_detail_coupon&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_refund_payment&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_refund_payment&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_sku_attr_value&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_sku_attr_value&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_sku_sale_attr_value&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_sku_sale_attr_value&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_province&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_province&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_region&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_region&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;all&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_info$ods_order_detail$ods_sku_info$ods_user_info$ods_payment_info$ods_base_category1$ods_base_category2$ods_base_category3$ods_base_trademark$ods_activity_info$ods_cart_info$ods_comment_info$ods_coupon_info$ods_coupon_use$ods_favor_info$ods_order_refund_info$ods_order_status_log$ods_spu_info$ods_activity_rule$ods_base_dic$ods_order_detail_activity$ods_order_detail_coupon$ods_refund_payment$ods_sku_attr_value$ods_sku_sale_attr_value$ods_base_province$ods_base_region&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="ods-to-dim-db-every-day-sh"><a href="#ods-to-dim-db-every-day-sh" class="headerlink" title="ods_to_dim_db_every_day.sh"></a>ods_to_dim_db_every_day.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">APP=gmall</span><br><span class="line"></span><br><span class="line"># 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天</span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else </span><br><span class="line">    do_date=`date -d &quot;-1 day&quot; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">dim_user_info=&quot;</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">with</span><br><span class="line">tmp as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        old.id old_id,</span><br><span class="line">        old.login_name old_login_name,</span><br><span class="line">        old.nick_name old_nick_name,</span><br><span class="line">        old.name old_name,</span><br><span class="line">        old.phone_num old_phone_num,</span><br><span class="line">        old.email old_email,</span><br><span class="line">        old.user_level old_user_level,</span><br><span class="line">        old.birthday old_birthday,</span><br><span class="line">        old.gender old_gender,</span><br><span class="line">        old.create_time old_create_time,</span><br><span class="line">        old.operate_time old_operate_time,</span><br><span class="line">        old.start_date old_start_date,</span><br><span class="line">        old.end_date old_end_date,</span><br><span class="line">        new.id new_id,</span><br><span class="line">        new.login_name new_login_name,</span><br><span class="line">        new.nick_name new_nick_name,</span><br><span class="line">        new.name new_name,</span><br><span class="line">        new.phone_num new_phone_num,</span><br><span class="line">        new.email new_email,</span><br><span class="line">        new.user_level new_user_level,</span><br><span class="line">        new.birthday new_birthday,</span><br><span class="line">        new.gender new_gender,</span><br><span class="line">        new.create_time new_create_time,</span><br><span class="line">        new.operate_time new_operate_time,</span><br><span class="line">        new.start_date new_start_date,</span><br><span class="line">        new.end_date new_end_date</span><br><span class="line">    from</span><br><span class="line">    (</span><br><span class="line">        select</span><br><span class="line">            id,</span><br><span class="line">            login_name,</span><br><span class="line">            nick_name,</span><br><span class="line">            name,</span><br><span class="line">            phone_num,</span><br><span class="line">            email,</span><br><span class="line">            user_level,</span><br><span class="line">            birthday,</span><br><span class="line">            gender,</span><br><span class="line">            create_time,</span><br><span class="line">            operate_time,</span><br><span class="line">            start_date,</span><br><span class="line">            end_date</span><br><span class="line">        from $&#123;APP&#125;.dim_user_info</span><br><span class="line">        where dt=&#x27;9999-99-99&#x27;</span><br><span class="line">        and start_date&lt;&#x27;$do_date&#x27;</span><br><span class="line">    )old</span><br><span class="line">    full outer join</span><br><span class="line">    (</span><br><span class="line">        select</span><br><span class="line">            id,</span><br><span class="line">            login_name,</span><br><span class="line">            nick_name,</span><br><span class="line">            md5(name) name,</span><br><span class="line">            md5(phone_num) phone_num,</span><br><span class="line">            md5(email) email,</span><br><span class="line">            user_level,</span><br><span class="line">            birthday,</span><br><span class="line">            gender,</span><br><span class="line">            create_time,</span><br><span class="line">            operate_time,</span><br><span class="line">            &#x27;$do_date&#x27; start_date,</span><br><span class="line">            &#x27;9999-99-99&#x27; end_date</span><br><span class="line">        from $&#123;APP&#125;.ods_user_info</span><br><span class="line">        where dt=&#x27;$do_date&#x27;</span><br><span class="line">    )new</span><br><span class="line">    on old.id=new.id</span><br><span class="line">)</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_user_info partition(dt)</span><br><span class="line">select</span><br><span class="line">    nvl(new_id,old_id),</span><br><span class="line">    nvl(new_login_name,old_login_name),</span><br><span class="line">    nvl(new_nick_name,old_nick_name),</span><br><span class="line">    nvl(new_name,old_name),</span><br><span class="line">    nvl(new_phone_num,old_phone_num),</span><br><span class="line">    nvl(new_email,old_email),</span><br><span class="line">    nvl(new_user_level,old_user_level),</span><br><span class="line">    nvl(new_birthday,old_birthday),</span><br><span class="line">    nvl(new_gender,old_gender),</span><br><span class="line">    nvl(new_create_time,old_create_time),</span><br><span class="line">    nvl(new_operate_time,old_operate_time),</span><br><span class="line">    nvl(new_start_date,old_start_date),</span><br><span class="line">    nvl(new_end_date,old_end_date),</span><br><span class="line">    nvl(new_end_date,old_end_date) dt</span><br><span class="line">from tmp</span><br><span class="line">union all</span><br><span class="line">select</span><br><span class="line">    old_id,</span><br><span class="line">    old_login_name,</span><br><span class="line">    old_nick_name,</span><br><span class="line">    old_name,</span><br><span class="line">    old_phone_num,</span><br><span class="line">    old_email,</span><br><span class="line">    old_user_level,</span><br><span class="line">    old_birthday,</span><br><span class="line">    old_gender,</span><br><span class="line">    old_create_time,</span><br><span class="line">    old_operate_time,</span><br><span class="line">    old_start_date,</span><br><span class="line">    cast(date_add(&#x27;$do_date&#x27;,-1) as string),</span><br><span class="line">    cast(date_add(&#x27;$do_date&#x27;,-1) as string) dt</span><br><span class="line">from tmp</span><br><span class="line">where new_id is not null and old_id is not null;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line">dim_sku_info=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">with</span><br><span class="line">sku as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        price,</span><br><span class="line">        sku_name,</span><br><span class="line">        sku_desc,</span><br><span class="line">        weight,</span><br><span class="line">        is_sale,</span><br><span class="line">        spu_id,</span><br><span class="line">        category3_id,</span><br><span class="line">        tm_id,</span><br><span class="line">        create_time</span><br><span class="line">    from $&#123;APP&#125;.ods_sku_info</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">spu as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        spu_name</span><br><span class="line">    from $&#123;APP&#125;.ods_spu_info</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">c3 as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        name,</span><br><span class="line">        category2_id</span><br><span class="line">    from $&#123;APP&#125;.ods_base_category3</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">c2 as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        name,</span><br><span class="line">        category1_id</span><br><span class="line">    from $&#123;APP&#125;.ods_base_category2</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">c1 as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        name</span><br><span class="line">    from $&#123;APP&#125;.ods_base_category1</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">tm as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        tm_name</span><br><span class="line">    from $&#123;APP&#125;.ods_base_trademark</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">attr as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        sku_id,</span><br><span class="line">        collect_set(named_struct(&#x27;attr_id&#x27;,attr_id,&#x27;value_id&#x27;,value_id,&#x27;attr_name&#x27;,attr_name,&#x27;value_name&#x27;,value_name)) attrs</span><br><span class="line">    from $&#123;APP&#125;.ods_sku_attr_value</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">    group by sku_id</span><br><span class="line">),</span><br><span class="line">sale_attr as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        sku_id,</span><br><span class="line">        collect_set(named_struct(&#x27;sale_attr_id&#x27;,sale_attr_id,&#x27;sale_attr_value_id&#x27;,sale_attr_value_id,&#x27;sale_attr_name&#x27;,sale_attr_name,&#x27;sale_attr_value_name&#x27;,sale_attr_value_name)) sale_attrs</span><br><span class="line">    from $&#123;APP&#125;.ods_sku_sale_attr_value</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">    group by sku_id</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_sku_info partition(dt=&#x27;$do_date&#x27;)</span><br><span class="line">select</span><br><span class="line">    sku.id,</span><br><span class="line">    sku.price,</span><br><span class="line">    sku.sku_name,</span><br><span class="line">    sku.sku_desc,</span><br><span class="line">    sku.weight,</span><br><span class="line">    sku.is_sale,</span><br><span class="line">    sku.spu_id,</span><br><span class="line">    spu.spu_name,</span><br><span class="line">    sku.category3_id,</span><br><span class="line">    c3.name,</span><br><span class="line">    c3.category2_id,</span><br><span class="line">    c2.name,</span><br><span class="line">    c2.category1_id,</span><br><span class="line">    c1.name,</span><br><span class="line">    sku.tm_id,</span><br><span class="line">    tm.tm_name,</span><br><span class="line">    attr.attrs,</span><br><span class="line">    sale_attr.sale_attrs,</span><br><span class="line">    sku.create_time</span><br><span class="line">from sku</span><br><span class="line">left join spu on sku.spu_id=spu.id</span><br><span class="line">left join c3 on sku.category3_id=c3.id</span><br><span class="line">left join c2 on c3.category2_id=c2.id</span><br><span class="line">left join c1 on c2.category1_id=c1.id</span><br><span class="line">left join tm on sku.tm_id=tm.id</span><br><span class="line">left join attr on sku.id=attr.sku_id</span><br><span class="line">left join sale_attr on sku.id=sale_attr.sku_id;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line">dim_base_province=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_base_province</span><br><span class="line">select</span><br><span class="line">    bp.id,</span><br><span class="line">    bp.name,</span><br><span class="line">    bp.area_code,</span><br><span class="line">    bp.iso_code,</span><br><span class="line">    bp.iso_3166_2,</span><br><span class="line">    bp.region_id,</span><br><span class="line">    bp.name</span><br><span class="line">from $&#123;APP&#125;.ods_base_province bp</span><br><span class="line">join $&#123;APP&#125;.ods_base_region br on bp.region_id = br.id;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line">dim_coupon_info=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_coupon_info partition(dt=&#x27;$do_date&#x27;)</span><br><span class="line">select</span><br><span class="line">    id,</span><br><span class="line">    coupon_name,</span><br><span class="line">    coupon_type,</span><br><span class="line">    condition_amount,</span><br><span class="line">    condition_num,</span><br><span class="line">    activity_id,</span><br><span class="line">    benefit_amount,</span><br><span class="line">    benefit_discount,</span><br><span class="line">    create_time,</span><br><span class="line">    range_type,</span><br><span class="line">    limit_num,</span><br><span class="line">    taken_count,</span><br><span class="line">    start_time,</span><br><span class="line">    end_time,</span><br><span class="line">    operate_time,</span><br><span class="line">    expire_time</span><br><span class="line">from $&#123;APP&#125;.ods_coupon_info</span><br><span class="line">where dt=&#x27;$do_date&#x27;;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line">dim_activity_rule_info=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_activity_rule_info partition(dt=&#x27;$do_date&#x27;)</span><br><span class="line">select</span><br><span class="line">    ar.id,</span><br><span class="line">    ar.activity_id,</span><br><span class="line">    ai.activity_name,</span><br><span class="line">    ar.activity_type,</span><br><span class="line">    ai.start_time,</span><br><span class="line">    ai.end_time,</span><br><span class="line">    ai.create_time,</span><br><span class="line">    ar.condition_amount,</span><br><span class="line">    ar.condition_num,</span><br><span class="line">    ar.benefit_amount,</span><br><span class="line">    ar.benefit_discount,</span><br><span class="line">    ar.benefit_level</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        activity_id,</span><br><span class="line">        activity_type,</span><br><span class="line">        condition_amount,</span><br><span class="line">        condition_num,</span><br><span class="line">        benefit_amount,</span><br><span class="line">        benefit_discount,</span><br><span class="line">        benefit_level</span><br><span class="line">    from $&#123;APP&#125;.ods_activity_rule</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">)ar</span><br><span class="line">left join</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        activity_name,</span><br><span class="line">        start_time,</span><br><span class="line">        end_time,</span><br><span class="line">        create_time</span><br><span class="line">    from $&#123;APP&#125;.ods_activity_info</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">)ai</span><br><span class="line">on ar.activity_id=ai.id;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;dim_user_info&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_user_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;dim_sku_info&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_sku_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;dim_base_province&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_base_province&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;dim_coupon_info&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_coupon_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;dim_activity_rule_info&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_activity_rule_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;all&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_user_info$dim_sku_info$dim_coupon_info$dim_activity_rule_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="ods-to-dim-db-first-time-sh"><a href="#ods-to-dim-db-first-time-sh" class="headerlink" title="ods_to_dim_db_first_time.sh"></a>ods_to_dim_db_first_time.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">APP=gmall</span><br><span class="line"></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">   do_date=$2</span><br><span class="line">else </span><br><span class="line">   echo &quot;请传入日期参数&quot;</span><br><span class="line">   exit</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line"># 用户维度表（拉链表）</span><br><span class="line">dim_user_info=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_user_info partition(dt=&#x27;9999-99-99&#x27;)</span><br><span class="line">select</span><br><span class="line">    id,</span><br><span class="line">    login_name,</span><br><span class="line">    nick_name,</span><br><span class="line">    md5(name),</span><br><span class="line">    md5(phone_num),</span><br><span class="line">    md5(email),</span><br><span class="line">    user_level,</span><br><span class="line">    birthday,</span><br><span class="line">    gender,</span><br><span class="line">    create_time,</span><br><span class="line">    operate_time,</span><br><span class="line">    &#x27;$do_date&#x27;,</span><br><span class="line">    &#x27;9999-99-99&#x27;</span><br><span class="line">from $&#123;APP&#125;.ods_user_info</span><br><span class="line">where dt=&#x27;$do_date&#x27;;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line"># 商品维度表（全量）</span><br><span class="line">dim_sku_info=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">with</span><br><span class="line">sku as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        price,</span><br><span class="line">        sku_name,</span><br><span class="line">        sku_desc,</span><br><span class="line">        weight,</span><br><span class="line">        is_sale,</span><br><span class="line">        spu_id,</span><br><span class="line">        category3_id,</span><br><span class="line">        tm_id,</span><br><span class="line">        create_time</span><br><span class="line">    from $&#123;APP&#125;.ods_sku_info</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">spu as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        spu_name</span><br><span class="line">    from $&#123;APP&#125;.ods_spu_info</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">c3 as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        name,</span><br><span class="line">        category2_id</span><br><span class="line">    from $&#123;APP&#125;.ods_base_category3</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">c2 as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        name,</span><br><span class="line">        category1_id</span><br><span class="line">    from $&#123;APP&#125;.ods_base_category2</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">c1 as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        name</span><br><span class="line">    from $&#123;APP&#125;.ods_base_category1</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">tm as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        tm_name</span><br><span class="line">    from $&#123;APP&#125;.ods_base_trademark</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">attr as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        sku_id,</span><br><span class="line">        collect_set(named_struct(&#x27;attr_id&#x27;,attr_id,&#x27;value_id&#x27;,value_id,&#x27;attr_name&#x27;,attr_name,&#x27;value_name&#x27;,value_name)) attrs</span><br><span class="line">    from $&#123;APP&#125;.ods_sku_attr_value</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">    group by sku_id</span><br><span class="line">),</span><br><span class="line">sale_attr as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        sku_id,</span><br><span class="line">        collect_set(named_struct(&#x27;sale_attr_id&#x27;,sale_attr_id,&#x27;sale_attr_value_id&#x27;,sale_attr_value_id,&#x27;sale_attr_name&#x27;,sale_attr_name,&#x27;sale_attr_value_name&#x27;,sale_attr_value_name)) sale_attrs</span><br><span class="line">    from $&#123;APP&#125;.ods_sku_sale_attr_value</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">    group by sku_id</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_sku_info partition(dt=&#x27;$do_date&#x27;)</span><br><span class="line">select</span><br><span class="line">    sku.id,</span><br><span class="line">    sku.price,</span><br><span class="line">    sku.sku_name,</span><br><span class="line">    sku.sku_desc,</span><br><span class="line">    sku.weight,</span><br><span class="line">    sku.is_sale,</span><br><span class="line">    sku.spu_id,</span><br><span class="line">    spu.spu_name,</span><br><span class="line">    sku.category3_id,</span><br><span class="line">    c3.name,</span><br><span class="line">    c3.category2_id,</span><br><span class="line">    c2.name,</span><br><span class="line">    c2.category1_id,</span><br><span class="line">    c1.name,</span><br><span class="line">    sku.tm_id,</span><br><span class="line">    tm.tm_name,</span><br><span class="line">    attr.attrs,</span><br><span class="line">    sale_attr.sale_attrs,</span><br><span class="line">    sku.create_time</span><br><span class="line">from sku</span><br><span class="line">left join spu on sku.spu_id=spu.id</span><br><span class="line">left join c3 on sku.category3_id=c3.id</span><br><span class="line">left join c2 on c3.category2_id=c2.id</span><br><span class="line">left join c1 on c2.category1_id=c1.id</span><br><span class="line">left join tm on sku.tm_id=tm.id</span><br><span class="line">left join attr on sku.id=attr.sku_id</span><br><span class="line">left join sale_attr on sku.id=sale_attr.sku_id;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line"># 地区维度表（特殊）</span><br><span class="line">dim_base_province=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_base_province</span><br><span class="line">select</span><br><span class="line">    bp.id,</span><br><span class="line">    bp.name,</span><br><span class="line">    bp.area_code,</span><br><span class="line">    bp.iso_code,</span><br><span class="line">    bp.iso_3166_2,</span><br><span class="line">    bp.region_id,</span><br><span class="line">    br.region_name</span><br><span class="line">from $&#123;APP&#125;.ods_base_province bp</span><br><span class="line">join $&#123;APP&#125;.ods_base_region br on bp.region_id = br.id;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line"># 优惠券维度表（全量）</span><br><span class="line">dim_coupon_info=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_coupon_info partition(dt=&#x27;$do_date&#x27;)</span><br><span class="line">select</span><br><span class="line">    id,</span><br><span class="line">    coupon_name,</span><br><span class="line">    coupon_type,</span><br><span class="line">    condition_amount,</span><br><span class="line">    condition_num,</span><br><span class="line">    activity_id,</span><br><span class="line">    benefit_amount,</span><br><span class="line">    benefit_discount,</span><br><span class="line">    create_time,</span><br><span class="line">    range_type,</span><br><span class="line">    limit_num,</span><br><span class="line">    taken_count,</span><br><span class="line">    start_time,</span><br><span class="line">    end_time,</span><br><span class="line">    operate_time,</span><br><span class="line">    expire_time</span><br><span class="line">from $&#123;APP&#125;.ods_coupon_info</span><br><span class="line">where dt=&#x27;$do_date&#x27;;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line"># 活动维度表（全量）</span><br><span class="line">dim_activity_rule_info=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_activity_rule_info partition(dt=&#x27;$do_date&#x27;)</span><br><span class="line">select</span><br><span class="line">    ar.id,</span><br><span class="line">    ar.activity_id,</span><br><span class="line">    ai.activity_name,</span><br><span class="line">    ar.activity_type,</span><br><span class="line">    ai.start_time,</span><br><span class="line">    ai.end_time,</span><br><span class="line">    ai.create_time,</span><br><span class="line">    ar.condition_amount,</span><br><span class="line">    ar.condition_num,</span><br><span class="line">    ar.benefit_amount,</span><br><span class="line">    ar.benefit_discount,</span><br><span class="line">    ar.benefit_level</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        activity_id,</span><br><span class="line">        activity_type,</span><br><span class="line">        condition_amount,</span><br><span class="line">        condition_num,</span><br><span class="line">        benefit_amount,</span><br><span class="line">        benefit_discount,</span><br><span class="line">        benefit_level</span><br><span class="line">    from $&#123;APP&#125;.ods_activity_rule</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">)ar</span><br><span class="line">left join</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        activity_name,</span><br><span class="line">        start_time,</span><br><span class="line">        end_time,</span><br><span class="line">        create_time</span><br><span class="line">    from $&#123;APP&#125;.ods_activity_info</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">)ai</span><br><span class="line">on ar.activity_id=ai.id;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;dim_user_info&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_user_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;dim_sku_info&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_sku_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;dim_base_province&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_base_province&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;dim_coupon_info&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_coupon_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;dim_activity_rule_info&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_activity_rule_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;all&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_user_info$dim_sku_info$dim_coupon_info$dim_activity_rule_info$dim_base_province&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="tail-f-sh"><a href="#tail-f-sh" class="headerlink" title="tail-f.sh"></a>tail-f.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">echo ================== 展示本地日志文件最后几行输出:tail -f ./$1 ==================</span><br><span class="line">tail -f ./$1</span><br></pre></td></tr></table></figure><h1 id="which-sh"><a href="#which-sh" class="headerlink" title="which.sh"></a>which.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">echo ================== 查看 $1 文件位置的命令 :which $1 ==================</span><br><span class="line">which $1</span><br></pre></td></tr></table></figure><h1 id="azkaban-sh"><a href="#azkaban-sh" class="headerlink" title="azkaban.sh"></a>azkaban.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">start-web()&#123;</span><br><span class="line">        for i in hadoop102</span><br><span class="line">        do</span><br><span class="line">                ssh $i &quot;cd /opt/module/azkaban/azkaban-web-server-3.84.4;bin/start-web.sh&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stop-web()&#123;</span><br><span class="line">        for i in hadoop102</span><br><span class="line">        do</span><br><span class="line">                ssh $i &quot;cd /opt/module/azkaban/azkaban-web-server-3.84.4;bin/shutdown-web.sh&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">start-exec()&#123;</span><br><span class="line">        for i in hadoop100 hadoop101 hadoop102</span><br><span class="line">        do</span><br><span class="line">                ssh $i &quot;cd /opt/module/azkaban/azkaban-exec-server-3.84.4;bin/start-exec.sh&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">active-exec()&#123;</span><br><span class="line">        for i in hadoop100 hadoop101 hadoop102</span><br><span class="line">        do</span><br><span class="line">                ssh $i &quot;curl -G &#x27;$i:12321/executor?action=activate&#x27; &amp;&amp; echo&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stop-exec()&#123;</span><br><span class="line">        for i in hadoop100 hadoop101 hadoop102</span><br><span class="line">        do</span><br><span class="line">                ssh $i &quot;cd /opt/module/azkaban/azkaban-exec-server-3.84.4;bin/shutdown-exec.sh&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">case $1 in   </span><br><span class="line">        start-exec)</span><br><span class="line">                start-exec</span><br><span class="line">        ;;</span><br><span class="line">        active-exec)</span><br><span class="line">                active-exec</span><br><span class="line">        ;;</span><br><span class="line">        stop-exec)</span><br><span class="line">                stop-exec</span><br><span class="line">        ;;</span><br><span class="line">        start-web)</span><br><span class="line">                start-web</span><br><span class="line">        ;;</span><br><span class="line">        stop-web)</span><br><span class="line">                stop-web</span><br><span class="line">        ;;</span><br><span class="line">        </span><br><span class="line">        start)</span><br><span class="line">        echo &quot;=============== 启动executor ================&quot;</span><br><span class="line">                start-exec</span><br><span class="line">                sleep 10</span><br><span class="line">        echo &quot;=============== 激活executor ================&quot;</span><br><span class="line">                active-exec</span><br><span class="line">                sleep 5</span><br><span class="line">        echo &quot;=============== 启动webserver ===============&quot;</span><br><span class="line">                start-web</span><br><span class="line">        ;;</span><br><span class="line"></span><br><span class="line">        stop)</span><br><span class="line">        echo &quot;=============== 关闭executor ================&quot;</span><br><span class="line">                stop-exec</span><br><span class="line">                sleep 5</span><br><span class="line">        echo &quot;=============== 关闭webserver ===============&quot;</span><br><span class="line">                stop-web</span><br><span class="line">        ;;</span><br><span class="line">        *)</span><br><span class="line">        echo &#x27;Input args error...&#x27;</span><br><span class="line">esac</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="cluster5-0-all-sh"><a href="#cluster5-0-all-sh" class="headerlink" title="cluster5.0_all.sh"></a>cluster5.0_all.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">        echo ================== 启动 离线集群 ==================</span><br><span class="line"></span><br><span class="line">        #启动 Zookeeper集群</span><br><span class="line">        myzookeeper.sh start</span><br><span class="line"></span><br><span class="line">        #启动 Hadoop集群</span><br><span class="line">        myhadoop.sh start</span><br><span class="line"></span><br><span class="line">        #启动 Kafka采集集群</span><br><span class="line">        mykafka.sh start</span><br><span class="line"></span><br><span class="line">        #启动 Flume采集集群</span><br><span class="line">        applog-flume-kafka-topic_log.sh start</span><br><span class="line"></span><br><span class="line">        #启动 Flume消费集群</span><br><span class="line">        kafka-topic_log-hdfs-origin_data-gmall-log-topic_log.sh start</span><br><span class="line"></span><br><span class="line">        #启动flume消费maxwell传到kafka的数据</span><br><span class="line">        db_maxwell_kafka_flume_hdfs.sh start</span><br><span class="line"></span><br><span class="line">        #启动hive的metastore元数据服务</span><br><span class="line">        hive_metastore.sh start</span><br><span class="line"></span><br><span class="line">        #启动maxwell</span><br><span class="line">        maxwell.sh start</span><br><span class="line"></span><br><span class="line">        #启动hive的hiveserver2服务</span><br><span class="line">        hive_hiveserver2.sh start</span><br><span class="line"></span><br><span class="line">        &#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">        echo ================== 停止 离线集群 ==================</span><br><span class="line"></span><br><span class="line">        #启动hive的hiveserver2服务</span><br><span class="line">        hive_hiveserver2.sh stop</span><br><span class="line"></span><br><span class="line">        #关闭maxwell</span><br><span class="line">        maxwell.sh stop</span><br><span class="line"></span><br><span class="line">        #启动hive的metastore元数据服务</span><br><span class="line">        hive_metastore.sh stop</span><br><span class="line"></span><br><span class="line">        #关闭flume消费maxwell传到kafka的数据</span><br><span class="line">        db_maxwell_kafka_flume_hdfs.sh stop</span><br><span class="line"></span><br><span class="line">        #停止 Flume消费集群</span><br><span class="line">        kafka-topic_log-hdfs-origin_data-gmall-log-topic_log.sh stop</span><br><span class="line"></span><br><span class="line">        #停止 Flume采集集群</span><br><span class="line">        applog-flume-kafka-topic_log.sh stop</span><br><span class="line"></span><br><span class="line">        #停止 Kafka采集集群</span><br><span class="line">        mykafka.sh stop</span><br><span class="line"></span><br><span class="line">        #停止 Hadoop集群</span><br><span class="line">        myhadoop.sh stop</span><br><span class="line"></span><br><span class="line">        #停止 Zookeeper集群</span><br><span class="line">        myzookeeper.sh stop</span><br><span class="line"></span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="xcall-sh"><a href="#xcall-sh" class="headerlink" title="xcall.sh"></a>xcall.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line">echo &quot;查看每台节点上内存使用情况的命令:  xcall.sh free -h &quot;</span><br><span class="line">echo &quot;jps -ml 显示更多进程的信息,防止重名进程傻傻分不清&quot;</span><br><span class="line">for i in hadoop100 hadoop101 hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">    echo --------- $i ----------</span><br><span class="line">    ssh $i &quot;$*&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h1 id="xsync"><a href="#xsync" class="headerlink" title="xsync"></a>xsync</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">#1. 判断参数个数</span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo Not Enough Arguement!</span><br><span class="line">    exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#2. 遍历集群所有机器</span><br><span class="line">for host in hadoop100 hadoop101 hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">    echo ====================  $host  ====================</span><br><span class="line">    #3. 遍历所有目录，挨个发送</span><br><span class="line"></span><br><span class="line">    for file in $@</span><br><span class="line">    do</span><br><span class="line">        #4. 判断文件是否存在</span><br><span class="line">        if [ -e $file ]</span><br><span class="line">            then</span><br><span class="line">                #5. 获取父目录</span><br><span class="line">                pdir=$(cd -P $(dirname $file); pwd)</span><br><span class="line"></span><br><span class="line">                #6. 获取当前文件的名称</span><br><span class="line">                fname=$(basename $file)</span><br><span class="line">                ssh $host &quot;mkdir -p $pdir&quot;</span><br><span class="line">                rsync -av $pdir/$fname $host:$pdir</span><br><span class="line">            else</span><br><span class="line">                echo $file does not exists!</span><br><span class="line">        fi</span><br><span class="line">    done</span><br><span class="line">done</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="myhbase-sh"><a href="#myhbase-sh" class="headerlink" title="myhbase.sh"></a>myhbase.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 hbase ===================&quot;</span><br><span class="line"></span><br><span class="line">        # echo &quot; --------------- 启动 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hbase-2.0.5/bin/start-hbase.sh&quot;</span><br><span class="line">        # sleep 5</span><br><span class="line">        # ssh hadoop102 &quot;/opt/module/hbase-2.0.5/bin/hbase-daemon.sh start regionserver&quot;</span><br><span class="line">        </span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 hbase ===================&quot;</span><br><span class="line"></span><br><span class="line">        # echo &quot; --------------- 关闭 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hbase-2.0.5/bin/stop-hbase.sh&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="hive-load-hdfsDB-to-hiveHdfs5-0-sh"><a href="#hive-load-hdfsDB-to-hiveHdfs5-0-sh" class="headerlink" title="hive_load_hdfsDB_to_hiveHdfs5.0.sh"></a>hive_load_hdfsDB_to_hiveHdfs5.0.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">APP=gmall</span><br><span class="line"></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">   do_date=$2</span><br><span class="line">else </span><br><span class="line">   do_date=`date -d &#x27;-1 day&#x27; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">load_data()&#123;</span><br><span class="line">    sql=&quot;&quot;</span><br><span class="line">    for i in $*; do</span><br><span class="line">        #判断路径是否存在</span><br><span class="line">        hadoop fs -test -e /origin_data/$APP/db/$&#123;i:4&#125;/$do_date</span><br><span class="line">        #路径存在方可装载数据</span><br><span class="line">        if [[ $? = 0 ]]; then</span><br><span class="line">            sql=$sql&quot;load data inpath &#x27;/origin_data/$APP/db/$&#123;i:4&#125;/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.$i partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line">        fi</span><br><span class="line">    done</span><br><span class="line">    hive -e &quot;$sql&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    &quot;ods_activity_info_full&quot;)</span><br><span class="line">        load_data &quot;ods_activity_info_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_activity_rule_full&quot;)</span><br><span class="line">        load_data &quot;ods_activity_rule_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_base_category1_full&quot;)</span><br><span class="line">        load_data &quot;ods_base_category1_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_base_category2_full&quot;)</span><br><span class="line">        load_data &quot;ods_base_category2_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_base_category3_full&quot;)</span><br><span class="line">        load_data &quot;ods_base_category3_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_base_dic_full&quot;)</span><br><span class="line">        load_data &quot;ods_base_dic_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_base_province_full&quot;)</span><br><span class="line">        load_data &quot;ods_base_province_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_base_region_full&quot;)</span><br><span class="line">        load_data &quot;ods_base_region_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_base_trademark_full&quot;)</span><br><span class="line">        load_data &quot;ods_base_trademark_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_cart_info_full&quot;)</span><br><span class="line">        load_data &quot;ods_cart_info_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_coupon_info_full&quot;)</span><br><span class="line">        load_data &quot;ods_coupon_info_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_sku_attr_value_full&quot;)</span><br><span class="line">        load_data &quot;ods_sku_attr_value_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_sku_info_full&quot;)</span><br><span class="line">        load_data &quot;ods_sku_info_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_sku_sale_attr_value_full&quot;)</span><br><span class="line">        load_data &quot;ods_sku_sale_attr_value_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_spu_info_full&quot;)</span><br><span class="line">        load_data &quot;ods_spu_info_full&quot;</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    &quot;ods_cart_info_inc&quot;)</span><br><span class="line">        load_data &quot;ods_cart_info_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_comment_info_inc&quot;)</span><br><span class="line">        load_data &quot;ods_comment_info_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_coupon_use_inc&quot;)</span><br><span class="line">        load_data &quot;ods_coupon_use_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_favor_info_inc&quot;)</span><br><span class="line">        load_data &quot;ods_favor_info_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_order_detail_inc&quot;)</span><br><span class="line">        load_data &quot;ods_order_detail_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_order_detail_activity_inc&quot;)</span><br><span class="line">        load_data &quot;ods_order_detail_activity_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_order_detail_coupon_inc&quot;)</span><br><span class="line">        load_data &quot;ods_order_detail_coupon_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_order_info_inc&quot;)</span><br><span class="line">        load_data &quot;ods_order_info_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_order_refund_info_inc&quot;)</span><br><span class="line">        load_data &quot;ods_order_refund_info_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_order_status_log_inc&quot;)</span><br><span class="line">        load_data &quot;ods_order_status_log_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_payment_info_inc&quot;)</span><br><span class="line">        load_data &quot;ods_payment_info_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_refund_payment_inc&quot;)</span><br><span class="line">        load_data &quot;ods_refund_payment_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_user_info_inc&quot;)</span><br><span class="line">        load_data &quot;ods_user_info_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;all&quot;)</span><br><span class="line">        load_data &quot;ods_activity_info_full&quot; &quot;ods_activity_rule_full&quot; &quot;ods_base_category1_full&quot; &quot;ods_base_category2_full&quot; &quot;ods_base_category3_full&quot; &quot;ods_base_dic_full&quot; &quot;ods_base_province_full&quot; &quot;ods_base_region_full&quot; &quot;ods_base_trademark_full&quot; &quot;ods_cart_info_full&quot; &quot;ods_coupon_info_full&quot; &quot;ods_sku_attr_value_full&quot; &quot;ods_sku_info_full&quot; &quot;ods_sku_sale_attr_value_full&quot; &quot;ods_spu_info_full&quot; &quot;ods_cart_info_inc&quot; &quot;ods_comment_info_inc&quot; &quot;ods_coupon_use_inc&quot; &quot;ods_favor_info_inc&quot; &quot;ods_order_detail_inc&quot; &quot;ods_order_detail_activity_inc&quot; &quot;ods_order_detail_coupon_inc&quot; &quot;ods_order_info_inc&quot; &quot;ods_order_refund_info_inc&quot; &quot;ods_order_status_log_inc&quot; &quot;ods_payment_info_inc&quot; &quot;ods_refund_payment_inc&quot; &quot;ods_user_info_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="hive-load-hdfsLog-to-hiveHdfs5-0-sh"><a href="#hive-load-hdfsLog-to-hiveHdfs5-0-sh" class="headerlink" title="hive_load_hdfsLog_to_hiveHdfs5.0.sh"></a>hive_load_hdfsLog_to_hiveHdfs5.0.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line"># 定义变量方便修改</span><br><span class="line">APP=gmall</span><br><span class="line"></span><br><span class="line"># 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天</span><br><span class="line">if [ -n &quot;$1&quot; ] ;then</span><br><span class="line">   do_date=$1</span><br><span class="line">else </span><br><span class="line">   do_date=`date -d &quot;-1 day&quot; +%F`</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line">echo ================== 日志日期为 $do_date ==================</span><br><span class="line">#/origin_data/gmall/log/topic_log/2021-12-02</span><br><span class="line">sql=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/log/topic_log/$do_date&#x27; into table $&#123;APP&#125;.ods_log_inc partition(dt=&#x27;$do_date&#x27;);</span><br><span class="line">&quot;</span><br><span class="line">#第一步: 执行load data 将hdfs上的数据加载进hive里面</span><br><span class="line">hive -e &quot;$sql&quot;</span><br><span class="line"></span><br><span class="line">#第二步: 创建lzop索引   ------&gt; 数仓5.0使用的是gzip,所以不用建立索引</span><br><span class="line"># hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/$APP/ods/ods_log/dt=$do_date</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="myjps-sh"><a href="#myjps-sh" class="headerlink" title="myjps.sh"></a>myjps.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">for host in hadoop100 hadoop101 hadoop102 hadoop103 hadoop104 </span><br><span class="line">do</span><br><span class="line">        echo =============== $host ===============</span><br><span class="line">        ssh $host jps </span><br><span class="line">done</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="hive-hiveserver2-sh"><a href="#hive-hiveserver2-sh" class="headerlink" title="hive_hiveserver2.sh"></a>hive_hiveserver2.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot;------------ 开启hive hiveserver2服务去连接mysql中的元数据,后台启动 ---------------&quot;</span><br><span class="line">        # echo &quot;------------ hive连接mysql中的元数据有2种方式: 1）直接连接：直接去mysql中连接metastore库；2）通过服务连：hive有2种服务分别是metastore和hiveserver2，hive通过metastore服务去连接mysql中的元数据。---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;nohup /opt/module/hive-3.1.2/bin/hiveserver2 1&gt;/dev/null 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; ----------- 关闭hive hiveserver2服务去连接mysql中的元数据,后台启动 --------&quot;</span><br><span class="line">        ssh hadoop102 &quot;ps -ef | grep hive_hiveserver2 | grep -v grep |awk &#x27;&#123;print \$2&#125;&#x27; | xargs -n1 kill&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="hive-metastore-sh"><a href="#hive-metastore-sh" class="headerlink" title="hive_metastore.sh"></a>hive_metastore.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot;------------ 开启hive metastore 服务去连接mysql中的元数据,后台启动 ---------------&quot;</span><br><span class="line">        # echo &quot;------------ hive连接mysql中的元数据有2种方式: 1）直接连接：直接去mysql中连接metastore库；2）通过服务连：hive有2种服务分别是metastore和hiveserver2，hive通过metastore服务去连接mysql中的元数据。---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;nohup /opt/module/hive-3.1.2/bin/hive --service metastore&gt;log.txt 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; ----------- 关闭hive metastore 服务去连接mysql中的元数据,后台启动 --------&quot;</span><br><span class="line">        ssh hadoop102 &quot;ps -ef | grep hive | grep -v grep |awk &#x27;&#123;print \$2&#125;&#x27; | xargs -n1 kill&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="ls-al-sh"><a href="#ls-al-sh" class="headerlink" title="ls-al.sh"></a>ls-al.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">echo ================== 展示隐藏文件,Linux命令: ls - al ==================</span><br><span class="line">ls -al</span><br></pre></td></tr></table></figure><h1 id="ls-grep-sh"><a href="#ls-grep-sh" class="headerlink" title="ls-grep.sh"></a>ls-grep.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">#$1: 脚本的第一个参数 做非空判断</span><br><span class="line">if [ -n &quot;$1&quot; ] ;then</span><br><span class="line">   name=$1</span><br><span class="line">else </span><br><span class="line">   echo &quot;请传入一个查询参数&quot;</span><br><span class="line">   exit</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line">echo ================== 查看文件夹下是否有包含 $name 名称的文件 ==================</span><br><span class="line">ls | grep $name</span><br></pre></td></tr></table></figure><h1 id="maxwell-sh"><a href="#maxwell-sh" class="headerlink" title="maxwell.sh"></a>maxwell.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">MAXWELL_HOME=/opt/module/maxwell-1.29.2</span><br><span class="line"></span><br><span class="line">status_maxwell()&#123;</span><br><span class="line">    result=`ps -ef | grep maxwell | grep -v grep | wc -l`</span><br><span class="line">    return $result</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">start_maxwell()&#123;</span><br><span class="line">    # status_maxwell</span><br><span class="line">    # if [[ $? -lt 1 ]]; then</span><br><span class="line">    #     echo &quot;启动Maxwell&quot;</span><br><span class="line">    #     $MAXWELL_HOME/bin/maxwell --config $MAXWELL_HOME/config.properties --daemon</span><br><span class="line">    # else</span><br><span class="line">    #     echo &quot;Maxwell正在运行&quot;</span><br><span class="line">    # fi</span><br><span class="line">    $MAXWELL_HOME/bin/maxwell --config $MAXWELL_HOME/config.properties --daemon</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">stop_maxwell()&#123;</span><br><span class="line">    status_maxwell</span><br><span class="line">    if [[ $? -gt 0 ]]; then</span><br><span class="line">        echo &quot;停止Maxwell&quot;</span><br><span class="line">        ps -ef | grep maxwell | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27; | xargs kill -9</span><br><span class="line">    else</span><br><span class="line">        echo &quot;Maxwell未在运行&quot;</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    start )</span><br><span class="line">        start_maxwell</span><br><span class="line">    ;;</span><br><span class="line">    stop )</span><br><span class="line">        stop_maxwell</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="mydolphinscheduler-sh"><a href="#mydolphinscheduler-sh" class="headerlink" title="mydolphinscheduler.sh"></a>mydolphinscheduler.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 dolphinscheduler 集群 ===================&quot;</span><br><span class="line">        echo &quot;</span><br><span class="line">                1）一键启停所有服务</span><br><span class="line">                ./bin/start-all.sh</span><br><span class="line">                ./bin/stop-all.sh</span><br><span class="line">                2）启停 Master</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start master-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop master-server</span><br><span class="line">                3）启停 Worker</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start worker-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop worker-server</span><br><span class="line">                4）启停 Api</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start api-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop api-server</span><br><span class="line">                5）启停 Logger</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start logger-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop logger-server</span><br><span class="line">                6）启停 Alert</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start alert-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop alert-server</span><br><span class="line">            &quot;</span><br><span class="line"></span><br><span class="line">        ssh hadoop102 &quot;/opt/module/dolphinscheduler-1.3.9/bin/start-all.sh&quot;</span><br><span class="line"></span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 dolphinscheduler 集群 ===================&quot;</span><br><span class="line">        echo &quot;</span><br><span class="line">                1）一键启停所有服务</span><br><span class="line">                ./bin/start-all.sh</span><br><span class="line">                ./bin/stop-all.sh</span><br><span class="line">                2）启停 Master</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start master-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop master-server</span><br><span class="line">                3）启停 Worker</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start worker-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop worker-server</span><br><span class="line">                4）启停 Api</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start api-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop api-server</span><br><span class="line">                5）启停 Logger</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start logger-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop logger-server</span><br><span class="line">                6）启停 Alert</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start alert-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop alert-server</span><br><span class="line">            &quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/dolphinscheduler-1.3.9/bin/stop-all.sh&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="mydolphinscheduler-standalone-sh"><a href="#mydolphinscheduler-standalone-sh" class="headerlink" title="mydolphinscheduler-standalone.sh"></a>mydolphinscheduler-standalone.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 dolphinscheduler 单机模式(standalone) 单机模式内置zookeeper,所以必须关闭zookeeper===================&quot;</span><br><span class="line">        echo &quot;</span><br><span class="line">                1）一键启停所有服务</span><br><span class="line">                ./bin/start-all.sh</span><br><span class="line">                ./bin/stop-all.sh</span><br><span class="line">                2）启停 Master</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start master-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop master-server</span><br><span class="line">                3）启停 Worker</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start worker-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop worker-server</span><br><span class="line">                4）启停 Api</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start api-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop api-server</span><br><span class="line">                5）启停 Logger</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start logger-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop logger-server</span><br><span class="line">                6）启停 Alert</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start alert-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop alert-server</span><br><span class="line">            &quot;</span><br><span class="line"></span><br><span class="line">        ssh hadoop102 &quot;/opt/module/dolphinscheduler-1.3.9/bin/dolphinscheduler-daemon.sh start standalone-server&quot;</span><br><span class="line"></span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 dolphinscheduler 单机模式(standalone) 单机模式内置zookeeper,所以必须关闭zookeeper===================&quot;</span><br><span class="line">        echo &quot;</span><br><span class="line">                1）一键启停所有服务</span><br><span class="line">                ./bin/start-all.sh</span><br><span class="line">                ./bin/stop-all.sh</span><br><span class="line">                2）启停 Master</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start master-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop master-server</span><br><span class="line">                3）启停 Worker</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start worker-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop worker-server</span><br><span class="line">                4）启停 Api</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start api-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop api-server</span><br><span class="line">                5）启停 Logger</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start logger-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop logger-server</span><br><span class="line">                6）启停 Alert</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start alert-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop alert-server</span><br><span class="line">            &quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/dolphinscheduler-1.3.9/bin/dolphinscheduler-daemon.sh stop standalone-server&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="mygrafana-sh"><a href="#mygrafana-sh" class="headerlink" title="mygrafana.sh"></a>mygrafana.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 Grafana ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; sudo systemctl start grafana-server &quot;</span><br><span class="line"></span><br><span class="line">        ssh hadoop102 &quot;sudo systemctl start grafana-server&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 Grafana ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; sudo systemctl stop grafana-server &quot;</span><br><span class="line"></span><br><span class="line">        ssh hadoop102 &quot;sudo systemctl stop grafana-server&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       </span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="sqoop-import-mysql-to-hdfs-ods-every-day-sh"><a href="#sqoop-import-mysql-to-hdfs-ods-every-day-sh" class="headerlink" title="sqoop_import_mysql_to_hdfs_ods_every_day.sh"></a>sqoop_import_mysql_to_hdfs_ods_every_day.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line">#每日同步脚本 , 需要考虑到新增及变化的 增量同步</span><br><span class="line">APP=gmall</span><br><span class="line">sqoop=/opt/module/sqoop-1.4.6/bin/sqoop</span><br><span class="line"></span><br><span class="line">#每天一天的日期</span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else</span><br><span class="line">    do_date=`date -d &#x27;-1 day&#x27; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">import_data()&#123;</span><br><span class="line">$sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/$APP \</span><br><span class="line">--username root \</span><br><span class="line">--password shangbaishuyao \</span><br><span class="line">--target-dir /origin_data/$APP/db/$1/$do_date \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--query &quot;$2 and  \$CONDITIONS&quot; \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; \</span><br><span class="line">--compress \</span><br><span class="line">--compression-codec lzop \</span><br><span class="line">--null-string &#x27;\\N&#x27; \</span><br><span class="line">--null-non-string &#x27;\\N&#x27;</span><br><span class="line"></span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 每日同步策略是新增及变化的 sqoop增量导入</span><br><span class="line">import_order_info()&#123;</span><br><span class="line">  import_data order_info &quot;select</span><br><span class="line">                            id, </span><br><span class="line">                            total_amount, </span><br><span class="line">                            order_status, </span><br><span class="line">                            user_id, </span><br><span class="line">                            payment_way,</span><br><span class="line">                            delivery_address,</span><br><span class="line">                            out_trade_no, </span><br><span class="line">                            create_time, </span><br><span class="line">                            operate_time,</span><br><span class="line">                            expire_time,</span><br><span class="line">                            tracking_no,</span><br><span class="line">                            province_id,</span><br><span class="line">                            activity_reduce_amount,</span><br><span class="line">                            coupon_reduce_amount,                            </span><br><span class="line">                            original_total_amount,</span><br><span class="line">                            feight_fee,</span><br><span class="line">                            feight_fee_reduce      </span><br><span class="line">                        from order_info</span><br><span class="line">                        where </span><br><span class="line">                        (date_format(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27; </span><br><span class="line">                        or date_format(operate_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;)&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_coupon_use()&#123;</span><br><span class="line">  import_data coupon_use &quot;select</span><br><span class="line">                          id,</span><br><span class="line">                          coupon_id,</span><br><span class="line">                          user_id,</span><br><span class="line">                          order_id,</span><br><span class="line">                          coupon_status,</span><br><span class="line">                          get_time,</span><br><span class="line">                          using_time,</span><br><span class="line">                          used_time,</span><br><span class="line">                          expire_time</span><br><span class="line">                        from coupon_use</span><br><span class="line">                        where (date_format(get_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;</span><br><span class="line">                        or date_format(using_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;</span><br><span class="line">                        or date_format(used_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;</span><br><span class="line">                        or date_format(expire_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;)&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_order_status_log()&#123;</span><br><span class="line">  import_data order_status_log &quot;select</span><br><span class="line">                                  id,</span><br><span class="line">                                  order_id,</span><br><span class="line">                                  order_status,</span><br><span class="line">                                  operate_time</span><br><span class="line">                                from order_status_log</span><br><span class="line">                                where date_format(operate_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_user_info()&#123;</span><br><span class="line">  import_data &quot;user_info&quot; &quot;select </span><br><span class="line">                            id,</span><br><span class="line">                            login_name,</span><br><span class="line">                            nick_name,</span><br><span class="line">                            name,</span><br><span class="line">                            phone_num,</span><br><span class="line">                            email,</span><br><span class="line">                            user_level, </span><br><span class="line">                            birthday,</span><br><span class="line">                            gender,</span><br><span class="line">                            create_time,</span><br><span class="line">                            operate_time</span><br><span class="line">                          from user_info </span><br><span class="line">                          where (DATE_FORMAT(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27; </span><br><span class="line">                          or DATE_FORMAT(operate_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;)&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_order_detail()&#123;</span><br><span class="line">  import_data order_detail &quot;select </span><br><span class="line">                              id,</span><br><span class="line">                              order_id, </span><br><span class="line">                              sku_id,</span><br><span class="line">                              sku_name,</span><br><span class="line">                              order_price,</span><br><span class="line">                              sku_num, </span><br><span class="line">                              create_time,</span><br><span class="line">                              source_type,</span><br><span class="line">                              source_id,</span><br><span class="line">                              split_total_amount,</span><br><span class="line">                              split_activity_amount,</span><br><span class="line">                              split_coupon_amount</span><br><span class="line">                            from order_detail </span><br><span class="line">                            where DATE_FORMAT(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_payment_info()&#123;</span><br><span class="line">  import_data &quot;payment_info&quot;  &quot;select </span><br><span class="line">                                id,  </span><br><span class="line">                                out_trade_no, </span><br><span class="line">                                order_id, </span><br><span class="line">                                user_id, </span><br><span class="line">                                payment_type, </span><br><span class="line">                                trade_no, </span><br><span class="line">                                total_amount,  </span><br><span class="line">                                subject, </span><br><span class="line">                                payment_status,</span><br><span class="line">                                create_time,</span><br><span class="line">                                callback_time </span><br><span class="line">                              from payment_info </span><br><span class="line">                              where (DATE_FORMAT(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27; </span><br><span class="line">                              or DATE_FORMAT(callback_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;)&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 同步策略是增量同步 创建时间是每天的</span><br><span class="line">import_comment_info()&#123;</span><br><span class="line">  import_data comment_info &quot;select</span><br><span class="line">                              id,</span><br><span class="line">                              user_id,</span><br><span class="line">                              sku_id,</span><br><span class="line">                              spu_id,</span><br><span class="line">                              order_id,</span><br><span class="line">                              appraise,</span><br><span class="line">                              create_time</span><br><span class="line">                            from comment_info</span><br><span class="line">                            where date_format(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_order_refund_info()&#123;</span><br><span class="line">  import_data order_refund_info &quot;select</span><br><span class="line">                                id,</span><br><span class="line">                                user_id,</span><br><span class="line">                                order_id,</span><br><span class="line">                                sku_id,</span><br><span class="line">                                refund_type,</span><br><span class="line">                                refund_num,</span><br><span class="line">                                refund_amount,</span><br><span class="line">                                refund_reason_type,</span><br><span class="line">                                refund_status,</span><br><span class="line">                                create_time</span><br><span class="line">                              from order_refund_info</span><br><span class="line">                              where date_format(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 同步策略是全量同步,理论上where1=1可以不用写,但是import_data 这个函数 最终执行的sql是 --query &quot;$2 and  \$CONDITIONS&quot; \ 这个 我们为了满足sql的语法要求,因为前面有个and所以我们必须有where 1=1</span><br><span class="line">import_sku_info()&#123;</span><br><span class="line">  import_data sku_info &quot;select </span><br><span class="line">                          id,</span><br><span class="line">                          spu_id,</span><br><span class="line">                          price,</span><br><span class="line">                          sku_name,</span><br><span class="line">                          sku_desc,</span><br><span class="line">                          weight,</span><br><span class="line">                          tm_id,</span><br><span class="line">                          category3_id,</span><br><span class="line">                          is_sale,</span><br><span class="line">                          create_time</span><br><span class="line">                        from sku_info where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_category1()&#123;</span><br><span class="line">  import_data &quot;base_category1&quot; &quot;select </span><br><span class="line">                                  id,</span><br><span class="line">                                  name </span><br><span class="line">                                from base_category1 where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_category2()&#123;</span><br><span class="line">  import_data &quot;base_category2&quot; &quot;select</span><br><span class="line">                                  id,</span><br><span class="line">                                  name,</span><br><span class="line">                                  category1_id </span><br><span class="line">                                from base_category2 where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_category3()&#123;</span><br><span class="line">  import_data &quot;base_category3&quot; &quot;select</span><br><span class="line">                                  id,</span><br><span class="line">                                  name,</span><br><span class="line">                                  category2_id</span><br><span class="line">                                from base_category3 where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_province()&#123;</span><br><span class="line">  import_data base_province &quot;select</span><br><span class="line">                              id,</span><br><span class="line">                              name,</span><br><span class="line">                              region_id,</span><br><span class="line">                              area_code,</span><br><span class="line">                              iso_code,</span><br><span class="line">                              iso_3166_2</span><br><span class="line">                            from base_province</span><br><span class="line">                            where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_region()&#123;</span><br><span class="line">  import_data base_region &quot;select</span><br><span class="line">                              id,</span><br><span class="line">                              region_name</span><br><span class="line">                            from base_region</span><br><span class="line">                            where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_trademark()&#123;</span><br><span class="line">  import_data base_trademark &quot;select</span><br><span class="line">                                id,</span><br><span class="line">                                tm_name</span><br><span class="line">                              from base_trademark</span><br><span class="line">                              where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_spu_info()&#123;</span><br><span class="line">  import_data spu_info &quot;select</span><br><span class="line">                            id,</span><br><span class="line">                            spu_name,</span><br><span class="line">                            category3_id,</span><br><span class="line">                            tm_id</span><br><span class="line">                          from spu_info</span><br><span class="line">                          where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_favor_info()&#123;</span><br><span class="line">  import_data favor_info &quot;select</span><br><span class="line">                          id,</span><br><span class="line">                          user_id,</span><br><span class="line">                          sku_id,</span><br><span class="line">                          spu_id,</span><br><span class="line">                          is_cancel,</span><br><span class="line">                          create_time,</span><br><span class="line">                          cancel_time</span><br><span class="line">                        from favor_info</span><br><span class="line">                        where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_cart_info()&#123;</span><br><span class="line">  import_data cart_info &quot;select</span><br><span class="line">                        id,</span><br><span class="line">                        user_id,</span><br><span class="line">                        sku_id,</span><br><span class="line">                        cart_price,</span><br><span class="line">                        sku_num,</span><br><span class="line">                        sku_name,</span><br><span class="line">                        create_time,</span><br><span class="line">                        operate_time,</span><br><span class="line">                        is_ordered,</span><br><span class="line">                        order_time,</span><br><span class="line">                        source_type,</span><br><span class="line">                        source_id</span><br><span class="line">                      from cart_info</span><br><span class="line">                      where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_coupon_info()&#123;</span><br><span class="line">  import_data coupon_info &quot;select</span><br><span class="line">                          id,</span><br><span class="line">                          coupon_name,</span><br><span class="line">                          coupon_type,</span><br><span class="line">                          condition_amount,</span><br><span class="line">                          condition_num,</span><br><span class="line">                          activity_id,</span><br><span class="line">                          benefit_amount,</span><br><span class="line">                          benefit_discount,</span><br><span class="line">                          create_time,</span><br><span class="line">                          range_type,</span><br><span class="line">                          limit_num,</span><br><span class="line">                          taken_count,</span><br><span class="line">                          start_time,</span><br><span class="line">                          end_time,</span><br><span class="line">                          operate_time,</span><br><span class="line">                          expire_time</span><br><span class="line">                        from coupon_info</span><br><span class="line">                        where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_activity_info()&#123;</span><br><span class="line">  import_data activity_info &quot;select</span><br><span class="line">                              id,</span><br><span class="line">                              activity_name,</span><br><span class="line">                              activity_type,</span><br><span class="line">                              start_time,</span><br><span class="line">                              end_time,</span><br><span class="line">                              create_time</span><br><span class="line">                            from activity_info</span><br><span class="line">                            where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_activity_rule()&#123;</span><br><span class="line">    import_data activity_rule &quot;select</span><br><span class="line">                                    id,</span><br><span class="line">                                    activity_id,</span><br><span class="line">                                    activity_type,</span><br><span class="line">                                    condition_amount,</span><br><span class="line">                                    condition_num,</span><br><span class="line">                                    benefit_amount,</span><br><span class="line">                                    benefit_discount,</span><br><span class="line">                                    benefit_level</span><br><span class="line">                                from activity_rule</span><br><span class="line">                                where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_dic()&#123;</span><br><span class="line">    import_data base_dic &quot;select</span><br><span class="line">                            dic_code,</span><br><span class="line">                            dic_name,</span><br><span class="line">                            parent_code,</span><br><span class="line">                            create_time,</span><br><span class="line">                            operate_time</span><br><span class="line">                          from base_dic</span><br><span class="line">                          where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import_order_detail_activity()&#123;</span><br><span class="line">    import_data order_detail_activity &quot;select</span><br><span class="line">                                                                id,</span><br><span class="line">                                                                order_id,</span><br><span class="line">                                                                order_detail_id,</span><br><span class="line">                                                                activity_id,</span><br><span class="line">                                                                activity_rule_id,</span><br><span class="line">                                                                sku_id,</span><br><span class="line">                                                                create_time</span><br><span class="line">                                                            from order_detail_activity</span><br><span class="line">                                                            where date_format(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import_order_detail_coupon()&#123;</span><br><span class="line">    import_data order_detail_coupon &quot;select</span><br><span class="line">                                                                id,</span><br><span class="line">                                                order_id,</span><br><span class="line">                                                                order_detail_id,</span><br><span class="line">                                                                coupon_id,</span><br><span class="line">                                                                coupon_use_id,</span><br><span class="line">                                                                sku_id,</span><br><span class="line">                                                                create_time</span><br><span class="line">                                                            from order_detail_coupon</span><br><span class="line">                                                            where date_format(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import_refund_payment()&#123;</span><br><span class="line">    import_data refund_payment &quot;select</span><br><span class="line">                                                        id,</span><br><span class="line">                                                        out_trade_no,</span><br><span class="line">                                                        order_id,</span><br><span class="line">                                                        sku_id,</span><br><span class="line">                                                        payment_type,</span><br><span class="line">                                                        trade_no,</span><br><span class="line">                                                        total_amount,</span><br><span class="line">                                                        subject,</span><br><span class="line">                                                        refund_status,</span><br><span class="line">                                                        create_time,</span><br><span class="line">                                                        callback_time</span><br><span class="line">                                                    from refund_payment</span><br><span class="line">                                                    where (DATE_FORMAT(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27; </span><br><span class="line">                                                    or DATE_FORMAT(callback_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;)&quot;                                                    </span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_sku_attr_value()&#123;</span><br><span class="line">    import_data sku_attr_value &quot;select</span><br><span class="line">                                                    id,</span><br><span class="line">                                                    attr_id,</span><br><span class="line">                                                    value_id,</span><br><span class="line">                                                    sku_id,</span><br><span class="line">                                                    attr_name,</span><br><span class="line">                                                    value_name</span><br><span class="line">                                                from sku_attr_value</span><br><span class="line">                                                where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import_sku_sale_attr_value()&#123;</span><br><span class="line">    import_data sku_sale_attr_value &quot;select</span><br><span class="line">                                                            id,</span><br><span class="line">                                                            sku_id,</span><br><span class="line">                                                            spu_id,</span><br><span class="line">                                                            sale_attr_value_id,</span><br><span class="line">                                                            sale_attr_id,</span><br><span class="line">                                                            sale_attr_name,</span><br><span class="line">                                                            sale_attr_value_name</span><br><span class="line">                                                        from sku_sale_attr_value</span><br><span class="line">                                                        where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">  &quot;order_info&quot;)</span><br><span class="line">     import_order_info</span><br><span class="line">;;</span><br><span class="line">  &quot;base_category1&quot;)</span><br><span class="line">     import_base_category1</span><br><span class="line">;;</span><br><span class="line">  &quot;base_category2&quot;)</span><br><span class="line">     import_base_category2</span><br><span class="line">;;</span><br><span class="line">  &quot;base_category3&quot;)</span><br><span class="line">     import_base_category3</span><br><span class="line">;;</span><br><span class="line">  &quot;order_detail&quot;)</span><br><span class="line">     import_order_detail</span><br><span class="line">;;</span><br><span class="line">  &quot;sku_info&quot;)</span><br><span class="line">     import_sku_info</span><br><span class="line">;;</span><br><span class="line">  &quot;user_info&quot;)</span><br><span class="line">     import_user_info</span><br><span class="line">;;</span><br><span class="line">  &quot;payment_info&quot;)</span><br><span class="line">     import_payment_info</span><br><span class="line">;;</span><br><span class="line">  &quot;base_province&quot;)</span><br><span class="line">     import_base_province</span><br><span class="line">;;</span><br><span class="line">  &quot;activity_info&quot;)</span><br><span class="line">      import_activity_info</span><br><span class="line">;;</span><br><span class="line">  &quot;cart_info&quot;)</span><br><span class="line">      import_cart_info</span><br><span class="line">;;</span><br><span class="line">  &quot;comment_info&quot;)</span><br><span class="line">      import_comment_info</span><br><span class="line">;;</span><br><span class="line">  &quot;coupon_info&quot;)</span><br><span class="line">      import_coupon_info</span><br><span class="line">;;</span><br><span class="line">  &quot;coupon_use&quot;)</span><br><span class="line">      import_coupon_use</span><br><span class="line">;;</span><br><span class="line">  &quot;favor_info&quot;)</span><br><span class="line">      import_favor_info</span><br><span class="line">;;</span><br><span class="line">  &quot;order_refund_info&quot;)</span><br><span class="line">      import_order_refund_info</span><br><span class="line">;;</span><br><span class="line">  &quot;order_status_log&quot;)</span><br><span class="line">      import_order_status_log</span><br><span class="line">;;</span><br><span class="line">  &quot;spu_info&quot;)</span><br><span class="line">      import_spu_info</span><br><span class="line">;;</span><br><span class="line">  &quot;activity_rule&quot;)</span><br><span class="line">      import_activity_rule</span><br><span class="line">;;</span><br><span class="line">  &quot;base_dic&quot;)</span><br><span class="line">      import_base_dic</span><br><span class="line">;;</span><br><span class="line">  &quot;order_detail_activity&quot;)</span><br><span class="line">      import_order_detail_activity</span><br><span class="line">;;</span><br><span class="line">  &quot;order_detail_coupon&quot;)</span><br><span class="line">      import_order_detail_coupon</span><br><span class="line">;;</span><br><span class="line">  &quot;refund_payment&quot;)</span><br><span class="line">      import_refund_payment</span><br><span class="line">;;</span><br><span class="line">  &quot;sku_attr_value&quot;)</span><br><span class="line">      import_sku_attr_value</span><br><span class="line">;;</span><br><span class="line">  &quot;sku_sale_attr_value&quot;)</span><br><span class="line">      import_sku_sale_attr_value</span><br><span class="line">;;</span><br><span class="line">&quot;all&quot;)</span><br><span class="line">   import_base_category1</span><br><span class="line">   import_base_category2</span><br><span class="line">   import_base_category3</span><br><span class="line">   import_order_info</span><br><span class="line">   import_order_detail</span><br><span class="line">   import_sku_info</span><br><span class="line">   import_user_info</span><br><span class="line">   import_payment_info</span><br><span class="line">   import_base_trademark</span><br><span class="line">   import_activity_info</span><br><span class="line">   import_cart_info</span><br><span class="line">   import_comment_info</span><br><span class="line">   import_coupon_use</span><br><span class="line">   import_coupon_info</span><br><span class="line">   import_favor_info</span><br><span class="line">   import_order_refund_info</span><br><span class="line">   import_order_status_log</span><br><span class="line">   import_spu_info</span><br><span class="line">   import_activity_rule</span><br><span class="line">   import_base_dic</span><br><span class="line">   import_order_detail_activity</span><br><span class="line">   import_order_detail_coupon</span><br><span class="line">   import_refund_payment</span><br><span class="line">   import_sku_attr_value</span><br><span class="line">   import_sku_sale_attr_value</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="sqoop-import-mysql-to-hdfs-ods-first-day-sh"><a href="#sqoop-import-mysql-to-hdfs-ods-first-day-sh" class="headerlink" title="sqoop_import_mysql_to_hdfs_ods_first_day.sh"></a>sqoop_import_mysql_to_hdfs_ods_first_day.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line">#mysql 业务数据库库名</span><br><span class="line">APP=gmall</span><br><span class="line">#sqoop命令的绝对路径</span><br><span class="line">sqoop=/opt/module/sqoop-1.4.6/bin/sqoop</span><br><span class="line"></span><br><span class="line">#$2: 脚本的第二个参数 做非空判断</span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">   do_date=$2</span><br><span class="line">else </span><br><span class="line">   echo &quot;请传入日期参数&quot;</span><br><span class="line">   exit</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line">#公用的函数 主要是sqoop的imoport命令</span><br><span class="line">import_data()&#123;</span><br><span class="line">$sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/$APP \</span><br><span class="line">--username root \</span><br><span class="line">--password shangbaishuyao \</span><br><span class="line">--target-dir /origin_data/$APP/db/$1/$do_date \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--query &quot;$2 where \$CONDITIONS&quot; \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; \</span><br><span class="line">--compress \</span><br><span class="line">--compression-codec lzop \</span><br><span class="line">--null-string &#x27;\\N&#x27; \</span><br><span class="line">--null-non-string &#x27;\\N&#x27;</span><br><span class="line"></span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 通过创建时间和变化的时间获取每天新增及变化的数据 也就是  where 创建时间 = 今天 or 操作时间 = 今天</span><br><span class="line">import_order_info()&#123;</span><br><span class="line">  import_data order_info &quot;select</span><br><span class="line">                            id, </span><br><span class="line">                            total_amount, </span><br><span class="line">                            order_status, </span><br><span class="line">                            user_id, </span><br><span class="line">                            payment_way,</span><br><span class="line">                            delivery_address,</span><br><span class="line">                            out_trade_no, </span><br><span class="line">                            create_time, </span><br><span class="line">                            operate_time,</span><br><span class="line">                            expire_time,</span><br><span class="line">                            tracking_no,</span><br><span class="line">                            province_id,</span><br><span class="line">                            activity_reduce_amount,</span><br><span class="line">                            coupon_reduce_amount,                            </span><br><span class="line">                            original_total_amount,</span><br><span class="line">                            feight_fee,</span><br><span class="line">                            feight_fee_reduce      </span><br><span class="line">                        from order_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_coupon_use()&#123;</span><br><span class="line">  import_data coupon_use &quot;select</span><br><span class="line">                          id,</span><br><span class="line">                          coupon_id,</span><br><span class="line">                          user_id,</span><br><span class="line">                          order_id,</span><br><span class="line">                          coupon_status,</span><br><span class="line">                          get_time,</span><br><span class="line">                          using_time,</span><br><span class="line">                          used_time,</span><br><span class="line">                          expire_time</span><br><span class="line">                        from coupon_use&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_order_status_log()&#123;</span><br><span class="line">  import_data order_status_log &quot;select</span><br><span class="line">                                  id,</span><br><span class="line">                                  order_id,</span><br><span class="line">                                  order_status,</span><br><span class="line">                                  operate_time</span><br><span class="line">                                from order_status_log&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_user_info()&#123;</span><br><span class="line">  import_data &quot;user_info&quot; &quot;select </span><br><span class="line">                            id,</span><br><span class="line">                            login_name,</span><br><span class="line">                            nick_name,</span><br><span class="line">                            name,</span><br><span class="line">                            phone_num,</span><br><span class="line">                            email,</span><br><span class="line">                            user_level, </span><br><span class="line">                            birthday,</span><br><span class="line">                            gender,</span><br><span class="line">                            create_time,</span><br><span class="line">                            operate_time</span><br><span class="line">                          from user_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_order_detail()&#123;</span><br><span class="line">  import_data order_detail &quot;select </span><br><span class="line">                              id,</span><br><span class="line">                              order_id, </span><br><span class="line">                              sku_id,</span><br><span class="line">                              sku_name,</span><br><span class="line">                              order_price,</span><br><span class="line">                              sku_num, </span><br><span class="line">                              create_time,</span><br><span class="line">                              source_type,</span><br><span class="line">                              source_id,</span><br><span class="line">                              split_total_amount,</span><br><span class="line">                              split_activity_amount,</span><br><span class="line">                              split_coupon_amount</span><br><span class="line">                            from order_detail&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_payment_info()&#123;</span><br><span class="line">  import_data &quot;payment_info&quot;  &quot;select </span><br><span class="line">                                id,  </span><br><span class="line">                                out_trade_no, </span><br><span class="line">                                order_id, </span><br><span class="line">                                user_id, </span><br><span class="line">                                payment_type, </span><br><span class="line">                                trade_no, </span><br><span class="line">                                total_amount,  </span><br><span class="line">                                subject, </span><br><span class="line">                                payment_status,</span><br><span class="line">                                create_time,</span><br><span class="line">                                callback_time </span><br><span class="line">                              from payment_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_comment_info()&#123;</span><br><span class="line">  import_data comment_info &quot;select</span><br><span class="line">                              id,</span><br><span class="line">                              user_id,</span><br><span class="line">                              sku_id,</span><br><span class="line">                              spu_id,</span><br><span class="line">                              order_id,</span><br><span class="line">                              appraise,</span><br><span class="line">                              create_time</span><br><span class="line">                            from comment_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_order_refund_info()&#123;</span><br><span class="line">  import_data order_refund_info &quot;select</span><br><span class="line">                                id,</span><br><span class="line">                                user_id,</span><br><span class="line">                                order_id,</span><br><span class="line">                                sku_id,</span><br><span class="line">                                refund_type,</span><br><span class="line">                                refund_num,</span><br><span class="line">                                refund_amount,</span><br><span class="line">                                refund_reason_type,</span><br><span class="line">                                refund_status,</span><br><span class="line">                                create_time</span><br><span class="line">                              from order_refund_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_sku_info()&#123;</span><br><span class="line">  import_data sku_info &quot;select </span><br><span class="line">                          id,</span><br><span class="line">                          spu_id,</span><br><span class="line">                          price,</span><br><span class="line">                          sku_name,</span><br><span class="line">                          sku_desc,</span><br><span class="line">                          weight,</span><br><span class="line">                          tm_id,</span><br><span class="line">                          category3_id,</span><br><span class="line">                          is_sale,</span><br><span class="line">                          create_time</span><br><span class="line">                        from sku_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_category1()&#123;</span><br><span class="line">  import_data &quot;base_category1&quot; &quot;select </span><br><span class="line">                                  id,</span><br><span class="line">                                  name </span><br><span class="line">                                from base_category1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_category2()&#123;</span><br><span class="line">  import_data &quot;base_category2&quot; &quot;select</span><br><span class="line">                                  id,</span><br><span class="line">                                  name,</span><br><span class="line">                                  category1_id </span><br><span class="line">                                from base_category2&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_category3()&#123;</span><br><span class="line">  import_data &quot;base_category3&quot; &quot;select</span><br><span class="line">                                  id,</span><br><span class="line">                                  name,</span><br><span class="line">                                  category2_id</span><br><span class="line">                                from base_category3&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_province()&#123;</span><br><span class="line">  import_data base_province &quot;select</span><br><span class="line">                              id,</span><br><span class="line">                              name,</span><br><span class="line">                              region_id,</span><br><span class="line">                              area_code,</span><br><span class="line">                              iso_code,</span><br><span class="line">                              iso_3166_2</span><br><span class="line">                            from base_province&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_region()&#123;</span><br><span class="line">  import_data base_region &quot;select</span><br><span class="line">                              id,</span><br><span class="line">                              region_name</span><br><span class="line">                            from base_region&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_trademark()&#123;</span><br><span class="line">  import_data base_trademark &quot;select</span><br><span class="line">                                id,</span><br><span class="line">                                tm_name</span><br><span class="line">                              from base_trademark&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_spu_info()&#123;</span><br><span class="line">  import_data spu_info &quot;select</span><br><span class="line">                            id,</span><br><span class="line">                            spu_name,</span><br><span class="line">                            category3_id,</span><br><span class="line">                            tm_id</span><br><span class="line">                          from spu_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_favor_info()&#123;</span><br><span class="line">  import_data favor_info &quot;select</span><br><span class="line">                          id,</span><br><span class="line">                          user_id,</span><br><span class="line">                          sku_id,</span><br><span class="line">                          spu_id,</span><br><span class="line">                          is_cancel,</span><br><span class="line">                          create_time,</span><br><span class="line">                          cancel_time</span><br><span class="line">                        from favor_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_cart_info()&#123;</span><br><span class="line">  import_data cart_info &quot;select</span><br><span class="line">                        id,</span><br><span class="line">                        user_id,</span><br><span class="line">                        sku_id,</span><br><span class="line">                        cart_price,</span><br><span class="line">                        sku_num,</span><br><span class="line">                        sku_name,</span><br><span class="line">                        create_time,</span><br><span class="line">                        operate_time,</span><br><span class="line">                        is_ordered,</span><br><span class="line">                        order_time,</span><br><span class="line">                        source_type,</span><br><span class="line">                        source_id</span><br><span class="line">                      from cart_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_coupon_info()&#123;</span><br><span class="line">  import_data coupon_info &quot;select</span><br><span class="line">                          id,</span><br><span class="line">                          coupon_name,</span><br><span class="line">                          coupon_type,</span><br><span class="line">                          condition_amount,</span><br><span class="line">                          condition_num,</span><br><span class="line">                          activity_id,</span><br><span class="line">                          benefit_amount,</span><br><span class="line">                          benefit_discount,</span><br><span class="line">                          create_time,</span><br><span class="line">                          range_type,</span><br><span class="line">                          limit_num,</span><br><span class="line">                          taken_count,</span><br><span class="line">                          start_time,</span><br><span class="line">                          end_time,</span><br><span class="line">                          operate_time,</span><br><span class="line">                          expire_time</span><br><span class="line">                        from coupon_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_activity_info()&#123;</span><br><span class="line">  import_data activity_info &quot;select</span><br><span class="line">                              id,</span><br><span class="line">                              activity_name,</span><br><span class="line">                              activity_type,</span><br><span class="line">                              start_time,</span><br><span class="line">                              end_time,</span><br><span class="line">                              create_time</span><br><span class="line">                            from activity_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_activity_rule()&#123;</span><br><span class="line">    import_data activity_rule &quot;select</span><br><span class="line">                                    id,</span><br><span class="line">                                    activity_id,</span><br><span class="line">                                    activity_type,</span><br><span class="line">                                    condition_amount,</span><br><span class="line">                                    condition_num,</span><br><span class="line">                                    benefit_amount,</span><br><span class="line">                                    benefit_discount,</span><br><span class="line">                                    benefit_level</span><br><span class="line">                                from activity_rule&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_dic()&#123;</span><br><span class="line">    import_data base_dic &quot;select</span><br><span class="line">                            dic_code,</span><br><span class="line">                            dic_name,</span><br><span class="line">                            parent_code,</span><br><span class="line">                            create_time,</span><br><span class="line">                            operate_time</span><br><span class="line">                          from base_dic&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import_order_detail_activity()&#123;</span><br><span class="line">    import_data order_detail_activity &quot;select</span><br><span class="line">                                                                id,</span><br><span class="line">                                                                order_id,</span><br><span class="line">                                                                order_detail_id,</span><br><span class="line">                                                                activity_id,</span><br><span class="line">                                                                activity_rule_id,</span><br><span class="line">                                                                sku_id,</span><br><span class="line">                                                                create_time</span><br><span class="line">                                                            from order_detail_activity&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import_order_detail_coupon()&#123;</span><br><span class="line">    import_data order_detail_coupon &quot;select</span><br><span class="line">                                                                id,</span><br><span class="line">                                                order_id,</span><br><span class="line">                                                                order_detail_id,</span><br><span class="line">                                                                coupon_id,</span><br><span class="line">                                                                coupon_use_id,</span><br><span class="line">                                                                sku_id,</span><br><span class="line">                                                                create_time</span><br><span class="line">                                                            from order_detail_coupon&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import_refund_payment()&#123;</span><br><span class="line">    import_data refund_payment &quot;select</span><br><span class="line">                                                        id,</span><br><span class="line">                                                        out_trade_no,</span><br><span class="line">                                                        order_id,</span><br><span class="line">                                                        sku_id,</span><br><span class="line">                                                        payment_type,</span><br><span class="line">                                                        trade_no,</span><br><span class="line">                                                        total_amount,</span><br><span class="line">                                                        subject,</span><br><span class="line">                                                        refund_status,</span><br><span class="line">                                                        create_time,</span><br><span class="line">                                                        callback_time</span><br><span class="line">                                                    from refund_payment&quot;                                                    </span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_sku_attr_value()&#123;</span><br><span class="line">    import_data sku_attr_value &quot;select</span><br><span class="line">                                                    id,</span><br><span class="line">                                                    attr_id,</span><br><span class="line">                                                    value_id,</span><br><span class="line">                                                    sku_id,</span><br><span class="line">                                                    attr_name,</span><br><span class="line">                                                    value_name</span><br><span class="line">                                                from sku_attr_value&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import_sku_sale_attr_value()&#123;</span><br><span class="line">    import_data sku_sale_attr_value &quot;select</span><br><span class="line">                                                            id,</span><br><span class="line">                                                            sku_id,</span><br><span class="line">                                                            spu_id,</span><br><span class="line">                                                            sale_attr_value_id,</span><br><span class="line">                                                            sale_attr_id,</span><br><span class="line">                                                            sale_attr_name,</span><br><span class="line">                                                            sale_attr_value_name</span><br><span class="line">                                                        from sku_sale_attr_value&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">  &quot;order_info&quot;)</span><br><span class="line">     import_order_info</span><br><span class="line">;;</span><br><span class="line">  &quot;base_category1&quot;)</span><br><span class="line">     import_base_category1</span><br><span class="line">;;</span><br><span class="line">  &quot;base_category2&quot;)</span><br><span class="line">     import_base_category2</span><br><span class="line">;;</span><br><span class="line">  &quot;base_category3&quot;)</span><br><span class="line">     import_base_category3</span><br><span class="line">;;</span><br><span class="line">  &quot;order_detail&quot;)</span><br><span class="line">     import_order_detail</span><br><span class="line">;;</span><br><span class="line">  &quot;sku_info&quot;)</span><br><span class="line">     import_sku_info</span><br><span class="line">;;</span><br><span class="line">  &quot;user_info&quot;)</span><br><span class="line">     import_user_info</span><br><span class="line">;;</span><br><span class="line">  &quot;payment_info&quot;)</span><br><span class="line">     import_payment_info</span><br><span class="line">;;</span><br><span class="line">  &quot;base_province&quot;)</span><br><span class="line">     import_base_province</span><br><span class="line">;;</span><br><span class="line">  &quot;base_region&quot;)</span><br><span class="line">     import_base_region</span><br><span class="line">;;</span><br><span class="line">  &quot;base_trademark&quot;)</span><br><span class="line">     import_base_trademark</span><br><span class="line">;;</span><br><span class="line">  &quot;activity_info&quot;)</span><br><span class="line">      import_activity_info</span><br><span class="line">;;</span><br><span class="line">  &quot;cart_info&quot;)</span><br><span class="line">      import_cart_info</span><br><span class="line">;;</span><br><span class="line">  &quot;comment_info&quot;)</span><br><span class="line">      import_comment_info</span><br><span class="line">;;</span><br><span class="line">  &quot;coupon_info&quot;)</span><br><span class="line">      import_coupon_info</span><br><span class="line">;;</span><br><span class="line">  &quot;coupon_use&quot;)</span><br><span class="line">      import_coupon_use</span><br><span class="line">;;</span><br><span class="line">  &quot;favor_info&quot;)</span><br><span class="line">      import_favor_info</span><br><span class="line">;;</span><br><span class="line">  &quot;order_refund_info&quot;)</span><br><span class="line">      import_order_refund_info</span><br><span class="line">;;</span><br><span class="line">  &quot;order_status_log&quot;)</span><br><span class="line">      import_order_status_log</span><br><span class="line">;;</span><br><span class="line">  &quot;spu_info&quot;)</span><br><span class="line">      import_spu_info</span><br><span class="line">;;</span><br><span class="line">  &quot;activity_rule&quot;)</span><br><span class="line">      import_activity_rule</span><br><span class="line">;;</span><br><span class="line">  &quot;base_dic&quot;)</span><br><span class="line">      import_base_dic</span><br><span class="line">;;</span><br><span class="line">  &quot;order_detail_activity&quot;)</span><br><span class="line">      import_order_detail_activity</span><br><span class="line">;;</span><br><span class="line">  &quot;order_detail_coupon&quot;)</span><br><span class="line">      import_order_detail_coupon</span><br><span class="line">;;</span><br><span class="line">  &quot;refund_payment&quot;)</span><br><span class="line">      import_refund_payment</span><br><span class="line">;;</span><br><span class="line">  &quot;sku_attr_value&quot;)</span><br><span class="line">      import_sku_attr_value</span><br><span class="line">;;</span><br><span class="line">  &quot;sku_sale_attr_value&quot;)</span><br><span class="line">      import_sku_sale_attr_value</span><br><span class="line">;;</span><br><span class="line">  &quot;all&quot;)</span><br><span class="line">   import_base_category1</span><br><span class="line">   import_base_category2</span><br><span class="line">   import_base_category3</span><br><span class="line">   import_order_info</span><br><span class="line">   import_order_detail</span><br><span class="line">   import_sku_info</span><br><span class="line">   import_user_info</span><br><span class="line">   import_payment_info</span><br><span class="line">   import_base_region</span><br><span class="line">   import_base_province</span><br><span class="line">   import_base_trademark</span><br><span class="line">   import_activity_info</span><br><span class="line">   import_cart_info</span><br><span class="line">   import_comment_info</span><br><span class="line">   import_coupon_use</span><br><span class="line">   import_coupon_info</span><br><span class="line">   import_favor_info</span><br><span class="line">   import_order_refund_info</span><br><span class="line">   import_order_status_log</span><br><span class="line">   import_spu_info</span><br><span class="line">   import_activity_rule</span><br><span class="line">   import_base_dic</span><br><span class="line">   import_order_detail_activity</span><br><span class="line">   import_order_detail_coupon</span><br><span class="line">   import_refund_payment</span><br><span class="line">   import_sku_attr_value</span><br><span class="line">   import_sku_sale_attr_value</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;myhadoop-sh&quot;&gt;&lt;a href=&quot;#myhadoop-sh&quot; class=&quot;headerlink&quot; title=&quot;myhadoop.sh&quot;&gt;&lt;/a&gt;myhadoop.sh&lt;/h1&gt;&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;#!/bin/bash&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;if [ $# -lt 1 ]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;then&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    echo &amp;quot;No Args Input...&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    exit ;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;fi&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;case $1 in&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;quot;start&amp;quot;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        echo &amp;quot; =================== 启动 hadoop集群 ===================&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        echo &amp;quot; --------------- 启动 hdfs ---------------&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ssh hadoop102 &amp;quot;/opt/module/hadoop-3.1.3/sbin/start-dfs.sh&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        echo &amp;quot; --------------- 启动 yarn ---------------&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ssh hadoop103 &amp;quot;/opt/module/hadoop-3.1.3/sbin/start-yarn.sh&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        echo &amp;quot; --------------- 启动 historyserver ---------------&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ssh hadoop102 &amp;quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;quot;stop&amp;quot;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        echo &amp;quot; =================== 关闭 hadoop集群 ===================&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        echo &amp;quot; --------------- 关闭 historyserver ---------------&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ssh hadoop102 &amp;quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        echo &amp;quot; --------------- 关闭 yarn ---------------&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ssh hadoop103 &amp;quot;/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        echo &amp;quot; --------------- 关闭 hdfs ---------------&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ssh hadoop102 &amp;quot;/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;*)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    echo &amp;quot;Input Args Error...&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;esac&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="shell" scheme="http://xubatian.cn/tags/shell/"/>
    
  </entry>
  
</feed>
