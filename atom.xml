<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>我的梦想是星辰大海</title>
  
  <subtitle>知识源于积累,登峰造极源于自律</subtitle>
  <link href="http://xubatian.cn/atom.xml" rel="self"/>
  
  <link href="http://xubatian.cn/"/>
  <updated>2022-01-15T07:28:03.166Z</updated>
  <id>http://xubatian.cn/</id>
  
  <author>
    <name>xubatian</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>hadoop组成模块之MapReduce框架原理</title>
    <link href="http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BMapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/"/>
    <id>http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BMapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/</id>
    <published>2022-01-14T16:20:06.996Z</published>
    <updated>2022-01-15T07:28:03.166Z</updated>
    
    <content type="html"><![CDATA[<p>孤独这两个字拆开来看，有孩童，有瓜果，有小犬，有蚊蝇，足以撑起一个盛夏傍晚间的巷子口，人情味十足。 稚儿擎瓜柳棚下，细犬逐蝶窄巷中，人间繁华多笑语，惟我空余两鬓风。 孩童水果猫狗飞蝇当然热闹，可都和你无关，这就叫孤独。 …                            </p><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_94.jpg" width = "1201.56" height = "675.88" alt="xubatian的博客" align="center" /><p>前言</p><p>什么是MapReduce? </p><p>MapReduce是hadoop解决大数据计算问题的而落地的一个计算引擎. 他是一个计算框架.</p><p>mapreduce如何做到海量数据的计算的呢?</p><p>mapreduce的流程是什么样子的呢?</p><p>mapreduce的有哪些部分组成的的?</p><p>图示:</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mm2.png" width = "400" height = "200" alt="xubatian的博客" align="center" /><p>如上图所示, 这就是完整的mapreduce 的数据流图.</p><p>首先我们知道了hadoop的hdfs是将海量的数据进行了存储.  而hadoop的mapreduce是将存储的海量数据尽心计算得出我们想要的结果.</p><p>那么他要计算这些数据.  首先他得将数据输入到我们的计算引擎mapreduce当中, 即使用Inputformat将数据导入到mapreduce里面来, 然后通过mapTask将数据打散,即map任务, 再通过reduceTask任务将数据聚合,即reduce任务. 最后通过outputFormat将计算的结果输出到某个文件或者某个数据库中.</p><p>那么由此可知. 整个mapreduce是由. Inputformart输入数据.  MapTask 打散数据. ReduceTask聚合数据. Outputformart输出结果.这几个部分组成的. 所以,要知道mapreduce的框架原理.需要学习组成mapreduce的四个环节.</p><h1 id="MapReduce框架原理"><a href="#MapReduce框架原理" class="headerlink" title="MapReduce框架原理"></a>MapReduce框架原理</h1><h2 id="InputFormat数据输入"><a href="#InputFormat数据输入" class="headerlink" title="InputFormat数据输入"></a>InputFormat数据输入</h2><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mm.png" width = "500" height = "300" alt="xubatian的博客" align="center" /><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">InputFormat, 这个是整个MapReduce里面非常关键的一个对象.</span><br><span class="line"></span><br><span class="line">什么是InputFormat呢? 就是你输入到这个mapper里面的数据其实就是由InputFormat来负责的. 比如我怎么从文件里面去读这个数据. 这里还涉及到是否要切片这个概念.</span><br><span class="line"></span><br><span class="line">MapReduce数据流是比较简单的.一个是mapper阶段. 一个是reduce阶段. 在mapper阶段前面就有一个InputFormat帮我们去读数据. 在Reducer后面有一个OutPutFormat帮我们去写数据.在mapper与reducer中间就是我们讲的shuffle过程.大部分工作都是在shuffle里面做的</span><br></pre></td></tr></table></figure><h2 id="切片与MapTask并行度决定机制"><a href="#切片与MapTask并行度决定机制" class="headerlink" title="切片与MapTask并行度决定机制"></a>切片与MapTask并行度决定机制</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> 切片不从物理上切开,比如说,我有一个两百兆的文件.我上传到HDFS以后. 假如说的我一块的大小是128M. 那么这个200M的文件上传到HDFS以后被切成了两块了.</span><br><span class="line">第一块128M,第二块72M. 到此为止只是块的概念. </span><br><span class="line"> 接下来我就要通过map来分析你这个数据了.正常情况下,其实你的一个块的数据要交给mapTask去处理.但是严格意义上并不是这样的.而是一个切片要交给一个mapTask来处理. </span><br><span class="line">那么这个切片是怎么来的呢?</span><br><span class="line"> 切片和块的大小有点关系,但是我们可以自己去设置有多大. 比如假设我设置我的切片大小是64M. 那么对于128M块大小的数据在真正进入到map要处理的时候.会把这个128M的块的数据从逻辑上切成两片.</span><br><span class="line"> 第一片交给一个mapTask去处理.</span><br><span class="line">第二片也交给一个mapTask去处理. </span><br><span class="line">所以说,严格来说不是一个块的数据交给mapTask.而是一个切片的数据交给一个mapTask. 72M的也是切成两片.</span><br><span class="line">这里我们所说的切只是逻辑上的,这个块还是整个的一个块.他是不会从物理上给你切开的. 只是逻辑上的.</span><br><span class="line"> 一个切片,一个mapTask.  注意: 默认情况下切片的大小和块的大小是一样的.</span><br><span class="line">也就意味着不自己设置的情况下.他的一个切片的大小也是128M. 正好我的默认情况下块的大小是一样的.</span><br><span class="line"></span><br><span class="line">那么问题来了, 我是不是一个切片越大,mapTask越多,我并行能力越强,是否我的性能就越高呢?</span><br></pre></td></tr></table></figure><h3 id="1．问题引出"><a href="#1．问题引出" class="headerlink" title="1．问题引出"></a>1．问题引出</h3><p>MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度。<br>思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？</p><h3 id="2．MapTask并行度决定机制"><a href="#2．MapTask并行度决定机制" class="headerlink" title="2．MapTask并行度决定机制"></a>2．MapTask并行度决定机制</h3><p>数据块：Block是HDFS物理上把数据分成一块一块。<br>数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。(逻辑上对数据块进行划分,让多个mapTask并行处理数据块的不同切片部分)</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mm4.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">假如我有300M的数据.假设切片大小我设置为100M. 块大小设置为128M. 那么这个300M的文件将来我存到dataNode上面的时候,如上图. 128M一块 ,128M一块, 44M一块.</span><br><span class="line">块大小还是按照128M. 但是我的切片大小是100M. 那么将来我在处理我128M的数据的时候,那我就要从逻辑上将我的128M块 切片分成100M 和 28M.  其中这100M我交给mapTask去处理. 那么我剩下的28M怎么办呢? 他不是在往后面找72M数据合起来. 他有一个切片的概念. 切片他是不考虑数据的整体性. 正常情况下他是以块来作为一个单位来切的.  所以hadoop他默认就是按照128M来切的.这样就不用了跨机器读你的数据了.</span><br><span class="line"></span><br><span class="line">记住: 你有多少个切片,将来你就有多少个mapTask.  每一个切片都分配一个mapTask来进行并行处理.   默认情况下切片大小就是你的块大小.</span><br></pre></td></tr></table></figure><p>通过源码查看切片机制:</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mm5.png" width = "1201.56" height = "600" alt="xubatian的博客" align="center" /><h2 id="Job提交流程源码和切片源码详解"><a href="#Job提交流程源码和切片源码详解" class="headerlink" title="Job提交流程源码和切片源码详解"></a>Job提交流程源码和切片源码详解</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">Job提交流程(MapTask执行之前):</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> 在驱动类中job.waitForCompletion(<span class="keyword">true</span>) 提交Job</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> 执行submit()方法</span><br><span class="line">   [<span class="number">1</span>]. ensureState(JobState.DEFINE);  确认Job的状态</span><br><span class="line">   [<span class="number">2</span>]. setUseNewAPI(); 设置使用新API</span><br><span class="line">   [<span class="number">3</span>]. connect();</span><br><span class="line">   ① <span class="keyword">return</span> <span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line">   ② 调用构造器里面的重要方法initialize(jobTrackAddr, conf);</span><br><span class="line">      (<span class="number">1</span>). clientProtocol = provider.create(conf); 获取Job的运行方式(本地 LocalJobRunner / yarn YARNRunner)</span><br><span class="line"></span><br><span class="line">   [<span class="number">4</span>]. 获取到JobSubmitter submitter , 然后提交Job submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">   ① checkSpecs(job); 校验输出路径是否存在. 如果存在，直接抛出异常.</span><br><span class="line">   ②  Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line">       获取Job的临时的工作目录，例如: D:/tmp/hadoop-Administrator/mapred/staging/Administrator780971275/.staging </span><br><span class="line">   ③  JobID jobId = submitClient.getNewJobID(); 生成JobId</span><br><span class="line">   ④  Path submitJobDir = <span class="keyword">new</span> Path(jobStagingArea, jobId.toString()); </span><br><span class="line">       获取到提交Job的目录 ，实际就是   jobStagingArea + jobId 。</span><br><span class="line">   ⑤  copyAndConfigureFiles(job, submitJobDir); 复制并配置一些文件</span><br><span class="line">       (<span class="number">1</span>).  rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line">       (<span class="number">2</span>).  submitJobDir = jtFs.makeQualified(submitJobDir); 获取到提交job的全路径</span><br><span class="line">       (<span class="number">3</span>).  FileSystem.mkdirs(jtFs, submitJobDir, mapredSysPerms); 在文件系统创建Job提交的相关目录</span><br><span class="line">   ⑥  <span class="keyword">int</span> maps = writeSplits(job, submitJobDir); 生成切片信息，并将 job.split 切片信息的文件写到job提交目录中  (下面着重讲如何生成切片信息的)</span><br><span class="line">       (<span class="number">1</span>). maps = writeNewSplits(job, jobSubmitDir); </span><br><span class="line">       (<span class="number">2</span>). InputFormat&lt;?, ?&gt; input = ReflectionUtils.newInstance(job.getInputFormatClass(), conf);</span><br><span class="line">            切片信息是否InputFormat来生成的.  默认的的InputFormat是 TextInputFormat</span><br><span class="line">       (<span class="number">3</span>). List&lt;InputSplit&gt; splits = input.getSplits(job); 通过InputFormat来获取切片信息</span><br><span class="line">    a.  <span class="keyword">long</span> minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job)); ==&gt;<span class="number">1</span></span><br><span class="line">    b.  <span class="keyword">long</span> maxSize = getMaxSplitSize(job);  ==&gt; Long.MAX_VALUE</span><br><span class="line">    c.  <span class="keyword">long</span> blockSize = file.getBlockSize(); 获取块大小，本地默认的块大小32M.</span><br><span class="line">    d.  <span class="keyword">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize); 计算切片大小</span><br><span class="line">             ****** Math.max(minSize, Math.min(maxSize, blockSize));假如我想要切片大小变大我需要将minSize调大就可以了.如果我想要切片大小变的更小我就需要调整maxSize了.</span><br><span class="line">    e.   bytesRemaining)/splitSize &gt; SPLIT_SLOP(<span class="number">1.1</span>) 如果剩余文件的大小超过切片</span><br><span class="line">         大小的<span class="number">1.1</span>倍,继续切片，反之, 不切片.</span><br><span class="line"></span><br><span class="line">   ⑦  conf.setInt(MRJobConfig.NUM_MAPS, maps); 设置MapTask的个数</span><br><span class="line">   ⑧   writeConf(conf, submitJobFile); 将job相关的 job.xml 配置信息等写到job提交目录中</span><br><span class="line">   ⑨  status = submitClient.submitJob(</span><br><span class="line">             jobId, submitJobDir.toString(), job.getCredentials()); 真正提交Job，提交了job之后就需要执行我的mapTask了</span><br><span class="line">   ⑩  jtFs.delete(submitJobDir, <span class="keyword">true</span>); 等Job执行结束后，删除临时目录.</span><br></pre></td></tr></table></figure><p>&emsp;     </p><p>&emsp;     </p><p>&emsp;     </p><h3 id="InputFormat-抽象类-负责读取数据到Map阶段进行处理"><a href="#InputFormat-抽象类-负责读取数据到Map阶段进行处理" class="headerlink" title="InputFormat(抽象类):  负责读取数据到Map阶段进行处理."></a>InputFormat(抽象类):  负责读取数据到Map阶段进行处理.</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">重要的方法: </span><br><span class="line">getSplits() : 生成切片信息</span><br><span class="line">createRecordReader(): 创建RecordReader对象，负责数据的读取</span><br><span class="line">子抽象类: </span><br><span class="line">FileInputFormat  (抽象类)</span><br><span class="line">重要的方法: </span><br><span class="line">isSplitable(): 设置是否可切分</span><br><span class="line">实现类:(因为两个都是抽象类,所以就代表着我们不能直接去使用,所以我们需要看他们有哪些具体的实现类) </span><br><span class="line">TextInputFormat  默认的,它里面大部分方法都是用的FileInputFormat里面的.</span><br><span class="line">CombineTextInputFormat  它主要是针对小文件过多的情况下来使用的.combine就是合并</span><br><span class="line">KeyValueTextInputFormat</span><br><span class="line">NLineInputFormat</span><br></pre></td></tr></table></figure><p>&emsp;     </p><h3 id="OutputFormat-负责Reduce端数据的输出"><a href="#OutputFormat-负责Reduce端数据的输出" class="headerlink" title="OutputFormat:  负责Reduce端数据的输出"></a>OutputFormat:  负责Reduce端数据的输出</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">重要的方法: </span><br><span class="line">getRecordWriter() : 创建RecordWriter,负责数据的写出</span><br><span class="line">子抽象类:</span><br><span class="line">FileOutputFormat</span><br><span class="line">实现类:</span><br><span class="line">TextOutputFormat  默认的</span><br><span class="line">SequenceFileOutputFormat</span><br></pre></td></tr></table></figure><p>&emsp;     </p><h3 id="Partitioner-分区器"><a href="#Partitioner-分区器" class="headerlink" title="Partitioner: 分区器"></a>Partitioner: 分区器</h3><pre><code>默认的分区器:  HashPartitioner ,通过key的hashcode值对ReduceTasks的个数取余确定去往哪个分区.</code></pre><p>​            </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">如何确定分区:</span><br><span class="line">   partitions = jobContext.getNumReduceTasks();</span><br><span class="line">   <span class="keyword">if</span> (partitions &gt; <span class="number">1</span>) &#123;</span><br><span class="line">partitioner = (org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;)</span><br><span class="line"> ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">partitioner = <span class="keyword">new</span> org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;() &#123;</span><br><span class="line"> <span class="meta">@Override</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> partitions - <span class="number">1</span>;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure><h2 id="Job提交流程源码详解"><a href="#Job提交流程源码详解" class="headerlink" title="Job提交流程源码详解"></a>Job提交流程源码详解</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">waitForCompletion()</span><br><span class="line"></span><br><span class="line">submit();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1建立连接</span></span><br><span class="line">connect();</span><br><span class="line"><span class="comment">// 1）创建提交Job的代理</span></span><br><span class="line"><span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line"><span class="comment">// （1）判断是本地yarn还是远程</span></span><br><span class="line">initialize(jobTrackAddr, conf); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 提交job</span></span><br><span class="line">submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster)</span><br><span class="line"><span class="comment">// 1）创建给集群提交数据的Stag路径</span></span><br><span class="line">Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2）获取jobid ，并创建Job路径</span></span><br><span class="line">JobID jobId = submitClient.getNewJobID();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3）拷贝jar包到集群</span></span><br><span class="line"></span><br><span class="line">copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4）计算切片，生成切片规划文件</span></span><br><span class="line">writeSplits(job, submitJobDir);</span><br><span class="line">maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">input.getSplits(job);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5）向Stag路径写XML配置文件</span></span><br><span class="line">writeConf(conf, submitJobFile);</span><br><span class="line">conf.writeXml(out);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6）提交Job,返回提交状态</span></span><br><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/job1.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><h2 id="FileInputFormat切片源码解析-input-getSplits-job"><a href="#FileInputFormat切片源码解析-input-getSplits-job" class="headerlink" title="FileInputFormat切片源码解析(input.getSplits(job))"></a>FileInputFormat切片源码解析(input.getSplits(job))</h2><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/job2.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><h2 id="FileInputFormat切片机制"><a href="#FileInputFormat切片机制" class="headerlink" title="FileInputFormat切片机制"></a>FileInputFormat切片机制</h2><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/job3.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/job4.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><h2 id="CombineTextInputFormat切片机制"><a href="#CombineTextInputFormat切片机制" class="headerlink" title="CombineTextInputFormat切片机制"></a>CombineTextInputFormat切片机制</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">如果说我们使用框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。</span><br><span class="line"></span><br><span class="line">比如说我有5个文件, 第一个是1M,第二个是2M,3M,4M,5M. 因为他们都小于我们的块大小128M.  所以说按照单个文件来去切的话, 每个文件都是一个切片, 最终生成5个切片. 且我的每一个切片都是对应一个mapTask, 但是这么小的切片对应我的mapTask不是杀鸡用牛刀吗?它都不值得我去启动一次mapTask.因为你数据量太小了.所以对于小文件来说我就不能使用TextInputFormat了, 我必须对你做一个处理.所以就用到我们的CombineTextInputFormat. 他是用的场景就是小文件过多的情况下.他的思想是什么呢?</span><br><span class="line">首先我们使用CombineTextInputFormat的时候得先设置虚拟存储切片的最大值. 具体设置多大按照实际情况来定. 假如说我把虚拟存储最大值设置为4M,那么他会根据这个4M进行判断,我这个文件到底要不要去切. 然后最终我怎么去帮你生成切片.所以CombineTextInputFormat也是可以避免数据倾斜问题.</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/job6.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><p>如上图所示:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们设置虚拟存储的文件大小(setMaxInputSplitSize)为4M. 比如说我们有几个文件(如上图) 这些小文件.  因为我们设置虚拟存储大小是4M.  在虚拟存储过程中有一个算法,就是如果你的文件的大小小于你虚拟存储的最大值(即4M). 那么对于你这个文件来说就将你划分成一块.这一块是虚拟的.不是真正划分的. 但是如果你实际文件大小是大于你设置的文件最大值的(4M) , 但是他又小于两倍的最大值. 即大于4M 小于8M.  在这种情况下我就将你这个文件一分为2. 如上面的两块的2.55M .最后生成最终存储的文件即虚拟文件. 接下来他就按照这个规划帮你进行切片. 生成切片的过程中他会判断你虚拟存储的文件大小是否大于我们设置的值(4M). 如果大于则单独形成一个切片.  如果你虚拟存储的大小不大于4M. 那么他就会和你下一个虚拟存储文件进行合并, 共同形成一个切片.  如上图的1.7M+2.55M 形成一个切片.  最终形成3个切片. 三个切片不会差太多.这就规避了数据倾斜问题.</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/job7.png" width = "1201.56" height = "600" alt="xubatian的博客" align="center" /><p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/job8.png" width = "1201.56" height = "600" alt="xubatian的博客" align="center" />“ </p><h3 id="1、应用场景："><a href="#1、应用场景：" class="headerlink" title="1、应用场景："></a>1、应用场景：</h3><p>CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。</p><h3 id="2、虚拟存储切片最大值设置"><a href="#2、虚拟存储切片最大值设置" class="headerlink" title="2、虚拟存储切片最大值设置"></a>2、虚拟存储切片最大值设置</h3><p>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m<br>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</p><h3 id="3、切片机制"><a href="#3、切片机制" class="headerlink" title="3、切片机制"></a>3、切片机制</h3><p>生成切片过程包括：虚拟存储过程和切片过程二部分。</p><h2 id="自定义InputFormat"><a href="#自定义InputFormat" class="headerlink" title="自定义InputFormat"></a>自定义InputFormat</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">在企业开发中，Hadoop框架自带的InputFormat类型不能满足所有应用场景，需要自定义InputFormat来解决实际问题。</span><br><span class="line">自定义InputFormat步骤如下：</span><br><span class="line">（1）自定义一个类继承FileInputFormat。</span><br><span class="line">（2）改写RecordReader，实现一次读取一个完整文件封装为KV。</span><br><span class="line">（3）在输出时使用SequenceFileOutPutFormat输出合并文件。</span><br><span class="line"></span><br><span class="line">无论HDFS还是MapReduce，在处理小文件时效率都非常低，但又难免面临处理大量小文件的场景，此时，就需要有相应解决方案。可以自定义InputFormat实现小文件的合并。</span><br><span class="line"></span><br><span class="line">比如说我有n多个小文件,我想将他存到HDFS中,之前的一种方式是使用har文件. </span><br><span class="line">但是现在我想把我的n多个小文件最终存到一个文件里面.就是死将这n多个小文件里面的内容拿出来存到一个文件里面,存成一个稍微大一点的文件. 但是我不能简单存成一个txt文件. 因为这样的话就会乱了.就揉到一起了.  我还是希望你存到这大文件里面以后他们之间还是有一个很明确的划分的. 所以这一次我会用到一个SequenceFlie.  SequenceFile就是一个序列文件. 这个文件的格式比较特殊. 虽然是一个文件, 但是它里面也是以k,v 的形式来存的. k 可以是原来文件的路径.v是文件的内容 (D:/one.txt  , 文件内容  ) . 虽然说我将多个文件存到一个文件当中, 但是这个文件里面他也是有明确的划分的.  将来我们去读的时候也是可以通过 k, v 的形式去读出来. 而且这种格式存起来特别的紧凑.  比如原先我n个小文件存起来是5M, 但是通过 SequenceFile这种方式存的话可能就 3M. 因为他是二进制的存储方式.  sequenceFile这种文件的格式hadoop是支持的 , 但是我们得自己操作, 将小文件里面的内容读出来,以 k ,v 的形式给他写进去.这个过程需要我们自己做.所以我在读我们每一个小文件的时候希望一次性将文件里面的内容都读出来然后向sequenceFile中去放. 比如说k 就是我们小文件的路径加名字. v就是我文件的内容. 我一次性将这个k,v 写到sequenceFile中.  但是我们上面的InputFormat没有一次性读整个文件的. 都是一行一行读的. 所以这就需要我们自定义了.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>具体案例: 略.</p><h1 id="MapReduce工作流程"><a href="#MapReduce工作流程" class="headerlink" title="MapReduce工作流程"></a>MapReduce工作流程</h1><p>1.MapReduce工作流程:</p><p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/map1.png" width = "700" height = "400" alt="xubatian的博客" align="center" />“ </p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/map2.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><p>2.流程详解:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">上面的流程是整个MapReduce最全工作流程，但是Shuffle过程只是从第<span class="number">7</span>步开始到第<span class="number">16</span>步结束，具体Shuffle过程详解，如下：</span><br><span class="line"></span><br><span class="line"><span class="number">1</span>）MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>）多个溢出文件会被合并成大的溢出文件</span><br><span class="line"></span><br><span class="line"><span class="number">4</span>）在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序</span><br><span class="line"></span><br><span class="line"><span class="number">5</span>）ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据</span><br><span class="line"></span><br><span class="line"><span class="number">6</span>）ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）</span><br><span class="line"></span><br><span class="line"><span class="number">7</span>）合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>．注意</span><br><span class="line">Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。</span><br><span class="line">缓冲区的大小可以通过参数调整，参数：io.sort.mb默认100M。</span><br><span class="line"></span><br><span class="line"><span class="number">4</span>．源码解析流程</span><br><span class="line">context.write(k, NullWritable.get());</span><br><span class="line">output.write(key, value);</span><br><span class="line">collector.collect(key, value,partitioner.getPartition(key, value, partitions));</span><br><span class="line">HashPartitioner();</span><br><span class="line">collect()</span><br><span class="line">close()</span><br><span class="line">collect.flush()</span><br><span class="line">          sortAndSpill()</span><br><span class="line">         sort()  </span><br><span class="line">          <span class="function">QuickSort</span></span><br><span class="line"><span class="function"><span class="title">mergeParts</span><span class="params">()</span></span>;</span><br><span class="line">          file.out;</span><br><span class="line">file.out.index;</span><br><span class="line">collector.close();</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;孤独这两个字拆开来看，有孩童，有瓜果，有小犬，有蚊蝇，足以撑起一个盛夏傍晚间的巷子口，人情味十足。 稚儿擎瓜柳棚下，细犬逐蝶窄巷中，人间繁华多笑语，惟我空余两鬓风。 孩童水果猫狗飞蝇当然热闹，可都和你无关，这就叫孤独。 …                            &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>hadoop组成模块之mapreduce的MapTask机制和reduceTask机制</title>
    <link href="http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8Bmapreduce%E7%9A%84MapTask%E6%9C%BA%E5%88%B6%E5%92%8CreduceTask%E6%9C%BA%E5%88%B6/"/>
    <id>http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8Bmapreduce%E7%9A%84MapTask%E6%9C%BA%E5%88%B6%E5%92%8CreduceTask%E6%9C%BA%E5%88%B6/</id>
    <published>2022-01-14T14:22:08.000Z</published>
    <updated>2022-01-14T15:26:25.285Z</updated>
    
    <content type="html"><![CDATA[<p>MapReduce程序分为map阶段,和reduce阶段. map阶段有mapTask任务,reduce阶段有reduceTask任务.</p><p>那么什么是mapTask? 什么是reduceTask?</p><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_76.jpg" width = "1201.56" height = "675.88" alt="xubatian的博客" align="center" /><h1 id="MapTask工作机制"><a href="#MapTask工作机制" class="headerlink" title="MapTask工作机制"></a>MapTask工作机制</h1><h2 id="MapTask流程图解读"><a href="#MapTask流程图解读" class="headerlink" title="MapTask流程图解读"></a>MapTask流程图解读</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_78.png" width = "900.56" height = "450" alt="xubatian的博客" align="center" /><p>（1）Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。</p><p>（2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。</p><p>（3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部,</p><p>​          它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。</p><p>   (4)  Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入   </p><p>​          本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p><h2 id="溢写阶段详情解读"><a href="#溢写阶段详情解读" class="headerlink" title="溢写阶段详情解读"></a>溢写阶段详情解读</h2><p>步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，   经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。</p><p>步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p><p>步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。</p><p>Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p><p>在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p><p>让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p><h1 id="ReduceTask工作机制"><a href="#ReduceTask工作机制" class="headerlink" title="ReduceTask工作机制"></a>ReduceTask工作机制</h1><ol><li>runTasks(reduceRunnables, reduceService, “reduce”); 准备执行所有的ReduceTaskRunnable</li><li>执行ReduceTaskRunnable 的run方法</li><li>创建ReduceTask对象  ReduceTask reduce = new ReduceTask(systemJobFile.toString(),reduceId, taskId, mapIds.size(), 1);</li><li>执行ReduceTask的run方法  reduce.run(localConf, Job.this);</li><li>runNewReducer(job, umbilical, reporter, rIter, comparator, keyClass, valueClass);</li><li>reducer.run(reducerContext);</li><li>执行到Reducer中的reduce方法。</li></ol><h2 id="ReduceTask流程图解读"><a href="#ReduceTask流程图解读" class="headerlink" title="ReduceTask流程图解读"></a>ReduceTask流程图解读</h2><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_91.png" width = "900.56" height = "450" alt="xubatian的博客" align="center" /><p>​    （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上， 否则直接放到内存中。<br>​    （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。<br>​    （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。<br>​    （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。</p><h2 id="设置ReduceTask并行度（个数）"><a href="#设置ReduceTask并行度（个数）" class="headerlink" title="设置ReduceTask并行度（个数）"></a>设置ReduceTask并行度（个数）</h2><p>ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 默认值是1，手动设置为4</span><br><span class="line"></span><br><span class="line">job.setNumReduceTasks(4);</span><br></pre></td></tr></table></figure><h2 id="ReduceTask注意事项"><a href="#ReduceTask注意事项" class="headerlink" title="ReduceTask注意事项"></a>ReduceTask注意事项</h2><p>（1）ReduceTask=0，表示没有Reduce阶段，输出文件个数和Map个数一致。<br>（2）ReduceTask默认值就是1，所以输出文件个数为一个。<br>（3）如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜<br>（4）ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask。<br>（5）具体多少个ReduceTask，需要根据集群性能而定。<br>（6）如果分区数不是1，但是ReduceTask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;MapReduce程序分为map阶段,和reduce阶段. map阶段有mapTask任务,reduce阶段有reduceTask任务.&lt;/p&gt;
&lt;p&gt;那么什么是mapTask? 什么是reduceTask?&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="MapReduce" scheme="http://xubatian.cn/tags/MapReduce/"/>
    
    <category term="MapTask" scheme="http://xubatian.cn/tags/MapTask/"/>
    
  </entry>
  
  <entry>
    <title>Java</title>
    <link href="http://xubatian.cn/Java/"/>
    <id>http://xubatian.cn/Java/</id>
    <published>2022-01-14T09:53:14.000Z</published>
    <updated>2022-01-15T06:49:38.074Z</updated>
    
    <content type="html"><![CDATA[<p>侧脸 偷看 数学 操场 阳光 篮球 这是暗恋 情书 摸头 换座位 讲题 壁咚 陪跑 这是明恋       –来自网易音乐《Abelvolks》                            </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.xn_02.gif" width = "1201.56" height = "675.88" alt="xubatian的博客" align="center" />]]></content>
    
    
    <summary type="html">&lt;p&gt;侧脸 偷看 数学 操场 阳光 篮球 这是暗恋 情书 摸头 换座位 讲题 壁咚 陪跑 这是明恋       –来自网易音乐《Abelvolks》                            &lt;/p&gt;</summary>
    
    
    
    <category term="Java" scheme="http://xubatian.cn/categories/Java/"/>
    
    
  </entry>
  
  <entry>
    <title>纪念册</title>
    <link href="http://xubatian.cn/%E7%BA%AA%E5%BF%B5%E5%86%8C/"/>
    <id>http://xubatian.cn/%E7%BA%AA%E5%BF%B5%E5%86%8C/</id>
    <published>2022-01-14T09:40:55.000Z</published>
    <updated>2022-01-15T07:38:43.564Z</updated>
    
    <content type="html"><![CDATA[<p>现在我也用上了.那年,那月,那一天…. 回忆带着苦涩….</p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_01.gif" width = "1201.56" height = "675.88" alt="xubatian的博客" align="center" />]]></content>
    
    
    <summary type="html">&lt;p&gt;现在我也用上了.那年,那月,那一天…. 回忆带着苦涩….&lt;/p&gt;</summary>
    
    
    
    <category term="纪念册" scheme="http://xubatian.cn/categories/%E7%BA%AA%E5%BF%B5%E5%86%8C/"/>
    
    
    <category term="纪念册" scheme="http://xubatian.cn/tags/%E7%BA%AA%E5%BF%B5%E5%86%8C/"/>
    
  </entry>
  
  <entry>
    <title>动态</title>
    <link href="http://xubatian.cn/%E5%8A%A8%E6%80%81/"/>
    <id>http://xubatian.cn/%E5%8A%A8%E6%80%81/</id>
    <published>2022-01-14T09:12:58.000Z</published>
    <updated>2022-01-15T07:38:31.573Z</updated>
    
    <content type="html"><![CDATA[<p>理想三旬♡ 一旬败给年少轻狂的理想 二旬败给青涩无知的爱情 三旬败给沧桑寂寥的现实        –来自网易云音乐《 理想三旬》                            </p><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_74.png" width = "1201.56" height = "675.88" alt="xubatian的博客" align="center" />]]></content>
    
    
    <summary type="html">&lt;p&gt;理想三旬♡ 一旬败给年少轻狂的理想 二旬败给青涩无知的爱情 三旬败给沧桑寂寥的现实        –来自网易云音乐《 理想三旬》                            &lt;/p&gt;</summary>
    
    
    
    <category term="动态" scheme="http://xubatian.cn/categories/%E5%8A%A8%E6%80%81/"/>
    
    
    <category term="动态" scheme="http://xubatian.cn/tags/%E5%8A%A8%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>hadoop组成模块之Yarn-HA高可用</title>
    <link href="http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BYarn-HA%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    <id>http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BYarn-HA%E9%AB%98%E5%8F%AF%E7%94%A8/</id>
    <published>2022-01-14T07:54:34.000Z</published>
    <updated>2022-01-14T12:35:00.010Z</updated>
    
    <content type="html"><![CDATA[<p>“一起看日落的人比日落更浪漫。”       –来自网易有音乐《你站在日落的海平面》                                                        </p><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.2/aliyun/www.xubatian.cn_29.jpg" width = "1201.56" height = "675.88" alt="xubatian的博客" align="center" /><h1 id="YARN-HA配置"><a href="#YARN-HA配置" class="headerlink" title="YARN-HA配置"></a>YARN-HA配置</h1><h2 id="YARN-HA工作机制"><a href="#YARN-HA工作机制" class="headerlink" title="YARN-HA工作机制"></a>YARN-HA工作机制</h2><h3 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a>官方文档</h3><p><a href="https://hadoop.apache.org/docs/r3.1.3/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">https://hadoop.apache.org/docs/r3.1.3/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html</a></p><h3 id="YARN-HA工作机制-1"><a href="#YARN-HA工作机制-1" class="headerlink" title="YARN-HA工作机制"></a>YARN-HA工作机制</h3><p>如图所示:</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_70.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><h3 id="配置YARN-HA集群"><a href="#配置YARN-HA集群" class="headerlink" title="配置YARN-HA集群"></a>配置YARN-HA集群</h3><ol><li>环境准备<br> （1）修改IP<br> （2）修改主机名及主机名和IP地址的映射<br> （3）关闭防火墙<br> （4）ssh免密登录<br> （5）安装JDK，配置环境变量等<br> （6）配置Zookeeper集群</li><li>   集群规划</li></ol><pre><code>| hadoop102       | hadoop103       | hadoop104   || --------------- | --------------- | ----------- || JournalNode     | JournalNode     | JournalNode || NameNode        | NameNode        |             || DataNode        | DataNode        | DataNode    || ZK              | ZK              | ZK          || ResourceManager | ResourceManager |             || NodeManager     | NodeManager     | NodeManager |</code></pre><ol start="3"><li>   具体配置</li></ol><p>​        （1）yarn-site.xml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">  &lt;configuration&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!--启用resourcemanager ha--&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!--声明两台resourcemanager的地址--&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;cluster-yarn1&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;rm1,rm2&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hadoop102&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hadoop103&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!--指定zookeeper集群的地址--&gt; </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!--启用自动恢复--&gt; </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt; </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">             &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;</span><br><span class="line">             &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>​      (2) 同步更新其他节点的配置信息</p><ol start="4"><li><p>启动hdfs[第一次启动]<br>（1）在各个JournalNode节点上，输入以下命令启动journalnode服务</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure><p>（2）在[nn1]上，对其进行格式化，并启动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs namenode -format</span><br><span class="line">sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure><p>（3）在[nn2]上，同步nn1的元数据信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure><p>（4）启动[nn2]</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure><p>（5）启动所有DataNode</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemons.sh start datanode</span><br></pre></td></tr></table></figure><p>（6）将[nn1]切换为Active</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs haadmin -transitionToActive nn1</span><br></pre></td></tr></table></figure></li><li><p>启动YARN<br>（1）在hadoop102中执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><p>（2）在hadoop103中执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure><p>（3）查看服务状态，如图所示</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn rmadmin -getServiceState rm1</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_71.png" width = "700" height = "400" alt="xubatian的博客" align="center" /></li></ol><p>​        查看所有进程:</p><p>​                                     <img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_72.png" width = "700" height = "400" alt="xubatian的博客" align="center" /></p><h2 id="HDFS-Federation架构设计"><a href="#HDFS-Federation架构设计" class="headerlink" title="HDFS Federation架构设计"></a>HDFS Federation架构设计</h2><p>NameNode架构的局限性（内存瓶颈, 毕竟是一台服务器嘛, 内存在高也就那么多）<br>（1）Namespace（命名空间）的限制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">由于NameNode在内存中存储所有的元数据（metadata），因此单个NameNode所能存储的对象（文件+块）数目受到NameNode  所在JVM的heap size(堆大小)的限制。50G的heap(堆)能够存储20亿（200million）个对象，这20亿个对象支持4000个DataNode，12PB的存储（假设文件平均大小为40MB）。随着数据的飞速增长，存储的需求也随之增长。单个DataNode从4T增长到36T，集群的尺寸增长到8000个DataNode。存储的需求从12PB增长到大于100PB。</span><br></pre></td></tr></table></figure><p>（2）隔离问题</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">由于HDFS仅有一个NameNode，无法隔离各个程序，因此HDFS上的一个实验程序就很有可能影响整个HDFS上运行的程序。</span><br></pre></td></tr></table></figure><p>（3）性能的瓶颈</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">由于是单个NameNode的HDFS架构，因此整个HDFS文件系统的吞吐量受限于单个NameNode的吞吐量。</span><br></pre></td></tr></table></figure><ol><li><p>HDFS Federation架构设计，如图所示<br>能不能有多个NameNode</p><table><thead><tr><th>NameNode</th><th>NameNode</th><th>NameNode</th></tr></thead><tbody><tr><td>元数据</td><td>元数据</td><td>元数据</td></tr><tr><td>Log</td><td>machine</td><td>电商数据/话单数据</td></tr></tbody></table></li></ol><p>HDFS Federation架构设计图:</p><p>​                <img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_73.png" width = "700" height = "400" alt="xubatian的博客" align="center" /></p><p>​        </p><h2 id="HDFS-Federation应用思考"><a href="#HDFS-Federation应用思考" class="headerlink" title="HDFS Federation应用思考"></a>HDFS Federation应用思考</h2><p>不同应用可以使用不同NameNode进行数据管理<br>        图片业务、爬虫业务、日志审计业务<br>Hadoop生态系统中，不同的框架使用不同的NameNode进行管理NameSpace。（隔离性）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;“一起看日落的人比日落更浪漫。”       –来自网易有音乐《你站在日落的海平面》                                                        &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="Yarn" scheme="http://xubatian.cn/tags/Yarn/"/>
    
    <category term="Yarn HA" scheme="http://xubatian.cn/tags/Yarn-HA/"/>
    
  </entry>
  
  <entry>
    <title>hadoop组成模块之HDFS HA高可用</title>
    <link href="http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BHDFS-HA%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    <id>http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BHDFS-HA%E9%AB%98%E5%8F%AF%E7%94%A8/</id>
    <published>2022-01-14T06:55:00.000Z</published>
    <updated>2022-01-14T09:09:25.934Z</updated>
    
    <content type="html"><![CDATA[<p>或许是物质世界泛滥了 人们越来越孤独 或许是现实形式太泛泛了 人们越来越追求简单淡泊了 或许是功利的人与事太思空见惯了 所以很多人灵魂深处渴望一位知己。 只知我曲中暖，却不知我心寒…       –来自网易云音乐《弦外知音》                            </p><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.2/aliyun/www.xubatian.cn_28.png" width = "1201.56" height = "675.88" alt="xubatian的博客" align="center" /><h1 id="HA概述"><a href="#HA概述" class="headerlink" title="HA概述"></a>HA概述</h1><h2 id="为什么会提出高可用这个概念呢"><a href="#为什么会提出高可用这个概念呢" class="headerlink" title="为什么会提出高可用这个概念呢?"></a><strong>为什么会提出高可用这个概念呢</strong>?</h2><p>比如说我有一个对外提供服务的集群. 我对外提供服务的集群是不能随随便便就停止的. 提出高可用的原因是,在我们的hadoop中,比如像我们hdfs,yarn中,这两个都会存在一个问题. 这个问题就是单点故障问题. 所谓的单点故障就是说, 在我们的hdfs或者yarn中, 只要其中一个节点出现故障,其实你整个hdfs或者yarn就不能正常工作了. hdfs说的是namenode, yarn说的是ResourceManager. 为什么会这样呢? 因为现在对于我的hdfs来说,我有一个namenode ,下面有许多个DataNode. 这就构成整个集群.  其实在整个集群中他都是围绕着namenode来进行工作的.  所以你发现, 当namenode出现问题的时候. 即使你DataNode还好好的,但是我们的DataNode是不能直接对外提供服务的. 因为我整个集群的核心是namenode.  所以说只要namenode节点故障了. 其实对我整个集群来讲,他就故障了.  就不能对外提供服务了.  yarn也是一样 . 我们yarn的NodeManager首先得先和我们的NodeManager进行交互. 如果我们NodeManager挂了. 整个集群就停止了对外服务了. </p><p>说明单点故障不是hdfs或者yarn上面有, 而是我整个分布式里面都有. 只要你整个分布式,整个集群,如果他都要围绕着某一个节点来干活,它都存在单点故障问题.</p><p>Redis也是有. 他有主从关系.  他是如何解决单点故障呢? 是主挂掉,从上位.</p><h2 id="NameNode和secondNameNode关系"><a href="#NameNode和secondNameNode关系" class="headerlink" title="NameNode和secondNameNode关系?"></a><strong>NameNode和secondNameNode关系?</strong></h2><p> secondNameNode不是NameNode的热备. 因为他不能替换NameNode去干活. secondNameNode只是辅助namenode去工作. 比如说合并编辑日志.  所以说,当我的NameNode挂掉之后, 我的secondNameNode只是保证我的数据不丢失,不能保证我的机器能正常干活.</p><p>那我的高可用如何去解决这个问题呢? 那就是弄两个NameNode.</p><p>但是两个NameNode不能同时进行工作. </p><p>所以我们一个是作为active活跃状态. 一个作为standby时刻准备着.</p><p>我们的active活跃状态的NameNode是正在对外提供服务的. 那就意味这将来客户端的读写请求操作都要经过active活跃状态的NameNode. </p><p>如果说我们一个写请求,正在操作活跃状态的NameNode, 但是突然挂掉了.另一台standby状态的NameNode如何和active状态的保持一致呢? </p><p>即 ,如何实现active状态的NameNode和standby状态的NameNode数据同步问题?</p><p>这个时候光靠两台NameNode,即一台active状态,一台standby状态. 已经不行了. 这时候就有一个叫journalNode来了. 它实际上也是一个进程. 他和NameNode,DataNode,secondNameNode一样的, 也是一个进程. 他就是用来完成一台active状态的NameNode和一台standby状态的NameNode,这来两台数据同步性的中间进程.    将来我客户端操作active(活跃)状态的NameNode时候, 不仅在内存中修改. 还会写自己的编辑日志, 并且要往journalNode中去写一份. 注意.active(活跃)状态的NameNode往journalnode中去写, standby状态的NameNode往journalName里面去读.这就解决数据同步性的问题.</p><p>假如说我这个active状态的NameNode性能超级强悍. 他一直都不会出现问题. 这个时候我们客户端一致对Actvie状态的NameNode做修改的操作. 那么他的编辑日志会越来越大. 当你达到一定程度, 我们是不是得合并编辑日志呢. 我们合并编辑日志是secondNameNode做的. 但是在hadoopHA高可用情况下, 有了standby状态的NameNode时候,就不需要配置secondNameNode了. 因为我们的standby状态的NameNode就可以完成合并编辑日志的功能.而且当active状态的NameNode出现故障的时候, 我们standby状态的NameNode立马能提供服务.</p><p>实际上,NameNode,secondNameNode,等都是对应的一个java类. </p><h2 id="总结什么是HDFS-HA"><a href="#总结什么是HDFS-HA" class="headerlink" title="总结什么是HDFS HA"></a>总结什么是HDFS HA</h2><p>1）所谓HA（High Availablity），即高可用（7*24小时不中断服务）。<br>2）实现高可用最关键的策略是消除单点故障。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。<br>3）Hadoop2.0之前，在HDFS集群中NameNode存在单点故障SPOF(单点故障)（Single Points Of Failure）。<br>4）NameNode主要在以下两个方面影响HDFS集群<br>      NameNode机器发生意外，如宕机，集群将无法使用，直到管理员重启<br>      NameNode机器需要升级，包括软件、硬件升级，此时集群也将无法使用<br>HDFS HA功能通过配置Active/Standby两个NameNodes实现在集群中对NameNode的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，<br>这时可通过此种方式将NameNode很快的切换到另外一台机器。</p><p>说白了就是搞多台主机, 其中一台挂了能迅速切换到另一台. 不影响整个集群的使用.</p><h2 id="HDFS-HA工作机制"><a href="#HDFS-HA工作机制" class="headerlink" title="HDFS-HA工作机制"></a>HDFS-HA工作机制</h2><p>通过双NameNode消除单点故障.</p><h2 id="HDFS-HA手动故障转移-不好用"><a href="#HDFS-HA手动故障转移-不好用" class="headerlink" title="HDFS-HA手动故障转移(不好用)"></a>HDFS-HA手动故障转移(不好用)</h2><p>如果一台主机挂了,咱们手动让另一台主机代替挂掉的主机工作,从未不影响集群工作.    </p><p>手动进行故障转移，在该模式下，即使现役NameNode已经失效，系统也不会自动从现役NameNode转移到待机NameNode</p><h2 id="配置HDFS-HA自动故障转移-用到了zookeeper"><a href="#配置HDFS-HA自动故障转移-用到了zookeeper" class="headerlink" title="配置HDFS-HA自动故障转移(用到了zookeeper)"></a>配置HDFS-HA自动故障转移(用到了zookeeper)</h2><p>如果一台主机挂了,zookeeper让另一台主机代替挂掉的主机工作,从未不影响集群工作. 提前是在zookeeper上配置好.    </p><p>自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）这是hadoop的进程,不是zookeeper的进程。ZKFC可以简单理解为zookeeper的客户端. ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。HA的自动故障转移依赖于ZooKeeper</p><p>注意: </p><p>ZKFailoverController(ZKFC)是一个新的组件，它是一个ZooKeeper客户端，它还监视和管理NameNode的状态。运行NameNode的每台机器也运行ZKFC，他们之间是一对一的关系。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>）故障检测：集群中的每个NameNode在ZooKeeper中维护了一个持久会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。</span><br><span class="line"><span class="number">2</span>）现役NameNode选择：ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。</span><br></pre></td></tr></table></figure><h2 id="ZKFC负责那些工作"><a href="#ZKFC负责那些工作" class="headerlink" title="ZKFC负责那些工作"></a>ZKFC负责那些工作</h2><p>1）健康监测：ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认      为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。<br>2）ZooKeeper会话管理：当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状      态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。<br>3）基于ZooKeeper的选择：如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如        果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为Active。故障转移进程与前面描述的手动故障转         移相似，首先如果必要保护之前的现役NameNode，然后本地NameNode转换为Active状态</p><p>1.具体配置</p><p>(1) 在hdfs-site.xml中增加</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p> (2) 在core-site.xml文件中增加</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br></pre></td></tr></table></figure><ol start="2"><li> 启动</li></ol>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@Hadoop102 hadoop]$ cd ~</span><br><span class="line">[shangbaishuyao@Hadoop102 ~]$ cd bin/</span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ vim startZookeeper</span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ vim startZookeeper</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">for i in shangbaishuyao@Hadoop102 shangbaishuyao@Hadoop103 shangbaishuyao@Hadoop104</span><br><span class="line">do</span><br><span class="line">        echo&quot;==========================start $i Zookeeper===============================&quot;</span><br><span class="line">        ssh $i &#x27;/opt/module/zookeeper-3.4.10/bin/zkServer.sh start&#x27;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ chmod 777 startZookeeper</span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 170 3月   1 09:58 myjps</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 249 3月  10 13:21 startZookeeper</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 499 2月  29 20:16 xsync</span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ cp startZookeeper statusZookeeper</span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ ll</span><br><span class="line">总用量 16</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 170 3月   1 09:58 myjps</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 249 3月  10 13:21 startZookeeper</span><br><span class="line">-rwxrwxr-x. 1 shangbaishuyao shangbaishuyao 249 3月  10 13:25 statusZookeeper</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 499 2月  29 20:16 xsync</span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ vim statusZookeeper </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">for i in shangbaishuyao@Hadoop102 shangbaishuyao@Hadoop103 shangbaishuyao@Hadoop104</span><br><span class="line">do</span><br><span class="line">        echo&quot;==========================status $i Zookeeper===============================&quot;</span><br><span class="line">        ssh $i &#x27;/opt/module/zookeeper-3.4.10/bin/zkServer.sh status&#x27;</span><br><span class="line">done</span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ </span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ cp startZookeeper stopZookeeper</span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ vim stopZookeer</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">for i in shangbaishuyao@Hadoop102 shangbaishuyao@Hadoop103 shangbaishuyao@Hadoop104</span><br><span class="line">do</span><br><span class="line">        echo&quot;==========================stop $i Zookeeper===============================&quot;</span><br><span class="line">        ssh $i &#x27;/opt/module/zookeeper-3.4.10/bin/zkServer.sh stop&#x27;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ ll</span><br><span class="line">总用量 20</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 170 3月   1 09:58 myjps</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 249 3月  10 13:21 startZookeeper</span><br><span class="line">-rwxrwxr-x. 1 shangbaishuyao shangbaishuyao 251 3月  10 13:26 statusZookeeper</span><br><span class="line">-rwxrwxr-x. 1 shangbaishuyao shangbaishuyao 247 3月  10 13:29 stopZookeeper</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 499 2月  29 20:16 xsync</span><br></pre></td></tr></table></figure><p>   以上操作完成后还是起不起来,有问题,因为他ssh过去之后他不会去加载profile,就意味着他找不到java,因为我的java环境变量是在这里面配置的,所以我需要他ssh过去之后可以加载java文件:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@Hadoop102 ~]$ cd /opt/module/zookeeper-3.4.10/</span><br><span class="line">[shangbaishuyao@Hadoop102 zookeeper-3.4.10]$ start</span><br><span class="line">start                 start-all.sh          start-dfs.cmd         start-pulseaudio-x11  start_udev            start-yarn.cmd        startZookeeper        </span><br><span class="line">start-all.cmd         start-balancer.sh     start-dfs.sh          start-secure-dns.sh   startx                start-yarn.sh         </span><br><span class="line">[shangbaishuyao@Hadoop102 zookeeper-3.4.10]$ startZookeeper </span><br><span class="line">/home/shangbaishuyao/bin/startZookeeper: line 4: echo==========================start shangbaishuyao@Hadoop102 Zookeeper===============================: command not found</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">/home/shangbaishuyao/bin/startZookeeper: line 4: echo==========================start shangbaishuyao@Hadoop103 Zookeeper===============================: command not found</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">/home/shangbaishuyao/bin/startZookeeper: line 4: echo==========================start shangbaishuyao@Hadoop104 Zookeeper===============================: command not found</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">[shangbaishuyao@Hadoop102 zookeeper-3.4.10]$ statusZookeeper </span><br><span class="line">/home/shangbaishuyao/bin/statusZookeeper: line 4: echo==========================status shangbaishuyao@Hadoop102 Zookeeper===============================: command not found</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Error contacting service. It is probably not running.</span><br><span class="line">/home/shangbaishuyao/bin/statusZookeeper: line 4: echo==========================status shangbaishuyao@Hadoop103 Zookeeper===============================: command not found</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Error contacting service. It is probably not running.</span><br><span class="line">/home/shangbaishuyao/bin/statusZookeeper: line 4: echo==========================status shangbaishuyao@Hadoop104 Zookeeper===============================: command not found</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Error contacting service. It is probably not running.</span><br><span class="line">[shangbaishuyao@Hadoop102 zookeeper-3.4.10]$ myjps</span><br><span class="line">========================hadoop102========================</span><br><span class="line">2783 Jps</span><br><span class="line">========================hadoop103========================</span><br><span class="line">4111 Jps</span><br><span class="line">========================hadoop104========================</span><br></pre></td></tr></table></figure><p>解决办法是在隐藏文件中</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@Hadoop102 zookeeper-3.4.10]$ cd ~</span><br><span class="line">[shangbaishuyao@Hadoop102 ~]$ ll -a</span><br><span class="line">总用量 60</span><br><span class="line">drwx------. 7 shangbaishuyao shangbaishuyao 4096 3月  10 13:31 .</span><br><span class="line">drwxr-xr-x. 3 root           root           4096 2月  27 14:12 ..</span><br><span class="line">-rw-------. 1 shangbaishuyao shangbaishuyao 3554 3月   6 13:23 .bash_history</span><br><span class="line">-rw-r--r--. 1 shangbaishuyao shangbaishuyao   18 5月  11 2016 .bash_logout</span><br><span class="line">-rw-r--r--. 1 shangbaishuyao shangbaishuyao  176 5月  11 2016 .bash_profile</span><br><span class="line">-rw-r--r--. 1 shangbaishuyao shangbaishuyao  124 5月  11 2016 .bashrc</span><br><span class="line">drwxrwxr-x. 2 shangbaishuyao shangbaishuyao 4096 3月  10 13:29 bin</span><br><span class="line">drwxr-xr-x. 2 shangbaishuyao shangbaishuyao 4096 11月 12 2010 .gnome2</span><br><span class="line">drwxr-xr-x. 4 shangbaishuyao shangbaishuyao 4096 2月  27 19:53 .mozilla</span><br><span class="line">drwxrwxr-x. 2 shangbaishuyao shangbaishuyao 4096 2月  27 16:21 .oracle_jre_usage</span><br><span class="line">drwx------. 2 shangbaishuyao shangbaishuyao 4096 3月   1 00:41 .ssh</span><br><span class="line">-rw-------. 1 shangbaishuyao shangbaishuyao 8777 3月  10 13:29 .viminfo</span><br><span class="line">-rw-rw-r--. 1 shangbaishuyao shangbaishuyao   61 3月  10 13:31 zookeeper.out</span><br><span class="line">[shangbaishuyao@Hadoop102 ~]$ vim .bashrc </span><br><span class="line"></span><br><span class="line"># .bashrc</span><br><span class="line"></span><br><span class="line"># Source global definitions</span><br><span class="line"></span><br><span class="line">if [ -f /etc/bashrc ]; then</span><br><span class="line">        . /etc/bashrc</span><br><span class="line">fi</span><br><span class="line">JAVA_HOME=/opt/module/jdk1.8.0_144</span><br><span class="line">export JAVA_HOME</span><br><span class="line"></span><br><span class="line"># User specific aliases and functions</span><br><span class="line"></span><br><span class="line">~                                        </span><br><span class="line">[shangbaishuyao@Hadoop102 ~]$ xsync .bashrc</span><br><span class="line">[shangbaishuyao@Hadoop102 ~]$ echo $JAVA_HOME</span><br><span class="line">/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;或许是物质世界泛滥了 人们越来越孤独 或许是现实形式太泛泛了 人们越来越追求简单淡泊了 或许是功利的人与事太思空见惯了 所以很多人灵魂深处渴望一位知己。 只知我曲中暖，却不知我心寒…       –来自网易云音乐《弦外知音》                            &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="HDFS" scheme="http://xubatian.cn/tags/HDFS/"/>
    
    <category term="HDFS HA" scheme="http://xubatian.cn/tags/HDFS-HA/"/>
    
  </entry>
  
  <entry>
    <title>从Hadoop框架讨论大数据生态</title>
    <link href="http://xubatian.cn/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E8%AE%A8%E8%AE%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/"/>
    <id>http://xubatian.cn/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E8%AE%A8%E8%AE%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/</id>
    <published>2022-01-13T05:53:25.434Z</published>
    <updated>2022-01-14T07:09:58.999Z</updated>
    
    <content type="html"><![CDATA[<p>若是问我最想和谁在一起 我想到的只有你 我既想缠着你 又想放弃你 又想跟你联系 又不想跟你联系 既想慢慢退出你的世界 又怕失去你 终其一生满是遗憾      –来自网易音乐《要命》              </p><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/images/11.jpg" width = "1201.56" height = "675.88" alt="xubatian的博客" align="center" /><h1 id="大数据和hadoop的关系"><a href="#大数据和hadoop的关系" class="headerlink" title="大数据和hadoop的关系"></a>大数据和hadoop的关系</h1><p>上文说道 <strong>大数据</strong> 其实说白了就是主要解决海量数据的存储和海量数据的分析计算问题.</p><p> 那么这个问题是如何解决的呢? 是有什么框架或者说什么工具来解决呢的? </p><p>答案就是 hadoop.  它是整个大数据体系中最主要的也是最核心的部分.</p><p> 因为它解决了大数据的痛点: 海量数据的存储问题,分析计算问题.</p><p>所以,要讨论大数据,那么 hadoop就是它的起点…因为解决了海量数据的存储和计算问题呀!</p><p>&emsp;     </p><p>在知道了什么是大数据, 为什么学习大数据需要从hadoop开始学之后,下面就要具体了解hadoop框架了.</p><p>在我看来学习一个框架最基本的步骤:</p><ol><li>是什么?</li><li>能做什么?</li><li>怎么学?</li></ol><p>那么 下面我们具体看看,hadoop到底是个什么东西? 它是怎么解决海量数据的存储和计算的问题的呢?</p><p>&emsp;     </p><h1 id="Hadoop是什么"><a href="#Hadoop是什么" class="headerlink" title="Hadoop是什么?"></a><a href="https://www.leixue.com/ask/what-is-hadoop?btwaf=52655100">Hadoop是什么?</a></h1><p>备注: hadoop官网地址: <a href="https://hadoop.apache.org/">https://hadoop.apache.org/</a></p><p><strong>首先从hadoop的官网入手,看看官网说的hadoop的是什么?</strong></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/hadoopguangwang1.png" width = "900" height = "500" alt="xubatian的博客" align="center" /><p><strong>由官网总结得出hadoop是什么?</strong></p><p>&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; Hadoop是一个开源软件框架，用于在商用硬件集群上存储数据和运行应用程序。它为任何类型的数据提供海量存储，巨大的处理能力以及处理几乎无限的并发任务或作业的能力。</p><p>① Hadoop是一个由Apache基金会所开发的分布式系统基础架构。<br>② 主要解决，海量数据的存储和海量数据的分析计算问题。<br>③ 广义上来说，Hadoop通常是指一个更广泛的概念-Hadoop生态</p><p><strong>那么什么是hadoop生态呢?</strong> </p><p>hadoop生态是指由hadoop中心衍生的一系列解决大数据问题的一些大数据组件或者框架. 目的依然是针对大数据的海量数据存储和海量数据计算问题开展或者研发的针对不同问题的解决方式.</p><p>如图下图: 展示了大数据是以hadoop为中心的生态体系,所以hadoop是大数据的核心. 他解决了大数据的存储和计算问题.</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/hadoopshengtaiquan2.png" width = "900" height = "500" alt="xubatian的博客" align="center" /><h1 id="Hadoop能做什么"><a href="#Hadoop能做什么" class="headerlink" title="Hadoop能做什么?"></a>Hadoop能做什么?</h1><p> hadoop就是为了解决大数据的痛点而孕育出的,所以hadoop一定是解决了海量数据的存储和计算的问题.但是海量数据的存储和海量数据的计算问题仅仅是大数据问题的解决方案. 比如: hadoop就利用了分布式文件存储来存储大量的数据. 但是因为用到分布式的解决方案,所以,他还得解决分布式方案出现的问题.eg: 容错. </p><p>伴随者一个问题的解决和新问题出现再次解决,最终解决完所有问题得到的hadoop有了这些能力:</p><ol><li><strong>能够快速存储和处理大量任何类型的数据</strong>。随着数据量和品种的不断增加，特别是来自社交媒体和物联网（IoT），这是一个关键考虑因素。</li><li><strong>计算能力</strong>。Hadoop 的分布式计算模型可以快速处理大数据。您使用的计算节点越多，您拥有的处理能力就越强。</li><li><strong>容错</strong>。数据和应用程序处理可防止硬件故障。如果节点发生故障，作业将自动重定向到其他节点，以确保分布式计算不会失败。自动存储所有数据的多个副本。</li><li><strong>灵活性</strong>。与传统的关系数据库不同，您不必在存储数据之前对其进行预处理。您可以根据需要存储尽可能多的数据，并决定以后如何使用它。这包括非结构化数据，如文本，图像和视频。</li><li><strong>低成本</strong>。开源框架是免费的，使用商用硬件来存储大量数据。</li><li><strong>可扩展性</strong>。只需添加节点，您就可以轻松扩展系统以处理更多数据。需要很少的管理。</li></ol><p>既然hadoop能做这些,他能解决大数据问题. 那么难道就没有其他框架可以解决这些问题了吗? 又是什么原因使得hadoop奠定了如今大数据的核心地位呢? 要知道这个我们就必须了解hadoop有哪些优势.</p><p>&emsp;     </p><h1 id="Hadoop的优势是什么"><a href="#Hadoop的优势是什么" class="headerlink" title="Hadoop的优势是什么?"></a>Hadoop的优势是什么?</h1><p>因为hadoop有这些优势,所以同时期,大多数公司更愿意使用hadoop,使得hadoop独领风骚.</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1）高可靠性：Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。</span><br><span class="line">2）高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。</span><br><span class="line">3）高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。</span><br><span class="line">4）高容错性：能够自动将失败的任务重新分配。</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/hadoop102.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/hadoop3.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><p>也正是因为hadoop具有这些优势,使得大数据初期公司都是使用的hadoop来解决大数据问题的. 所以,后期的框架也是由hadoop为基础进行的衍生, 这就造成了hadoop在大数据技术方面不可动摇的核心地位,一直独领风骚到现在.</p><p>&emsp;     </p><p>在了解到hadoop能做什么之后,又为啥学大数据必须用到hadoop之后,我们还需要知道,hadoop从研发到发行有哪些版本.最常用的是什么哪款的?</p><h1 id="hadoop从研发到发行有哪些版本呢"><a href="#hadoop从研发到发行有哪些版本呢" class="headerlink" title="hadoop从研发到发行有哪些版本呢?"></a>hadoop从研发到发行有哪些版本呢?</h1><p>Hadoop三大发行版本</p><p>Hadoop三大发行版本：Apache、Cloudera、Hortonworks。</p><p>Apache版本最原始（最基础）的版本，对于入门学习最好。06年,我们学习的版本</p><p>Cloudera在大型互联网企业中用的较多。09年出来的,收费的</p><p>Hortonworks文档较好。11-12年,收费的</p><p>&emsp; </p><ol><li> Apache Hadoop</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">官网地址：http:<span class="comment">//hadoop.apache.org/releases.html</span></span><br><span class="line"></span><br><span class="line">下载地址：https:<span class="comment">//archive.apache.org/dist/hadoop/common/</span></span><br></pre></td></tr></table></figure><ol start="2"><li> Cloudera Hadoop </li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">官网地址：https:<span class="comment">//www.cloudera.com/downloads/cdh/5-10-0.html</span></span><br><span class="line"></span><br><span class="line">下载地址：http:<span class="comment">//archive-primary.cloudera.com/cdh5/cdh/5/</span></span><br><span class="line"></span><br><span class="line">（<span class="number">1</span>）<span class="number">2008</span>年成立的Cloudera是最早将Hadoop商用的公司，为合作伙伴提供Hadoop的商用解决方案，主要是包括支持、咨询服务、培训。</span><br><span class="line"></span><br><span class="line">（<span class="number">2</span>）<span class="number">2009</span>年Hadoop的创始人Doug Cutting也加盟Cloudera公司。Cloudera产品主要为CDH，Cloudera Manager，Cloudera Support</span><br><span class="line"></span><br><span class="line">（<span class="number">3</span>    CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全性，稳定性上有所增强。</span><br><span class="line"></span><br><span class="line">（<span class="number">4</span>）Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。Cloudera Support即是对Hadoop的技术支持。</span><br><span class="line"></span><br><span class="line">（<span class="number">5</span>）Cloudera的标价为每年每个节点<span class="number">4000</span>美元。Cloudera开发并贡献了可实时处理大数据的Impala项目。</span><br></pre></td></tr></table></figure><ol start="3"><li> Hortonworks Hadoop</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">官网地址：https:<span class="comment">//hortonworks.com/products/data-center/hdp/</span></span><br><span class="line"></span><br><span class="line">下载地址：https:<span class="comment">//hortonworks.com/downloads/#data-platform</span></span><br><span class="line"></span><br><span class="line">（<span class="number">1</span>）<span class="number">2011</span>年成立的Hortonworks是雅虎与硅谷风投公司Benchmark Capital合资组建。</span><br><span class="line"></span><br><span class="line">（<span class="number">2</span>）公司成立之初就吸纳了大约<span class="number">25</span>名至<span class="number">30</span>名专门研究Hadoop的雅虎工程师，上述工程师均在<span class="number">2005</span>年开始协助雅虎开发Hadoop，贡献了Hadoop80%的代码。</span><br><span class="line"></span><br><span class="line">（<span class="number">3</span>）雅虎工程副总裁、雅虎Hadoop开发团队负责人Eric Baldeschwieler出任Hortonworks的首席执行官。</span><br><span class="line"></span><br><span class="line">（<span class="number">4</span>）Hortonworks的主打产品是Hortonworks Data Platform（HDP），也同样是<span class="number">100</span>%开源的产品，HDP除常见的项目外还包括了Ambari，一款开源的安装和管理系统。</span><br><span class="line"></span><br><span class="line">（<span class="number">5</span>）HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。</span><br><span class="line"></span><br><span class="line">（<span class="number">6</span>）Hortonworks开发了很多增强特性并提交至核心主干，这使得Apache Hadoop能够在包括Window Server和Windows Azure在内的Microsoft Windows平台上本地运行。定价以集群为基础，每<span class="number">10</span>个节点每年为<span class="number">12500</span>美元。</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>其中最常用的就是Apache 开源的hadoop版本. 而且不收费,所以很多公司用的就是他,所以他成为了主流.</p><p>&emsp;     </p><p>该了解的都了解清楚了,所以我们需要在正式的学习hadoop了, 因为hadoop框架有四个模块,每个模块又有不同的功能,所以我们需要了解hadoop的组成…避免文章很长,所以另立一篇文章.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;若是问我最想和谁在一起 我想到的只有你 我既想缠着你 又想放弃你 又想跟你联系 又不想跟你联系 既想慢慢退出你的世界 又怕失去你 终其一生满是遗憾      –来自网易音乐《要命》              &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据是什么?</title>
    <link href="http://xubatian.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%98%AF%E4%BB%80%E4%B9%88/"/>
    <id>http://xubatian.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%98%AF%E4%BB%80%E4%B9%88/</id>
    <published>2022-01-13T05:05:36.000Z</published>
    <updated>2022-01-15T07:37:50.886Z</updated>
    
    <content type="html"><![CDATA[<p>经历了多少委屈 才有一身好脾气。    –来自网易云音乐《习惯》          </p><p>​                  </p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/1.gif" alt="图片"></p><h1 id="大数据概念"><a href="#大数据概念" class="headerlink" title="大数据概念"></a><a href="https://baike.baidu.com/item/%E5%A4%A7%E6%95%B0%E6%8D%AE/1356941">大数据概念</a></h1><p>大数据（Big Data）：指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产.</p><p>说白了大数据就是主要解决海量数据的存储和海量数据的分析计算问题.</p><p>一般数据直接存放在mysql中,通过SQL语言进行分析. 但是数据量特别大的时候到达TB,PB级别的时候,数据再使用mysql等数据库工作就显得不是那么容易了. 这种海量的数据的分析,计算,存储已经是寻常数据库无法完成的了. 所以,大数据孕育而生.大数据所负责的范围是在PB和EB范围居多.</p><p>&emsp;     </p><p>按顺序给出数据存储单位：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">从小到大的存储单位: bit、Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB</span><br><span class="line">1Byte = 8bit  </span><br><span class="line">1K = 1024Byte  </span><br><span class="line">1MB = 1024K</span><br><span class="line">1G = 1024M  </span><br><span class="line">1T = 1024G      </span><br><span class="line">1P = 1024T</span><br><span class="line"></span><br><span class="line">计算机常用的存储单位：</span><br><span class="line"><span class="number">8</span> bit = <span class="number">1</span> Byte 一字节</span><br><span class="line"><span class="number">1024</span> B = <span class="number">1</span> KB （KiloByte） 千字节</span><br><span class="line"><span class="number">1024</span> KB = <span class="number">1</span> MB （MegaByte） 兆字节</span><br><span class="line"><span class="number">1024</span> MB = <span class="number">1</span> GB （GigaByte） 吉字节</span><br><span class="line"><span class="number">1024</span> GB = <span class="number">1</span> TB （TeraByte） 太字节</span><br><span class="line"><span class="number">1024</span> TB = <span class="number">1</span> PB （PetaByte） 拍字节</span><br><span class="line"><span class="number">1024</span> PB = <span class="number">1</span> EB （ExaByte） 艾字节</span><br><span class="line"><span class="number">1024</span> EB = <span class="number">1</span> ZB （ZetaByte） 泽字节</span><br><span class="line"><span class="number">1024</span> ZB = <span class="number">1</span> YB （YottaByte） 尧字节</span><br><span class="line"><span class="number">1024</span> YB = 1BB（Brontobyte）珀字节</span><br><span class="line"><span class="number">1024</span> BB = <span class="number">1</span> NB （NonaByte） 诺字节</span><br><span class="line"><span class="number">1024</span> NB = <span class="number">1</span> DB （DoggaByte）刀字节</span><br></pre></td></tr></table></figure><p>&emsp;     </p><h1 id="大数据应用场景"><a href="#大数据应用场景" class="headerlink" title="大数据应用场景"></a><a href="https://baijiahao.baidu.com/s?id=1707322739345909800&wfr=spider&for=pc">大数据应用场景</a></h1><p>大数据到底在现实生活中有哪些应用场景呢? 说白了就是大数据能干啥? </p><h2 id="电商行业"><a href="#电商行业" class="headerlink" title="电商行业"></a>电商行业</h2><p>不知道大家有没有这样的记忆，你在手机淘宝上搜索了一下衬衫这个商品，在你下一次打开的时候，首页上推荐的绝对有相关产品；你在头条上连着关注了几条疫情相关的信息，那么类似信息就会一直给你推荐。这就是大数据的应用之一。它可以<strong>根据你的的消费习惯为你提供相关产品与服务，而且很精细化</strong>。随着数据量的不断扩大，可以根据特定时间段特定区域等分析出区域消费特征，男女消费特征，消费习惯等等，这样在未来的市场布局中，就可以很有针对性地<strong>预测市场走向，调整销售策略、产品结构及产品备货量等</strong>，创造商业价值</p><h2 id="金融行业"><a href="#金融行业" class="headerlink" title="金融行业"></a>金融行业</h2><p>炒股的人都知道要看K线，<strong>那么K线怎么来的？都是交易过程的一些数据加工而来</strong>，可以说大数据在金融行业的应用非常广泛，行家关注大消息，菜鸟就只能看那些线，看不看得懂另说，这些线都是由<strong>大数据统计</strong>而来。在<strong>交易过程也大都是使用大数据算法进行的。</strong>买卖双方可以根据这些数据以及新闻，决定接下来的几秒内是选择购买还是出售。</p><h2 id="生物技术"><a href="#生物技术" class="headerlink" title="生物技术"></a>生物技术</h2><p>前两天<strong>百度布局苏州，打造生物计算发展新高地</strong>。计算机算，当然要用到数据。用李彦宏自己的话来讲，<strong>生物计算是高度融合的学科。</strong>生物和计算的融合，能够有效利用大量的<strong>生物数据，把药物发现的“大海捞针”变成“按图索骥”</strong>，为人类的生命健康谋福祉。</p><p>借助大数据和人工智能，医生可以检测出不同癌症病人的不同病变，找到个性化的用药，实现<strong>精准医疗</strong>，降低治疗成本。</p><p>大数据能在自身基因技术的多方面发挥作用，如<strong>基因测序和重组方面</strong>，大数据可以将复杂的工程简单化，带来更好的科技成果。</p><h2 id="汽车行业"><a href="#汽车行业" class="headerlink" title="汽车行业"></a>汽车行业</h2><p>说起汽车，不得不提最近互联网巨头的造车潮。小米、华为、360都说要造车，那么哪家更强？<strong>华为、360、腾讯标榜的是不造整车，以技术赋能汽车行业。</strong></p><p>汽车上的<strong>传感器</strong>，随时测量和传递着有关位置、运动、震动、温度、湿度乃至空气中化学物质的变化，<strong>这也是大数据</strong>。</p><p>互联网赋能汽车的软件技术，大多也要通过大数据来实现。如一些操作系统，一些智能云服务等。</p><p>360的周鸿祎明确声明，360 将不会独立造车：“做手机失败之后，我知道自己不能造车，<strong>硬件是好的身体，软件是灵魂</strong>，没有灵魂的肉体是行尸走肉。”</p><p>&emsp;     </p><p>现在，人们越来越多地意识到大数据的价值，把大数据模型系统地应用到公共商业服务中，为政府、企业或个人提供服务；根据用户的查询浏览购买记录来推荐产品……可以说人们的生产生活正在被数字所定义，可以说无数据不存储，无数据不计算，无数据不真相，未来大数据所能发挥的作用更会超越我们的想象。</p><h1 id="大数据特点"><a href="#大数据特点" class="headerlink" title="大数据特点"></a><a href="https://blog.csdn.net/arsaycode/article/details/70847184">大数据特点</a></h1><p>一、Volume：数据量大，包括采集、存储和计算的量都非常大。大数据的起始计量单位至少是P（1000个T）、E（100万个T）或Z（10亿个T）。</p><p>二、Variety：种类和来源多样化。包括结构化、半结构化和非结构化数据，具体表现为网络日志、音频、视频、图片、地理位置信息等等，多类型的数据对数据的处理能力提出了更高的要求。</p><p>三、Value：数据价值密度相对较低，或者说是浪里淘沙却又弥足珍贵。随着互联网以及物联网的广泛应用，信息感知无处不在，信息海量，但价值密度较低，如何结合业务逻辑并通过强大的机器算法来挖掘数据价值，是大数据时代最需要解决的问题。</p><p>四、Velocity：数据增长速度快，处理速度也快，时效性要求高。比如搜索引擎要求几分钟前的新闻能够被用户查询到，个性化推荐算法尽可能要求实时完成推荐。这是大数据区别于传统数据挖掘的显著特征。</p><p>五、Veracity：数据的准确性和可信赖度，即数据的质量。</p><h1 id="大数据发展前景"><a href="#大数据发展前景" class="headerlink" title="大数据发展前景"></a><a href="http://www.npc.gov.cn/npc/c30834/201910/653fc6300310412f841c90972528be67.shtml">大数据发展前景</a></h1><p>随着科技的进步，大数据从科学前沿逐渐深入到各行业。纵观国内外，大数据已经形成产业规模，并上升到国家战略层面，大数据技术和应用呈现纵深发展趋势。面向大数据的云计算技术、大数据计算框架等不断推出，新型大数据挖掘方法和算法大量出现，大数据新模式、新业态层出不穷，传统产业开始利用大数据实现转型升级。</p><p>趋势一：数据的资源化</p><p>趋势二：与云计算的深度结合</p><p>趋势三：科学理论的突破</p><p>趋势四：数据科学和数据联盟的成立</p><p>大数据作为一种重要的战略资产，已经不同程度地渗透到每个行业领域和部门，其深度应用不仅有助于企业经营活动，还有利于推动国民经济发展。它对于推动信息产业创新、大数据存储管理挑战、改变经济社会管理面貌等方面也意义重大。</p><p>大数据的技术发展与物联网、云计算、人工智能等新技术领域的联系将更加紧密，物联网的发展将极大提高数据的获取能力，云计算与人工智能将深刻地融入数据分析体系，融合创新将会不断地涌现和持续深入。</p><p>总体来说，大数据产业发展将迎来快速增长期，创新成为大数据发展主要基调，大数据与各大产业融合将加速，为做大做强数字经济、带动传统产业转型升级提供新动力。</p><h1 id="了解大数据技术生态体系"><a href="#了解大数据技术生态体系" class="headerlink" title="了解大数据技术生态体系"></a>了解大数据技术生态体系</h1><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/bigdata.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><p>图中涉及的技术名词解释如下：<br>1）Sqoop：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库（MySQL）间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。<br>2）Flume：Flume是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；<br>3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统；<br>4）Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。<br>5）Flink：Flink是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多。<br>6）Oozie：Oozie是一个管理Hadoop作业（job）的工作流程调度管理系统。<br>7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。<br>8）Hive：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。<br>9）ZooKeeper：它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;经历了多少委屈 才有一身好脾气。    –来自网易云音乐《习惯》          &lt;/p&gt;
&lt;p&gt;​                  &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>mapreduce过程中的shuffle机制原理</title>
    <link href="http://xubatian.cn/mapreduce%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84shuffle%E6%9C%BA%E5%88%B6%E5%8E%9F%E7%90%86/"/>
    <id>http://xubatian.cn/mapreduce%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84shuffle%E6%9C%BA%E5%88%B6%E5%8E%9F%E7%90%86/</id>
    <published>2022-01-12T15:09:02.000Z</published>
    <updated>2022-01-14T16:01:01.582Z</updated>
    
    <content type="html"><![CDATA[<p>那个果断删了你的人 却在另一个地方悄悄地关注着你 你认为毫无瓜葛的人 在无数个夜里忍住了一万次 想和你联系的冲动         –来自网易云音乐《再也不会遇见了》</p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_01.gif" width = "1201.56" height = "675.88" alt="xubatian的博客" align="center" /><h1 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a>Shuffle机制</h1><p>什么是shuffle?</p><p>shuffle就是数据落盘的过程.</p><p>hadoop在mapreduce计算过程中,什么阶段会有落盘呢? 在map打散数据之后,reduce聚合数据之前.</p><p>Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/map4.png" width = "600" height = "300" alt="xubatian的博客" align="center" /><p>其实我们的MapReduce整个阶段我们细细的分可以分为5个阶段.<br>Map——&gt; sort(排序) ——-&gt; copy ——&gt;sort(排序) ——-&gt; Reducer</p><p>首先我们在看shuffle机制之前先了解一下 shuffle过程的前一个过程和后一个过程, 即,mapTask过程和ReduceTask过程.</p><h2 id="通过源码简单了解一下mapTask和ReduceTask"><a href="#通过源码简单了解一下mapTask和ReduceTask" class="headerlink" title="通过源码简单了解一下mapTask和ReduceTask:"></a>通过源码简单了解一下mapTask和ReduceTask:</h2><p>因为你我们的mapreduce程序分为mapTask 和 reduceTask. 在中间的过程就是shuffle过程.所以简单看一下mapTask和reduceTask的源码.</p><h3 id="mapTask"><a href="#mapTask" class="headerlink" title="mapTask"></a>mapTask</h3><p>这是shuffle之前的过程</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/map6.png" width = "1201.56" height = "600" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/oo1.png" width = "1201.56" height = "600" alt="xubatian的博客" align="center" /><h3 id="ReduceTask"><a href="#ReduceTask" class="headerlink" title="ReduceTask"></a>ReduceTask</h3><p>这是shuffle之后的过程</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/oo2.png" width = "1201.56" height = "600" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/oo3.png" width = "900" height = "600" alt="xubatian的博客" align="center" /><h3 id="shuffle机制的整个过程图"><a href="#shuffle机制的整个过程图" class="headerlink" title="shuffle机制的整个过程图"></a>shuffle机制的整个过程图</h3><p>shuffle过程是在MapTask方法之后，ReduceTask方法之前的数据处理过程称之为Shuffle。</p><p><strong>shuffle机制展示图:</strong></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/shuffle1.png" width = "1201.56" height = "600" alt="xubatian的博客" align="center" /><h3 id="对于shuffle机制的解读"><a href="#对于shuffle机制的解读" class="headerlink" title="对于shuffle机制的解读"></a><strong>对于shuffle机制的解读</strong></h3><p>Map方法出来之后先进入到getPartition方法,获取是哪一个分区的.<br>然后进入环形缓冲区.(默认100M,企业开发一般调整到200M,这是优化.) 到达80%溢写.溢写对数据排序. 排序手段是快排. 对key的索引按照字典顺序排 .溢写后产生大量的溢写文件.针对溢写文件进行归并排序. 按照分区放到对应磁盘上等待拉取. reduce拉取对应分区数据放入内存.内存不够放入磁盘. 内存中数据或者磁盘中数据都需要进行归并排序. 排完序后分组. 然后进入到reduce方法.    </p><p>map端的溢写过程产生的溢写文件进行归并,默认是一次归并10个. 但是随着机器性能提高. 这个也可以提高到20 或者30.<br>总体上默认的mapTask的内存是1G. 默认的reduceTask也是1G.<br>通常我们调整做多的是ReduceTask为什么是reduceTask呢?<br>因为他是一个聚合汇总. 因为mapTask默认是128M的数据.<br>而reduceTask是将所有数据都聚合到这里面,数据量相对来说大一些<br>真正开发的时候可以适当调整到4个G左右.<br>yarn单个节点(就是一个服务器)默认内存是8个G,通长也是需要调整的,一般是跟你的集群的节点内存相等,正常是128G.<br>yarn的单个任务(就是处理一件事) 默认内存也是8G. 生产过程这些都是需要调整的. </p><p>优化:<br>环形缓冲区.(默认100M,企业开发一般调整到200M,这是优化.)<br>到达80%溢写.(企业一般调整到90%或者95% 这是优化,调大了减少溢写文件的次数)<br>这样做可以减少溢写次数这就优化了. 对文件进行归并排序前可以进行一次Combiner.前提条件是不影响业务.比如求和,汇总业务不影响. 求平均值就影响.</p><p>Combiner之后进行归并排序. 然后放到对应分区的磁盘上.<br>为了减少磁盘io,即为了减少从map端到reduce端的拉取过程采用压缩.减少磁盘io.  在MapReduce整个过程中,map输入端,map输出端,reduce输出端都可以压缩. 输入端需要注意切片,输入端谁支持切片呢?那种压缩支持切片呢?LZO支持切片.LZO里面需要额外的建立索引. 如果LZO里面没有建立索引,那就不支持切分.<br>reduce默认拉取磁盘数据是5个.但是我们可以增大到10个或者20个.前提条件是机器性能和内存.</p><p>通常map输出端的压缩使用snappy或者LZO</p><h3 id="对于shuffle机制中的缓存区解释图"><a href="#对于shuffle机制中的缓存区解释图" class="headerlink" title="对于shuffle机制中的缓存区解释图"></a><strong>对于shuffle机制中的缓存区解释图</strong></h3><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/shuffle2.png" width = "1301.56" height = "600" alt="xubatian的博客" align="center" /><p>以后我们在做一些操作的时候, 能没有reducer的话, 就不写Reducer.<br>因为整个MapReduce任务中, shuffle是最耗费时间的. 因为你Reducer没有shuffle也就没有了. 那么这个程序性能就比较高. </p><h3 id="Shuffle过程源码解读"><a href="#Shuffle过程源码解读" class="headerlink" title="Shuffle过程源码解读"></a>Shuffle过程源码解读</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">1.根据ReduceTask的个数决定获取哪个输出对象。</span><br><span class="line">  new NewDirectOutputCollector(taskContext, job, umbilical, reporter); ==&gt;ReduceTask为0 </span><br><span class="line">  output = new NewOutputCollector(taskContext, job, umbilical, reporter);</span><br><span class="line">  获取collector</span><br><span class="line">  [1]. collector = createSortingCollector(job, reporter); 创建一个输出对象，收集map输出的kv</span><br><span class="line">① Class&lt;?&gt;[] collectorClasses = job.getClasses(</span><br><span class="line">           JobContext.MAP_OUTPUT_COLLECTOR_CLASS_ATTR, MapOutputBuffer.class); </span><br><span class="line">   获取到collector的类型</span><br><span class="line">② MapOutputCollector&lt;KEY, VALUE&gt; collector = ReflectionUtils.newInstance(subclazz, job);</span><br><span class="line">   获取到collector的对象</span><br><span class="line">③ collector.init(context); 调用init方法</span><br><span class="line">   (1). job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);</span><br><span class="line">                job.getInt(JobContext.IO_SORT_MB, 100);</span><br><span class="line">获取缓冲区的大小(100MB) 以及 溢写百分比 (80%). </span><br><span class="line">           (2). sorter = ReflectionUtils.newInstance(job.getClass(&quot;map.sort.class&quot;,</span><br><span class="line">QuickSort.class, IndexedSorter.class), job);</span><br><span class="line">获取到排序对象，默认用的是QuickSort</span><br><span class="line">   (3).comparator = job.getOutputKeyComparator();  获取分组比较器</span><br><span class="line">       a. WritableComparator.get(getMapOutputKeyClass().asSubclass(WritableComparable.class), this);</span><br><span class="line">   (4). combiner</span><br><span class="line">   (5). spillThread.start();  启动溢写线程</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">   拿到collector之后</span><br><span class="line">   [2].  if (partitions &gt; 1) &#123;</span><br><span class="line">partitioner = (org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;)</span><br><span class="line">ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span><br><span class="line">         &#125; else &#123;</span><br><span class="line">                partitioner = new org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;() &#123;</span><br><span class="line">               @Override</span><br><span class="line">                public int getPartition(K key, V value, int numPartitions) &#123;</span><br><span class="line">                    return partitions - 1;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line">         &#125;</span><br><span class="line"> 获取到分区器, 计算每个KV对应的分区号，溢写的时候将kv写到对应的分区文件中.</span><br></pre></td></tr></table></figure><h3 id="Shuffle过程源码解读图示"><a href="#Shuffle过程源码解读图示" class="headerlink" title="Shuffle过程源码解读图示"></a>Shuffle过程源码解读图示</h3><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_79.png" width = "1301.56" height = "600" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_80.png" width = "1301.56" height = "600" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_81.png" width = "1301.56" height = "600" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_82.png" width = "1301.56" height = "600" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_83.png" width = "1301.56" height = "600" alt="xubatian的博客" align="center" /><p><strong>进入collector.init(context)里面.</strong></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_84.png" width = "1301.56" height = "600" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_85.png" width = "1301.56" height = "600" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_86.png" width = "1301.56" height = "600" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_87.png" width = "1301.56" height = "600" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_88.png" width = "1301.56" height = "600" alt="xubatian的博客" align="center" /><p><strong>获取到分区器, 计算每个KV对应的分区号，溢写的时候将kv写到对应的分区文件中.</strong></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_89.png" width = "1301.56" height = "600" alt="xubatian的博客" align="center" /><p>源码: (上面部分只是准备过程, 这里开始是执行过程)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">2. Mapper方法执行，写出去KV ,collector收集， 达到阈值， 溢写， 最后合并. </span><br><span class="line">   [1]. context.write(k, v);  Mapper中的map方法中写出KV</span><br><span class="line">   [2]. mapContext.write(key, value);</span><br><span class="line">   [3]. output.write(key, value);</span><br><span class="line">   [4]. collector.collect(key, value, partitioner.getPartition(key, value, partitions));</span><br><span class="line">① startSpill(); 当满足溢写条件后，开始溢写</span><br><span class="line">   (1). spillReady.signal();  发信号给溢写线程，让溢写线程开始工作,实际就是调用</span><br><span class="line">sortAndSpill()方法进行排序和溢写. </span><br><span class="line">           (2). 在sortAndSpill()方法中：</span><br><span class="line">a. final Path filename = mapOutputFile.getSpillFileForWrite(numSpills, size);</span><br><span class="line">   获取到溢写的文件的位置</span><br><span class="line">    D:/tmp/hadoopAdministrator/mapred/local/localRunner/Administrator/jobcache/job_local685727973_0001/attempt_local685727973_0001_m_000000_0/output/spill0.out</span><br><span class="line">        </span><br><span class="line">b. sorter.sort(MapOutputBuffer.this, mstart, mend, reporter); 溢写之前先排序</span><br><span class="line"></span><br><span class="line">c. 生成溢写文件 ，并溢写   spill0.out  splill1.out .....</span><br><span class="line">d. 如果存储index的内存达到阈值，也会溢写到磁盘中。 </span><br><span class="line"></span><br><span class="line">       </span><br><span class="line">  ② 当map端所有的kv全部都写出后， 会触发最后一次溢写， 之后会进行合并. </span><br><span class="line">   合并完成以后，最终会保留两个文件</span><br><span class="line">   file.out  ==&gt;   kv</span><br><span class="line">   file.out.index  == &gt;kv的index</span><br></pre></td></tr></table></figure><p><strong>如下图只是一个mapTask的过程, 假如有多个mapTask, 每一个mapTask最终都会去生成file.out或者file.out.index文件</strong></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_90.png" width = "1301.56" height = "300" alt="xubatian的博客" align="center" /><h3 id="收集器源码"><a href="#收集器源码" class="headerlink" title="收集器源码"></a><strong>收集器源码</strong></h3><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_61.png" width = "1201.56" height = "600" alt="xubatian的博客" align="center" /><h3 id="shuffle完成后-从Map写出来的数据过程图"><a href="#shuffle完成后-从Map写出来的数据过程图" class="headerlink" title="shuffle完成后,从Map写出来的数据过程图"></a>shuffle完成后,从Map写出来的数据过程图</h3><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/pp1.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><h3 id="Partition分区"><a href="#Partition分区" class="headerlink" title="Partition分区"></a>Partition分区</h3><p>什么是partition分区呢? 他的作用使用么?</p><h4 id="1、问题引出"><a href="#1、问题引出" class="headerlink" title="1、问题引出"></a>1、问题引出</h4><p>要求将统计结果照条件输出到不同文件中（分区）比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区）</p><h4 id="2、默认Partitioner分区"><a href="#2、默认Partitioner分区" class="headerlink" title="2、默认Partitioner分区"></a>2、默认Partitioner分区</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;</span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numReduceTasks)</span></span></span><br><span class="line"><span class="function"><span class="title">return</span>  <span class="params">(key.hashCode()</span> &amp; Integer.MAX_VALUE) % numReduceTasks </span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>默认分区是根据key的hashCode对ReduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。</p><h4 id="3、自定义Partitioner步骤"><a href="#3、自定义Partitioner步骤" class="headerlink" title="3、自定义Partitioner步骤"></a>3、自定义Partitioner步骤</h4><p>（1）自定义类继承Partitioner，重写getPartition()方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, FlowBean value, <span class="keyword">int</span> numPartitions)</span></span></span><br><span class="line"><span class="function"><span class="comment">// 控制分区代码逻辑</span></span></span><br><span class="line"><span class="function">        ... ...</span></span><br><span class="line"><span class="function">return partition</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（2）在Job驱动中，设置自定义Partitioner</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setPartitionerClass(CustomPartitioner.class);</span><br></pre></td></tr></table></figure><p>（3）自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h4 id="4、分区总结"><a href="#4、分区总结" class="headerlink" title="4、分区总结"></a>4、分区总结</h4><p>（1）如果ReduceTask的数量&gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx；<br>（2）如果1&lt;ReduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception；<br>（3）如果ReduceTask的数量=1，则不管MapTask端输出多少个分区文件,最终结果都交给这一个ReduceTask，最终也就只会产生一个          结果文件part-r-00000；<br>（4）分区号必须从零开始，逐一累加。</p><h4 id="5、案例分析"><a href="#5、案例分析" class="headerlink" title="5、案例分析"></a>5、案例分析</h4><p>例如：假设自定义分区数为5，则</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(1)job.setNumReduceTasks(1); 会正常运行，只不过会产生一个输出文件</span><br><span class="line">(2)job.setNumReduceTasks(2); 会报错</span><br><span class="line">(3）job.setNumReduceTasks(6); 大于5，程序会正常运行，会产生空文件</span><br></pre></td></tr></table></figure><h4 id="对于partition分区的总结"><a href="#对于partition分区的总结" class="headerlink" title="对于partition分区的总结"></a>对于partition分区的总结</h4><p>块(block): 就是存到hdfs的时候是以什么块为单位.<br>切片就是MapReduce程序在处理你每一块数据的时候生成的逻辑上的切片信息.<br><strong>分区就是你的MapReduce处理完我这个数据后,最终往磁盘上输出结果的时候,我最终要输出几个文件</strong>. 比如说我要输出3个文件,那也就代  表着我有三个分区.  如果我输出一个文件,就代表只有一个分区.  所以分区简单理解为将来MapReduce处理完成以后他要帮我输出几个文件就是几个分区.<br>      比如说我一堆单词, a-q ,q-p各自放在一个文件中. 那么这两个文件就必须有两个分区来完成.  说白了这两个文件就是两个分区.</p><p>如图展示什么是partition分区:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_62.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><p>对于一个wordCount案例来讲, 我希望将统计的结果输出到两个文件中, 但是我没有说什么样的分区给到一分区, 什么样的分区给带二分区. 也就是说没有明确告诉那个k,v按照什么样的条件进入什么样的分区呢? 如图设置两个分区:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_63.png" width = "900" height = "550" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_64.png" width = "800" height = "600" alt="xubatian的博客" align="center" /> <p>其实他是根据我们的hash算法来帮你区分的. 他会拿上你每一个key的hashcode值去对我的reduce的个数取模,上面我的reduce是2, 任何数字对reduce取模只有可能是0和1.  所以最后我的分区是00000, 00001 即0号分区和1号分区. 简单理解就是我的使用key的hashcode对reduce个数取模, 如果得到是0就往0号分区放. 得到是1就往1号分区放. 所以他默认情况下用的是hash运算. 这就是默认的分区器Partition. 如下图:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_65.png" width = "500" height = "200" alt="xubatian的博客" align="center" /><h4 id="Partitioner分区器源码解读"><a href="#Partitioner分区器源码解读" class="headerlink" title="Partitioner分区器源码解读"></a>Partitioner分区器源码解读</h4><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_66.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><p>Partitioner: 分区器<br>    默认的分区器:  HashPartitioner ,通过key的hashcode值对ReduceTasks的个数取余确定去往哪个分区.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">如何确定分区的:</span><br><span class="line">      partitions = jobContext.getNumReduceTasks();</span><br><span class="line">      <span class="keyword">if</span> (partitions &gt; <span class="number">1</span>) &#123;</span><br><span class="line">partitioner = (org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;)</span><br><span class="line">  ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">partitioner = <span class="keyword">new</span> org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;() &#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> partitions - <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_67.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><h3 id="Combiner合并"><a href="#Combiner合并" class="headerlink" title="Combiner合并"></a>Combiner合并</h3><p>什么是combiner合并呢? 他和partition分区又有什么联系呢? </p><p>分区? 合并? 你联想到了什么?</p><p>（1）Combiner是MR程序中Mapper和Reducer之外的一种组件。<br>（2）Combiner组件的父类就是Reducer。<br>（3）Combiner和Reducer的区别在于运行的位置<br>                    <strong>Combiner是在每一个MapTask所在的节点运行</strong>;<br>                    <strong>Reducer是接收全局所有Mapper的输出结果</strong>；<br>（4）Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量。<br>（5）Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出key,value应该跟Reducer的输入key,value类型要           对应起来。</p><p>如下所示:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Mapper                                     Reducer</span><br><span class="line"><span class="number">3</span> <span class="number">5</span> <span class="number">7</span> -&gt;(<span class="number">3</span>+<span class="number">5</span>+<span class="number">7</span>)/<span class="number">3</span>=<span class="number">5</span>                 (<span class="number">3</span>+<span class="number">5</span>+<span class="number">7</span>+<span class="number">2</span>+<span class="number">6</span>)/<span class="number">5</span>=<span class="number">23</span>/<span class="number">5</span>       不等于     (<span class="number">5</span>+<span class="number">4</span>)/<span class="number">2</span>=<span class="number">9</span>/<span class="number">2</span></span><br><span class="line"><span class="number">2</span> <span class="number">6</span> -&gt;(<span class="number">2</span>+<span class="number">6</span>)/<span class="number">2</span>=<span class="number">4</span></span><br></pre></td></tr></table></figure><p>（  6）自定义Combiner实现步骤</p><p>​            (a) 自定义一个Combiner继承Reducer，重写Reduce方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>,<span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 1 汇总操作</span></span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(IntWritable v :values)&#123;</span><br><span class="line">          count += v.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 2 写出</span></span><br><span class="line">        context.write(key, <span class="keyword">new</span> IntWritable(count));</span><br><span class="line">      &#125;</span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure><p>​              (b)  在Job驱动类中设置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(WordcountCombiner.class);</span><br></pre></td></tr></table></figure><p> 对于运行结果展示:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_68.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_69.png" width = "700" height = "400" alt="xubatian的博客" align="center" />]]></content>
    
    
    <summary type="html">&lt;p&gt;那个果断删了你的人 却在另一个地方悄悄地关注着你 你认为毫无瓜葛的人 在无数个夜里忍住了一万次 想和你联系的冲动         –来自网易云音乐《再也不会遇见了》&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="mapreduce" scheme="http://xubatian.cn/tags/mapreduce/"/>
    
    <category term="shuffle" scheme="http://xubatian.cn/tags/shuffle/"/>
    
  </entry>
  
  <entry>
    <title>hadoop组成模块之MapReduce概述</title>
    <link href="http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BMapReduce%E6%A6%82%E8%BF%B0/"/>
    <id>http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BMapReduce%E6%A6%82%E8%BF%B0/</id>
    <published>2022-01-12T13:12:15.000Z</published>
    <updated>2022-01-14T16:21:24.104Z</updated>
    
    <content type="html"><![CDATA[<p>亲爱的女孩，如果还能跟你心平气和的聊天的话，我希望能告诉你我那些无视和冷笑都是在伪装，可是已经过去了。淡月梨花，借梦来，桥边廊庑。     –来自网易云音乐《任你》</p><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/images/14.jpg" width = "1201.56" height = "675.88" alt="xubatian的博客" align="center" /><p>前言</p><p>我们知道大数据面临的痛点就是:</p><p>①数据存储问题.数据超级大,可达TB级别.所以没有任何合适的工具存储这些数据.mysql等数据也无法有效的存储. </p><p>②数据在有效的时间整合出结果,即计算问题.就算能够存储下来,也无法有效的操作这些数据. 即,无法通过类似sql语句查询mysql一样去整合数据得到有效的信息.</p><p>但是我们又说到, hadoop解决了这两个问题. </p><p>首先hadoop解决数据存储问题的模块就是HDFS. 也就是说HDFS就是针对大数据存储问题的一套落地的解决方案. </p><p>那么数据存储了,我需要分析呀.如何取分析呢?使用什么引擎去分析呢?<br>答案就是: MapReduce.   一个模块负责存储, 一个模块负责分析.</p><h1 id="MapReduce概述"><a href="#MapReduce概述" class="headerlink" title="MapReduce概述"></a>MapReduce概述</h1><h2 id="MapReduce定义"><a href="#MapReduce定义" class="headerlink" title="MapReduce定义"></a>MapReduce定义</h2><p>MapReduce是一个分布式运算架，是用户开发“基于Hadoop的数据分析应用”的核心框架。<br>MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。</p><p>&emsp;     </p><h2 id="MapReduce优缺点"><a href="#MapReduce优缺点" class="headerlink" title="MapReduce优缺点"></a>MapReduce优缺点</h2><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_92.png" width = "700" height = "600" alt="xubatian的博客" align="center" /><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">什么叫离线处理?</span><br><span class="line">他对比的就是实时处理,就像mysql等就可以实时的返回结果;</span><br><span class="line">就是我已经有大量的数据了,你通过hadoop里面的hdfs,mapReduce去做运算,最终经过一段时间的运算给我得出结果,这是要耗费一定时间的,这就叫离线处理</span><br></pre></td></tr></table></figure><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>其实也不能叫缺点,以为他本身设计的时候就没有去考虑这些东西,如果考虑这些东西就可能解决不了大量数据的处理了</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_93.png" width = "700" height = "390" alt="xubatian的博客" align="center" /><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">静态的就表示,数据已经放好了在这里了,而不是你时不时还往里面添加数据,这是不支持的</span><br><span class="line">就是如下图,什么叫有向图(DAG)计算?就是后面应用程序输入的数据是前面数据输出的数据,而mapReduce不擅长做这个,但是也不是不能做. 如果这么做的话会有大量的磁盘io操作. 就是你前面的MapReduce程序完成后结果放在磁盘中,下一个MapReduce程序读取磁盘的结果来操作. 这个过程会发生大量的磁盘io.所以你这个性能是非常低下的</span><br></pre></td></tr></table></figure><h1 id="MapReduce核心思想"><a href="#MapReduce核心思想" class="headerlink" title="MapReduce核心思想"></a>MapReduce核心思想</h1><p>之前map  和 reduce 一个负责分,一个负责合,这是一个很笼统的认识</p><p>现在MapReduce核心编程思想，如图所示:</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mapreduce5.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><h2 id="对上图MapReduce核心思想解释"><a href="#对上图MapReduce核心思想解释" class="headerlink" title="对上图MapReduce核心思想解释"></a>对上图MapReduce核心思想解释</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">第一个文件200m 我们立马想到,我们存到hdfs中后立马存两块(128M一块). 这里有两个文件,所以有三块数据.在map阶段会读取你的每一块内容,每一块内容都会交给maptask来处理.这个maptask就是map阶段最核心的一个对象. 整个maptask的过程中就是你的map阶段.所有的工作都是围绕着maptask来进行工作的.maptask就是一个任务.是map阶段的一个任务.正常情况下你的每一块数据都会交给maptask来进行处理.在我的maptask里面做什么操作呢?我要把你文件中的每一行数据都给他读出来.</span><br><span class="line">1)读数据,并按行处理       (就是我读一行进来)</span><br><span class="line">2)按空格切分行内单词     (具体要不要按照行处理看你每个单词与单词之间是否已空格划分)</span><br><span class="line">3)K键值对(单词,1)        (hadoop,1)(hive,1)(spark,1)</span><br><span class="line">4)将所有的KV键值对中的词,按照单词首字母,分成2个分区溢写到磁盘    (为什么要分成两个分区呢?因为我最终的结果我要的是两个,那我就是将a-z,q-p开头的单词写到不同的文件中)</span><br><span class="line"></span><br><span class="line">最终我的三个maptask中一共六个文件,即分区1(z-p),分区2(q-z)各两个.</span><br><span class="line">把三个maptask里面分区1的给一个reduce中,分区2的给一个reduce中.</span><br><span class="line">所以在这种情况下他就有两个reducetask,为什么有两个呢? 其实就是由你这个分区来决定的.因为你最终有两个文件,所以我就开启两个reduceTask来处理. 这reduceTask到每一个mapTask生成的文件里面去拷贝数据,比如说分区1里面拷贝到处理a-p的ReduceTask, 分区2拷贝到处理q-z的ReduceTask中.</span><br><span class="line"></span><br><span class="line">Mapreduce程序只能包含一个map阶段和一个reduce阶段.你不能有多个.如果说你的一个MapReduce程序处理完成后还没有得到你想要的结果,那你只能再重新再开一个MapReduce程序接着去处理你这个数据.不能再一个MapReduce中写多个map阶段或者多个reduce阶段程序.</span><br><span class="line"></span><br><span class="line">问题:</span><br><span class="line">为什么一个mapTask要处理一块数据?</span><br><span class="line">我的kv对处理完成以后如何写到多个分区里面呢?</span><br><span class="line">我的reduce阶段如何去拷贝我的多个分区的数据?</span><br></pre></td></tr></table></figure><h2 id="mapreduce核心思想总结"><a href="#mapreduce核心思想总结" class="headerlink" title="mapreduce核心思想总结"></a>mapreduce核心思想总结</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1）分布式的运算程序往往需要分成至少2个阶段。</span><br><span class="line">2）第一个阶段的MapTask并发实例，完全并行运行，互不相干。</span><br><span class="line">3）第二个阶段的ReduceTask并发实例互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出。</span><br><span class="line">4）MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。</span><br><span class="line">总结：分析WordCount数据流走向深入理解MapReduce核心思想。</span><br></pre></td></tr></table></figure><h2 id="MapReduce进程"><a href="#MapReduce进程" class="headerlink" title="MapReduce进程"></a>MapReduce进程</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">一个完整的MapReduce程序在分布式运行时有三类实例进程：</span><br><span class="line">1）MrAppMaster：负责整个程序的过程调度及状态协调。</span><br><span class="line">2）MapTask:  负责Map阶段的整个数据处理流程。</span><br><span class="line">3）ReduceTask:  负责Reduce阶段的整个数据处理流程。</span><br></pre></td></tr></table></figure><h2 id="通过官方WordCount源码查看我们如何取去写MapReduce程序"><a href="#通过官方WordCount源码查看我们如何取去写MapReduce程序" class="headerlink" title="通过官方WordCount源码查看我们如何取去写MapReduce程序"></a>通过官方WordCount源码查看我们如何取去写MapReduce程序</h2><p>采用反编译工具反编译源码，发现WordCount案例有Map类、Reduce类和驱动类。且数据的类型是Hadoop自身封装的序列化类型。</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mapreduce8.png" width = "1201.56" height = "600" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mapreduce9.png" width = "1201.56" height = "600" alt="xubatian的博客" align="center" /><h2 id="Hadoop常用数据序列化类型"><a href="#Hadoop常用数据序列化类型" class="headerlink" title="Hadoop常用数据序列化类型"></a>Hadoop常用数据序列化类型</h2><p>在mapreduce中用的数据类型都是hadoop重新提供出来的一套数据类型,他没有直接使用java中提供好的8个基本数据类型,因为他认为java定义好的在序列化和反序列化操作时的数据量比较大,所以他自己搞了一套数据类型和一套序列化,实际上特别好记,就是在原java后面加了writable,除了字符串</p><p>​    </p><p>常用的数据类型对应的Hadoop数据序列化类型</p><table><thead><tr><th>Java类型</th><th>Hadoop Writable类型</th></tr></thead><tbody><tr><td>Boolean</td><td>BooleanWritable</td></tr><tr><td>Byte</td><td>ByteWritable</td></tr><tr><td>Int</td><td>IntWritable</td></tr><tr><td>Float</td><td>FloatWritable</td></tr><tr><td>Long</td><td>LongWritable</td></tr><tr><td>Double</td><td>DoubleWritable</td></tr><tr><td>String</td><td><strong>Text</strong></td></tr><tr><td>Map</td><td>MapWritable</td></tr><tr><td>Array</td><td>ArrayWritable</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">&lt;p&gt;亲爱的女孩，如果还能跟你心平气和的聊天的话，我希望能告诉你我那些无视和冷笑都是在伪装，可是已经过去了。淡月梨花，借梦来，桥边廊庑。     –来自网易云音乐《任你》&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="MapReduce" scheme="http://xubatian.cn/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>hadoop组成模块之HDFS分布式存储详解</title>
    <link href="http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BHDFS%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E8%AF%A6%E8%A7%A3/"/>
    <id>http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BHDFS%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E8%AF%A6%E8%A7%A3/</id>
    <published>2022-01-12T11:06:38.000Z</published>
    <updated>2022-01-14T07:35:31.324Z</updated>
    
    <content type="html"><![CDATA[<p>“你和萤火虫有两个共同点，在我的眼里都会发光，同时，都已经很多年没见了。”    –来自网易云音乐《夜、萤火虫和你》                            </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/20.jpg" width = "1201.56" height = "675.88" alt="xubatian的博客" align="center" /><p>前言</p><p>上文了解到了hadoop由四个模块组成: HDFS,MapReduce,Yarn,Common.</p><p>本篇文章具体了解hadoop的HDFS模块.</p><p>什么是HDFS?</p><p>HDFS有什么功能?</p><p>如何掌握HDFS?</p><h1 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h1><h2 id="是什么原因使得HDFS产生的呢"><a href="#是什么原因使得HDFS产生的呢" class="headerlink" title="是什么原因使得HDFS产生的呢?"></a>是什么原因使得HDFS产生的呢?</h2><p>我们知道大数据面临的痛点就是:</p><p>① <strong>数据存储问题.</strong> 数据超级大,可达TB级别.所以没有任何合适的工具存储这些数据.mysql等数据也无法有效的存储. </p><p>② <strong>数据在有效的时间整合出结果,即计算问题</strong>.就算能够存储下来,也无法有效的操作这些数据. 即,无法通过类似sql语句查询mysql一样去整合数据得到有效的信息.</p><p>但是我们又说到, hadoop解决了这两个问题. </p><p>首先hadoop解决数据存储问题的模块就是HDFS. 也就是说HDFS就是针对大数据存储问题的一套落地的解决方案. </p><p>那么为什么连mysql这样的数据库都无法解决的问题,hadoop的HDFS能够解决呢?</p><p>答案就是: 分布式文件存储. 可以理解为, 将十台服务器的磁盘整合成一个磁盘,如果每台服务器是10T,那么,整个HDFS就是100T. 如果后期磁盘不够了,我还可以增加服务器的方式继续扩容.</p><h2 id="HDFS产生背景"><a href="#HDFS产生背景" class="headerlink" title="HDFS产生背景"></a>HDFS产生背景</h2><p>随着数据量越来越大,在一个操作系统存不下所有的数据,那么就分配到更多的操作系统管理的磁盘中,但是不方便管理和维护,迫切需要一种系统来管理多台机器上的文件,这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种。它主要解决什么问题呢?它主要解决的是,当我的是数据量大到一定程度的时候,我把我的数据分布到多台机器上去存储,但是我可以多多台机器上的数据进行统一的管理.</p><p>&emsp;     </p><h2 id="HDFS定义"><a href="#HDFS定义" class="headerlink" title="HDFS定义"></a>HDFS定义</h2><p>HDFS( Hadoop Distributed File System),它是一个文件系统,用于存储文件,通过目录树来定位文件;</p><p>其次,它是分布式的,由很多服务器联合起来实现其功能,集群中的服务器有各自的角色. 因为有很多服务器联合起来是一个HDFS.</p><p>每台服务器上都有HDFS.如何区分谁是主,谁是从呢? 因为有了主从才能更好的管理. 就像董事长管着经理,经理管着员工,这样才能掌控整个大的集团. 毕竟一个角色的人是有限的. 此逻辑在这里也适用.</p><p>&emsp; </p><h2 id="HDFS的使用场景"><a href="#HDFS的使用场景" class="headerlink" title="HDFS的使用场景"></a>HDFS的使用场景</h2><p>适合一次写入,多次读出的场景,且不支持文件的修改,也不支持删除,只支持末尾追加。适合用来做数据分析,并不适合用来做网盘应用.</p><p>Namenode所有元数据信息都是维护在内存中的</p><p>&emsp;     </p><h2 id="HDFS优缺点"><a href="#HDFS优缺点" class="headerlink" title="HDFS优缺点"></a>HDFS优缺点</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/hdfs1.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/hdfs2.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">要知道,namenode给datanode下达指令,但是NameNode具体给datanode下达什么指令是由我们客户端来操作NameNode来决定的;注意在HDFS当中他的文件单位是以块(Block)为单位的存储的.</span><br></pre></td></tr></table></figure><p>&emsp;     </p><h2 id="HDFS组成架构"><a href="#HDFS组成架构" class="headerlink" title="HDFS组成架构"></a>HDFS组成架构</h2><p>上面说了HDFS是有很多服务器联合起来的. 但是需要工作你得有一个领导人.你得分清楚哪一个是领导. 将来数据来了之后我需要存在哪台机器上得由一个小领导来指挥.</p><p>Namenode具体要给datanode下达什么指令,都是我们客户端来操作的.</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/hdfs3.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><p>要知道,namenode给datanode下达指令,但是NameNode具体给datanode下达什么指令是由我们客户端来操作NameNode来决定的;注意在HDFS当中他的文件单位是以块(Block)为单位的</p><p>Client:    就是客户端</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">(1)文件切分。文件上传HDFS的时候,Client将文件切分成个个的Bock,然后进行上传;</span></span><br><span class="line"><span class="string">(2)与</span> <span class="string">Namenode交获取文件的位置信息;</span></span><br><span class="line"><span class="string">(3)与</span> <span class="string">Datanode交互,读取或者写入数据;</span></span><br><span class="line"><span class="string">(4)</span> <span class="string">Client提供些命令来管理HDFS,比如</span> <span class="string">Namenode格式化;</span></span><br><span class="line"><span class="string">(5)</span> <span class="string">Client可以通过些命令来访问HDFS,比如对HDFS增删查改操作;</span></span><br></pre></td></tr></table></figure><p>SecondaryNameNode:    并非 NameNode的热备。当 NameNode挂掉的时候,它并不能马上替换 NameNode并提供服务</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>)辅助 NameNode,分担其工作量,比如定期合并 Fsimage和Edits,并推送给 NameNode;</span><br><span class="line">(<span class="number">2</span>)在紧急情况下,可辅助恢复 NameNode</span><br></pre></td></tr></table></figure><p>&emsp;     &emsp;     </p><h2 id="HDFS文件块大小"><a href="#HDFS文件块大小" class="headerlink" title="HDFS文件块大小"></a>HDFS文件块大小</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/block1.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/block2.png" width = "600" height = "400" alt="xubatian的博客" align="center" /><p>&emsp;     </p><h1 id="HDFS的Shell操作"><a href="#HDFS的Shell操作" class="headerlink" title="HDFS的Shell操作"></a>HDFS的Shell操作</h1><h2 id="命令大全"><a href="#命令大全" class="headerlink" title="命令大全"></a>命令大全</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ bin/hadoop fs</span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">[-checksum &lt;src&gt; ...]</span><br><span class="line">[-chgrp [-R] GROUP PATH...]</span><br><span class="line">[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">[-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">[-copyFromLocal [-f] [-p] [-l] [-d] [-t &lt;thread count&gt;] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">[-count [-q] [-h] [-v] [-t [&lt;storage type&gt;]] [-u] [-x] [-e] &lt;path&gt; ...]</span><br><span class="line">[-cp [-f] [-p | -p[topax]] [-d] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">[-df [-h] [&lt;path&gt; ...]]</span><br><span class="line">[-du [-s] [-h] [-v] [-x] &lt;path&gt; ...]</span><br><span class="line">[-expunge]</span><br><span class="line">[-find &lt;path&gt; ... &lt;expression&gt; ...]</span><br><span class="line">[-get [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">[-getfacl [-R] &lt;path&gt;]</span><br><span class="line">[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">[-getmerge [-nl] [-skip-empty-file] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">[-head &lt;file&gt;]</span><br><span class="line">[-help [cmd ...]]</span><br><span class="line">[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [&lt;path&gt; ...]]</span><br><span class="line">[-mkdir [-p] &lt;path&gt; ...]</span><br><span class="line">[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">[-mv &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">[-put [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">[-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ...]</span><br><span class="line">[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span><br><span class="line">[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line">[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">[-stat [format] &lt;path&gt; ...]</span><br><span class="line">[-tail [-f] [-s &lt;sleep interval&gt;] &lt;file&gt;]</span><br><span class="line">[-test -[defsz] &lt;path&gt;]</span><br><span class="line">[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">[-touch [-a] [-m] [-t TIMESTAMP ] [-c] &lt;path&gt; ...]</span><br><span class="line">[-touchz &lt;path&gt; ...]</span><br><span class="line">[-truncate [-w] &lt;length&gt; &lt;path&gt; ...]</span><br><span class="line">[-usage [cmd ...]]</span><br></pre></td></tr></table></figure><p>图示:</p><p>&emsp; <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/mingling.png" width = "1201.56" height = "900" alt="xubatian的博客" align="center" /></p><h2 id="Hadoop常用命令实操"><a href="#Hadoop常用命令实操" class="headerlink" title="Hadoop常用命令实操"></a>Hadoop常用命令实操</h2><h3 id="启动类命令"><a href="#启动类命令" class="headerlink" title="启动类命令"></a>启动类命令</h3><table><thead><tr><th>功能说明</th><th>命令脚本</th></tr></thead><tbody><tr><td>启动hdfs集群</td><td>sbin/start-dfs.sh</td></tr><tr><td>启动yarn</td><td>sbin/start-yarn.sh</td></tr></tbody></table><h3 id="hadoop-fs-hdfs-dfs-命令"><a href="#hadoop-fs-hdfs-dfs-命令" class="headerlink" title="hadoop fs/hdfs dfs 命令"></a>hadoop fs/hdfs dfs 命令</h3><table><thead><tr><th>功能说明</th><th>命令</th></tr></thead><tbody><tr><td>创建目录</td><td>hdfs dfs -mkdir -p /data/flink</td></tr><tr><td>显示目录</td><td>hdfs dfs -ls /</td></tr><tr><td>从HDFS拷贝到本地</td><td>hdfs dfs -copyToLocal /data/data.txt ./</td></tr><tr><td>文件上传到集群(从本地)</td><td>hhdfs dfs -copyFromLocal data.txt /</td></tr><tr><td>文件下载</td><td>hdfs dfs -get /data/flink</td></tr><tr><td>删除集群的文件</td><td>hdfs dfs -rm /data/flink</td></tr><tr><td>删除文件夹</td><td>hdfs dfs -rm -r -skipTrash /data</td></tr><tr><td>从本地剪切粘贴到HDFS</td><td>hdfs dfs  -moveFromLocal data.txt /data/</td></tr><tr><td>追加一个文件到已经存在的文件末尾hdfs dfs -appendToFile data1.txt /data/data.txt</td><td></td></tr><tr><td>显示文件内容</td><td>hdfs dfs -cat data.txt</td></tr><tr><td>修改文件所属权限</td><td>hdfs dfs  -chmod  777 xxx.sh</td></tr><tr><td>修改文件所属用户组</td><td>hdfs dfs  -chown  root:root data.txt</td></tr><tr><td>从HDFS的一个路径拷贝到HDFS的另一个路径</td><td>hdfs dfs -cp data.txt /data1.txt</td></tr><tr><td>在HDFS目录中移动文件</td><td>hdfs dfs -mv data.txt /opt/</td></tr><tr><td>合并下载多个文件</td><td>hdfs dfs  -getmerge /data/* ./data_merge.txt</td></tr><tr><td>hadoop fs -put</td><td>等同于copyFromLocal</td></tr><tr><td>显示一个文件的末尾</td><td>hdfs dfs -tail data.txt</td></tr><tr><td>删除文件或文件夹</td><td>hdfs dfs -rm /data/data.txt</td></tr><tr><td>删除空目录</td><td>hdfs dfs -rmdir /data</td></tr><tr><td>统计文件夹的大小信息</td><td>hdfs dfs -s -h /data</td></tr><tr><td>统计文件夹下的文件大小信息</td><td>hdfs dfs  -h /data</td></tr><tr><td>设置HDFS中文件的副本数量</td><td>hdfs dfs -setrep 3 /data/data.txt</td></tr></tbody></table><h3 id="yarn命令"><a href="#yarn命令" class="headerlink" title="yarn命令"></a>yarn命令</h3><table><thead><tr><th>功能说明</th><th align="left">命令</th></tr></thead><tbody><tr><td>查看正在运行的yarn任务列表</td><td align="left">yarn application -list appID</td></tr><tr><td>kill掉指定id的yarn任务</td><td align="left">yarn application -kill appID</td></tr><tr><td>查看任务日志信息</td><td align="left">yarn logs -applicationId appID</td></tr></tbody></table><p>&emsp; </p><h1 id="HDFS的API操作"><a href="#HDFS的API操作" class="headerlink" title="HDFS的API操作"></a>HDFS的API操作</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.alibaba.hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.*;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> 上白书妖</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2020/2/26 21:48</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Desription</span>:获取客户端的连接对象,操作hadoop集群</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsClient</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     创建目录</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testMkdirs</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置在集群上运行</span></span><br><span class="line">        <span class="comment">// configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop102:9000&quot;);</span></span><br><span class="line">        <span class="comment">// FileSystem fs = FileSystem.get(configuration);</span></span><br><span class="line"></span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span> );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 创建目录</span></span><br><span class="line">        Boolean result = fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">&quot;/1108/xw/shangbaishuyao&quot;</span>));</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;result:&quot;</span> + result);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    HDFS文件上传（测试参数优先级）</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        configuration.set(<span class="string">&quot;dfs.replication&quot;</span>, <span class="string">&quot;2&quot;</span>);</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 上传文件</span></span><br><span class="line">        fs.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">&quot;H:&quot;</span>+ File.separator+<span class="string">&quot;hello1.txt&quot;</span>), <span class="keyword">new</span> Path(<span class="string">&quot;/bigdata0615&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        fs.close();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;over&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 文件下载</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 执行下载操作</span></span><br><span class="line">        <span class="comment">// boolean delSrc 指是否将原文件删除</span></span><br><span class="line">        <span class="comment">// Path src 指要下载的文件路径</span></span><br><span class="line">        <span class="comment">// Path dst 指将文件下载到的路径</span></span><br><span class="line">        <span class="comment">// boolean useRawLocalFileSystem 是否开启文件校验</span></span><br><span class="line">        fs.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">&quot;/banzhang.txt&quot;</span>), <span class="keyword">new</span> Path(<span class="string">&quot;H:/banhua.txt&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        fs.close();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;over&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    HDFS文件夹删除</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span>  <span class="keyword">void</span> <span class="title">testDelete</span><span class="params">()</span> <span class="keyword">throws</span>  IOException,InterruptedException,URISyntaxException</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 执行删除</span></span><br><span class="line">        fs.delete(<span class="keyword">new</span> Path(<span class="string">&quot;/1108/&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     HDFS文件详情查看</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 获取文件详情</span></span><br><span class="line">        RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">&quot;/&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(listFiles.hasNext())&#123;</span><br><span class="line">            LocatedFileStatus status = listFiles.next();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 输出详情</span></span><br><span class="line">            <span class="comment">// 文件名称</span></span><br><span class="line">            System.out.println(<span class="string">&quot;文件名:&quot;</span>+status.getPath().getName());</span><br><span class="line">            <span class="comment">// 长度</span></span><br><span class="line">            System.out.println(<span class="string">&quot;文件长度:&quot;</span>+status.getLen());</span><br><span class="line">            <span class="comment">// 权限</span></span><br><span class="line">            System.out.println(<span class="string">&quot;文件长度:&quot;</span>+status.getPermission());</span><br><span class="line">            <span class="comment">// 分组</span></span><br><span class="line">            System.out.println(<span class="string">&quot;文件分组:&quot;</span>+status.getGroup());</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取存储的块信息</span></span><br><span class="line">            BlockLocation[] blockLocations = status.getBlockLocations();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (BlockLocation blockLocation : blockLocations) &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 获取块存储的主机节点</span></span><br><span class="line">                String[] hosts = blockLocation.getHosts();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">                    System.out.println(<span class="string">&quot;文件块所在的主机节点:&quot;</span>+host);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            System.out.println(<span class="string">&quot;-----------班长的分割线----------&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 3 关闭资源</span></span><br><span class="line">             fs.close();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;over&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     HDFS文件和文件夹判断,如果要判断其子子孙孙的话,就要用到递归</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListStatus</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件配置信息</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;========================递归判断其子子孙孙是文件还是文件夹========================&quot;</span>);</span><br><span class="line">        listDirectoryAndFile(<span class="string">&quot;/&quot;</span>,fs);</span><br><span class="line">        System.out.println(<span class="string">&quot;==================================递归判断完毕====================================&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 判断是文件还是文件夹</span></span><br><span class="line">        FileStatus[] listStatus = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">&quot;/&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (FileStatus fileStatus : listStatus) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 如果是文件</span></span><br><span class="line">            <span class="keyword">if</span> (fileStatus.isFile())</span><br><span class="line">            &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;文件:&quot;</span>+fileStatus.getPath().getName());</span><br><span class="line">            &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;目录:&quot;</span>+fileStatus.getPath().getName());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        fs.close();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;运行结束&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     需求: 指定一个目录,递归查看多有子目录,及文件</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listDirectoryAndFile</span><span class="params">(String path , FileSystem fs)</span> <span class="keyword">throws</span> IOException</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//处理当前目录下的子目录及文件</span></span><br><span class="line">        FileStatus[] fileStatuses = fs.listStatus(<span class="keyword">new</span> Path(path));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> ( fileStatus.isFile())</span><br><span class="line">            &#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (path.equals(<span class="string">&quot;/&quot;</span>))</span><br><span class="line">                &#123;</span><br><span class="line">                    System.out.println(<span class="string">&quot;文件:&quot;</span> + path +fileStatus.getPath().getName());</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    System.out.println(<span class="string">&quot;文件:&quot;</span> + path + <span class="string">&quot;/&quot;</span> + fileStatus.getPath().getName());</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//文件</span></span><br><span class="line">                System.out.println(<span class="string">&quot;文件:&quot;</span> + path+ <span class="string">&quot;/&quot;</span>+fileStatus.getPath().getName());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">//获取整个路径 即 :  hdfs://hadoop102:9000/bigdata0615</span></span><br><span class="line">                String currentpath = fileStatus.getPath().toString().substring(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>.length());</span><br><span class="line">                <span class="comment">//System.out.println(fileStatus.getPath());</span></span><br><span class="line">                <span class="comment">//目录</span></span><br><span class="line">                <span class="comment">//System.out.println(&quot;目录:&quot; + fileStatus.getPath().getName());</span></span><br><span class="line">                System.out.println(<span class="string">&quot;目录:&quot;</span> +currentpath);</span><br><span class="line"></span><br><span class="line">                <span class="comment">//递归显示当前目录下的子目录及文件</span></span><br><span class="line">                listDirectoryAndFile(currentpath,fs);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     HDFS文件名更改</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testRename</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 修改文件名称</span></span><br><span class="line">        fs.rename(<span class="keyword">new</span> Path(<span class="string">&quot;/shangbaishuyao.txt&quot;</span>), <span class="keyword">new</span> Path(<span class="string">&quot;/shangbaishuyao.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> <span class="comment">/*</span></span><br><span class="line"><span class="comment">     HDFS文件上传</span></span><br><span class="line"><span class="comment">     需求：把本地H盘上的shangbaishuyao.txt文件上传到HDFS根目录</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">putFileToHDFS</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 创建输入流</span></span><br><span class="line">        FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(<span class="string">&quot;H:&quot;</span>+File.separator+<span class="string">&quot;shangbaishuyao.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 获取输出流</span></span><br><span class="line">        FSDataOutputStream fos = fs.create(<span class="keyword">new</span> Path(<span class="string">&quot;/shangbaishuyao.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 流对拷</span></span><br><span class="line">        IOUtils.copyBytes(fis, fos, configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 关闭资源</span></span><br><span class="line">        IOUtils.closeStream(fos);</span><br><span class="line">        IOUtils.closeStream(fis);</span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    定位文件读取</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    需求：分块读取HDFS上的大文件，比如根目录下的/hadoop-2.7.2.tar.gz</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    （1）下载第一块</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek1</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 获取输入流</span></span><br><span class="line">        FSDataInputStream fis = fs.open(<span class="keyword">new</span> Path(<span class="string">&quot;/hadoop-2.7.2.tar.gz&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 创建输出流</span></span><br><span class="line">        FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">&quot;H:/hadoop-2.7.2.tar.gz.part1&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 流的拷贝</span></span><br><span class="line">        <span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i =<span class="number">0</span> ; i &lt; <span class="number">1024</span> * <span class="number">128</span>; i++)&#123;</span><br><span class="line">            fis.read(buf);</span><br><span class="line">            fos.write(buf);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5关闭资源</span></span><br><span class="line">        IOUtils.closeStream(fis);</span><br><span class="line">        IOUtils.closeStream(fos);</span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    （2）下载第二块</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek2</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 打开输入流</span></span><br><span class="line">        FSDataInputStream fis = fs.open(<span class="keyword">new</span> Path(<span class="string">&quot;/hadoop-2.7.2.tar.gz&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 定位输入数据位置</span></span><br><span class="line">        fis.seek(<span class="number">1024</span>*<span class="number">1024</span>*<span class="number">128</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 创建输出流</span></span><br><span class="line">        FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">&quot;H:/hadoop-2.7.2.tar.gz.part2&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 流的对拷</span></span><br><span class="line">        IOUtils.copyBytes(fis, fos, configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 关闭资源</span></span><br><span class="line">        IOUtils.closeStream(fis);</span><br><span class="line">        IOUtils.closeStream(fos);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">        （3）合并文件</span></span><br><span class="line"><span class="comment">                在Window命令窗口中进入到目录H:\，然后执行如下命令，对数据进行合并</span></span><br><span class="line"><span class="comment">                type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1</span></span><br><span class="line"><span class="comment">                合并完成后，将hadoop-2.7.2.tar.gz.part1重新命名为hadoop-2.7.2.tar.gz。解压发现该tar包非常完整。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>&emsp; </p><h1 id="HDFS的读写流程"><a href="#HDFS的读写流程" class="headerlink" title="HDFS的读写流程"></a>HDFS的读写流程</h1><h2 id="HDFS写数据流程-剖析文件写入"><a href="#HDFS写数据流程-剖析文件写入" class="headerlink" title="HDFS写数据流程(剖析文件写入)"></a>HDFS写数据流程(剖析文件写入)</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/hdfswrite.png" width = "1201.56" height = "600" alt="xubatian的博客" align="center" /><p>Bytebuffer 缓存</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1）客户端通过Distributed FileSystem(分布式文件系统)模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。</span><br><span class="line">2）NameNode返回是否可以上传。</span><br><span class="line">3）客户端请求第一个 Block(块)上传到哪几个DataNode服务器上。</span><br><span class="line">4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。</span><br><span class="line">5）客户端通过FSDataOutputStream(拿到输出流)模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。(FSDataOutputStream，这个类重载了很多write方法，用于写入很多类型的数据：比如字节数组，long，int，char等等。)</span><br><span class="line">6）dn1、dn2、dn3逐级应答客户端。</span><br><span class="line">7）客户端开始往dn1上传第一个Block(128M)（先从磁盘读取数据放到一个本地内存缓存），他不可能一次性传128M,他是以Packet(数据包)为单位的形式传,(丢包就是这种形式,有部分数据没收到)，dn1收到一个Packet就会先放进他的缓存里面然后落盘,存到内存的目的是因为他还要传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答,假如我成功收到了,我放在队列当中的东西会被响应的。</span><br><span class="line">8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</span><br></pre></td></tr></table></figure><p>&emsp; </p><h2 id="HDFS读数据流程"><a href="#HDFS读数据流程" class="headerlink" title="HDFS读数据流程"></a>HDFS读数据流程</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/hdfsread.png" width = "1201.56" height = "600" alt="xubatian的博客" align="center" /><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</span><br><span class="line">2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。</span><br><span class="line">3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet(数据包)为单位来做校验）。</span><br><span class="line">4）客户端以Packet(数据包)为单位接收，先在本地缓存，然后写入目标文件。 </span><br></pre></td></tr></table></figure><p>&emsp; </p><h1 id="NameNode和SecondaryNameNode"><a href="#NameNode和SecondaryNameNode" class="headerlink" title="NameNode和SecondaryNameNode"></a>NameNode和SecondaryNameNode</h1><p>对于现在hadoop升级到3.x之后,其实现在namenode已经不太是重点了. 在早期的版本当中很重要,因为在早期的版本中namenode存在一个单点故障. 早期的版本中没有很好的解决方案,只能有secondarynamenode来进行恢复大部分的数据.<br>但是对于现在来说并不是特别重要了. 因为我们有了一个高可用.即hadoopHA.他解决了namenode的一个单点故障的问题. 实际上我们有了高可用之后我们的secondarynamenode我们已经不需要再用他了.</p><p>&emsp; </p><h2 id="NameNode和SecondaryNameNode工作机制"><a href="#NameNode和SecondaryNameNode工作机制" class="headerlink" title="NameNode和SecondaryNameNode工作机制"></a>NameNode和SecondaryNameNode工作机制</h2><p>思考：NameNode中的元数据是存储在哪里的？<br>首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage(镜像文件)。<br>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits(修改记录)文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。<br>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。</p><p>&emsp; </p><p>NameNode和SecondaryNameNode工作机制图示:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/namenode1.png" width = "1201.56" height = "600" alt="xubatian的博客" align="center" /><ol><li><p>第一阶段：NameNode启动<br> （1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。<br> （2）客户端对元数据进行增删改的请求。<br> （3）NameNode记录操作日志，更新滚动日志。<br> （4）NameNode在内存中对元数据进行增删改。</p></li><li><p>第二阶段：Secondary NameNode工作<br> （1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。<br> （2）Secondary NameNode请求执行CheckPoint。<br> （3）NameNode滚动正在写的Edits日志。<br> （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。<br> （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。<br> （6）生成新的镜像文件fsimage.chkpoint。<br> （7）拷贝fsimage.chkpoint到NameNode。<br> （8）NameNode将fsimage.chkpoint重新命名成fsimage。</p><p> &emsp; </p><p> NN和2NN工作机制详解：</p></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Fsimage：NameNode内存中元数据序列化后形成的文件。</span><br><span class="line">Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。</span><br><span class="line">NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。</span><br><span class="line">由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。</span><br><span class="line">SecondaryNameNode首先会询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中</span><br></pre></td></tr></table></figure><p>&emsp; </p><h2 id="Fsimage和Edits解析"><a href="#Fsimage和Edits解析" class="headerlink" title="Fsimage和Edits解析"></a>Fsimage和Edits解析</h2><h3 id="Fsimage和Edits概念"><a href="#Fsimage和Edits概念" class="headerlink" title="Fsimage和Edits概念"></a>Fsimage和Edits概念</h3><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/edits.png" width = "1201.56" height = "500" alt="xubatian的博客" align="center" /><h3 id="oiv查看Fsimage文件"><a href="#oiv查看Fsimage文件" class="headerlink" title="oiv查看Fsimage文件"></a>oiv查看Fsimage文件</h3><p>（1）查看oiv和oev命令,查看镜像文件用oiv,查看编辑日志用oev</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 current]$ hdfs</span><br><span class="line">oiv            apply the offline fsimage viewer to an fsimage</span><br><span class="line">oev            apply the offline edits viewer to an edits file</span><br><span class="line">[shangbaishuyao@Hadoop102 current]$ hdfs oiv -p xml -i fsimage_0000000000000000125 -o ./fsimage125.xml</span><br><span class="line">[shangbaishuyao@Hadoop102 current]$ cat fsimage125.xml </span><br></pre></td></tr></table></figure><p>（2）基本语法<br>hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</p><p>思考：可以看出，Fsimage中没有记录块所对应DataNode，为什么？<br>在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报。</p><p>&emsp; </p><h3 id="oev查看Edits文件"><a href="#oev查看Edits文件" class="headerlink" title="oev查看Edits文件"></a>oev查看Edits文件</h3><p>1）基本语法<br>hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径</p><p>2）案例实操</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/edits.xml</span><br></pre></td></tr></table></figure><p>思考：NameNode如何确定下次开机启动的时候合并哪些Edits？</p><p>&emsp; </p><h3 id="CheckPoint-检查点-时间设置"><a href="#CheckPoint-检查点-时间设置" class="headerlink" title="CheckPoint(检查点)时间设置"></a>CheckPoint(检查点)时间设置</h3><p>1）通常情况下，SecondaryNameNode每隔一小时执行一次。<br>    [hdfs-default.xml]</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;3600&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>2）一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1000000&lt;/value&gt;</span><br><span class="line">&lt;description&gt;操作动作次数&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;60&lt;/value&gt;</span><br><span class="line">&lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt;</span><br><span class="line">&lt;/property &gt;</span><br></pre></td></tr></table></figure><h3 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h3><p>NameNode故障后，可以采用如下两种方法恢复数据。<br>    方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录；</p><ol><li><p>kill -9 NameNode进程</p></li><li><p>删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</p></li></ol>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</span><br></pre></td></tr></table></figure><ol start="3"><li>拷贝SecondaryNameNode中数据到原NameNode存储数据目录</li></ol>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/</span><br></pre></td></tr></table></figure><ol start="4"><li>重新启动NameNode</li></ol>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure><p>  方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。<br>  1.修改hdfs-site.xml中的</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;120&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><ol start="2"><li><p>kill -9 NameNode进程</p></li><li><p>删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</span><br></pre></td></tr></table></figure></li><li><p>如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./</span><br><span class="line">[shangbaishuyao@hadoop102 namesecondary]$ rm -rf in_use.lock</span><br><span class="line">[shangbaishuyao@hadoop102 dfs]$ pwd</span><br><span class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs</span><br><span class="line">[shangbaishuyao@hadoop102 dfs]$ ls</span><br><span class="line">data  name  namesecondary</span><br></pre></td></tr></table></figure><p>导入检查点数据（等待一会ctrl+c结束掉）<br>[shangbaishuyao@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint</p></li><li><p>启动NameNode<br>[shangbaishuyao@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</p><p>&emsp; </p></li></ol><h3 id="Hadoop集群安全模式"><a href="#Hadoop集群安全模式" class="headerlink" title="Hadoop集群安全模式"></a>Hadoop集群安全模式</h3><p>概述:</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/namenodeSafe.png" width = "800" height = "400" alt="xubatian的博客" align="center" /><p>基本语法:</p><p>集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。<br>（1）bin/hdfs dfsadmin -safemode get        （功能描述：查看安全模式状态）<br>（2）bin/hdfs dfsadmin -safemode enter      （功能描述：进入安全模式状态）<br>（3）bin/hdfs dfsadmin -safemode leave    （功能描述：离开安全模式状态）<br>（4）bin/hdfs dfsadmin -safemode wait    （功能描述：等待安全模式状态）</p><p>&emsp; </p><h3 id="NameNode多目录配置"><a href="#NameNode多目录配置" class="headerlink" title="NameNode多目录配置"></a>NameNode多目录配置</h3><ol><li><p>NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性</p></li><li><p>具体配置如下<br> （1）在hdfs-site.xml文件中增加如下内容,引用了hdfs-core.xml的hadoop.tmp.dir配置</p></li></ol><pre><code><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>（2）停止集群，删除data和logs中所有数据。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ rm -rf data/ logs/</span><br><span class="line">[shangbaishuyao@hadoop103 hadoop-2.7.2]$ rm -rf data/ logs/</span><br><span class="line">[shangbaishuyao@hadoop104 hadoop-2.7.2]$ rm -rf data/ logs/</span><br></pre></td></tr></table></figure>（3）格式化集群并启动。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode –format</span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>（4）查看结果<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 dfs]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">drwx------. 3 shangbaishuyao shangbaishuyao 4096 12月 11 08:03 data</span><br><span class="line">drwxrwxr-x. 3 shangbaishuyao shangbaishuyao 4096 12月 11 08:03 name1</span><br><span class="line">drwxrwxr-x. 3 shangbaishuyao shangbaishuyao 4096 12月 11 08:03 name2</span><br></pre></td></tr></table></figure></code></pre><p>&emsp; </p><h2 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/datanode.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><p>1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。<br>2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。<br>3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟30秒没有收到某个DataNode的心跳，则认为该节点不可用。<br>4）集群运行中可以安全加入和退出一些机器。</p><h3 id="数据完整性-如何保存存储的数据是完整的"><a href="#数据完整性-如何保存存储的数据是完整的" class="headerlink" title="数据完整性(如何保存存储的数据是完整的?)"></a>数据完整性(如何保存存储的数据是完整的?)</h3><p>思考：如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号（1）和绿灯信号（0），但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理DataNode节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？<br>如下是DataNode节点保证数据完整性的方法。<br>1）当DataNode读取Block的时候，它会计算CheckSum(校验和)。校验和,就是经过一定的算法的到一个数据的结果,这个结果就是校验和; </p><p> 一个数据在上传前经过算法变成校验盒,然后再把数据放到hdfs上,经过算法变成校验盒,如果前后两个校验盒相同,则说明数据完整,数据没发生破坏对于银行等机构,数据完整性是相当重要的<br>2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。<br>3）Client读取其他DataNode上的Block。<br>4）DataNode在其文件创建后周期验证CheckSum，如图所示。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/shuju.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><p>&emsp; </p><h3 id="掉线时限参数设置-10分钟30秒"><a href="#掉线时限参数设置-10分钟30秒" class="headerlink" title="掉线时限参数设置 10分钟30秒"></a>掉线时限参数设置 10分钟30秒</h3><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/diaoxian.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval(心跳核查间隔)的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;300000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;“你和萤火虫有两个共同点，在我的眼里都会发光，同时，都已经很多年没见了。”    –来自网易云音乐《夜、萤火虫和你》                            &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="HDFS" scheme="http://xubatian.cn/tags/HDFS/"/>
    
    <category term="Namenode" scheme="http://xubatian.cn/tags/Namenode/"/>
    
    <category term="DataNode" scheme="http://xubatian.cn/tags/DataNode/"/>
    
    <category term="SecondaryNameNode" scheme="http://xubatian.cn/tags/SecondaryNameNode/"/>
    
  </entry>
  
  <entry>
    <title>hadoop组成模块及各模块的简介</title>
    <link href="http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E5%8F%8A%E5%90%84%E6%A8%A1%E5%9D%97%E7%9A%84%E7%AE%80%E4%BB%8B/"/>
    <id>http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E5%8F%8A%E5%90%84%E6%A8%A1%E5%9D%97%E7%9A%84%E7%AE%80%E4%BB%8B/</id>
    <published>2022-01-12T05:56:07.000Z</published>
    <updated>2022-01-14T07:10:14.033Z</updated>
    
    <content type="html"><![CDATA[<p> 太阳在坠落 海浪在发愁 你在干什么       –来自网易云音乐《白墙》                            </p><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/images/11.jpg" width = "1201.56" height = "675.88" alt="xubatian的博客" align="center" /><p>&emsp;</p><p>前言<br>前面了解到了什么是大数据, 学习大数据为什么要学习hadoop, hadoop是什么.<br>现在我们需要具体的了解hadoop.<br>此篇文章具体了解hadoop的组成.</p><h1 id="从官网看hadoop组成结构"><a href="#从官网看hadoop组成结构" class="headerlink" title="从官网看hadoop组成结构"></a>从官网看hadoop组成结构</h1><p>官网地址:  <a href="https://hadoop.apache.org/">https://hadoop.apache.org/</a></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/hadoopguangwang1.png" width = "900" height = "500" alt="xubatian的博客" align="center" /><p>由上图官网可知, hadoop可以分为四个模块: HDFS,MapReduce,Yarn,Common. </p><p>那么hadoop从发行版本,即hadoop1.X 到现如今的 hadoop3.x,他都是这四个模块没有变化吗? </p><h1 id="Hadoop各个版本的不同模块及负责的功能变化"><a href="#Hadoop各个版本的不同模块及负责的功能变化" class="headerlink" title="Hadoop各个版本的不同模块及负责的功能变化"></a>Hadoop各个版本的不同模块及负责的功能变化</h1><p>&emsp;</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/hadoop4.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><p>由图可知, hadoop最开始是只有三个模块的,及 common,hdfs,mapreduce. 最开始是将计算和资源调度都由mapreduce掌控.演化到hadoop2.x之后才增加了yarn模块,将原本mapreduce掌控计算和资源调度的工作转变成了 mapreduce只负责计算. yarn负责资源调度的方式. 一直沿用至今hadoop3.x</p><p>&emsp;</p><h1 id="Hadoop四个模块具体的功能如何理解呢"><a href="#Hadoop四个模块具体的功能如何理解呢" class="headerlink" title="Hadoop四个模块具体的功能如何理解呢?"></a>Hadoop四个模块具体的功能如何理解呢?</h1><h2 id="①-HDFS架构"><a href="#①-HDFS架构" class="headerlink" title="① HDFS架构"></a>① HDFS架构</h2><p>Hadoop Distributed File System，简称HDFS，是一个分布式文件系统</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/hdfs.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><p>&emsp;</p><h2 id="②YARN架构"><a href="#②YARN架构" class="headerlink" title="②YARN架构"></a>②YARN架构</h2><p>Yet Another Resource Negotiator简称YARN ，另一种资源协调者，是Hadoop的资源管理器。</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/yarn.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><p>&emsp;</p><h2 id="③MapReduce架构"><a href="#③MapReduce架构" class="headerlink" title="③MapReduce架构"></a>③MapReduce架构</h2><p>MapReduce将计算过程分为两个阶段：Map和Reduce</p><p>1）Map阶段并行处理输入数据</p><p>2）Reduce阶段对Map结果进行汇总</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mr.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><h2 id="④Common模块"><a href="#④Common模块" class="headerlink" title="④Common模块"></a>④Common模块</h2><p>Hadoop-common是指支持Hadoop模块的常用实用程序和库的集合.类似Java的公共类. 学习hadoop的时候设计Common不多.看源码的时候遇到的比较多.此处暂不介绍.</p><p>&emsp;</p><h2 id="⑤HDFS、YARN、MapReduce三者关系"><a href="#⑤HDFS、YARN、MapReduce三者关系" class="headerlink" title="⑤HDFS、YARN、MapReduce三者关系"></a>⑤HDFS、YARN、MapReduce三者关系</h2><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/hmy.png" width = "700" height = "400" alt="xubatian的博客" align="center" /><p>&emsp;</p><p>此处先简要了解hadoop四个模块的组成. 后面会一一详解.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt; 太阳在坠落 海浪在发愁 你在干什么       –来自网易云音乐《白墙》                            &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Linux命令收集</title>
    <link href="http://xubatian.cn/Linux%E5%91%BD%E4%BB%A4%E6%94%B6%E9%9B%86/"/>
    <id>http://xubatian.cn/Linux%E5%91%BD%E4%BB%A4%E6%94%B6%E9%9B%86/</id>
    <published>2022-01-11T15:43:31.000Z</published>
    <updated>2022-01-14T00:53:45.843Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;对于常用的Linux命令进行持续收集。持续更新中…</p><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/images/2.png" width = "1201.56" height = "675.88" alt="xubatian的博客" align="center" /><h1 id="Linux（vi-vim）"><a href="#Linux（vi-vim）" class="headerlink" title="Linux（vi/vim）"></a>Linux（vi/vim）</h1><h2 id="一般模式"><a href="#一般模式" class="headerlink" title="一般模式"></a>一般模式</h2><table><thead><tr><th>语法</th><th>功能描述</th></tr></thead><tbody><tr><td>yy</td><td>复制光标当前一行</td></tr><tr><td>y数字y</td><td>复制一段（从第几行到第几行）</td></tr><tr><td>p</td><td>箭头移动到目的行粘贴</td></tr><tr><td>u</td><td>撤销上一步</td></tr><tr><td>dd</td><td>删除光标当前行</td></tr><tr><td>d数字d</td><td>删除光标（含）后多少行</td></tr><tr><td>x</td><td>删除一个字母，相当于del</td></tr><tr><td>X</td><td>删除一个字母，相当于Backspace</td></tr><tr><td>yw</td><td>复制一个词</td></tr><tr><td>dw</td><td>删除一个词</td></tr><tr><td>shift+^</td><td>移动到行头</td></tr><tr><td>shift+$</td><td>移动到行尾</td></tr><tr><td>1+shift+g</td><td>移动到页头，数字</td></tr><tr><td>shift+g</td><td>移动到页尾</td></tr><tr><td>数字N+shift+g</td><td>移动到目标行</td></tr></tbody></table><h2 id="编辑模式"><a href="#编辑模式" class="headerlink" title="编辑模式"></a>编辑模式</h2><table><thead><tr><th>按键</th><th>功能</th></tr></thead><tbody><tr><td>i</td><td>当前光标前</td></tr><tr><td>a</td><td>当前光标后</td></tr><tr><td>o</td><td>当前光标行的下一行</td></tr><tr><td>I</td><td>光标所在行最前</td></tr><tr><td>A</td><td>光标所在行最后</td></tr><tr><td>O</td><td>当前光标行的上一行</td></tr></tbody></table><h2 id="指令模式"><a href="#指令模式" class="headerlink" title="指令模式"></a>指令模式</h2><table><thead><tr><th>命令</th><th>功能</th></tr></thead><tbody><tr><td>:w</td><td>保存</td></tr><tr><td>:q</td><td>退出</td></tr><tr><td>:!</td><td>强制执行</td></tr><tr><td>/要查找的词</td><td>n 查找下一个，N 往上查找</td></tr><tr><td>? 要查找的词</td><td>n是查找上一个，shift+n是往下查找</td></tr><tr><td>:set nu</td><td>显示行号</td></tr><tr><td>:set nonu</td><td>关闭行号</td></tr></tbody></table><p>&emsp;</p><h2 id="压缩和解压"><a href="#压缩和解压" class="headerlink" title="压缩和解压"></a>压缩和解压</h2><h4 id="gzip-gunzip-压缩"><a href="#gzip-gunzip-压缩" class="headerlink" title="gzip/gunzip 压缩"></a>gzip/gunzip 压缩</h4><p>（1）只能压缩文件不能压缩目录</p><p>（2）不保留原来的文件</p><p>gzip压缩：gzip hello.txt</p><p>gunzip解压缩文件：gunzip hello.txt.gz</p><p>&emsp;</p><h4 id="zip-unzip-压缩"><a href="#zip-unzip-压缩" class="headerlink" title="zip/unzip 压缩"></a>zip/unzip 压缩</h4><p>可以压缩目录且保留源文件</p><p>zip压缩（压缩 1.txt 和2.txt，压缩后的名称为mypackage.zip）：zip hello.zip hello.txt world.txt</p><p>unzip解压：unzip hello.zip</p><p>unzip解压到指定目录：unzip hello.zip -d /opt</p><p>&emsp;</p><h4 id="tar-打包"><a href="#tar-打包" class="headerlink" title="tar 打包"></a>tar 打包</h4><p>tar压缩多个文件：tar -zcvf hello.txt world.txt</p><p>tar压缩目录：tar -zcvf hello.tar.gz opt/</p><p>tar解压到当前目录：tar -zxvf hello.tar.gz</p><p>tar解压到指定目录：tar -zxvf hello.tar.gz -C /opt</p><p>&emsp;</p><h3 id="RPM"><a href="#RPM" class="headerlink" title="RPM"></a>RPM</h3><p>RPM查询命令：rpm -qa |grep firefox</p><p>RPM卸载命令：</p><p>rpm -e xxxxxx</p><p>rpm -e –nodeps xxxxxx（不检查依赖）</p><p>RPM安装命令：</p><p>rpm -ivh xxxxxx.rpm</p><p>rpm -ivh –nodeps fxxxxxx.rpm（–nodeps，不检测依赖进度）</p><p>&emsp;</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-i</td><td>-i=install，安装</td></tr><tr><td>-v</td><td>-v=verbose，显示详细信息</td></tr><tr><td>-h</td><td>-h=hash，进度条</td></tr><tr><td>–nodeps</td><td>–nodeps，不检测依赖进度</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;nbsp;对于常用的Linux命令进行持续收集。持续更新中…&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Linux" scheme="http://xubatian.cn/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>博客文章目录</title>
    <link href="http://xubatian.cn/Directory/"/>
    <id>http://xubatian.cn/Directory/</id>
    <published>2022-01-08T21:36:28.000Z</published>
    <updated>2022-01-09T11:33:22.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>知识源于积累,登峰造极源于自律. 整理自己发在博客上的系列文章，方便查找。索引持续更新中…</strong>  </p><p>注意: </p><ol><li>每篇文章右下角小猫旁边就是目录哦~~  鼠标移至此处就会展示我<strong>精心</strong>分化好的目录.</li><li>NO ￥,所以多数图片使用CDN+Guthub,少部分使用阿里云OSS.请耐心加载. 什么? 赏里面的二维码加载的很快? 来人,将此人拉出去枪毙一百遍,下一个问题~~ &nbsp;&nbsp;</li><li>文章最下评论系统部分可能需要二次刷新页面才能展示~ 虽然评论没有实名制,但也不要MR哦,( ´◔ ‸◔’)！</li><li>啥?音乐有bug? 错,这是我精心设计的,为的就是不打扰咱们看文章, 看文章需静心(￢_￢) ~~</li></ol><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/images/1.png" width = "1201.56" height = "675.88" alt="xubatian的博客" align="center" /><p> &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;博客索引目录</p><h1 id="Java编程-笔记"><a href="#Java编程-笔记" class="headerlink" title="Java编程(笔记)"></a>Java编程(笔记)</h1><h1 id="大数据开发-笔记"><a href="#大数据开发-笔记" class="headerlink" title="大数据开发(笔记)"></a>大数据开发(笔记)</h1><h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><p><a href="https://www.xubatian.cn/Linux%E5%91%BD%E4%BB%A4%E6%94%B6%E9%9B%86/">Linux命令收集</a></p><h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><p><a href="https://www.xubatian.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%98%AF%E4%BB%80%E4%B9%88/">什么是大数据</a></p><p><a href="https://www.xubatian.cn/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E8%AE%A8%E8%AE%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/">从Hadoop框架讨论大数据生态</a></p><p><a href="https://www.xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E5%8F%8A%E5%90%84%E6%A8%A1%E5%9D%97%E7%9A%84%E7%AE%80%E4%BB%8B/">hadoop组成模块及各模块的简介</a></p><p><a href="https://www.xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BHDFS%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E8%AF%A6%E8%A7%A3/">hadoop组成模块之HDFS分布式存储详解</a></p><p><a href="https://www.xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BHDFS-HA%E9%AB%98%E5%8F%AF%E7%94%A8/">hadoop组成模块之HDFS-HA高可用</a></p><p><a href="https://www.xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BYarn-HA%E9%AB%98%E5%8F%AF%E7%94%A8/">hadoop组成模块之Yarn-HA高可用</a></p><p><a href="https://www.xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BMapReduce%E8%AF%A6%E8%A7%A3/">hadoop组成模块之MapReduce概述</a></p><p>hadoop组成模块之MapReduce框架原理</p><p><a href="https://www.xubatian.cn/mapreduce%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84shuffle%E6%9C%BA%E5%88%B6%E5%8E%9F%E7%90%86/">mapreduce过程中的shuffle机制原理</a></p><p>hadoop组成模块之mapreduce的MapTask机制和reduceTask机制</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;知识源于积累,登峰造极源于自律. 整理自己发在博客上的系列文章，方便查找。索引持续更新中…&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;注意: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每篇文章右下角小猫旁边就是目录哦~~  鼠标移至此处就会展示我&lt;strong&gt;精心&lt;/strong&gt;分化好的目录.&lt;/li&gt;
&lt;li&gt;NO ￥,所以多数图片使用CDN+Guthub,少部分使用阿里云OSS.请耐心加载. 什么? 赏里面的二维码加载的很快? 来人,将此人拉出去枪毙一百遍,下一个问题~~ &amp;nbsp;&amp;nbsp;&lt;/li&gt;
&lt;li&gt;文章最下评论系统部分可能需要二次刷新页面才能展示~ 虽然评论没有实名制,但也不要MR哦,( ´◔ ‸◔’)！&lt;/li&gt;
&lt;li&gt;啥?音乐有bug? 错,这是我精心设计的,为的就是不打扰咱们看文章, 看文章需静心(￢_￢) ~~&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="目录" scheme="http://xubatian.cn/categories/%E7%9B%AE%E5%BD%95/"/>
    
    
    <category term="xubatian博客导航栏" scheme="http://xubatian.cn/tags/xubatian%E5%8D%9A%E5%AE%A2%E5%AF%BC%E8%88%AA%E6%A0%8F/"/>
    
  </entry>
  
</feed>
