<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>我的梦想是星辰大海</title>
  
  <subtitle>知识源于积累,登峰造极源于自律</subtitle>
  <link href="http://xubatian.cn/atom.xml" rel="self"/>
  
  <link href="http://xubatian.cn/"/>
  <updated>2022-01-28T06:38:21.611Z</updated>
  <id>http://xubatian.cn/</id>
  
  <author>
    <name>xubatian</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>到底是什么是我心态不稳?</title>
    <link href="http://xubatian.cn/%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%E6%98%AF%E6%88%91%E5%BF%83%E6%80%81%E4%B8%8D%E7%A8%B3/"/>
    <id>http://xubatian.cn/%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%E6%98%AF%E6%88%91%E5%BF%83%E6%80%81%E4%B8%8D%E7%A8%B3/</id>
    <published>2022-01-28T05:47:56.000Z</published>
    <updated>2022-01-28T06:38:21.611Z</updated>
    
    <content type="html"><![CDATA[<p>编译spark源码的三天时间,我到底经历了什么?</p><span id="more"></span><p>原本我是打算使用本地进行编译的…结果撑了一天,不行了.一大波bug正在赶来….</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128135223.png" alt="博客: www.xubatian.cn"></p><p>这种bug,我解决了不下二十个. 最关键的是,这种报错都是些jar包下载不下来….</p><div align="center">    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128135425.png" alt="博客: www.xubatian.cn"></img></div><p>想想还是算了吧,换服务器编译吧… 在准备了一堆maven,scala等一堆环境变量之后,终于走上了编译之路.</p><p>打死我都没想,这条路黑暗了我两天美好的人生….</p><p>再哭一下…..</p><div align="center">    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128135700.png" alt="博客: www.xubatian.cn"></img></div><p>——————————————————— 以下是我最常遇到的bug  , 经常光顾我 ,也是老熟人了  ——————————————————-</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128135009.png" alt="博客: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128135024.png" alt="博客: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128140050.png" alt="博客: www.xubatian.cn"></p><p>阳光总在风雨后…</p><p>历经三天的折磨,终于编译出了合适我hadoop版本的spark…..</p><p>此处咧嘴大笑….</p><div align="center">    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128141051.png" alt="博客: www.xubatian.cn"></img></div><p>炫耀版的展示一下….嘻嘻…</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/www.xubatian.cn_400.png" alt="博客: www.xubatian.cn"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;编译spark源码的三天时间,我到底经历了什么?&lt;/p&gt;</summary>
    
    
    
    <category term="动态" scheme="http://xubatian.cn/categories/%E5%8A%A8%E6%80%81/"/>
    
    
    <category term="动态" scheme="http://xubatian.cn/tags/%E5%8A%A8%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>streamx源码编译及安装部署-本地编译(推荐)</title>
    <link href="http://xubatian.cn/streamx%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-%E6%9C%AC%E5%9C%B0%E7%BC%96%E8%AF%91(%E6%8E%A8%E8%8D%90)/"/>
    <id>http://xubatian.cn/streamx%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-%E6%9C%AC%E5%9C%B0%E7%BC%96%E8%AF%91(%E6%8E%A8%E8%8D%90)/</id>
    <published>2022-01-22T12:21:54.000Z</published>
    <updated>2022-01-23T03:05:51.541Z</updated>
    
    <content type="html"><![CDATA[<p>锚定既定奋斗目标，意气风发走向未来。——人民日报</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_394.jpg" alt="blog: www.xubatian.cn"></p><p>在服务器端进行了streamx编译的那文章也说了,我没有flink,hadoop 的配置,所以我重新进行了streamx的源码编译,版本依旧是streamx-1.2.0 稳定版本</p><p><strong>现在编译的是Flink版本为1.14.3 , hadoop版本为3.1.3.</strong></p><h1 id="源码编译的前提条件"><a href="#源码编译的前提条件" class="headerlink" title="源码编译的前提条件"></a>源码编译的前提条件</h1><p>我使用的是maven 3.8.3版本. node js , jdk1.8.3 ,npm. </p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/20220122203333.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/20220122203432.png" alt="blog: www.xubatian.cn"></p><h1 id="streamx源码编译"><a href="#streamx源码编译" class="headerlink" title="streamx源码编译"></a>streamx源码编译</h1><h2 id="从官网下载streamx稳定版本"><a href="#从官网下载streamx稳定版本" class="headerlink" title="从官网下载streamx稳定版本"></a>从官网下载streamx稳定版本</h2><p>官网地址: <a href="https://www.streamxhub.com/#">https://www.streamxhub.com/#</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_391.png" alt="blog: www.xubatian.cn"></p><p>github地址: <a href="https://github.com/streamxhub/streamx">https://github.com/streamxhub/streamx</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_390.png" alt="blog: www.xubatian.cn"></p><h2 id="修改streamx的相关版本"><a href="#修改streamx的相关版本" class="headerlink" title="修改streamx的相关版本"></a>修改streamx的相关版本</h2><p>修改为公司hadoop,flink,spark等相符合的大数据组件版本. </p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122201007.png" alt="blog: www.xubatian.cn"></p><h2 id="编译streamx源码"><a href="#编译streamx源码" class="headerlink" title="编译streamx源码"></a>编译streamx源码</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/20220122203910.png" alt="blog: www.xubatian.cn"></p><h2 id="报错及解决方式"><a href="#报错及解决方式" class="headerlink" title="报错及解决方式"></a>报错及解决方式</h2><p>常常报错的问题就是 jar包下载不下来. 或者maven镜像无法来取jar包. </p><p>解决方式:</p><p>① 看看是那个jar包下载不下来. </p><p>② 复制该jar名称,去maven中央仓库直接下载版本相同的jar</p><p>maven中央仓库地址: <a href="https://mvnrepository.com/">https://mvnrepository.com/</a> </p><p>注: 也可以直接放到谷歌浏览器上直接搜索.</p><p>③ 删除之前编译残留的文件,将下载好的jar包拷贝到本地maven仓库(注意:一定要放到指定的文件夹下)</p><p>④ 然后重新编译</p><h3 id="如下是示例"><a href="#如下是示例" class="headerlink" title="如下是示例:"></a>如下是示例:</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122194838.png" alt="blog: www.xubatian.cn"></p><h3 id="查看本地maven仓库-删除全部残余文件"><a href="#查看本地maven仓库-删除全部残余文件" class="headerlink" title="查看本地maven仓库,删除全部残余文件"></a>查看本地maven仓库,删除全部残余文件</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122195038.png" alt="blog: www.xubatian.cn"></p><h3 id="下载版本一样的jar包"><a href="#下载版本一样的jar包" class="headerlink" title="下载版本一样的jar包"></a>下载版本一样的jar包</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122195126.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122195208.png" alt="blog: www.xubatian.cn"></p><p>将下载好的jar拷贝至本地maven仓库</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122195342.png" alt="blog: www.xubatian.cn"></p><h3 id="重新编译"><a href="#重新编译" class="headerlink" title="重新编译"></a>重新编译</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/20220122203910.png" alt="blog: www.xubatian.cn"></p><h2 id="反复经过N次上述行为后-恭喜你-成功了"><a href="#反复经过N次上述行为后-恭喜你-成功了" class="headerlink" title="反复经过N次上述行为后,恭喜你,成功了"></a>反复经过N次上述行为后,恭喜你,成功了</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122200810.png" alt="blog: www.xubatian.cn"></p><h2 id="在此目录下的压缩包拷贝至服务器上解压"><a href="#在此目录下的压缩包拷贝至服务器上解压" class="headerlink" title="在此目录下的压缩包拷贝至服务器上解压"></a>在此目录下的压缩包拷贝至服务器上解压</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122201341.png" alt="blog: www.xubatian.cn"></p><p>后缀是macOS的是我在本地进行编译的. 后缀是Linux的是我在服务器端编译的.</p><p>二者的区别就是 macOS端的我修改了hadoop版本为3.1.3 ,Flink版本为1.14.3</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/20220122205512.png" alt="blog: www.xubatian.cn"></p><h2 id="streamx部署"><a href="#streamx部署" class="headerlink" title="streamx部署"></a>streamx部署</h2><p>请看我的另一篇文章: <a href="https://www.xubatian.cn/streamx%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E7%BC%96%E8%AF%91/">streamx源码编译及安装部署-服务器端编译</a></p><h2 id="启动streamx"><a href="#启动streamx" class="headerlink" title="启动streamx"></a>启动streamx</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122201928.png" alt="blog: www.xubatian.cn"></p><h2 id="访问Web页面"><a href="#访问Web页面" class="headerlink" title="访问Web页面"></a>访问Web页面</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122202012.png" alt="blog: www.xubatian.cn"></p><h2 id="编译好的streamx存放地址"><a href="#编译好的streamx存放地址" class="headerlink" title="编译好的streamx存放地址"></a>编译好的streamx存放地址</h2><p>streamx1.12.0 源码默认配置 直接编译 编译后的压缩包为: streamx-console-service-1.2.0-Linux-bin.tar.gz<br>streamx1.12.0 源码,修改hadoop版本为3.1.3, flink版本为1.14.3 编译后的压缩包为: streamx-console-service-1.2.0-macOS-bin.tar.gz<br>两个压缩包都方式这里,链接: <a href="https://pan.baidu.com/s/1M4R0K3rOzNZOdilnJiduFw">https://pan.baidu.com/s/1M4R0K3rOzNZOdilnJiduFw</a> 提取码: 2olc</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;锚定既定奋斗目标，意气风发走向未来。——人民日报&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="streamx" scheme="http://xubatian.cn/tags/streamx/"/>
    
  </entry>
  
  <entry>
    <title>streamx源码编译及安装部署-服务器端编译</title>
    <link href="http://xubatian.cn/streamx%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E7%BC%96%E8%AF%91/"/>
    <id>http://xubatian.cn/streamx%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E7%BC%96%E8%AF%91/</id>
    <published>2022-01-22T06:38:59.000Z</published>
    <updated>2022-01-23T03:05:56.394Z</updated>
    
    <content type="html"><![CDATA[<p>遇到问题，改变苛求别人的惯性，重新塑造思考问题的方式。换个角度看世界，换个方向看问题，就会豁然开朗。——人民日报</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_387.jpg" alt="blog: www.xubatian.cn"></p><h1 id="什么是streamx"><a href="#什么是streamx" class="headerlink" title="什么是streamx"></a>什么是streamx</h1><p> 大数据技术如今发展的如火如荼，已经呈现百花齐放欣欣向荣的景象，实时处理流域 Apache Spark 和 Apache Flink 更是一个伟大的进步，尤其是 Apache Flink 被普遍认为是下一代大数据流计算引擎， 我们在使用 Flink 时发现从编程模型， 启动配置到运维管理都有很多可以抽象共用的地方， 我们将一些好的经验固化下来并结合业内的最佳实践， 通过不断努力终于诞生了今天的框架 —— StreamX， 项目的初衷是 —— 让 Flink 开发更简单， 使用 StreamX 开发，可以极大降低学习成本和开发门槛， 让开发者只用关心最核心的业务， StreamX 规范了项目的配置，鼓励函数式编程，定义了最佳的编程方式，提供了一系列开箱即用的 Connectors ，标准化了配置、开发、测试、部署、监控、运维的整个过程， 提供 Scala 和 Java 两套api， 其最终目的是打造一个一站式大数据平台，流批一体，湖仓一体的解决方案.</p><h1 id="源码编译的前提条件"><a href="#源码编译的前提条件" class="headerlink" title="源码编译的前提条件"></a>源码编译的前提条件</h1><p>我使用的是CentOS Linux release 7.5.1804 (Core).  mysql5.7. 以及maven 3.8.3版本. node js 和 jdk1.8.3 .最少2个多G的磁盘空间</p><p>   <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122150459.png" alt="blog: www.xubatian.cn"></p><h2 id="安装node-js"><a href="#安装node-js" class="headerlink" title="安装node-js"></a>安装node-js</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ wget https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz</span><br></pre></td></tr></table></figure><h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ tar xf node-v16.13.1-linux-x64.tar.xz </span><br></pre></td></tr></table></figure><h3 id="进入解压目录"><a href="#进入解压目录" class="headerlink" title="进入解压目录"></a>进入解压目录</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ cd node-v16.13.1-linux-x64</span><br></pre></td></tr></table></figure><h3 id="修改Linux环境变量"><a href="#修改Linux环境变量" class="headerlink" title="修改Linux环境变量"></a>修改Linux环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ vim /etc/profile.d/shangbaishuyao_configurationfile.sh</span><br><span class="line"><span class="meta">#</span><span class="bash">JAVA_HOME 加<span class="built_in">export</span>是对全局有效,相当于对外暴露一个接口</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">HADOOP_HOME</span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">KAFKA_HOME</span></span><br><span class="line">export KAFKA_HOME=/opt/module/kafka_2.11-2.4.1</span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">HIVE_HOME</span></span><br><span class="line">export HIVE_HOME=/opt/module/hive-3.1.2</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">MAVEN_HOME</span></span><br><span class="line">export MAVEN_HOME=/opt/module/maven-3.8.3</span><br><span class="line">export MAVEN_HOME</span><br><span class="line">export PATH=$PATH:$MAVEN_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> SPARK_HOME</span></span><br><span class="line">export SPARK_HOME=/opt/module/spark-3.0.0-hadoop3.2</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> NODE_JS</span></span><br><span class="line">export NODE_HOME=/opt/module/node-v16.13.1-linux-x64</span><br><span class="line">export PATH=$PATH:$NODE_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">HBASE_HOME</span></span><br><span class="line">export HBASE_HOME=/opt/module/hbase-2.0.5</span><br><span class="line">export PATH=$PATH:$HBASE_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">FLINK_HOME</span></span><br><span class="line">export FLINK_HOME=/opt/module/flink-1.14.3</span><br><span class="line">export PATH=$PATH:$FLINK_HOME/bin</span><br><span class="line"></span><br><span class="line">export HADOOP_CLASSPATH=`hadoop classpath`</span><br><span class="line">[shangbaishuyao@hadoop102 module]$</span><br></pre></td></tr></table></figure><h3 id="刷新环境变量"><a href="#刷新环境变量" class="headerlink" title="刷新环境变量"></a>刷新环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ source /etc/profile.d/shangbaishuyao_configurationfile.sh</span><br></pre></td></tr></table></figure><h3 id="查看是否安装成功"><a href="#查看是否安装成功" class="headerlink" title="查看是否安装成功"></a>查看是否安装成功</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ node -v</span><br><span class="line">v16.13.1</span><br></pre></td></tr></table></figure><h2 id="安装maven"><a href="#安装maven" class="headerlink" title="安装maven"></a>安装maven</h2><p>网上例子很多,此处略.</p><p>参考文章: <a href="https://www.cnblogs.com/freeweb/p/5241013.html">https://www.cnblogs.com/freeweb/p/5241013.html</a></p><h2 id="安装npm"><a href="#安装npm" class="headerlink" title="安装npm"></a>安装npm</h2><p>直接安装好node.js就有npm命令了,此处略. 有个问题是. 我在编译streamx的时候因为npm版本过低所以失败三次. 所以我升级了npm命令为: npm install -g npm</p><h2 id="安装JDK"><a href="#安装JDK" class="headerlink" title="安装JDK"></a>安装JDK</h2><p>网上例子很多,此处略.</p><h2 id="安装mysql"><a href="#安装mysql" class="headerlink" title="安装mysql"></a>安装mysql</h2><p>网上例子很多,此处略.</p><h3 id="进入mysql修改配置"><a href="#进入mysql修改配置" class="headerlink" title="进入mysql修改配置"></a>进入mysql修改配置</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 streamx-release-1.2.0]$ vim /etc/my.cnf</span><br><span class="line"></span><br><span class="line">#streamx</span><br><span class="line">port=3306</span><br><span class="line">bind-address=0.0.0.0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122153152.png" alt="blog: www.xubatian.cn"></p><h3 id="重新启动mysql"><a href="#重新启动mysql" class="headerlink" title="重新启动mysql"></a>重新启动mysql</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql restart</span><br></pre></td></tr></table></figure><h1 id="streamx源码编译"><a href="#streamx源码编译" class="headerlink" title="streamx源码编译"></a>streamx源码编译</h1><h2 id="从官网下载streamx稳定版本"><a href="#从官网下载streamx稳定版本" class="headerlink" title="从官网下载streamx稳定版本"></a>从官网下载streamx稳定版本</h2><p>官网地址: <a href="https://www.streamxhub.com/#">https://www.streamxhub.com/#</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_391.png" alt="blog: www.xubatian.cn"></p><p>github地址: <a href="https://github.com/streamxhub/streamx">https://github.com/streamxhub/streamx</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_390.png" alt="blog: www.xubatian.cn"></p><h2 id="修改streamx的相关版本"><a href="#修改streamx的相关版本" class="headerlink" title="修改streamx的相关版本"></a>修改streamx的相关版本</h2><p>修改为公司hadoop,flink,spark等相符合的大数据组件版本. </p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_392.png" alt="blog: www.xubatian.cn"></p><p>然后将压缩包上传到服务器上并解压.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ unzip streamx-release-1.2.0.zip -d /opt/module/</span><br></pre></td></tr></table></figure><p>进入解压目录编译源码,1.2.0默认flink版本为1.4,如需更改修改pom.xml再进行编译。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 streamx-release-1.2.0]$ mvn clean install -DskipTests -Denv=prod</span><br></pre></td></tr></table></figure><p>等待……</p><p>最后成功!</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_376.png" alt="blog: www.xubatian.cn"></p><h2 id="进行解压编译成功后的压缩包"><a href="#进行解压编译成功后的压缩包" class="headerlink" title="进行解压编译成功后的压缩包"></a>进行解压编译成功后的压缩包</h2><p>编译后在/opt/module/streamx-release-1.2.0/streamx-console/streamx-console-service/target目录会有对应tar包</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_380.png" alt="blog: www.xubatian.cn"></p><h2 id="进入解压后的目录"><a href="#进入解压后的目录" class="headerlink" title="进入解压后的目录"></a>进入解压后的目录</h2><p>进入到对应目录，修改配置文件，需要使用mysql地址来存储数据。</p><p>注意：数据库不会自动创建，需要手动创建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ cd streamx-console-service-1.2.0/</span><br><span class="line">[shangbaishuyao@hadoop102 streamx-console-service-1.2.0]$ vim conf/application.yml</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_381.png" alt="blog: www.xubatian.cn"></p><p>手动创建streamx的数据库</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122153945.png" alt="blog: www.xubatian.cn"></p><h2 id="启动streamx"><a href="#启动streamx" class="headerlink" title="启动streamx"></a>启动streamx</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 streamx-release-1.2.0]$ bin/startup.sh </span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_382.png" alt="blog: www.xubatian.cn"></p><h2 id="查看是否启动成功"><a href="#查看是否启动成功" class="headerlink" title="查看是否启动成功"></a>查看是否启动成功</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_384.png" alt="blog: www.xubatian.cn"></p><p>如果没有streamXconsole说明出现错误. 去logs里面查看具体错误.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_383.png" alt="blog: www.xubatian.cn"></p><h2 id="使用浏览器访问streamx"><a href="#使用浏览器访问streamx" class="headerlink" title="使用浏览器访问streamx"></a>使用浏览器访问streamx</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_385.png" alt="blog: www.xubatian.cn"></p><p>账号为: admin 密码为: streamx</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_386.png" alt="blog: www.xubatian.cn"></p><h2 id="编译好的streamx存放地址"><a href="#编译好的streamx存放地址" class="headerlink" title="编译好的streamx存放地址"></a>编译好的streamx存放地址</h2><p>streamx1.12.0 源码默认配置 直接编译 编译后的压缩包为: streamx-console-service-1.2.0-Linux-bin.tar.gz<br>streamx1.12.0 源码,修改hadoop版本为3.1.3, flink版本为1.14.3 编译后的压缩包为: streamx-console-service-1.2.0-macOS-bin.tar.gz<br>两个压缩包都方式这里,链接: <a href="https://pan.baidu.com/s/1M4R0K3rOzNZOdilnJiduFw">https://pan.baidu.com/s/1M4R0K3rOzNZOdilnJiduFw</a> 提取码: 2olc</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;遇到问题，改变苛求别人的惯性，重新塑造思考问题的方式。换个角度看世界，换个方向看问题，就会豁然开朗。——人民日报&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="streamx" scheme="http://xubatian.cn/tags/streamx/"/>
    
  </entry>
  
  <entry>
    <title>Flink部署</title>
    <link href="http://xubatian.cn/Flink%E9%83%A8%E7%BD%B2/"/>
    <id>http://xubatian.cn/Flink%E9%83%A8%E7%BD%B2/</id>
    <published>2022-01-20T21:39:16.000Z</published>
    <updated>2022-01-23T02:58:21.611Z</updated>
    
    <content type="html"><![CDATA[<p>所处的位置不同，看到的风景和思考的问题也有所不同。——人民日报</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_360.jpg" alt="blog: www.xubatian.cn"></p><h1 id="Standalone模式Flink自带的"><a href="#Standalone模式Flink自带的" class="headerlink" title="Standalone模式Flink自带的"></a>Standalone模式Flink自带的</h1><p>首先运行我们standalone的环境    </p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_361.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_362.png" alt="blog: www.xubatian.cn"></p><p>解压缩  <strong>flink-1.7.2-bin-hadoop27-scala_2.11.tgz</strong>(如果你两个模式都想试一下就这个压缩包,实际上你要真正搭建独立模式只需要flink-1.7.2后面不需要接hadoop27-scala_2.11的压缩包)，进入conf目录中。</p><p>1）修改 flink/conf/flink-conf.yaml 文件：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_363.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_364.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_365.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">为了更好地看清图片内容,我复制出来这两行:</span><br><span class="line"><span class="meta">#</span><span class="bash"> The heap size <span class="keyword">for</span> the TaskManager JVM</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#整个TaskManager的内存大小</span></span></span><br><span class="line">taskmanager.heap.size: 1024m    </span><br><span class="line"></span><br><span class="line">下面是,这一个G的内存可以同时允许你同时并行运行多少个Task,每个task会占用一个插槽</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#插槽即可理解为流水槽,你的水从流水槽上流出去,这个slot就好比是两个木板上的流水槽,这个slot是TaskManager的</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#TaskManager说白了就暂时是我们的worker节点,实际上来说你也可以看成是executor节点也可以.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#但是这里和saprk的standalone去类比的话这就矛盾了,因为flink的TaskManager在这里有相当于executor,也相当</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#于worker.   spark的standalone模式中,一个worker下面可以有多个executor,每一个executor并行可以运行两个</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#以上的任务吗?可以的. 所以我们spark独立模式理解起来有点难度.他是一个worker下面有一个executor,executor下面</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#又可能会运行多个task,也可能只运行一个task.这对初学者不好理解.而这里我们的flink将他简化了,他把worker这</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 一层去掉了,worker这一层就是taskmanager,就还好比是executor一样.这个executor上到底运行了几个task是由</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># slot来决定的.即由插槽来决定的. 这里的插槽设置为1,表示每一个TaskManager上有多少个slot.这里是有1个slot.这就意味着</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#我这个taskmanager上只能同时运行一个任务.你如果想要增加我们的并行度,就必须修改插槽数量.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> taskmanager.numberOfTaskSlots: 1</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改为3个插槽,表示每一个TaskManager里面有三个插槽,这三个插槽可以同时允许运行三个并行度,这三个并行度可以是不一样的广告</span></span><br><span class="line">taskmanager.numberOfTaskSlots: 3</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The parallelism used <span class="keyword">for</span> programs that did not specify and other parallelism.</span></span><br></pre></td></tr></table></figure><p>就只需要配置下面两个文件就可以了</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_366.png" alt="blog: www.xubatian.cn"></p><p>2）修改 /conf/slave文件：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_367.png" alt="blog: www.xubatian.cn"></p><p>3）分发给另外两台机子：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_368.png" alt="blog: www.xubatian.cn"></p><p>4）启动：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_369.png" alt="blog: www.xubatian.cn"></p><p>访问<a href="http://localhost:8081可以对flink集群和任务进行监控管理">http://localhost:8081可以对flink集群和任务进行监控管理</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_370.png" alt="blog: www.xubatian.cn"></p><h2 id="提交任务"><a href="#提交任务" class="headerlink" title="提交任务"></a>提交任务</h2>]]></content>
    
    
    <summary type="html">&lt;p&gt;所处的位置不同，看到的风景和思考的问题也有所不同。——人民日报&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink简介</title>
    <link href="http://xubatian.cn/Flink%E7%AE%80%E4%BB%8B/"/>
    <id>http://xubatian.cn/Flink%E7%AE%80%E4%BB%8B/</id>
    <published>2022-01-19T07:57:22.000Z</published>
    <updated>2022-01-23T02:58:21.611Z</updated>
    
    <content type="html"><![CDATA[<p>一旦时机到来，我们要能迅速地发现时机、把握时机，不犹豫，不踌躇，乘风而起，破万里浪。——人民日报</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_347.jpg" alt="blog: www.xubatian.cn"></p><h1 id="Flink简介"><a href="#Flink简介" class="headerlink" title="Flink简介"></a>Flink简介</h1><p>Flink其中一半是java语言开发的,另一半是scala语言开发的;spark的源码是scala语言开发的.但是scala是基于jvm的,所有的语法直接照着jvm调就可以了    </p><p>大数据中比较重要的框架,Hadoop(mapreduce),Spark,Flink,只有这三个才叫计算框架.<br>Flink,不管你是开发流式计算也好,还是做离线计算也好,还是做批量计算也好,他就是那一套代码,他不像Spark,Spark开发流式计算用的是SparkStreaming,用批量计算使用RDD,sparkCore<br>Flink有一套代码,但是他有很多套API<br>Flink有中文版,你发现Apache顶级项目有中文版的有那几个?他们的特点是什么?<br>因为他们50%以上的代码都是由中国人贡献的,Flink也一样,50%以上的代码是由中国国内贡献的</p><p>Flink和spark类似,他们的数据都是在内存中直接计算的,甚至他的状态都是存在内存中的 </p><p>Flink是默认就是有状态的计算,Flink中没有无状态这个说法但是spark中有无状态这种说法</p><p>flink的kappa架构怎么实现: <a href="https://www.jianshu.com/p/5f5736656bd5">https://www.jianshu.com/p/5f5736656bd5</a><br>Flink数据倾斜问题: <a href="https://www.cnblogs.com/qiu-hua/p/14056747.html">https://www.cnblogs.com/qiu-hua/p/14056747.html</a><br>                  <a href="https://www.cnblogs.com/Christbao/p/13569616.html">https://www.cnblogs.com/Christbao/p/13569616.html</a></p><p>但是实际上大数据量经常出现，一个 Flink 作业包含 200 个 Task 节点，其中有 199 个节点可以在很短的时间内完成计算。但是有一个节点执行时间远超其他结果，并且随着数据量的持续增加，导致该计算节点挂掉，从而整个任务失败重启。<br>我们可以在 Flink 的管理界面中看到任务的某一个 Task 数据量远超其他节点。<br>Flink 任务出现数据倾斜的直观表现是任务节点频繁出现反压，但是增加并行度后并不能解决问题；部分节点出现 OOM 异常，是因为大量的数据集中在某个节点上，导致该节点内存被爆，任务失败重启。</p><h2 id="初识Flink"><a href="#初识Flink" class="headerlink" title="初识Flink"></a>初识Flink</h2><p>Flink起源于Stratosphere项目，Stratosphere是在2010~2014年由3所地处柏林的大学和欧洲的一些其他的大学共同进行的研究项目，2014年4月Stratosphere的代码被复制并捐赠给了Apache软件基金会，参加这个孵化项目的初始成员是Stratosphere系统的核心开发人员，2014年12月，Flink一跃成为Apache软件基金会的顶级项目。<br>在德语中，Flink一词表示快速和灵巧，项目采用一只松鼠的彩色图案作为logo，这不仅是因为松鼠具有快速和灵巧的特点，还因为柏林的松鼠有一种迷人的红棕色，而Flink的松鼠logo拥有可爱的尾巴，尾巴的颜色与Apache软件基金会的logo颜色相呼应，也就是说，这是一只Apache风格的松鼠。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_348.png" alt="blog: www.xubatian.cn">Flink项目的理念是：“Apache Flink是为分布式、高性能、随时可用以及准确的流处理应用程序打造的开源流处理框架”。</p><p>Apache Flink是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。Flink被设计在所有常见的集群环境中运行，以内存执行速度和任意规模来执行计算。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_349.png" alt="blog: www.xubatian.cn"></p><p>博主解析图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_350.png" alt="blog: www.xubatian.cn"></p><h2 id="Flink的重要特点"><a href="#Flink的重要特点" class="headerlink" title="Flink的重要特点"></a>Flink的重要特点</h2><h3 id="事件驱动型-Event-driven"><a href="#事件驱动型-Event-driven" class="headerlink" title="事件驱动型(Event-driven)"></a>事件驱动型(Event-driven)</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_351.png" alt="blog: www.xubatian.cn"></p><p>事件驱动型应用是一类具有状态的应用，它从一个或多个事件流提取数据(其实就是他可以从多个源中读取数据)，并根据到来的事件触发计算(就是来一条数据立马计算不等待(计算是根据业务来的,可以做聚合计算等))、状态更新或其他外部动作。比较典型的就是以kafka为代表的消息队列几乎都是事件驱动型应用。</p><p>与之不同的就是SparkStreaming微批次，如图：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_352.png" alt="blog: www.xubatian.cn"></p><p>事件驱动型：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_353.png" alt="blog: www.xubatian.cn"></p><p>博主解析图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_354.png" alt="blog: www.xubatian.cn"></p><h3 id="事件驱动型应用的优势？"><a href="#事件驱动型应用的优势？" class="headerlink" title="事件驱动型应用的优势？"></a>事件驱动型应用的优势？</h3><p>事件驱动型应用无须查询远程数据库，本地数据访问使得它具有更高的吞吐和更低的延迟。而由于定期向远程持久化存储的 checkpoint 工作可以异步、增量式完成，因此对于正常事件处理的影响甚微。事件驱动型应用的优势不仅限于本地数据访问。传统分层架构下，通常多个应用会共享同一个数据库，因而任何对数据库自身的更改（例如：由应用更新或服务扩容导致数据布局发生改变）都需要谨慎协调。反观事件驱动型应用，由于只需考虑自身数据，因此在更改数据表示或服务扩容时所需的协调工作将大大减少</p><h3 id="流与批的世界观"><a href="#流与批的世界观" class="headerlink" title="流与批的世界观"></a>流与批的世界观</h3><p>有界和无界分别对应的就是批处理和流处理<br><strong>批处理</strong>(就是所谓的有界数据)的特点是有界、持久、大量，非常适合需要访问全套记录才能完成的计算工作，一般用于离线统计。<br><strong>流处理</strong>(就是所谓的无界数据)的特点是无界、实时,  无需针对整个数据集执行操作，而是对通过系统传输的每个数据项执行操作，一般用于实时统计。<br>在spark的世界观中，一切都是由批次组成的，离线数据是一个大批次，而实时数据是由一个一个无限的小批次组成的。<br>而在flink的世界观中，一切都是由流组成的，离线数据是有界限的流，实时数据是一个没有界限的流，这就是所谓的有界流和无界流。</p><p><strong>无界数据流</strong>：无界数据流有一个开始但是没有结束，它们不会在生成时终止并提供数据，必须连续处理无界流，也就是说必须在获取后立即处理event。对于无界数据流我们无法等待所有数据都到达，因为输入是无界的，并且在任何时间点都不会完成。处理无界数据通常要求以特定顺序（例如事件发生的顺序）获取event，以便能够推断结果完整性。</p><p><strong>有界数据流</strong>：有界数据流有明确定义的开始和结束，可以在执行任何计算之前通过获取所有数据来处理有界流，处理有界流不需要有序获取，因为可以始终对有界数据集进行排序，有界流的处理也称为批处理。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_355.png" alt="blog: www.xubatian.cn"></p><p>这种以流为世界观的架构，获得的最大好处就是具有极低的延迟。</p><p>博主解析图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_356.png" alt="blog: www.xubatian.cn"></p><h3 id="分层API"><a href="#分层API" class="headerlink" title="分层API"></a>分层API</h3><p>(对于我们学flink来说,我们三层都必须得会,经常用到的是中间那层DataStream API)<br>Flink他本质上把批量的数据和流数据都看成是流了,所以他本质上是流处理,所以他也可以做批处理,他和saprk相反,spark把所有的数据都看成是批处理了,但是spark也是可以做流处理的<br>记住: DataStream API做流处理,流处理是无界的  DataSetAPI是做批处理是有界的    </p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_357.png" alt="blog: www.xubatian.cn"></p><p>分层API是Flink根据抽象程度,提供的三种不同的API,所谓抽象程度就是看你封装的程度,如果你不怎么封装,那就是底层的API,稍微封装一下就叫中间的API,封装的很厉害就叫高级API.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_358.png" alt="blog: www.xubatian.cn"></p><p>最底层级的抽象仅仅提供了有状态流，它将通过过程函数（Process Function）被嵌入到DataStream API中。底层过程函数（Process Function） 与 DataStream API 相集成，使其可以对某些特定的操作进行底层的抽象，它允许用户可以自由地处理来自一个或多个数据流的事件，并使用一致的容错的状态。除此之外，用户可以注册事件时间并处理时间回调，从而使程序可以处理复杂的计算。</p><p>实际上，大多数应用并不需要上述的底层抽象，而是针对核心API（Core APIs） 进行编程，比如DataStream API（有界或无界流数据）以及DataSet API（有界数据集）。这些API为数据处理提供了通用的构建模块，比如由用户定义的多种形式的转换（transformations），连接（joins），聚合（aggregations），窗口操作（windows）等等。DataSet API 为有界数据集提供了额外的支持，例如循环与迭代。这些API处理的数据类型以类（classes）的形式由各自的编程语言所表示。</p><p>Table API 是以表为中心的声明式编程，其中表可能会动态变化（在表达流数据时）。Table API遵循（扩展的）关系模型：表有二维数据结构（schema）（类似于关系数据库中的表），同时API提供可比较的操作，例如select、project、join、group-by、aggregate等。Table API程序声明式地定义了什么逻辑操作应该执行，而不是准确地确定这些操作代码的看上去如何。</p><p>尽管Table API可以通过多种类型的用户自定义函数（UDF）进行扩展，其仍不如核心API更具表达能力，但是使用起来却更加简洁（代码量更少）。除此之外，Table API程序在执行之前会经过内置优化器进行优化。<br>你可以在表与 DataStream/DataSet 之间无缝切换，以允许程序将 Table API 与 DataStream 以及 DataSet 混合使用。</p><p>Flink提供的最高层级的抽象是 SQL 。这一层抽象在语法与表达能力上与 Table API 类似，但是是以SQL查询表达式的形式表现程序。SQL抽象与Table API交互密切，同时SQL查询可以直接在Table API定义的表上执行。</p><p>目前Flink作为批处理还不是主流，不如Spark成熟，所以DataSet使用的并不是很多。Flink Table API和Flink SQL也并不完善，大多都由各大厂商自己定制。所以我们主要学习DataStream API的使用。实际上Flink作为最接近Google DataFlow模型的实现，是流批统一的观点，所以基本上使用DataStream就可以了。<br>Flink几大模块<br>Flink Table &amp; SQL(还没开发完)<br>Flink Gelly(图计算)<br>Flink CEP(复杂事件处理)</p><p>Flink官方给你提供的连接器,虽然都是flink的,但是他来自两个不同的库,一个来自flink的,一个来自Bahir的</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_359.png" alt="blog: www.xubatian.cn"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;一旦时机到来，我们要能迅速地发现时机、把握时机，不犹豫，不踌躇，乘风而起，破万里浪。——人民日报&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>SparkCore之RDD编程的编程模型</title>
    <link href="http://xubatian.cn/SparkCore%E4%B9%8BRDD%E7%BC%96%E7%A8%8B%E7%9A%84%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/"/>
    <id>http://xubatian.cn/SparkCore%E4%B9%8BRDD%E7%BC%96%E7%A8%8B%E7%9A%84%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/</id>
    <published>2022-01-19T06:50:36.000Z</published>
    <updated>2022-01-23T02:58:21.755Z</updated>
    
    <content type="html"><![CDATA[<p>闲适因为忙碌才获得意义。如果摸鱼成为常态，放松就失去了意义；如果划水占据人生，幸福就会失去方向。               ——人民日报    </p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_340.jpg" alt="blog: www.xubatian.cn"></p><h1 id="RDD编程"><a href="#RDD编程" class="headerlink" title="RDD编程"></a>RDD编程</h1><p>创建RDD ,RDD的转换, RDD的输出</p><h2 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h2><p>在spark中无论是Transformations方法还是Actions方法,我们都要把他们称作算子</p><p>在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的Transformations(转换)定义RDD之后，就可以调用Actions(行动)触发RDD的计算，Action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到Action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。 </p><p>要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行。Driver中定义了一个或多个RDD，并调用RDD上的Action，Executor则执行RDD分区计算任务。</p><p>Actions(行动)算子会真正的去触发job去执行<br>Transformation(转换)算子懒执行<br>所以返回值是RDD类型的是Transformation算子,返回值非RDD类型就是Action算子</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_341.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_342.png" alt="blog: www.xubatian.cn"></p><h2 id="RDD的创建"><a href="#RDD的创建" class="headerlink" title="RDD的创建"></a>RDD的创建</h2><p>在Spark中创建RDD的创建方式可以分为三种：<br>从scala集合中创建RDD；<br>从外部存储创建RDD；<br>从其他RDD创建(这个其实讲的就是转换)。</p><h3 id="从集合中创建"><a href="#从集合中创建" class="headerlink" title="从集合中创建"></a>从集合中创建</h3><p>从集合中创建RDD，Spark主要提供了两种函数：parallelize(并行化)和makeRDD(创建RDD)</p><p>1）使用parallelize()从集合创建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(Array(1,2,3,4,5,6,7,8))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br></pre></td></tr></table></figure><p>2）使用makeRDD()从集合创建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd1 = sc.makeRDD(Array(1,2,3,4,5,6,7,8))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at &lt;console&gt;:24</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_343.png" alt="blog: www.xubatian.cn"></p><h3 id="由外部存储系统的数据集创建"><a href="#由外部存储系统的数据集创建" class="headerlink" title="由外部存储系统的数据集创建"></a>由外部存储系统的数据集创建</h3><p>包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等，</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd2= sc.textFile(&quot;hdfs://hadoop102:9000/RELEASE&quot;)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] = hdfs://hadoop102:9000/RELEASE MapPartitionsRDD[4] at textFile at &lt;console&gt;:24</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="从其他RDD创建"><a href="#从其他RDD创建" class="headerlink" title="从其他RDD创建"></a>从其他RDD创建</h3><p>下面都是…此处略</p><h2 id="RDD的转换"><a href="#RDD的转换" class="headerlink" title="RDD的转换"></a>RDD的转换</h2><p>RDD整体上分为Value类型和Key-Value类型,  K-V形式其实也是value类型<br>eg:   (k,(k,v))是value, 是不是kv? 是,只不过value类型是二元组.<br>如果是单个值, 是value, 如果是形如二元组是K,V.  k,v形式是value类型, 我把整个k,v元组当成整体来看,他就是value类型.  他们两是包含的关系. 所有的RDD都可以看做是value类型, 只过不特殊的我们拎出来,如k-v等等</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_344.png" alt="blog: www.xubatian.cn"></p><h3 id="Value类型"><a href="#Value类型" class="headerlink" title="Value类型"></a>Value类型</h3><p>什么叫做value类型呢?<br>因为他里面传的函数都是操作当前这个RDD里面的元素. 可能是单个元素, 可能是一个分区里面的元素.但是他操作的是里面的数据.<br>什么叫双value类型呢?<br>双value类型它里面传的参数是任意一个RDD.<br>Eg : RDD1.调用一个算子(RDD2)<br>以为之前提过, scala也好,spark也好,他是面向数据处理的. 那这个双value类型就是数学里面的,集合之间的关系. 集合里面有哪些关系呢? 并集, 交叉 ,笛卡尔集</p><h4 id="map-func-案例"><a href="#map-func-案例" class="headerlink" title="map(func)案例"></a>map(func)案例</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. 作用：返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成</span><br><span class="line">2. 需求：创建一个1-10数组的RDD，将所有元素*2形成新的RDD</span><br></pre></td></tr></table></figure><p>  (1)   创建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var source  = sc.parallelize(1 to 10)</span><br><span class="line"></span><br><span class="line">source: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at &lt;console&gt;:24</span><br></pre></td></tr></table></figure><p>（2）打印</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; source.collect()</span><br><span class="line"></span><br><span class="line">res7: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</span><br></pre></td></tr></table></figure><p>（3）将所有元素*2</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val mapadd = source.map(_ * 2)</span><br><span class="line">这个map是算子</span><br><span class="line"></span><br><span class="line">mapadd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at map at &lt;console&gt;:26</span><br></pre></td></tr></table></figure><p>（4）打印最终结果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; mapadd.collect()</span><br><span class="line"></span><br><span class="line">res8: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_345.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_346.png" alt="blog: www.xubatian.cn"></p><h3 id="双Value类型交互"><a href="#双Value类型交互" class="headerlink" title="双Value类型交互"></a>双Value类型交互</h3><p>以后慢慢写……</p><h3 id="Key-Value类型"><a href="#Key-Value类型" class="headerlink" title="Key-Value类型"></a>Key-Value类型</h3><p>以后慢慢写……</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;闲适因为忙碌才获得意义。如果摸鱼成为常态，放松就失去了意义；如果划水占据人生，幸福就会失去方向。               ——人民日报    &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="spark" scheme="http://xubatian.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>SparkCore之RDD概述</title>
    <link href="http://xubatian.cn/SparkCore%E4%B9%8BRDD%E6%A6%82%E8%BF%B0/"/>
    <id>http://xubatian.cn/SparkCore%E4%B9%8BRDD%E6%A6%82%E8%BF%B0/</id>
    <published>2022-01-19T06:22:21.000Z</published>
    <updated>2022-01-23T02:58:21.728Z</updated>
    
    <content type="html"><![CDATA[<p>仰观天宇，时间更加深邃；俯身耕耘，未来无限可能                  ——人民日报    </p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_331.jpg" alt="blog: www.xubatian.cn"></p><p>我们Spark中的stage是按照shuffle来切的.</p><h1 id="RDD概述"><a href="#RDD概述" class="headerlink" title="RDD概述"></a>RDD概述</h1><h2 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h2><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象。代码中是一个抽象类，它代表一个弹性的(分区)、不可变(元素)、可分区、里面的元素可并行计算的集合。</p><p>RDD是抽象类</p><h2 id="RDD的属性"><a href="#RDD的属性" class="headerlink" title="RDD的属性"></a>RDD的属性</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_332.png" alt="blog: www.xubatian.cn"></p><p>（1）一组分区（Partition）,切片和分区是一样的，即数据集的基本组成单位；<br>（2）一个计算每个分区的函数；<br>（3）RDD之间的依赖关系；<br>（4）一个Partitioner，即RDD的分片函数；<br>（5）一个列表，存储存取每个Partition的优先位置（preferred location）。</p><h2 id="RDD特点"><a href="#RDD特点" class="headerlink" title="RDD特点"></a>RDD特点</h2><p>RDD表示只读(不可变性)的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。</p><h2 id="弹性"><a href="#弹性" class="headerlink" title="弹性"></a>弹性</h2><p>存储的弹性：内存与磁盘的自动切换；<br>容错的弹性：数据丢失可以自动恢复；<br>计算的弹性：计算出错重试机制；<br>分片的弹性：可根据需要重新分片。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_333.png" alt="blog: www.xubatian.cn"></p><h2 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h2><p>RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute(计算)函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute(计算)函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute(计算)函数是执行转换逻辑将其他RDD的数据进行转换</p><h2 id="只读"><a href="#只读" class="headerlink" title="只读"></a>只读</h2><p>RDD是只读的（元素不可变），要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。<br>由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了。</p><p>RDD的操作算子包括两类，<br>一类叫做Transformations(转换)，它是用来将RDD进行转化，构建RDD的血缘关系；<br>另一类叫做Actions(行动)，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。</p><h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p>RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，</p><p>依赖包括两种，<br>一种是窄依赖，RDDs之间分区是一一对应的，<br>另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。(一对一,多对多指的是分区) </p><p>这里的一对一,一对多指的是分区</p><p>注意: 依赖和shuffle有关系</p><p>原图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_334.png" alt="blog: www.xubatian.cn"></p><p>博主解读:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_336.png" alt="blog: www.xubatian.cn"></p><p>父RDD中的全部数据被某一个子RDD的某个分区全部拥有我们叫窄依赖</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_337.png" alt="blog: www.xubatian.cn"></p><h2 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h2><p>如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_338.png" alt="blog: www.xubatian.cn"></p><h2 id="CheckPoint"><a href="#CheckPoint" class="headerlink" title="CheckPoint"></a>CheckPoint</h2><p>切断血缘关系后可以从CheckPoint中拿数据</p><p>虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。</p><p>checkPoint将数据持久化了之后他就会切断血缘关系, 因为他认为你持久化到文件当中后这个数据就不会丢了,这个依赖关系就不需要了,就切断了. 因为你是文件嘛, 而且默认一般存在hdfs中,hdfs又默认有三个副本. 所以他觉得你数据不会丢了. 既然不会丢了,依赖关系就不要了, 因为依赖关系就是防止你数据丢了重新计算做数据恢复的.</p><p>但是你缓存到内存当中,你不能将依赖切断, 因为内存当中数据可能会掉的.他可能还要用这个依赖关系重新做计算的.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_339.png" alt="blog: www.xubatian.cn"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;仰观天宇，时间更加深邃；俯身耕耘，未来无限可能                  ——人民日报    &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="spark" scheme="http://xubatian.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark基础解析</title>
    <link href="http://xubatian.cn/Spark%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90/"/>
    <id>http://xubatian.cn/Spark%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90/</id>
    <published>2022-01-19T03:09:24.000Z</published>
    <updated>2022-01-23T02:58:21.755Z</updated>
    
    <content type="html"><![CDATA[<p> 征途漫漫，惟有奋斗；梦想成真，惟有实干。                     ——人民日报</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_308.jpg" alt="blog: www.xubatian.cn"></p><h1 id="Spark概述"><a href="#Spark概述" class="headerlink" title="Spark概述"></a>Spark概述</h1><h2 id="什么是Spark"><a href="#什么是Spark" class="headerlink" title="什么是Spark"></a>什么是Spark</h2><p>1、定义<br>Spark是一种基于内存的快速、通用、可扩展的大数据分析引擎。<br>2、历史<br>2009年诞生于加州大学伯克利分校AMPLab, 项目采用Scala编写;<br>2010年开源;<br>2013年6月成为Apache孵化项目；<br>2014年2月成为Apache顶级项目。</p><p><strong>xubatian解析:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在Scala当中的map ，reduce这些方法在spark当中同样也有这些方法。</span><br><span class="line"></span><br><span class="line">要知道的是Scala的这些方法是面向的是集合当中做处理的，面向的是数据集合，数组等等这些操作。而spark是面对的是海量数据处理的，他面向的数据分析的什么东西呢？叫分布式数据集。Scala处理的数据在一个集合当中，而spark处理的数据可能跨了很多台机器。因为他是用hdfs来存储的。而hdfs存储的时候不是把所有的数据都放在一台机器上的。而是很多台机器上都有。而spark就是同时处理很多台机器上的事情。所以Scala和spark都有map方法，可能功能上都是一样的，都是把里面每一个元素做一个转变。但是他们面向的数据集不一样，spark面向的数据集时RDD。</span><br><span class="line"></span><br><span class="line">SparkStream和kafka做对接, 你kafka过来的还是一行一行的数据.虽然封装成了Dstream,但是他还是一行一行的数据. 你要做分析转换输出等</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Spark内置模块"><a href="#Spark内置模块" class="headerlink" title="Spark内置模块"></a>Spark内置模块</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_309.png" alt="blog: www.xubatian.cn"></p><p><strong>Spark Core</strong>：实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core中还包含了对弹性分布式数据集(Resilient Distributed DataSet，简称RDD)的API定义；</p><p><strong>Spark SQL</strong>：是Spark用来操作结构化数据的程序包。通过Spark SQL，我们可以使用 SQL或者Apache Hive版本的SQL方言(HQL)来查询数据。Spark SQL支持多种数据源，比如Hive表、Parquet以及JSON等；</p><p><strong>Spark Streaming</strong>：是Spark提供的对实时数据进行流式计算的组件。提供了用来操作数据流的API，并且与Spark Core中的 RDD API高度对应；</p><p><strong>Spark MLlib</strong>：提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能；</p><p>集群管理器：Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计 算。为了实现这样的要求，同时获得最大灵活性，Spark支持在各种集群管理器(Cluster Manager)上运行，包括Hadoop YARN、Apache Mesos，以及Spark自带的一个调度器，叫作独立调度器。</p><p> Spark得到了众多大数据公司的支持，这些公司包括Hortonworks、IBM、Intel、Cloudera、MapR、Pivotal、百度、腾讯、京东、携程、优酷土豆。当前百度的Spark已应用于大搜索、直达号、百度大数据等业务；阿里利用GraphX构建了大规模的图计算和图挖掘系统，实现了很多生产系统的推荐算法；腾讯Spark集群达到8000台的规模，是当前已知的世界上最大的Spark集群。</p><h2 id="Spark特点-DAG"><a href="#Spark特点-DAG" class="headerlink" title="Spark特点(DAG)"></a>Spark特点(DAG)</h2><ol><li><p><strong>快</strong><br>与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。计算的中间结果是存在于内存中的。</p></li><li><p><strong>易用</strong><br>Spark支持Java、Python和Scala的API, 还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的Shell,可以非常方便地在这些Shell中使用Spark集群来验证解决问题的方法。</p></li></ol><ol start="3"><li><strong>通用</strong></li></ol><p>  Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理(Spark Streaming) 、机器学习 (Spark MLlib)和图计算(GraphX).这些不同类型的处理都可以在同一个应用中无缝使用。减少了开发和维护的人力成本和部署平台的物力成本。</p><ol start="4"><li><strong>兼容性</strong><br>Spark可以非常方便地与其他的开源产品进行融合。比如, Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。</li></ol><h2 id="博主补充"><a href="#博主补充" class="headerlink" title="博主补充"></a>博主补充</h2><p>Spark实现了高效的DAG执行引擎。DAG是有向无环图即多个任务之间通过内存来做交互。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_310.png" alt="blog: www.xubatian.cn"></p><p>多个任务之间直接通过内存来做交互。它可以直接将他们串联起来。而我们之前可能需要用到MR1，MR2，MR3进行落盘操作。</p><p>另一个spark快的原因是：<br>                对于MR来说，你整个Map任务和Reduce任务是计算的核心。而map任务和reduce任务你用jps能看到进程吗？不能。也看不到spark当中的maptask和reducetask。这也是spark比mr快的一个比较核心的一个点。一个呢，对于hadoop来说他是使用进程来调度的。我启动一个单独的task都是一个单独的进程。你能jps看到的是进程号。而在spark当中启动一个任务他是线程。你说是调用进程快呢还是线程快呢？我线程我可以事先启动好一个线程池。我要的时候去取一下就完了。嘿嘿~~阴险。这也是spark比mr快的一个很重要的一个点。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_311.png" alt="blog: www.xubatian.cn"></p><h1 id="Spark运行模式"><a href="#Spark运行模式" class="headerlink" title="Spark运行模式"></a>Spark运行模式</h1><p>重点local模式,和yarn模式,为什么不掌握spark自己的呢?<br>第一重点:<br>本地模式主要是用于教学和测试.公司当中的一些demo级别的测试也是用本地模式.因为他相对来说,资源消耗等等都比较简单一点.local模式相对来说简单一点.不需要启动很多进程去占用额外的资源<br>第二重点:<br>Yarn模式,这个是应用于公司的生产环境.为什么公司的生产环境会用到yarn模式呢?稍微结合standlone模式思考一下.standlone是spark他自己来管理这一套资源.而公司当中其实并不愿意采用standlone模式,而是采用yarn模式比较多.为什么公司当中不用呢?既然spark自己有一套独立的调度资源系统,那你说他和standlone模式兼容性更好还是yarn模式兼容性更好呢?肯定是standlone。因为这是他自己的。那为什么兼容性更好却不用呢？说明他两又有区别，而且区别在公司当中standlone模式比yarn模式更严重一点。我们的mapreduce是yarn分配资源的，我们学过的tez也是yarn分配资源的。Storm也是yarn分配资源的。如果说我们spark也用yarn分配调度资源有什么好处呢？是不是统一的资源调度呢呀！<br>如果说我们spark当中使用独立的一套呢？会产生资源争抢。因为yarn认为这块资源是我独有的，而spark的standlone也认为这块资源是我独有的，那我分配的时候有可能两个任务就冲突了。但是我交给某一个人统一的安排这个资源，不行就等待，就不会产生资源争抢的问题。这个就是公司当中用yarn模式做的一个点。<br>Yarn模式在生产环境中用的比较多。主要体现在中小型公司。他整个集群资源规模不大，他整个MR任务，spark任务，或者其他任务都是运行在同一套资源上的。如果告诉你你公司比较有钱，你的spark集群是独立的spark集群。那么我们就用spark的standlone模式。<br>但是绝大多数公司他的整个集群都是资源混布的。这就比较依赖与统一的资源管理了。这样就不至于产生资源争抢。<br>我们所讲的几种模式都是在Liunx环境当中开一个shell窗口。类似于之前写的hive，在里面写sql操作。但是实际生产当中，他更多的对于spark来说还是要写代码，打jar包来运行。所以最后是我们写的一个wordcount程序，打jar包来提交到集群上去运行。</p><h2 id="Spark安装地址"><a href="#Spark安装地址" class="headerlink" title="Spark安装地址"></a>Spark安装地址</h2><p>1．官网地址<br><a href="http://spark.apache.org/">http://spark.apache.org</a><br>2．文档查看地址<br><a href="https://spark.apache.org/docs">https://spark.apache.org/docs</a><br>3．下载地址<br><a href="https://spark.apache.org/downloads.html">https://spark.apache.org/downloads.html</a></p><h2 id="集群角色"><a href="#集群角色" class="headerlink" title="集群角色"></a>集群角色</h2><p>注意: 不是standlone模式就没有Master和Worker</p><h3 id="Master和Worker"><a href="#Master和Worker" class="headerlink" title="Master和Worker"></a>Master和Worker</h3><p>Master和Worker:   负责资源的,具体运行,哪个的代码他不管.用户客户端提交代码后,你告诉我分配3G内存,2核CPU我给你分配就完了.    Master和Worker:   是standlone模式所独有的. yarn模式没有. Applicationmaster提交任务前,master和work一定是启动状态.</p><p>1）Master</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Spark特有资源调度系统的Leader。掌管着整个集群的资源信息，类似于Yarn框架中的ResourceManager，主要功能：</span><br><span class="line">（1）监听Worker，看Worker是否正常工作；</span><br><span class="line">（2）Master对Worker、Application等的管理(接收Worker的注册并管理所有的Worker，接收Client提交的application，调度等待的Application并向Worker提交)。</span><br></pre></td></tr></table></figure><p>2）Worker</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Spark特有资源调度系统的Slave(奴隶,随从)，有多个。每个Slave掌管着所在节点的资源信息，类似于Yarn框架中的NodeManager，主要功能：</span><br><span class="line">（1）通过RegisterWorker注册到Master；</span><br><span class="line">（2）定时发送心跳给Master；</span><br><span class="line">（3）根据Master发送的Application配置进程环境，并启动ExecutorBackend(执行Task所需的临时进程)</span><br></pre></td></tr></table></figure><h3 id="Driver和Executor"><a href="#Driver和Executor" class="headerlink" title="Driver和Executor"></a>Driver和Executor</h3><p>Driver和Executor:  负责具体执行的任务.他和具体提执行的任务相关.驱动器和执行器.驱动器是主,执行器是从.M任务的resourcemanager和nodemanager是负责管理资源, 资源申请下来之后他先启动的是Applicationmaster,是当前这个任务的小组长. Driver类似于MR的Applicationmaster . Applicationmaster来了之后,他去执行执行相应的具体的任务.就是mapTask,ReduceTask等.这些task就executer中去运行. Driver和Executer是线程级别的任务.</p><p>1）Driver（驱动器）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Spark的驱动器是执行开发程序中的main方法的线程。它负责开发人员编写的用来创建SparkContext (sc)、创建RDD，以及进行RDD的转化操作和行动操作代码的执行。如果你是用Spark Shell，那么当你启动Spark shell的时候，系统后台自启了一个Spark驱动器程序，就是在Spark shell中预加载的一个叫作 sc的SparkContext对象。如果驱动器程序终止，那么Spark应用也就结束了。</span><br><span class="line">Driver(驱动器)主要负责：</span><br><span class="line">（1）将用户程序代码转化为作业（Job）；</span><br><span class="line">（2）在Executor之间调度任务（Task）；</span><br><span class="line">（3）跟踪Executor的执行情况；</span><br><span class="line">（4）通过UI展示查询运行情况。</span><br></pre></td></tr></table></figure><p>2）Executor（执行器）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Spark Executor是一个工作节点，负责在 Spark 作业(Job)中运行任务(Task)，任务间相互独立。Spark 应用启动时，Executor节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有Executor节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他Executor节点上继续运行。</span><br><span class="line">Executor（执行器）主要负责：</span><br><span class="line">（1）负责运行组成 Spark 应用的任务，并将状态信息返回给驱动器(Driver)程序；</span><br><span class="line">（2）通过自身的块管理器（Block Manager）为用户程序中要求缓存的RDD提供内存式存储。RDD是直接缓存在Executor内的，因此任务可以在运行时充分利用缓存数据加速运算。</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Master和Worker是Spark的守护进程(什么叫守护进程?即一直都在的)，即Spark在特定模式下正常运行所必须的进程。</p><p>Driver和Executor是临时程序，当有具体任务提交到Spark集群才会开启的程序。其实Driver和Executor是线程.  而Master和Worker是进程.</p><h3 id="博主补充-1"><a href="#博主补充-1" class="headerlink" title="博主补充"></a>博主补充</h3><p>Driver(驱动器) 和 Executer(执行器) 有主从关系, Driver是主,Executer是从.<br>Driver可以这样理解,MR中资源准备好了之后,要启一个ApplicationMaster,即当前这个任务的守护者,相当于Driver(驱动器).  ApplicationMaster启动好了之后,启动相应的任务, 如mapTask,ReduceTask等. 具体的一个个Task去运行,这就相当于Executer里面运行的内容</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_312.png" alt="blog: www.xubatian.cn"></p><p>  1.只要你用的是StandLone模式,master和worker将一直都有.如果你是yarn模式就不需要. </p><ol start="2"><li>对于Driver和Executer只有等任务来了才有. 而Driver和Executer,无论本地模式和yarn模式都有, 他和模式没有关系 </li></ol><p>以下是Spark的几个模式, 它运行的位置可能不一样. standlone的Drive和Executor是由master和Worker来决定位置的.<br>如果是yarn模式就有ResourceManager来决定位置.</p><h2 id="Local模式"><a href="#Local模式" class="headerlink" title="Local模式"></a>Local模式</h2><p>本地模式：解压完了就等于安装好了。就和Hadoop一样，解压完了，什么都没改，直接就可以运行jar包了。这个也一样的，直接可以运行jar包</p><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Local模式就是运行在一台计算机上的模式，通常就是用于在本机上练手和测试。它可以通过以下集中方式设置Master。</p><p>local: 所有计算都运行在一个Core当中，没有任何并行计算，通常我们在本机执行些测试代码, 或者练手, 就用这种模式;</p><p>local[K]: 指定使用K个Core来运行计算，比如local[4]就是运行4个Core来执行;</p><p>local[*]:  这种模式直接使用最大Core数。</p><p>master叫资源管理器. 我们统称为master.<br>我们将standlone里面的master, yarn里面的ResourceManager. 以及这里上图的资源管理器都成为master.</p><h3 id="安装使用"><a href="#安装使用" class="headerlink" title="安装使用"></a>安装使用</h3><p>1）上传并解压spark安装包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 sorfware]$ tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/</span><br><span class="line">[shangbaishuyao@hadoop102 module]$ mv spark-2.1.1-bin-hadoop2.7 spark</span><br></pre></td></tr></table></figure><p>2）官方求PI案例(类似java jar)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>（1）基本语法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class &lt;main-class&gt;</span><br><span class="line">--master &lt;master-url&gt; \</span><br><span class="line">--deploy-mode &lt;deploy-mode&gt; \</span><br><span class="line">--conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">... # other options</span><br><span class="line">&lt;application-jar&gt; \        -- jar 包所在路径</span><br><span class="line">[application-arguments]    --大括号表示可选的,有些main方法不需要参数</span><br><span class="line"></span><br><span class="line">===============上面是模板,下面是实例,对比====================</span><br><span class="line"></span><br><span class="line"> bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>（2）参数说明</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">--master 指定Master的地址；</span><br><span class="line">--class: 你的应用的启动类 (如 org.apache.spark.examples.SparkPi)；</span><br><span class="line">--deploy-mode: 是否发布你的驱动到worker节点(cluster) 或者作为一个本地客户端 (client) (default: client)；</span><br><span class="line">--conf: 任意的Spark配置属性， 格式key=value. 如果值包含空格，可以加引号“key=value” ；</span><br><span class="line">application-jar: 打包好的应用jar,包含依赖. 这个URL在集群中全局可见。 比如hdfs:// 共享存储系统， 如果是 file:// path， 那么所有的节点的path都包含同样的jar</span><br><span class="line">application-arguments: 传给main()方法的参数；</span><br><span class="line">--executor-memory 1G 指定每个executor可用内存为1G；</span><br><span class="line">--total-executor-cores 2 指定每个executor使用的cup核数为2个。</span><br></pre></td></tr></table></figure><p>3）结果</p><p>该算法是利用蒙特·卡罗算法求PI</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_313.png" alt="blog: www.xubatian.cn"></p><p>4）准备文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ mkdir input</span><br><span class="line">在input下创建3个文件1.txt和2.txt，并输入以下内容</span><br><span class="line">hello shangbaishuyao</span><br><span class="line">hello spark</span><br></pre></td></tr></table></figure><p>5）启动spark-shell</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-shell</span><br><span class="line">Using Spark&#x27;s default log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line">Setting default log level to &quot;WARN&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">18/09/29 08:50:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">18/09/29 08:50:58 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</span><br><span class="line">Spark context Web UI available at http://192.168.9.102:4040</span><br><span class="line">Spark context available as &#x27;sc&#x27; (master = local[*], app id = local-1538182253312).</span><br><span class="line">Spark session available as &#x27;spark&#x27;.</span><br><span class="line">Welcome to</span><br><span class="line"></span><br><span class="line">      / __/__  ___ _____/ /__</span><br><span class="line">     _\ \/ _ \/ _ `/ __/  &#x27;_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1</span><br><span class="line">       /_/</span><br><span class="line">          </span><br><span class="line">Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)</span><br><span class="line">Type in expressions to have them evaluated.</span><br><span class="line">Type :help for more information.</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span></span><br></pre></td></tr></table></figure><p>6）结果图示</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_314.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_315.png" alt="blog: www.xubatian.cn"></p><p>7）运行WordCount程序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;sc.textFile(&quot;input&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">res0: Array[(String, Int)] = Array((hadoop,6), (oozie,3), (spark,3), (hive,3), (shangbaishuyao,3), (hbase,6))</span><br><span class="line">scala&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可登录hadoop102:4040查看程序运行</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_316.png" alt="blog: www.xubatian.cn"></p><h3 id="提交流程"><a href="#提交流程" class="headerlink" title="提交流程"></a>提交流程</h3><p>1）提交任务分析</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_317.png" alt="blog: www.xubatian.cn"></p><h3 id="博主补充解析"><a href="#博主补充解析" class="headerlink" title="博主补充解析"></a>博主补充解析</h3><p>因为是Local模式,没有master和worker. 我们在提交任务之前,没有启动任何程序. 所以资源管理者就是本身,就是spark-submit,即他自己管理计算,自己管理资源.  正常提交,提交之后就会运行一个Driver. 其实你在起动spark-shell的时候就已经有了这个Driver了. Driver去资源管理者里注册应用程序,然后启动Executor. 至此,这一套就在起动saprk-shell的时候就已经搞好了. 接下来过程就是我们自己写代码了.  就是Executor反向注册到Driver中产生通信. 然后写代码, 如初始化sparkContext, 任务划分,任务调度等. 调度完后给Executor中去运行.</p><h3 id="数据流程"><a href="#数据流程" class="headerlink" title="数据流程"></a>数据流程</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_319.png" alt=" blog: www.xubatian.cn"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">textFile(&quot;input&quot;)：  读取本地文件input文件夹数据；</span><br><span class="line">flatMap(_.spl it(&quot; &quot;))：压平操作，按照空格分割符将一行数据映射成一个个单词；</span><br><span class="line">map((_,1))：对每一个元素操作，将单词映射为元组；</span><br><span class="line">reduceByKey(_+_)：按照key将值进行聚合，相加；</span><br><span class="line">collect：将数据收集到Driver端展示。</span><br></pre></td></tr></table></figure><p><strong>案例分析</strong></p><p> <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_320.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_321.png" alt="blog: www.xubatian.cn"></p><h2 id="Standalone模式"><a href="#Standalone模式" class="headerlink" title="Standalone模式"></a>Standalone模式</h2><p>Standalone模式有一组进程叫master和worker</p><p>单机模式，这里指的是spark自己来管理整个的计算资源，交给spark来管理了，他也是一个分布式的。但是这个计算资源不跟其他的mapreduce呀或者storm等程序所共用的，他自己来管理的。意思就是说，spark他自己玩自己的。他有一套独立的资源管理系统在里面此模式中有一组进程叫master和worker</p><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_323.png" alt="blog: www.xubatian.cn"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">构建一个由Master+Slave构成的Spark集群，使Spark程序运行在集群中，且有Cluster与Client模式(默认是这种)两种。主要区别在于：Driver程序的运行节点不一样。</span><br><span class="line">Driver是一个线程,是执行我写的程序的main方法,就是执行的spark-submit --class里面的main方法.</span><br><span class="line"></span><br><span class="line">Client模式指什么意思呢? 我们需要执行Spark-submit来提交一个任务. 如果我们采用的是client模式. 那么我们的Driver程序就在当前提交的机器的线程. 这个spark-submit是不是一个进程,这个进程的名字叫spark-submit. 这个线程就运行在进程spark-submit里面. 这是client模式.</span><br><span class="line">而Cluster模式,他这个Driver运行在哪? 他是由master来决定的一个位置. 所以cluster模式和Client模式他们两个区别就在这. </span><br><span class="line">如果生产环境中要用的话, 用的最多的是Cluster模式. 因为 Driver在整个运行过程中,他会和其他节点Executor做通信. 这样就对内存用的比较大了. 这样的话,我们让集群自己去做选择是更好一些. 因为client模式, 你在哪提交的,你的Driver就运行在哪. 很有可能,你提交的地方的这台机器本身资源不足等问题,所以用cluster模式更好一些</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">7077 standlone模式下master的服务端口</span><br><span class="line">8080 standlone模式下master的web端口</span><br><span class="line">4040 Driver的web端口</span><br><span class="line">18080 历史服务端口</span><br><span class="line">8088 ResourceManager的web端口</span><br><span class="line">19888 是MapReduce里面yarn的历史服务端口</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_322.png" alt="blog: www.xubatian.cn"></p><h3 id="安装使用-1"><a href="#安装使用-1" class="headerlink" title="安装使用"></a>安装使用</h3><p>1）进入spark安装目录下的conf文件夹</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ cd spark/conf/</span><br></pre></td></tr></table></figure><p>2）修改配置文件名称</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ mv slaves.template slaves</span><br><span class="line">[shangbaishuyao@hadoop102 conf]$ mv spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure><p>3）修改slave文件，添加work节点</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vim slaves</span><br><span class="line"></span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p>4）修改spark-env.sh文件，添加如下配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vim spark-env.sh</span><br><span class="line"></span><br><span class="line">SPARK_MASTER_HOST=hadoop102</span><br><span class="line">SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure><p>5）分发spark包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ xsync spark/</span><br></pre></td></tr></table></figure><p>6）启动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ sbin/start-all.sh</span><br><span class="line">[shangbaishuyao@hadoop102 spark]$ util.sh </span><br><span class="line">================shangbaishuyao@hadoop102================</span><br><span class="line">3330 Jps</span><br><span class="line">3238 Worker</span><br><span class="line">3163 Master</span><br><span class="line">================shangbaishuyao@hadoop103================</span><br><span class="line">2966 Jps</span><br><span class="line">2908 Worker</span><br><span class="line">================shangbaishuyao@hadoop104================</span><br><span class="line">2978 Worker</span><br><span class="line">3036 Jps</span><br></pre></td></tr></table></figure><p>网页查看：hadoop102:8080<br>注意：如果遇到 “JAVA_HOME not set” 异常，可以在sbin目录下的spark-config.sh 文件中加入如下配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure><p>7）官方求PI案例</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077 \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_324.png" alt="blog: www.xubatian.cn"></p><p>8）启动spark shell</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/spark/bin/spark-shell \</span><br><span class="line">--master spark://hadoop102:7077 \                     </span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--total-executor-cores 2</span><br><span class="line">参数：--master spark://hadoop102:7077指定要连接的集群的master</span><br><span class="line">执行WordCount程序</span><br><span class="line">scala&gt;sc.textFile(&quot;/opt/module/spark/input&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">res0: Array[(String, Int)] = Array((hadoop,6), (oozie,3), (spark,3), (hive,3), (shangbaishuyao,3), (hbase,6))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><h3 id="JobHistoryServer配置-查看历史用的-历史服务器"><a href="#JobHistoryServer配置-查看历史用的-历史服务器" class="headerlink" title="JobHistoryServer配置  (查看历史用的,历史服务器)"></a>JobHistoryServer配置  (查看历史用的,历史服务器)</h3><p>1）修改spark-default.conf.template名称</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ mv spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure><p>2）修改spark-default.conf文件，开启Log  (配置的是写)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vi spark-defaults.conf</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               hdfs://hadoop102:9000/directory</span><br></pre></td></tr></table></figure><p>注意：HDFS上的目录需要提前存在。<br>3）修改spark-env.sh文件，添加如下配置 (配置的是读取)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vi spark-env.sh</span><br><span class="line"></span><br><span class="line">export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=18080</span><br><span class="line">-Dspark.history.retainedApplications=30 </span><br><span class="line">-Dspark.history.fs.logDirectory=hdfs://hadoop102:9000/directory&quot;</span><br></pre></td></tr></table></figure><p>参数描述：<br>spark.eventLog.dir：Application在运行过程中所有的信息均记录在该属性指定的路径下<br>spark.history.ui.port=18080  WEBUI访问的端口号为18080<br>spark.history.fs.logDirectory=hdfs://hadoop102:9000/directory配置了该属性后，在start-history-server.sh时就无需再显式的指定路径，Spark History Server页面只展示该指定路径下的信息<br>spark.history.retainedApplications=30指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除。注意：这个是内存中的应用数，而不是页面上显示的应用数。<br>4）分发配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ xsync spark-defaults.conf</span><br><span class="line">[shangbaishuyao@hadoop102 conf]$ xsync spark-env.sh</span><br></pre></td></tr></table></figure><p>5）启动历史服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ sbin/start-history-server.sh</span><br></pre></td></tr></table></figure><p>6）再次执行任务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077 \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>7）查看历史服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop102:18080</span><br></pre></td></tr></table></figure><h3 id="HA配置"><a href="#HA配置" class="headerlink" title="HA配置"></a>HA配置</h3><p>我们worker有三个宕机一个还能用,但是master只有一个,我发高可用,所以我们要将master依赖zookeeper,由zookeeper来选举master,不能让我们直接指定</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_325.png" alt="blog: www.xubatian.cn"></p><p>1）zookeeper正常安装并启动<br>2）修改spark-env.sh文件，添加如下配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vi spark-env.sh</span><br><span class="line">注释掉如下内容：因为我们的master由zookeeper来选举,不能由我们自己指定了,故注释掉</span><br><span class="line">#SPARK_MASTER_HOST=hadoop102</span><br><span class="line">#SPARK_MASTER_PORT=7077</span><br><span class="line">添加上如下内容：</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS=&quot;</span><br><span class="line">-Dspark.deploy.recoveryMode=ZOOKEEPER </span><br><span class="line">-Dspark.deploy.zookeeper.url=hadoop102,hadoop103,hadoop104 </span><br><span class="line">-Dspark.deploy.zookeeper.dir=/spark&quot;</span><br></pre></td></tr></table></figure><p>3）分发配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ xsync spark-env.sh</span><br></pre></td></tr></table></figure><p>4）在hadoop102上启动全部节点</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ sbin/start-all.sh</span><br></pre></td></tr></table></figure><p>5）在hadoop103上单独启动master节点</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop103 spark]$ sbin/start-master.sh</span><br></pre></td></tr></table></figure><p>6）spark HA集群访问,一般先连接前面的master,前面挂掉了再连接后面的 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/spark/bin/spark-shell \</span><br><span class="line">--master spark://hadoop102:7077,hadoop103:7077 \</span><br><span class="line">--executor-memory 2g \</span><br><span class="line">--total-executor-cores 2</span><br></pre></td></tr></table></figure><h2 id="Yarn模式"><a href="#Yarn模式" class="headerlink" title="Yarn模式"></a>Yarn模式</h2><p>他不需要部署spark集群,我只需要部署yarn集群,因为我所解压的spark只是作为本地客户端,只是提交用,当然你也可以分发, 分发后的目的也就是hadoop102,hadoop103,hadoop104都是可以提交任务而已. 因为yarn模式,我们解压的spark仅仅作为客户端来用的<br>生产环境当中用的最多的一种模式，就是说spark他有一个计算任务。任务呢，我来执行，但是运行任务的CPU,还有内存这些东西交给yarn来管理。交给yarn来管理其实就是交给resourcemanager和nodemanager来管理。</p><h3 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h3><p>Spark客户端直接连接Yarn，不需要额外构建Spark集群。有yarn-client和yarn-cluster两种模式，主要区别在于：Driver程序的运行节点。<br>yarn-client：Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出<br>yarn-cluster：Driver程序运行在由RM（ResourceManager）启动的AM（APPMaster）适用于生产环境。</p><p>Yarn-cluster提交流程图:<br><a href="https://www.cnblogs.com/shi-qi/articles/12174206.html">https://www.cnblogs.com/shi-qi/articles/12174206.html</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_326.png" alt="blog: www.xubatian.cn"></p><h3 id="博主解析"><a href="#博主解析" class="headerlink" title="博主解析"></a>博主解析</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_327.png" alt="blog: www.xubatian.cn"></p><h3 id="安装使用-2"><a href="#安装使用-2" class="headerlink" title="安装使用"></a>安装使用</h3><p>1）修改hadoop配置文件yarn-site.xml,添加如下内容<br>[shangbaishuyao@hadoop102 hadoop]$ vi yarn-site.xml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>2）修改spark-env.sh，添加如下配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vi spark-env.sh</span><br><span class="line"></span><br><span class="line">YARN_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop</span><br></pre></td></tr></table></figure><p>3）分发配置文件,只是分发这个配置文件,我spark-yarn要分发吗?不需要.因为我们不需要额外去构建spark集群,yarn是分布式的,而本地的spark-yarn仅仅是做提交任务的客户端,所以<br>Spark-yarn不许要分发</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ xsync /opt/module/hadoop-2.7.2/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure><p>4）执行一个程序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>注意：在提交任务之前需启动HDFS以及YARN集群。</p><p>Yarn 模式读取的文件是HDFS里面的</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_328.png" alt="blog: www.xubatian.cn"></p><h3 id="日志查看"><a href="#日志查看" class="headerlink" title="日志查看"></a>日志查看</h3><p>1）修改配置文件spark-defaults.conf，添加如下内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.yarn.historyServer.address=hadoop102:18080</span><br><span class="line">spark.history.ui.port=18080</span><br></pre></td></tr></table></figure><p>2）重启Spark历史服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ sbin/stop-history-server.sh </span><br><span class="line">stopping org.apache.spark.deploy.history.HistoryServer</span><br><span class="line">[shangbaishuyao@hadoop102 spark]$ sbin/start-history-server.sh </span><br><span class="line">starting org.apache.spark.deploy.history.HistoryServer, logging to /opt/module/spark/logs/spark-shangbaishuyao-org.apache.spark.deploy.history.HistoryServer-1-hadoop102.out</span><br></pre></td></tr></table></figure><p>3）提交任务到Yarn执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>4）Web页面查看日志</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_329.png" alt="blog: www.xubatian.cn"></p><h2 id="Mesos模式"><a href="#Mesos模式" class="headerlink" title="Mesos模式"></a>Mesos模式</h2><p>（这种很少用，几乎没有公司在用。Mesos也是apache的一个资源调度框架，就和yarn是类似的东西）</p><p>Spark客户端直接连接Mesos；不需要额外构建Spark集群。国内应用比较少，更多的是运用yarn调度。</p><h2 id="几种模式对比"><a href="#几种模式对比" class="headerlink" title="几种模式对比"></a>几种模式对比</h2><table><thead><tr><th>模式</th><th>Spark安装机器数</th><th>需启动的进程(提交任务前)</th><th>所属者</th></tr></thead><tbody><tr><td>Local</td><td>1</td><td>无</td><td>Spark</td></tr><tr><td>Standalone</td><td>3</td><td>Master及Worker</td><td>Spark</td></tr><tr><td>Yarn</td><td>1</td><td>Yarn及HDFS</td><td>Hadoop</td></tr></tbody></table><h1 id="案例代码"><a href="#案例代码" class="headerlink" title="案例代码"></a>案例代码</h1><p>WordCount案例代码:</p><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/sparkCore/src/main/scala/com/shangbaishuyao/wordCount">https://github.com/ShangBaiShuYao/bigdata/blob/master/sparkCore/src/main/scala/com/shangbaishuyao/wordCount</a></p><p>整个Spark学习案例代码:</p><p><a href="https://github.com/ShangBaiShuYao/bigdata">https://github.com/ShangBaiShuYao/bigdata</a></p><p>调试补充:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_330.png" alt="https://www.xubatian.cn/"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt; 征途漫漫，惟有奋斗；梦想成真，惟有实干。                     ——人民日报&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="spark" scheme="http://xubatian.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>hive常用函数收录</title>
    <link href="http://xubatian.cn/hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E6%94%B6%E5%BD%95/"/>
    <id>http://xubatian.cn/hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E6%94%B6%E5%BD%95/</id>
    <published>2022-01-18T14:43:32.000Z</published>
    <updated>2022-01-23T02:58:21.757Z</updated>
    
    <content type="html"><![CDATA[<p>山再高，往上攀，总能登顶；路再长，走下去，定能到达。       ——人民日报                  </p><p>​                                              </p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_307.jpg" alt="blog: www.xubatian.cn"></p><h1 id="常用日期函数"><a href="#常用日期函数" class="headerlink" title="常用日期函数"></a>常用日期函数</h1><p>unix_timestamp:返回当前或指定时间的时间戳    </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select unix_timestamp();</span><br><span class="line">select unix_timestamp(&quot;2020-10-28&quot;,&#x27;yyyy-MM-dd&#x27;);</span><br></pre></td></tr></table></figure><p>from_unixtime：将时间戳转为日期格式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select from_unixtime(1603843200);</span><br></pre></td></tr></table></figure><p>current_date：当前日期</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select current_date;</span><br></pre></td></tr></table></figure><p>current_timestamp：当前的日期加时间</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select current_timestamp;</span><br></pre></td></tr></table></figure><p>to_date：抽取日期部分</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select to_date(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>year：获取年</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select year(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>month：获取月</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select month(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>day：获取日</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select day(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>hour：获取时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select hour(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>minute：获取分</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select minute(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>second：获取秒</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select second(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>weekofyear：当前时间是一年中的第几周</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select weekofyear(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>dayofmonth：当前时间是一个月中的第几天</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select dayofmonth(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>months_between： 两个日期间的月份</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select months_between(&#x27;2020-04-01&#x27;,&#x27;2020-10-28&#x27;);</span><br></pre></td></tr></table></figure><p>add_months：日期加减月</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select add_months(&#x27;2020-10-28&#x27;,-3);</span><br></pre></td></tr></table></figure><p>datediff：两个日期相差的天数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select datediff(&#x27;2020-11-04&#x27;,&#x27;2020-10-28&#x27;);</span><br></pre></td></tr></table></figure><p>date_add：日期加天数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select date_add(&#x27;2020-10-28&#x27;,4);</span><br></pre></td></tr></table></figure><p>date_sub：日期减天数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select date_sub(&#x27;2020-10-28&#x27;,-4);</span><br></pre></td></tr></table></figure><p>last_day：日期的当月的最后一天</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select last_day(&#x27;2020-02-30&#x27;);</span><br></pre></td></tr></table></figure><p>date_format(): 格式化日期</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select date_format(&#x27;2020-10-28 12:12:12&#x27;,&#x27;yyyy/MM/dd HH:mm:ss&#x27;);</span><br></pre></td></tr></table></figure><p>常用取整函数<br>round： 四舍五入</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select round(3.14);</span><br><span class="line">select round(3.54);</span><br></pre></td></tr></table></figure><p>ceil：  向上取整</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select ceil(3.14);</span><br><span class="line">select ceil(3.54);</span><br></pre></td></tr></table></figure><p>floor： 向下取整</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select floor(3.14);</span><br><span class="line">select floor(3.54);</span><br></pre></td></tr></table></figure><h1 id="常用字符串操作函数"><a href="#常用字符串操作函数" class="headerlink" title="常用字符串操作函数"></a>常用字符串操作函数</h1><p>upper： 转大写</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select upper(&#x27;low&#x27;);</span><br></pre></td></tr></table></figure><p>lower： 转小写</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select lower(&#x27;low&#x27;);</span><br></pre></td></tr></table></figure><p>length： 长度</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select length(&quot;shangbaishuyao&quot;);</span><br></pre></td></tr></table></figure><p>trim：  前后去空格</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select trim(&quot; shangbaishuyao &quot;);</span><br></pre></td></tr></table></figure><p>lpad： 向左补齐，到指定长度</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select lpad(&#x27;shangbaishuyao&#x27;,9,&#x27;g&#x27;);</span><br></pre></td></tr></table></figure><p>rpad：  向右补齐，到指定长度</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select rpad(&#x27;shangbaishuyao&#x27;,9,&#x27;g&#x27;);</span><br></pre></td></tr></table></figure><p>regexp_replace：使用正则表达式匹配目标字符串，匹配成功后替换！</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT regexp_replace(&#x27;2020/10/25&#x27;, &#x27;/&#x27;, &#x27;-&#x27;);</span><br></pre></td></tr></table></figure><h1 id="集合操作"><a href="#集合操作" class="headerlink" title="集合操作"></a>集合操作</h1><p>size： 集合中元素的个数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select size(friends) from test3;</span><br></pre></td></tr></table></figure><p>map_keys： 返回map中的key</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select map_keys(children) from test3;</span><br></pre></td></tr></table></figure><p>map_values: 返回map中的value</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select map_values(children) from test3;</span><br></pre></td></tr></table></figure><p>array_contains: 判断array中是否包含某个元素</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select array_contains(friends,&#x27;bingbing&#x27;) from test3;</span><br></pre></td></tr></table></figure><p>sort_array： 将array中的元素排序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select sort_array(friends) from test3;</span><br></pre></td></tr></table></figure><p>grouping_set:多维分析</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;山再高，往上攀，总能登顶；路再长，走下去，定能到达。       ——人民日报                  &lt;/p&gt;
&lt;p&gt;​                                              &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="hive" scheme="http://xubatian.cn/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>hive简介(二)</title>
    <link href="http://xubatian.cn/hive%E7%AE%80%E4%BB%8B(%E4%BA%8C)/"/>
    <id>http://xubatian.cn/hive%E7%AE%80%E4%BB%8B(%E4%BA%8C)/</id>
    <published>2022-01-18T10:35:45.000Z</published>
    <updated>2022-01-23T02:58:21.682Z</updated>
    
    <content type="html"><![CDATA[<p>白衣执甲，逆行而上，以勇气和辛劳诠释了医者仁心，用担当和奉献换来了山河无恙。新时代中国女性可亲、可敬、可爱，她们在热血奋斗中怒放生命，在应对挑战中成就不凡。          ——人民日报                    </p><p>​                                              </p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_268.png" alt="blog: www.xubatian.cn"></p><h1 id="DDL数据定义-创建库-创建表都属于他"><a href="#DDL数据定义-创建库-创建表都属于他" class="headerlink" title="DDL数据定义(创建库,创建表都属于他)"></a>DDL数据定义(创建库,创建表都属于他)</h1><h2 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h2><p>1）创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_269.png" alt="blog: www.xubatian.cn"></p><p>2）避免要创建的数据库已经存在错误，增加if not exists判断。（标准写法）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists</span><br><span class="line">hive (default)&gt; create database if not exists db_hive;</span><br></pre></td></tr></table></figure><p>3）创建一个数据库，指定数据库在HDFS上存放的位置,/db-hive2.db是名字</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive2 location &#x27;/db_hive2.db&#x27;;  放在根目录下</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_270.png" alt="blog: www.xubatian.cn"></p><h2 id="查询数据库"><a href="#查询数据库" class="headerlink" title="查询数据库"></a>查询数据库</h2><h3 id="显示数据库"><a href="#显示数据库" class="headerlink" title="显示数据库"></a>显示数据库</h3><p>1．显示数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure><p>2．过滤显示查询的数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases like &#x27;db_hive*&#x27;;</span><br><span class="line">OK</span><br><span class="line">db_hive</span><br><span class="line">db_hive_1</span><br></pre></td></tr></table></figure><p>4.2.2 查看数据库详情<br>1．显示数据库信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hivehdfs://hadoop102:9000/user/hive/warehouse/db_hive.dbshangbaishuyaoUSER</span><br></pre></td></tr></table></figure><p>上白书妖补充:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_271.png" alt="blog: www.xubatian.cn"></p><p>2．显示数据库详细信息，extended</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hivehdfs://hadoop102:9000/user/hive/warehouse/db_hive.dbshangbaishuyaoUSER</span><br><span class="line">40.3.3 切换当前数据库</span><br><span class="line">hive (default)&gt; use db_hive;</span><br></pre></td></tr></table></figure><ol start="3"><li>切换当前数据库 </li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; use db_hive;</span><br></pre></td></tr></table></figure><h3 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h3><p>用户可以使用ALTER DATABASE (alter database )命令为某个数据库的DBPROPERTIES(dbproperties)设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter database db_hive set dbproperties(&#x27;createtime&#x27;=&#x27;20170830&#x27;);</span><br></pre></td></tr></table></figure><p>在hive中查看修改结果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line">db_name comment location        owner_name      owner_type      parameters</span><br><span class="line">db_hive         hdfs://hadoop102:8020/user/hive/warehouse/db_hive.db    shangbaishuyao USER    &#123;createtime=20170830&#125;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_272.png" alt="blog: www.xubatian.cn"></p><h3 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h3><p>1．删除空数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;drop database db_hive2;</span><br></pre></td></tr></table></figure><p>2．如果删除的数据库不存在，最好采用 if exists判断数据库是否存在</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</span><br><span class="line">hive&gt; drop database if exists db_hive2;</span><br></pre></td></tr></table></figure><p>3．如果数据库不为空，里面有表的话，可以采用cascade命令，强制删除</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</span><br><span class="line">hive&gt; drop database db_hive cascade;</span><br></pre></td></tr></table></figure><h2 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h2><p>1．建表语法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name </span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[COMMENT table_comment] </span><br><span class="line">[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[CLUSTERED BY (col_name, col_name, ...) </span><br><span class="line">[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] </span><br><span class="line">[ROW FORMAT row_format] </span><br><span class="line">[STORED AS file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_273.png" alt="blog: www.xubatian.cn"></p><p>2．字段解释说明<br>（1）CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。<br>（2）EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。<br>（3）COMMENT：为表和列添加注释。<br>（4）PARTITIONED BY创建分区表<br>（5）CLUSTERED BY创建分桶表<br>（6）SORTED BY不常用<br>（7）ROW FORMAT<br>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]<br>        [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]<br>   | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)]<br>用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT 或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。<br>SerDe是Serialize/Deserilize的简称，目的是用于序列化和反序列化。<br>（8）STORED AS指定存储文件类型<br>常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）<br>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。<br>（9）LOCATION ：指定表在HDFS上的存储位置。<br>（10）LIKE允许用户复制现有的表结构，但是不复制数据。</p><h3 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h3><p>也叫内部表,将来删除表中数据会把对应HDFS中存的也删掉,在HDFS上的文件,目录也会删除掉</p><p>1．理论<br>我们默认创建的表都是内部表,外部表的创建需要单独去指定<br>默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。    当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据。</p><p>2．案例实操<br>（1）普通创建表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student2(</span><br><span class="line">id <span class="type">int</span>, name string</span><br><span class="line">) </span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">&#x27;/user/hive/warehouse/student2&#x27;</span>;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_275.png" alt="blog: www.xubatian.cn"></p><p>（2）根据查询结果创建表（查询的结果会添加到新创建的表中）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists student3 as select id, name from student;</span><br></pre></td></tr></table></figure><p>（3）根据已经存在的表结构创建表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists student4 like student;</span><br></pre></td></tr></table></figure><p>补充效果图:  说明这种方式只是按照表结构来创建一张表,里面并没有数据</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_276.png" alt="blog: www.xubatian.cn"></p><p>（4）查询表的类型,更详细的信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE  </span><br><span class="line">由这个就可以看出他是管理表,或者说就是内部表,注意，只要表删了，在hdfs中对应的目录就没了</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_277.png" alt="blog: www.xubatian.cn"></p><h3 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h3><p>hive认为这个表不归我管，所以叫外部表</p><p>1．理论<br>因为表是外部表，所以Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。就是在所对应的hdfs上的文件不会被删除<br>特点就是：删除数据时，如果你是内部表，我就把你数据删了，如果你是外部表，我就不删除你数据<br>2．管理表和外部表的使用场景<br>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。<br>外部表的创建就只需要在创建表的语法上加上external 就是创建的外部表</p><ol start="3"><li>案例实操<br>分别创建部门和员工外部表，并向表中导入数据。</li></ol><p>​      建表语句</p><p>创建部门表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create external  table if not exists default.dept(</span><br><span class="line">deptno int,</span><br><span class="line">dname string,</span><br><span class="line">loc int</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>创建员工表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create external table if not exists default.emp(</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string, </span><br><span class="line">sal double, </span><br><span class="line">comm double,</span><br><span class="line">deptno int)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>查看创建的表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line">dept</span><br><span class="line">emp</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>导入数据</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_278.png" alt="blog: www.xubatian.cn"></p><ol start="4"><li>向外部表中导入数据</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept;</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/emp.txt&#x27; into table default.emp;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_279.png" alt="blog: www.xubatian.cn"></p><p>查询结果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp;</span><br><span class="line">hive (default)&gt; select * from dept;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_280.png" alt="blog: www.xubatian.cn"></p><p>上面看的结构比较乱,如果想看的更清楚可以使用hiveserver2查看:如下图</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_281.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_282.png" alt="blog: www.xubatian.cn"></p><ol start="5"><li>查看表格式化数据</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted dept;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure><h3 id="管理表-内部表-与外部表的互相转换"><a href="#管理表-内部表-与外部表的互相转换" class="headerlink" title="管理表(内部表)与外部表的互相转换"></a>管理表(内部表)与外部表的互相转换</h3><p>（1）查询表的类型</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure><p>（2）修改内部表student2为外部表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table student2 set tblproperties(&#x27;EXTERNAL&#x27;=&#x27;TRUE&#x27;);</span><br></pre></td></tr></table></figure><p>（3）查询表的类型</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure><p>（4）修改外部表student2为内部表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table student2 set tblproperties(&#x27;EXTERNAL&#x27;=&#x27;FALSE&#x27;);</span><br></pre></td></tr></table></figure><p>（5）查询表的类型</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure><p>注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！</p><h2 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h2><p>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。 </p><h3 id="分区表基本操作"><a href="#分区表基本操作" class="headerlink" title="分区表基本操作"></a>分区表基本操作</h3><p>1．引入分区表（需要根据日期对日志进行管理,按月存储,按月分区）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/user/hive/warehouse/log_partition/20170702/20170702.log</span><br><span class="line">/user/hive/warehouse/log_partition/20170703/20170703.log</span><br><span class="line">/user/hive/warehouse/log_partition/20170704/20170704.log</span><br></pre></td></tr></table></figure><p>2．创建分区表语法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition(</span><br><span class="line">deptno int, dname string, loc string )</span><br><span class="line">partitioned by (month string)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>3．加载数据到分区表中</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition partition(month=&#x27;201709&#x27;);</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition partition(month=&#x27;201708&#x27;);</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition partition(month=&#x27;201707’);</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_283.png" alt="blog: www.xubatian.cn"></p><p>4．查询分区表中数据<br>    单分区查询</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month=&#x27;201709&#x27;;</span><br></pre></td></tr></table></figure><p>多分区联合查询</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month=&#x27;201709&#x27;</span><br><span class="line">              union</span><br><span class="line">              select * from dept_partition where month=&#x27;201708&#x27;</span><br><span class="line">              union</span><br><span class="line">              select * from dept_partition where month=&#x27;201707&#x27;;</span><br><span class="line"></span><br><span class="line">_u3.deptno      _u3.dname       _u3.loc _u3.month</span><br><span class="line">10      ACCOUNTING      NEW YORK        201707</span><br><span class="line">10      ACCOUNTING      NEW YORK        201708</span><br><span class="line">10      ACCOUNTING      NEW YORK        201709</span><br><span class="line">20      RESEARCH        DALLAS  201707</span><br><span class="line">20      RESEARCH        DALLAS  201708</span><br><span class="line">20      RESEARCH        DALLAS  201709</span><br><span class="line">30      SALES   CHICAGO 201707</span><br><span class="line">30      SALES   CHICAGO 201708</span><br><span class="line">30      SALES   CHICAGO 201709</span><br><span class="line">40      OPERATIONS      BOSTON  201707</span><br><span class="line">40      OPERATIONS      BOSTON  201708</span><br><span class="line">40      OPERATIONS      BOSTON  201709</span><br></pre></td></tr></table></figure><p>5．单独增加分区<br>    创建单个分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month=&#x27;201706&#x27;) ;</span><br></pre></td></tr></table></figure><p>​    同时创建多个分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month=&#x27;201705&#x27;) partition(month=&#x27;201704&#x27;);</span><br></pre></td></tr></table></figure><p>6．删除分区<br>    删除单个分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month=&#x27;201704&#x27;);</span><br></pre></td></tr></table></figure><p>同时删除多个分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month=&#x27;201705&#x27;), partition (month=&#x27;201706&#x27;);</span><br></pre></td></tr></table></figure><p>7．查看分区表有多少分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show partitions dept_partition;</span><br></pre></td></tr></table></figure><p>8．查看分区表结构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc formatted dept_partition;</span><br><span class="line">#Partition Information          </span><br><span class="line">#col_name              data_type               comment             </span><br><span class="line">month                   string    </span><br></pre></td></tr></table></figure><h3 id="分区表注意事项"><a href="#分区表注意事项" class="headerlink" title="分区表注意事项"></a>分区表注意事项</h3><p>1．创建二级分区表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> dept_partition2(</span><br><span class="line">                 deptno <span class="type">int</span>, dname string, loc string )</span><br><span class="line">                 partitioned <span class="keyword">by</span> (<span class="keyword">month</span> string, <span class="keyword">day</span> string)</span><br><span class="line">                 <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>2．正常的加载数据<br>（1）加载数据到二级分区表中</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table</span><br><span class="line"> default.dept_partition2 partition(month=&#x27;201709&#x27;, day=&#x27;13&#x27;);</span><br></pre></td></tr></table></figure><p>（2）查询分区数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;13&#x27;;</span><br></pre></td></tr></table></figure><p>3．把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_284.png" alt="blog: www.xubatian.cn"></p><p>（1）方式一：上传数据后修复<br>    上传数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=12;  我自己创建分区目录,和分区如何关联?</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br><span class="line">把文件上传到自己创建的的分区目录下</span><br></pre></td></tr></table></figure><p>查询数据（查询不到刚上传的数据）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;12&#x27;;</span><br></pre></td></tr></table></figure><p>执行修复命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; msck repair table dept_partition2;</span><br></pre></td></tr></table></figure><p>再次查询数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;12&#x27;;</span><br></pre></td></tr></table></figure><p>​    （2）方式二：上传数据后添加分区<br>​    上传数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br></pre></td></tr></table></figure><p>​    执行添加分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 add partition(month=&#x27;201709&#x27;, day=&#x27;11&#x27;);</span><br></pre></td></tr></table></figure><p>​    查询数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;11&#x27;;</span><br></pre></td></tr></table></figure><p>（3）方式三：创建文件夹后load数据到分区<br>        创建目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=10;</span><br></pre></td></tr></table></figure><p>上传数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table</span><br><span class="line"> dept_partition2 partition(month=&#x27;201709&#x27;,day=&#x27;10&#x27;);</span><br></pre></td></tr></table></figure><p>查询数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;10&#x27;;</span><br></pre></td></tr></table></figure><h2 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h2><p> 重命名表<br>1．语法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name RENAME TO new_table_name</span><br></pre></td></tr></table></figure><p>2．实操案例</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 rename to dept_partition3;</span><br></pre></td></tr></table></figure><h3 id="实操案例"><a href="#实操案例" class="headerlink" title="实操案例"></a>实操案例</h3><p>（1）查询表结构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><p>（2）添加列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add columns(deptdesc string);</span><br></pre></td></tr></table></figure><p>（3）查询表结构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><p>（4）更新列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition change column deptdesc desc int;</span><br></pre></td></tr></table></figure><p>（5）查询表结构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><p>（6）替换列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition replace columns(deptno string, dname, string, loc string);</span><br></pre></td></tr></table></figure><p>（7）查询表结构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><h2 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; drop table dept_partition;</span><br></pre></td></tr></table></figure><h1 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h1><h2 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h2><h3 id="向表中装载数据（Load）"><a href="#向表中装载数据（Load）" class="headerlink" title="向表中装载数据（Load）"></a>向表中装载数据（Load）</h3><p>1．语法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data [local] inpath &#x27;/opt/module/datas/student.txt&#x27; overwrite | into table student [partition (partcol1=val1,…)];</span><br></pre></td></tr></table></figure><p>（1）load data:表示加载数据<br>（2）local:表示从本地加载数据到hive表；没有写Local则从HDFS加载数据到hive表<br>（3）inpath:表示加载数据的路径<br>（4）overwrite:表示覆盖表中已有数据，没有写overwrite则表示追加<br>（5）into table:表示加载到哪张表<br>（6）student:表示具体的表<br>（7）partition:表示上传到指定分区<br>注意:从本地文件系统中取load数据,他其实是copy的操作.如果你从HDFS系统中去load数据,他其实是剪切的操作.<br>2．实操案例<br>    （0）创建一张表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table student(id string, name string) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>（1）加载本地文件到hive    </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/student.txt&#x27; into table default.student;</span><br></pre></td></tr></table></figure><p>（2）加载HDFS文件到hive中<br>    上传文件到HDFS</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/shangbaishuyao/hive;</span><br></pre></td></tr></table></figure><p>加载HDFS上数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data inpath &#x27;/user/shangbaishuyao/hive/student.txt&#x27; into table default.student;</span><br></pre></td></tr></table></figure><p>（3）加载数据覆盖表中已有的数据<br>上传文件到HDFS</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/shangbaishuyao/hive;</span><br></pre></td></tr></table></figure><p>加载数据覆盖表中已有的数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data inpath &#x27;/user/shangbaishuyao/hive/student.txt&#x27; overwrite into table default.student;</span><br></pre></td></tr></table></figure><p>上传的方式就是不能加””号</p><h3 id="通过查询语句向表中插入数据（Insert）"><a href="#通过查询语句向表中插入数据（Insert）" class="headerlink" title="通过查询语句向表中插入数据（Insert）"></a>通过查询语句向表中插入数据（Insert）</h3><p>1．创建一张分区表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>2．基本插入数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table  student partition(month=&#x27;201709&#x27;) values(1,&#x27;wangwu&#x27;);</span><br></pre></td></tr></table></figure><p>3．基本模式插入（根据单张表查询结果）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table student partition(month=&#x27;201708&#x27;)</span><br><span class="line">             select id, name from student where month=&#x27;201709&#x27;;</span><br></pre></td></tr></table></figure><p>4．多插入模式（根据多张表查询结果）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; from student</span><br><span class="line">               insert overwrite table student partition(month=&#x27;201707&#x27;)</span><br><span class="line">               select id, name where month=&#x27;201709&#x27;</span><br><span class="line">               insert overwrite table student partition(month=&#x27;201706&#x27;)</span><br><span class="line">              select id, name where month=&#x27;201709&#x27;;</span><br></pre></td></tr></table></figure><h3 id="查询语句中创建表并加载数据（As-Select）"><a href="#查询语句中创建表并加载数据（As-Select）" class="headerlink" title="查询语句中创建表并加载数据（As Select）"></a>查询语句中创建表并加载数据（As Select）</h3><p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists student3</span><br><span class="line">as select id, name from student;</span><br></pre></td></tr></table></figure><h3 id="创建表时通过Location指定加载数据路径"><a href="#创建表时通过Location指定加载数据路径" class="headerlink" title="创建表时通过Location指定加载数据路径"></a>创建表时通过Location指定加载数据路径</h3><p>1．创建表，并指定在hdfs上的位置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table if not exists student5(</span><br><span class="line">                 id int, name string  )</span><br><span class="line">                 row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">                 location &#x27;/user/hive/warehouse/student5&#x27;;</span><br></pre></td></tr></table></figure><p>2．上传数据到hdfs上</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hive/warehouse/student5;</span><br></pre></td></tr></table></figure><p>3．查询数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from student5;</span><br></pre></td></tr></table></figure><h3 id="Import数据到指定Hive表中"><a href="#Import数据到指定Hive表中" class="headerlink" title="Import数据到指定Hive表中"></a>Import数据到指定Hive表中</h3><p>注意：先用export导出后，再将数据导入。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; import table student2 partition(month=&#x27;201709&#x27;) from</span><br><span class="line"> &#x27;/user/hive/warehouse/export/student&#x27;;</span><br></pre></td></tr></table></figure><h2 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h2><h3 id="Insert导出"><a href="#Insert导出" class="headerlink" title="Insert导出"></a>Insert导出</h3><p>1．将查询的结果导出到本地</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/datas/export/student&#x27;</span><br><span class="line">            select * from student;</span><br></pre></td></tr></table></figure><p>2．将查询的结果格式化导出到本地</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;insert overwrite local directory &#x27;/opt/module/datas/export/student1&#x27;</span><br><span class="line">           ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;             select * from student;</span><br></pre></td></tr></table></figure><p>3．将查询的结果导出到HDFS上(没有local)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite directory &#x27;/user/shangbaishuyao/student2&#x27;</span><br><span class="line">             ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; </span><br><span class="line">             select * from student;</span><br></pre></td></tr></table></figure><h3 id="Hadoop命令导出到本地"><a href="#Hadoop命令导出到本地" class="headerlink" title="Hadoop命令导出到本地"></a>Hadoop命令导出到本地</h3><p>get就是从hdfs上去下载文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0</span><br><span class="line">/opt/module/datas/export/student3.txt;</span><br></pre></td></tr></table></figure><h3 id="Hive-Shell-命令导出"><a href="#Hive-Shell-命令导出" class="headerlink" title="Hive Shell 命令导出"></a>Hive Shell 命令导出</h3><p>基本语法：（hive -f/-e 执行语句或者脚本 &gt; file）<br>注意:   &gt;   这个是追加号,将他追加到我们的文件里面</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/hive -e &#x27;select * from default.student;&#x27; &gt;</span><br><span class="line"> /opt/module/datas/export/student4.txt;</span><br></pre></td></tr></table></figure><h3 id="Export导出到HDFS上"><a href="#Export导出到HDFS上" class="headerlink" title="Export导出到HDFS上"></a>Export导出到HDFS上</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(defahiveult)&gt; export table default.student to</span><br><span class="line"> &#x27;/user/hive/warehouse/export/student&#x27;;</span><br></pre></td></tr></table></figure><p>说明他不仅把你数据导出来,还把你的元数据也导出来了,说明,也就是只有这两者都具备的的文件才能够导入</p><p>导出的东西有元数据的,但是你student.txt导入的东西也是student.txt,是没有元数据的</p><h3 id="清除表中数据（Truncate）"><a href="#清除表中数据（Truncate）" class="headerlink" title="清除表中数据（Truncate）"></a>清除表中数据（Truncate）</h3><p>注意：Truncate只能删除管理表，不能删除外部表中数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; truncate table student;</span><br></pre></td></tr></table></figure><h1 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h1><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select</a></p><h2 id="查询语句语法"><a href="#查询语句语法" class="headerlink" title="查询语句语法"></a>查询语句语法</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[WITH CommonTableExpression (, CommonTableExpression)*]    (Note: Only available</span><br><span class="line"> starting with Hive 0.13.0)</span><br><span class="line"></span><br><span class="line">SELECT [ALL | DISTINCT] select_expr, select_expr, ...</span><br><span class="line">  FROM table_reference</span><br><span class="line">  [WHERE where_condition]</span><br><span class="line">  [GROUP BY col_list]</span><br><span class="line">  [ORDER BY col_list]</span><br><span class="line">  [CLUSTER BY col_list</span><br><span class="line">    | [DISTRIBUTE BY col_list] [SORT BY col_list]</span><br><span class="line">  ]</span><br><span class="line"> [LIMIT number]</span><br></pre></td></tr></table></figure><h3 id="基本查询（Select…From）"><a href="#基本查询（Select…From）" class="headerlink" title="基本查询（Select…From）"></a>基本查询（Select…From）</h3><h3 id="全表和特定列查询"><a href="#全表和特定列查询" class="headerlink" title="全表和特定列查询"></a>全表和特定列查询</h3><p>1．全表查询</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp;</span><br></pre></td></tr></table></figure><p>2．选择特定列查询</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select empno, ename from emp;</span><br></pre></td></tr></table></figure><p>注意：<br>（1）SQL 语言大小写不敏感。<br>（2）SQL 可以写在一行或者多行<br>（3）关键字不能被缩写也不能分行<br>（4）各子句一般要分行写。<br>（5）使用缩进提高语句的可读性。</p><h3 id="列别名"><a href="#列别名" class="headerlink" title="列别名"></a>列别名</h3><p>1．重命名一个列<br>2．便于计算<br>3．紧跟列名，也可以在列名和别名之间加入关键字‘AS’<br>4．案例实操<br>查询名称和部门</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename AS name, deptno dn from emp;</span><br></pre></td></tr></table></figure><h3 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h3><table><thead><tr><th>运算符</th><th>描述</th></tr></thead><tbody><tr><td>A+B</td><td>A和B 相加</td></tr><tr><td>A-B</td><td>A减去B</td></tr><tr><td>A*B</td><td>A和B 相乘</td></tr><tr><td>A/B</td><td>A除以B</td></tr><tr><td>A%B</td><td>A对B取余</td></tr><tr><td>A&amp;B</td><td>A和B按位取与</td></tr><tr><td>A|B</td><td>A和B按位取或</td></tr><tr><td>A^B</td><td>A和B按位取异或</td></tr><tr><td>~A</td><td>A按位取反</td></tr></tbody></table><h3 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h3><p>1．求总行数（count）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) cnt from emp;</span><br></pre></td></tr></table></figure><p>2．求工资的最大值（max）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select max(sal) max_sal from emp;</span><br></pre></td></tr></table></figure><p>3．求工资的最小值（min）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select min(sal) min_sal from emp;</span><br></pre></td></tr></table></figure><p>4．求工资的总和（sum）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select sum(sal) sum_sal from emp; </span><br></pre></td></tr></table></figure><p>5．求工资的平均值（avg）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select avg(sal) avg_sal from emp;</span><br></pre></td></tr></table></figure><h3 id="Limit语句"><a href="#Limit语句" class="headerlink" title="Limit语句"></a>Limit语句</h3><p>典型的查询会返回多行数据。LIMIT子句用于限制返回的行数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp limit 5;</span><br></pre></td></tr></table></figure><h3 id="Where语句"><a href="#Where语句" class="headerlink" title="Where语句"></a>Where语句</h3><p>1．使用WHERE子句，将不满足条件的行过滤掉<br>2．WHERE子句紧随FROM子句<br>3．案例实操<br>查询出薪水大于1000的所有员工</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal &gt;1000;</span><br></pre></td></tr></table></figure><h3 id="比较运算符（Between-In-Is-Null）"><a href="#比较运算符（Between-In-Is-Null）" class="headerlink" title="比较运算符（Between/In/ Is Null）"></a>比较运算符（Between/In/ Is Null）</h3><p>1）下面表中描述了谓词操作符，这些操作符同样可以用于JOIN…ON和HAVING语句中</p><table><thead><tr><th>操作符</th><th>支持的数据类型</th><th>描述</th></tr></thead><tbody><tr><td>A=B</td><td>基本数据类型</td><td>如果A等于B则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=&gt;B</td><td>基本数据类型</td><td>如果A和B都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL</td></tr><tr><td>A&lt;&gt;B, A!=B</td><td>基本数据类型</td><td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A [NOT] BETWEEN B AND C</td><td>基本数据类型</td><td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A IS NULL</td><td>所有数据类型</td><td>如果A等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>A IS NOT NULL</td><td>所有数据类型</td><td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>IN(数值1, 数值2)</td><td>所有数据类型</td><td>使用 IN运算显示列表中的值</td></tr><tr><td>A [NOT] LIKE B</td><td>STRING 类型</td><td>B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A RLIKE B, A REGEXP B</td><td>STRING 类型</td><td>B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td></tr></tbody></table><p>2）案例实操<br>（1）查询出薪水等于5000的所有员工</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal =5000;</span><br></pre></td></tr></table></figure><p>（2）查询工资在500到1000的员工信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal between 500 and 1000;</span><br></pre></td></tr></table></figure><p>（3）查询comm为空的所有员工信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where comm is null;</span><br></pre></td></tr></table></figure><p>（4）查询工资是1500或5000的员工信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal IN (1500, 5000);</span><br></pre></td></tr></table></figure><h3 id="Like和RLike"><a href="#Like和RLike" class="headerlink" title="Like和RLike"></a>Like和RLike</h3><p>1）使用LIKE运算选择类似的值<br>2）选择条件可以包含字符或数字:<br>% 代表零个或多个字符(任意个字符)。<br>_ 代表一个字符。<br>2）RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_285.png" alt="blog: www.xubatian.cn"></p><p>这个就是匹配我这里面有2和3的数字</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_286.png" alt="blog: www.xubatian.cn"></p><p>这个表示我要严格要求以2开头以3结束的匹配数据</p><p>4）案例实操<br>    （1）查找以2开头薪水的员工信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal LIKE &#x27;2%&#x27;;</span><br></pre></td></tr></table></figure><p>​    （2）查找第二个数值为2的薪水的员工信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal LIKE &#x27;_2%&#x27;;</span><br></pre></td></tr></table></figure><p>​    （3）查找薪水中含有2的员工信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal RLIKE &#x27;[2]&#x27;;</span><br></pre></td></tr></table></figure><h3 id="逻辑运算符（And-Or-Not）"><a href="#逻辑运算符（And-Or-Not）" class="headerlink" title="逻辑运算符（And/Or/Not）"></a>逻辑运算符（And/Or/Not）</h3><table><thead><tr><th>操作符</th><th>含义</th></tr></thead><tbody><tr><td>AND</td><td>逻辑并</td></tr><tr><td>OR</td><td>逻辑或</td></tr><tr><td>NOT</td><td>逻辑否</td></tr></tbody></table><p>案例实操<br>    （1）查询薪水大于1000，部门是30</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal&gt;1000 and dep tno=30;</span><br></pre></td></tr></table></figure><p>​    （2）查询薪水大于1000，或者部门是30</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal&gt;1000 or deptno=30;</span><br></pre></td></tr></table></figure><p>​    （3）查询除了20部门和30部门以外的员工信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where deptno not IN(30, 20);</span><br></pre></td></tr></table></figure><h1 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h1><h2 id="Group-By语句"><a href="#Group-By语句" class="headerlink" title="Group By语句"></a>Group By语句</h2><p>分组?什么时候下会用到分组?<br>把整个一张表拆分成多个数据<br>GROUP BY(分组)语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。<br>案例实操：<br>    （1）计算emp表每个部门的平均工资</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno;</span><br></pre></td></tr></table></figure><p>2）计算emp每个部门中每个岗位的最高薪水</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select t.deptno, t.job, max(t.sal) max_sal from emp t group by t.deptno, t.job;   主标识</span><br></pre></td></tr></table></figure><h2 id="Having语句"><a href="#Having语句" class="headerlink" title="Having语句"></a>Having语句</h2><p>1．having与where不同点</p><p>（1） where针对表中的列发挥作用，查询数据；<br>having针对查询结果中的列发挥作用，筛选数据。<br>（2）where后面不能写分组函数，而having后面可以使用分组函数。<br>（3）having只用于group by分组统计语句。</p><p>2．案例实操<br>（1）求每个部门的平均薪水大于2000的部门<br>求每个部门的平均工资</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select deptno, avg(sal) from emp group by deptno;</span><br></pre></td></tr></table></figure><p>​      求每个部门的平均薪水大于2000的部门</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select deptno, avg(sal) avg_sal from emp group by deptno having</span><br><span class="line"> avg_sal &gt; 2000;</span><br></pre></td></tr></table></figure><h1 id="Join语句"><a href="#Join语句" class="headerlink" title="Join语句"></a>Join语句</h1><h2 id="等值Join"><a href="#等值Join" class="headerlink" title="等值Join"></a>等值Join</h2><p>Hive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接。等值连接就是A表里面的某某字段等于B表里面的某某字段.<br>案例实操<br>（1）根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称；</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno, d.dname from emp e join dept d</span><br><span class="line"> on e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><h2 id="表的别名"><a href="#表的别名" class="headerlink" title="表的别名"></a>表的别名</h2><p>1．好处<br>（1）使用别名可以简化查询。<br>（2）使用表名前缀可以提高执行效率。<br>2．案例实操<br>合并员工表和部门表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno </span><br><span class="line">from emp e join dept d </span><br><span class="line">on e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><h2 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h2><p>内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno</span><br><span class="line"> = d.deptno;</span><br></pre></td></tr></table></figure><h2 id="左外连接"><a href="#左外连接" class="headerlink" title="左外连接"></a>左外连接</h2><p>左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno</span><br><span class="line"> = d.deptno;</span><br></pre></td></tr></table></figure><h2 id="右外连接"><a href="#右外连接" class="headerlink" title="右外连接"></a>右外连接</h2><p>右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno</span><br><span class="line"> = d.deptno;</span><br></pre></td></tr></table></figure><h2 id="满外连接"><a href="#满外连接" class="headerlink" title="满外连接"></a>满外连接</h2><p>​    满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno</span><br><span class="line"> = d.deptno;</span><br></pre></td></tr></table></figure><h2 id="多表连接"><a href="#多表连接" class="headerlink" title="多表连接"></a>多表连接</h2><p>注意：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</p><p>1．创建位置表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists default.location(</span><br><span class="line">loc int,</span><br><span class="line">loc_name string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>2．导入数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/location.txt&#x27; into table default.location;</span><br></pre></td></tr></table></figure><p>3．多表连接查询</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;SELECT e.ename, d.deptno, l.loc_name</span><br><span class="line">FROM   emp e </span><br><span class="line">JOIN   dept d</span><br><span class="line">ON     d.deptno = e.deptno </span><br><span class="line">JOIN   location l</span><br><span class="line">ON     d.loc = l.loc;</span><br></pre></td></tr></table></figure><p>大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l;进行连接操作。<br>注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的。</p><h2 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h2><p>1．笛卡尔集会在下面条件下产生<br>（1）省略连接条件<br>（2）连接条件无效<br>（3）所有表中的所有行互相连接<br>2．案例实操</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select empno, dname from emp, dept;</span><br></pre></td></tr></table></figure><h2 id="连接谓词中不支持or-但是and是支持的"><a href="#连接谓词中不支持or-但是and是支持的" class="headerlink" title="连接谓词中不支持or 但是and是支持的"></a>连接谓词中不支持or 但是and是支持的</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno</span><br><span class="line">= d.deptno or e.ename=d.ename;   错误的</span><br></pre></td></tr></table></figure><h1 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h1><h2 id="全局排序（Order-By）"><a href="#全局排序（Order-By）" class="headerlink" title="全局排序（Order By）"></a>全局排序（Order By）</h2><p>Order By：全局排序，会进入到一个Reducer里面,最终会生成一个结果文件<br>1．使用 ORDER BY 子句排序<br>ASC（ascend）: 升序（默    认）<br>DESC（descend）: 降序<br>2．ORDER BY 子句在SELECT语句的结尾<br>3．案例实操<br>    （1）查询员工信息按工资升序排列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp order by sal;</span><br></pre></td></tr></table></figure><p>​    （2）查询员工信息按工资降序排列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp order by sal desc;</span><br></pre></td></tr></table></figure><h2 id="按照别名排序"><a href="#按照别名排序" class="headerlink" title="按照别名排序"></a>按照别名排序</h2><p>按照员工薪水的2倍排序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename, sal*2 twosal from emp order by twosal;</span><br></pre></td></tr></table></figure><h2 id="多个列排序"><a href="#多个列排序" class="headerlink" title="多个列排序"></a>多个列排序</h2><p>按照部门和工资升序排序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename, deptno, sal from emp order by deptno, sal ;</span><br></pre></td></tr></table></figure><h2 id="每个MapReduce内部排序（Sort-By）"><a href="#每个MapReduce内部排序（Sort-By）" class="headerlink" title="每个MapReduce内部排序（Sort By）"></a>每个MapReduce内部排序（Sort By）</h2><p>什么情况下map进入多个reduce呢?  分区.  </p><p>Sort By：对每个Reducer里面的数据进行排序，对全局结果集来说不是排序。<br>Order by : 是对整个数据进行排序<br>1．设置reduce个数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces=3;</span><br></pre></td></tr></table></figure><p>2．查看设置reduce个数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces;</span><br></pre></td></tr></table></figure><p>3．根据部门编号降序查看员工信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp sort by empno desc;</span><br></pre></td></tr></table></figure><p>4．将查询结果导入到文件中（按照部门编号降序排序）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/datas/sortby-result&#x27;</span><br><span class="line"> select * from emp sort by deptno desc;</span><br></pre></td></tr></table></figure><p>MapReduce里面是按照hash来进行分区的</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_287.png" alt="blog: www.xubatian.cn"></p><p>可以看到,这是随机分配的,是没有规律的,我们一般排序肯定会先指定分区然后在排序,分区的话,一般我们会指定按照什么来分区,然而这里没有,说明这种排序方式不提供指定分区,从而出现了随机分配的现象</p><h2 id="分区排序（Distribute-By）"><a href="#分区排序（Distribute-By）" class="headerlink" title="分区排序（Distribute By）"></a>分区排序（Distribute By）</h2><p>这种方式确确实实分区了,分区之后你在哪个区就用sort by了</p><p>Distribute By：类似MR中partition，进行分区,就是先分好区,分好之后在每个partition里面进行sort by一下，结合sort by使用。</p><p>注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。<br>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><p>Distribute指定正常的分区字段,指定之后就可以正常的分区操作了</p><h2 id="Cluster-By"><a href="#Cluster-By" class="headerlink" title="Cluster By"></a>Cluster By</h2><p>当distribute by和sort by字段相同时，可以使用cluster by方式。<br>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。<br>Sort by可以指定是升序还是降序,但是用cluster by之后就不能指定了,只能是升序<br>1）以下两种写法等价</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp cluster by deptno;</span><br><span class="line">hive (default)&gt; select * from emp distribute by deptno sort by deptno;</span><br></pre></td></tr></table></figure><p>注意：按照部门编号分区，不一定就是固定死的数值，可以是30号和60号部门分到一个分区里面去</p><h1 id="其他常用查询函数"><a href="#其他常用查询函数" class="headerlink" title="其他常用查询函数"></a>其他常用查询函数</h1><h3 id="空字段赋值"><a href="#空字段赋值" class="headerlink" title="空字段赋值"></a>空字段赋值</h3><p>1.函数说明<br>NVL：给值为NULL的数据赋值，它的格式是NVL( string1, replace_with)。<br>它的功能是如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为NULL ，则返回NULL。<br>2.数据准备：采用员工表<br>3.查询：如果员工的comm为NULL，则用-1代替</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select nvl(comm,-1) from emp;</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">20.0</span><br><span class="line">300.0</span><br><span class="line">500.0</span><br><span class="line">-1.0</span><br><span class="line">1400.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br><span class="line">0.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br></pre></td></tr></table></figure><p>4.查询：如果员工的comm为NULL，则用领导id代替</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select nvl(comm,mgr) from emp;</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">20.0</span><br><span class="line">300.0</span><br><span class="line">500.0</span><br><span class="line">7839.0</span><br><span class="line">1400.0</span><br><span class="line">7839.0</span><br><span class="line">7839.0</span><br><span class="line">7566.0</span><br><span class="line">NULL</span><br><span class="line">0.0</span><br><span class="line">7788.0</span><br><span class="line">7698.0</span><br><span class="line">7566.0</span><br></pre></td></tr></table></figure><h3 id="CASE-WHEN-THEN-ELSE-END"><a href="#CASE-WHEN-THEN-ELSE-END" class="headerlink" title="CASE  WHEN  THEN  ELSE  END"></a>CASE  WHEN  THEN  ELSE  END</h3><h1 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h1><p>1．相关函数说明<br>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化<br>CURRENT ROW：当前行<br>n PRECEDING：往前n行数据<br>n FOLLOWING：往后n行数据<br>UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点<br>LAG(col,n)：往前第n行数据<br>LEAD(col,n)：往后第n行数据<br>NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。</p><h1 id="Rank"><a href="#Rank" class="headerlink" title="Rank"></a>Rank</h1><p>1．函数说明<br>RANK() 排序相同时会重复，总数不会变<br>DENSE_RANK() 排序相同时会重复，总数会减少<br>ROW_NUMBER() 会根据顺序计算</p><h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1><h2 id="系统内置函数"><a href="#系统内置函数" class="headerlink" title="系统内置函数"></a>系统内置函数</h2><p>1．查看系统自带的函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show functions;</span><br></pre></td></tr></table></figure><p>2．显示自带的函数的用法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc function upper;</span><br></pre></td></tr></table></figure><p>3．详细显示自带的函数的用法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc function extended upper;</span><br></pre></td></tr></table></figure><h2 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h2><p>1）Hive 自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。<br>2）当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。<br>3）根据用户自定义函数类别分为以下三种：<br>    （1）UDF（User-Defined-Function）<br>        一进一出<br>    （2）UDAF（User-Defined Aggregation Function）<br>        聚集函数，多进一出<br>        类似于：count/max/min<br>    （3）UDTF（User-Defined Table-Generating Functions）<br>        一进多出<br>        如lateral view explode()<br>4）官方文档地址<br><a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a><br>5）编程步骤：<br>    （1）继承org.apache.hadoop.hive.ql.UDF<br>    （2）需要实现evaluate函数；evaluate函数支持重载；<br>    （3）在hive的命令行窗口创建函数<br>        a）添加jar</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar linux_jar_path</span><br></pre></td></tr></table></figure><p>​        b）创建function，</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create [temporary] function [dbname.]function_name AS class_name;</span><br></pre></td></tr></table></figure><p>​    （4）在hive的命令行窗口删除函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Drop [temporary] function [if exists] [dbname.]function_name;</span><br></pre></td></tr></table></figure><p>6）注意事项<br>    （1）UDF必须要有返回类型，可以返回null，但是返回类型不能为void；</p><h2 id="自定义UDF函数"><a href="#自定义UDF函数" class="headerlink" title="自定义UDF函数"></a>自定义UDF函数</h2><p>1．创建一个Maven工程Hive<br>2．导入依赖</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hive-exec&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.2.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure><p>3．创建一个类</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">package com.shangbaishuyao.hive;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line">public class Lower extends UDF &#123;</span><br><span class="line"></span><br><span class="line">public String evaluate (final String s) &#123;</span><br><span class="line"></span><br><span class="line">if (s == null) &#123;</span><br><span class="line">return null;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">return s.toLowerCase();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="4"><li>打成jar包上传到服务器/opt/module/jars/udf.jar</li></ol><p>5．将jar包添加到hive的classpath,</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; add jar /opt/module/datas/udf.jar;</span><br></pre></td></tr></table></figure><p>6.创建临时函数与开发好的java class关联</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create [temporary] function [dbname.]function_name AS class_name;</span><br></pre></td></tr></table></figure><p>注意:如果你加了这个temporary的话,创建的函数就是临时的,窗口关闭就没了<br>如果你不加这个temporary的话,创建的就是永久的函数;function后面指定那个库,as后面关联的是你idea写的类的全类名</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create temporary function mylower as &quot;com.shangbaishuyao.hive.Lower&quot;;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_288.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_289.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_290.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_291.png" alt="blog: www.xubatian.cn"></p><p>7．即可在hql中使用自定义的函数strip </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename, mylower(ename) lowername from emp;</span><br></pre></td></tr></table></figure><h2 id="自定义UDTF函数"><a href="#自定义UDTF函数" class="headerlink" title="自定义UDTF函数"></a>自定义UDTF函数</h2><p>1）需求说明<br>自定义一个UDTF实现将一个任意分割符的字符串切割成独立的单词，例如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Line:<span class="string">&quot;hello,world,hadoop,hive&quot;</span></span><br><span class="line"></span><br><span class="line">Myudtf(line, <span class="string">&quot;,&quot;</span>)</span><br><span class="line"></span><br><span class="line">hello</span><br><span class="line">world</span><br><span class="line">hadoop</span><br><span class="line">hive</span><br></pre></td></tr></table></figure><p>2）代码实现</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shangbaishuyao.udtf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUDTF</span> <span class="keyword">extends</span> <span class="title">GenericUDTF</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> ArrayList&lt;String&gt; outList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StructObjectInspector <span class="title">initialize</span><span class="params">(StructObjectInspector argOIs)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">  <span class="comment">//1.定义输出数据的列名和类型</span></span><br><span class="line">    List&lt;String&gt; fieldNames = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    List&lt;ObjectInspector&gt; fieldOIs = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.添加输出数据的列名和类型</span></span><br><span class="line">    fieldNames.add(<span class="string">&quot;lineToWord&quot;</span>);</span><br><span class="line">    fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object[] args)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//1.获取原始数据</span></span><br><span class="line">    String arg = args[<span class="number">0</span>].toString();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.获取数据传入的第二个参数，此处为分隔符</span></span><br><span class="line">    String splitKey = args[<span class="number">1</span>].toString();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.将原始数据按照传入的分隔符进行切分</span></span><br><span class="line">    String[] fields = arg.split(splitKey);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.遍历切分后的结果，并写出</span></span><br><span class="line">    <span class="keyword">for</span> (String field : fields) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//集合为复用的，首先清空集合</span></span><br><span class="line">        outList.clear();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将每一个单词添加至集合</span></span><br><span class="line">        outList.add(field);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将集合内容写出</span></span><br><span class="line">        forward(outList);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>3）打成jar包上传到服务器/opt/module/data/udtf.jar<br>4）将jar包添加到hive的classpath下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; add jar /opt/module/data/udtf.jar;</span><br></pre></td></tr></table></figure><p>5）创建临时函数与开发好的java class关联</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create temporary function myudtf as &quot;com.shangbaishuyao.hive.MyUDTF&quot;;</span><br></pre></td></tr></table></figure><p>6）即可在hql中使用自定义的函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select myudtf(line, &quot;,&quot;) word from words;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_292.png" alt="blog: www.xubatian.cn"></p><h1 id="压缩和存储"><a href="#压缩和存储" class="headerlink" title="压缩和存储"></a>压缩和存储</h1><h2 id="snappy压缩"><a href="#snappy压缩" class="headerlink" title="snappy压缩"></a>snappy压缩</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_293.png" alt="blog: www.xubatian.cn"></p><p>所以hadoop本身是不支持的</p><p>不支持Snappy压缩的,如图,东西很少</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_294.png" alt="blog: www.xubatian.cn"></p><p>而支持Snapp压缩的东西就比较多了</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_295.png" alt="blog: www.xubatian.cn"></p><p>所以我们要把这些东西直接拷贝到我正在使用的hadoop下面把他覆盖一下就可以了</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_296.png" alt="blog: www.xubatian.cn"></p><p>拷贝完查看一下就ok了</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_297.png" alt="blog: www.xubatian.cn"></p><p>然后如果是集群的话,就分发一下</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_298.png" alt="blog: www.xubatian.cn"></p><h1 id="Hadoop压缩配置"><a href="#Hadoop压缩配置" class="headerlink" title="Hadoop压缩配置"></a>Hadoop压缩配置</h1><h2 id="MR支持的压缩编码"><a href="#MR支持的压缩编码" class="headerlink" title="MR支持的压缩编码"></a>MR支持的压缩编码</h2><table><thead><tr><th>压缩格式</th><th>工具</th><th>算法</th><th>文件扩展名</th><th>是否可切分</th></tr></thead><tbody><tr><td>DEFAULT</td><td>无</td><td>DEFAULT</td><td>.deflate</td><td>否</td></tr><tr><td>Gzip</td><td>gzip</td><td>DEFAULT</td><td>.gz</td><td>否</td></tr><tr><td>bzip2</td><td>bzip2</td><td>bzip2</td><td>.bz2</td><td>是</td></tr><tr><td>LZO</td><td>lzop</td><td>LZO</td><td>.lzo</td><td>是</td></tr><tr><td>Snappy</td><td>无</td><td>Snappy</td><td>.snappy</td><td>否</td></tr></tbody></table><p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示:</p><table><thead><tr><th>压缩格式</th><th>对应的编码/解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><p>压缩性能的比较:</p><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr></tbody></table><p><a href="http://google.github.io/snappy/">http://google.github.io/snappy/</a><br>On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.</p><h2 id="压缩参数配置"><a href="#压缩参数配置" class="headerlink" title="压缩参数配置"></a>压缩参数配置</h2><p>要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）</p><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs  （在core-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.Lz4Codec</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec</td><td>org.apache.hadoop.io.compress. DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.type</td><td>RECORD</td><td>reducer输出</td><td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td></tr></tbody></table><h2 id="开启Map输出阶段压缩"><a href="#开启Map输出阶段压缩" class="headerlink" title="开启Map输出阶段压缩"></a>开启Map输出阶段压缩</h2><p>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：<br>案例实操：<br>1．开启hive中间传输数据压缩功能</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set hive.exec.compress.intermediate=true;</span><br></pre></td></tr></table></figure><p>2．开启mapreduce中map输出压缩功能</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.map.output.compress=true;</span><br></pre></td></tr></table></figure><p>3．设置mapreduce中map输出数据的压缩方式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure><p>4．执行查询语句</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(ename) name from emp;</span><br></pre></td></tr></table></figure><p>结果展示:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_299.png" alt="blog: www.xubatian.cn"></p><h2 id="开启Reduce输出阶段压缩"><a href="#开启Reduce输出阶段压缩" class="headerlink" title="开启Reduce输出阶段压缩"></a>开启Reduce输出阶段压缩</h2><p>当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。<br>案例实操：<br>1．开启hive最终输出数据压缩功能</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set hive.exec.compress.output=true;</span><br></pre></td></tr></table></figure><p>2．开启mapreduce最终输出数据压缩</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.output.fileoutputformat.compress=true;</span><br></pre></td></tr></table></figure><p>3．设置mapreduce最终数据输出压缩方式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure><p>4．设置mapreduce最终数据输出压缩为块压缩,以为他默认的是record,所以我们改成块</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK;</span><br></pre></td></tr></table></figure><p>5．测试一下输出结果是否是压缩文件;如果我直接写在hive里面是看不到的<br>所以我们写在文件里面,用下面insert这种方式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory</span><br><span class="line"> &#x27;/opt/modules/datas/distribute-result&#x27; select * from emp distribute by deptno sort by empno desc;</span><br></pre></td></tr></table></figure><p>结果展示:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_300.png" alt="blog: www.xubatian.cn"></p><p>补充:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_300.png" alt="blog: www.xubatian.cn"></p><p>结果图示:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_302.png" alt="blog: www.xubatian.cn"></p><h1 id="文件存储格式"><a href="#文件存储格式" class="headerlink" title="文件存储格式"></a>文件存储格式</h1><p>Hive支持的存储数据的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。</p><h2 id="列式存储和行式存储"><a href="#列式存储和行式存储" class="headerlink" title="列式存储和行式存储"></a>列式存储和行式存储</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_303.png" alt="blog: www.xubatian.cn"></p><p>如图所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p><h3 id="1．行存储的特点"><a href="#1．行存储的特点" class="headerlink" title="1．行存储的特点"></a>1．行存储的特点</h3><p>查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p><h3 id="2．列存储的特点"><a href="#2．列存储的特点" class="headerlink" title="2．列存储的特点"></a>2．列存储的特点</h3><p>因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。<br>TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；<br>ORC和PARQUET是基于列式存储的。</p><h2 id="TextFile格式"><a href="#TextFile格式" class="headerlink" title="TextFile格式"></a>TextFile格式</h2><p>TextFile格式,本身这种格式是不带压缩的,必须要借助其他的压缩来压缩他,压缩时要注意,如果我将来数据要切分的话,我就要用可切分的压缩格式来压缩他<br>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。</p><h2 id="Orc格式"><a href="#Orc格式" class="headerlink" title="Orc格式"></a>Orc格式</h2><p>Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。<br>如图所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于RowGroup概念，不过大小由4MB-&gt;250MB，这样应该能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data(索引)，Row Data(具体的每一行数据)，Stripe Footer：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_304.png" alt="blog: www.xubatian.cn"></p><p>1）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset。<br>2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。<br>3）Stripe Footer：存的是各个Stream的类型，长度等信息。<br>每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。</p><h2 id="Parquet格式"><a href="#Parquet格式" class="headerlink" title="Parquet格式"></a>Parquet格式</h2><p>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。<br>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。<br>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如图所示。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_306.png" alt="blog: www.xubatian.cn"></p><p>上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。</p><h2 id="存储文件的压缩比总结"><a href="#存储文件的压缩比总结" class="headerlink" title="存储文件的压缩比总结"></a>存储文件的压缩比总结</h2><p>ORC &gt;  Parquet &gt;  textFile</p><p><strong>注意orc是以256兆为单位来存的,其他方式就是按正常的128兆为一个块来存的</strong><br>在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;白衣执甲，逆行而上，以勇气和辛劳诠释了医者仁心，用担当和奉献换来了山河无恙。新时代中国女性可亲、可敬、可爱，她们在热血奋斗中怒放生命，在应对挑战中成就不凡。          ——人民日报                    &lt;/p&gt;
&lt;p&gt;​                                              &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="hive" scheme="http://xubatian.cn/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>hive简介(一)</title>
    <link href="http://xubatian.cn/hive%E7%AE%80%E4%BB%8B(%E4%B8%80)/"/>
    <id>http://xubatian.cn/hive%E7%AE%80%E4%BB%8B(%E4%B8%80)/</id>
    <published>2022-01-18T08:44:24.000Z</published>
    <updated>2022-01-23T02:58:21.777Z</updated>
    
    <content type="html"><![CDATA[<p>涓涓不塞，是为江河；源源不断，是为奋斗；生生不息，是为中国。——人民日报                </p><p>​                                              </p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_239.jpg" alt="blog: www.xubatian.cn"></p><h1 id="Hive入门"><a href="#Hive入门" class="headerlink" title="Hive入门"></a>Hive入门</h1><p>做数据分析用的,因为Hive里面不存数据. hive底层是MapReduce</p><h2 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h2><p>Hive：由Facebook开源用于解决海量结构化日志的数据统计。<br>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。<br>本质是：将HQL转化成MapReduce程序</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_241.png" alt="blog: www.xubatian.cn"></p><p>1）Hive处理的数据存储在HDFS<br>2）Hive分析数据底层的实现是MapReduce<br>3）执行程序运行在Yarn上</p><h2 id="Hive的优缺点"><a href="#Hive的优缺点" class="headerlink" title="Hive的优缺点"></a>Hive的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>)操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。</span><br><span class="line"><span class="number">2</span>)避免了去写MapReduce，减少开发人员的学习成本。</span><br><span class="line"><span class="number">3</span>)Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合。</span><br><span class="line"><span class="number">4</span>)Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。</span><br><span class="line"><span class="number">5</span>)Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</span><br></pre></td></tr></table></figure><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>1．Hive的HQL表达能力有限</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">（<span class="number">1</span>）迭代式算法无法表达(就是原始数据经过运算得到一个数据结果传给下一个在计算以此类推.这就是迭代)</span><br><span class="line">（<span class="number">2</span>）数据挖掘方面不擅长</span><br></pre></td></tr></table></figure><p>2．Hive的效率比较低(以为他通过HQL自动匹配模板,所以模板不能随便修改)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">（<span class="number">1</span>）Hive自动生成的MapReduce作业，通常情况下不够自主化</span><br><span class="line">（<span class="number">2</span>）Hive调优比较困难，粒度较粗(因为他封装了,所以调优比较困难)</span><br></pre></td></tr></table></figure><h2 id="Hive架构原理"><a href="#Hive架构原理" class="headerlink" title="Hive架构原理"></a>Hive架构原理</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_242.png" alt="blog: www.xubatian.cn"></p><p>1．用户接口：Client</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive）</span><br></pre></td></tr></table></figure><p>2．元数据：Metastore</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">元数据包括：表名、表所属的数据库（默认是<span class="keyword">default</span>）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；</span><br><span class="line">默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore</span><br></pre></td></tr></table></figure><p>3．Hadoop</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用HDFS进行存储，使用MapReduce进行计算。</span><br></pre></td></tr></table></figure><p>4．驱动器：Driver</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（<span class="number">1</span>）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</span><br><span class="line">（<span class="number">2</span>）编译器（Physical Plan）：将AST编译生成逻辑执行计划。</span><br><span class="line">（<span class="number">3</span>）优化器（Query Optimizer）：对逻辑执行计划进行优化。</span><br><span class="line">（<span class="number">4</span>）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_244.png" alt="blog: www.xubatian.cn"></p><p>Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。</p><h2 id="Hive和数据库比较"><a href="#Hive和数据库比较" class="headerlink" title="Hive和数据库比较"></a>Hive和数据库比较</h2><p>由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。</p><h3 id="数据存储位置"><a href="#数据存储位置" class="headerlink" title="数据存储位置"></a>数据存储位置</h3><p>Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中</p><h3 id="查询语言"><a href="#查询语言" class="headerlink" title="查询语言"></a>查询语言</h3><p>由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。</p><h3 id="数据更新"><a href="#数据更新" class="headerlink" title="数据更新"></a>数据更新</h3><p>由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO …  VALUES 添加数据，使用 UPDATE … SET修改数据。</p><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><p>Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于 MapReduce 的引入， Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询。</p><h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><p>Hive中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。而数据库通常有自己的执行引擎。</p><h3 id="执行延迟"><a href="#执行延迟" class="headerlink" title="执行延迟"></a>执行延迟</h3><p>Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。</p><h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><p>由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的（世界上最大的Hadoop 集群在 Yahoo!，2009年的规模在4000 台节点左右）。而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库 Oracle 在理论上的扩展能力也只有100台左右。</p><h3 id="数据规模"><a href="#数据规模" class="headerlink" title="数据规模"></a>数据规模</h3><p>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。</p><h1 id="Hive安装"><a href="#Hive安装" class="headerlink" title="Hive安装"></a>Hive安装</h1><h2 id="Hive安装地址"><a href="#Hive安装地址" class="headerlink" title="Hive安装地址"></a>Hive安装地址</h2><p>1．Hive官网地址<br><a href="http://hive.apache.org/">http://hive.apache.org/</a><br>2．文档查看地址<br><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a><br>3．下载地址<br><a href="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a><br>4．github地址<br><a href="https://github.com/apache/hive">https://github.com/apache/hive</a></p><h2 id="Hive安装部署"><a href="#Hive安装部署" class="headerlink" title="Hive安装部署"></a>Hive安装部署</h2><p>1．Hive安装及配置<br>（1）把apache-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下<br>（2）解压apache-hive-1.2.1-bin.tar.gz到/opt/module/目录下面</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 software]$ tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>（3）修改apache-hive-1.2.1-bin.tar.gz的名称为hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ mv apache-hive-1.2.1-bin/ hive</span><br></pre></td></tr></table></figure><p>（4）修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ mv hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure><p>​    （5）配置hive-env.sh文件<br>​    （a）配置HADOOP_HOME路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure><p>​    （b）配置HIVE_CONF_DIR路径(hive配置文件的路径)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_CONF_DIR=/opt/module/hive/conf</span><br></pre></td></tr></table></figure><p>2．Hadoop集群配置<br>（1）必须启动hdfs和yarn</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[shangbaishuyao@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><p>（2）在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir /tmp</span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir -p /user/hive/warehouse</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /tmp</span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_245.png" alt="blog: www.xubatian.cn"></p><h2 id="Hive基本操作"><a href="#Hive基本操作" class="headerlink" title="Hive基本操作"></a>Hive基本操作</h2><p>（1）启动hive</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/hive</span><br></pre></td></tr></table></figure><p>（2）查看数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure><p>（3）打开默认数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; use default;</span><br></pre></td></tr></table></figure><p>（4）显示default数据库中的表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure><p>（5）创建一张表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table student(id int, name string);</span><br></pre></td></tr></table></figure><p>（6）显示数据库中有几张表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure><p>（7）查看表的结构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc student;</span><br></pre></td></tr></table></figure><p>（8）向表中插入数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; insert into student values(1000,&quot;ss&quot;);</span><br></pre></td></tr></table></figure><p>（9）查询表中数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br></pre></td></tr></table></figure><p>（10）退出hive</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; quit;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_246.png" alt="blog: www.xubatian.cn"></p><h2 id="将本地文件导入Hive案例"><a href="#将本地文件导入Hive案例" class="headerlink" title="将本地文件导入Hive案例"></a>将本地文件导入Hive案例</h2><p>需求<br>将本地/opt/module/datas/student.txt这个目录下的数据导入到hive的student(id int, name string)表中。<br>1．数据准备<br>在/opt/module/datas这个目录下准备数据<br>（1）在/opt/module/目录下创建datas</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ mkdir datas</span><br></pre></td></tr></table></figure><p>（2）在/opt/module/datas/目录下创建student.txt文件并添加数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 datas]$ touch student.txt</span><br><span class="line">[shangbaishuyao@hadoop102 datas]$ vi student.txt</span><br><span class="line">1001zhangshan</span><br><span class="line">1002lishi</span><br><span class="line">1003zhaoliu</span><br></pre></td></tr></table></figure><p>注意以tab键间隔。<br>2．Hive实际操作<br>（1）启动hive</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/hive</span><br></pre></td></tr></table></figure><p>（2）显示数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure><p>（3）使用default数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; use default;</span><br></pre></td></tr></table></figure><p>（4）显示default数据库中的表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure><p>（5）删除已创建的student表 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop table student;</span><br></pre></td></tr></table></figure><p>（6）创建student表, 并声明文件分隔符’\t’</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED</span><br><span class="line"> BY &#x27;\t&#x27;;      我在创建表的时候就指定将来hive将来切割数据是如何切割,不然后面加载本地数据的时候出现这种情况</span><br><span class="line"> ROW FORMAT  :  行格式化</span><br><span class="line"> DELIMITED  : 每个字段的间隔</span><br><span class="line"> FIELDS TERMINATED BY &#x27;\t&#x27; : 通过’\t’形式切割 </span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_247.png" alt="blog: www.xubatian.cn"></p><p>本地文件导入hive案例<br>（7）加载/opt/module/datas/student.txt 文件到student数据库表中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data local inpath &#x27;/opt/module/datas/student.txt&#x27; into table student;</span><br></pre></td></tr></table></figure><p>将文件映射成student表<br>（8）Hive查询结果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br><span class="line">OK</span><br><span class="line">1001zhangshan</span><br><span class="line">1002lishi</span><br><span class="line">1003zhaoliu</span><br><span class="line">Time taken: 0.266 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure><p>3．遇到的问题<br>再打开一个客户端窗口启动hive，会产生java.sql.SQLException异常。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_248.png" alt="blog: www.xubatian.cn"></p><p>所以不能同时打开多个hive连接,如何解决呢?就是不用Derby数据库,使用mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.RuntimeException: java.lang.RuntimeException:</span><br><span class="line"> Unable to instantiate</span><br><span class="line"> org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</span><br><span class="line">        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)</span><br><span class="line">        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)</span><br><span class="line">        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)</span><br><span class="line">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">        at java.lang.reflect.Method.invoke(Method.java:606)</span><br><span class="line">        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)</span><br><span class="line">        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</span><br><span class="line">Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</span><br><span class="line">        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:86)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)</span><br><span class="line">        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)</span><br><span class="line">        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)</span><br><span class="line">        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)</span><br><span class="line">... 8 more</span><br></pre></td></tr></table></figure><p>原因是，Metastore默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore;</p><h2 id="MySql安装"><a href="#MySql安装" class="headerlink" title="MySql安装"></a>MySql安装</h2><h3 id="安装包准备"><a href="#安装包准备" class="headerlink" title="安装包准备"></a>安装包准备</h3><p>1．查看mysql是否安装，如果安装了，卸载mysql<br>    （1）查看</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 桌面]# rpm -qa|grep mysql</span><br><span class="line">mysql-libs-5.1.73-7.el6.x86_64</span><br></pre></td></tr></table></figure><p>​    （2）卸载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@Hadoop102 mysql-libs]$ sudo rpm -e mysql-libs-5.1.73-7.el6.x86_64    删不掉,因为有依赖,所以加上--nodeps</span><br><span class="line">error: Failed dependencies:</span><br><span class="line">libmysqlclient.so.16()(64bit) is needed by (installed) postfix-2:2.6.6-6.el6_7.1.x86_64</span><br><span class="line">libmysqlclient.so.16(libmysqlclient_16)(64bit) is needed by (installed) postfix-2:2.6.6-6.el6_7.1.x86_64</span><br><span class="line">mysql-libs is needed by (installed) postfix-2:2.6.6-6.el6_7.1.x86_64</span><br><span class="line">[shangbaishuyao@Hadoop102 mysql-libs]$ </span><br><span class="line">[root@hadoop102 桌面]# rpm -e --nodeps mysql-libs-5.1.73-7.el6.x86_64</span><br></pre></td></tr></table></figure><p>2．解压mysql-libs.zip文件到当前目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# unzip mysql-libs.zip</span><br><span class="line">[root@hadoop102 software]# ls</span><br><span class="line">mysql-libs.zip</span><br><span class="line">mysql-libs</span><br></pre></td></tr></table></figure><p>3．进入到mysql-libs文件夹下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# ll</span><br><span class="line">总用量 76048</span><br><span class="line">-rw-r--r--. 1 root root 18509960 3月  26 2015 MySQL-client-5.6.24-1.el6.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 root root  3575135 12月  1 2013 mysql-connector-java-5.1.27.tar.gz</span><br><span class="line">-rw-r--r--. 1 root root 55782196 3月  26 2015 MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure><h3 id="安装MySql服务器"><a href="#安装MySql服务器" class="headerlink" title="安装MySql服务器"></a>安装MySql服务器</h3><p>1．安装mysql服务端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@Hadoop102 mysql-libs]$ rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br><span class="line">error: can&#x27;t create transaction lock on /var/lib/rpm/.rpm.lock (权限不够)</span><br><span class="line">[root@hadoop102 mysql-libs]# sudo rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure><p>2．查看产生的随机密码</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_249.png" alt="blog: www.xubatian.cn"></p><p>所以必须修改随机密码:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# cat /root/.mysql_secret</span><br><span class="line">OEXaQuS8IWkG19Xs</span><br></pre></td></tr></table></figure><p>如果无法进入则:切换用户来进入</p><p>3．查看mysql状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# service mysql status</span><br></pre></td></tr></table></figure><p>启动mysql和查看状态:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@Hadoop102 ~]$ su - root</span><br><span class="line">密码：</span><br><span class="line">[root@Hadoop102 ~]# service mysql start</span><br><span class="line">Starting MySQL..                                           [确定]</span><br><span class="line">[root@Hadoop102 ~]# service mysql status</span><br><span class="line">MySQL running (5912)                                       [确定]</span><br><span class="line">[root@Hadoop102 ~]# </span><br></pre></td></tr></table></figure><p>4．启动mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# service mysql start</span><br></pre></td></tr></table></figure><h3 id="安装MySql客户端"><a href="#安装MySql客户端" class="headerlink" title="安装MySql客户端"></a>安装MySql客户端</h3><p>1．安装mysql客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure><p>2．链接mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# mysql -uroot -pOEXaQuS8IWkG19Xs</span><br></pre></td></tr></table></figure><p>3．修改密码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash">SET PASSWORD=PASSWORD(<span class="string">&#x27;000000&#x27;</span>);</span></span><br></pre></td></tr></table></figure><p>4．退出mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"><span class="built_in">exit</span></span></span><br></pre></td></tr></table></figure><p>删除mysql上不干净怎么办?一定要把这个删了</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_250.png" alt="blog: www.xubatian.cn"></p><h3 id="MySql中user表中主机配置"><a href="#MySql中user表中主机配置" class="headerlink" title="MySql中user表中主机配置"></a>MySql中user表中主机配置</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@Hadoop102</span> mysql<span class="operator">-</span>libs]# mysql <span class="operator">-</span>uroot <span class="operator">-</span>pshangbaishuyao</span><br><span class="line">Warning: <span class="keyword">Using</span> a password <span class="keyword">on</span> the command line interface can be insecure.</span><br><span class="line">Welcome <span class="keyword">to</span> the MySQL monitor.  Commands <span class="keyword">end</span> <span class="keyword">with</span> ; <span class="keyword">or</span> \g.</span><br><span class="line">Your MySQL connection id <span class="keyword">is</span> <span class="number">2</span></span><br><span class="line">Server version: <span class="number">5.6</span><span class="number">.24</span> MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) <span class="number">2000</span>, <span class="number">2015</span>, Oracle <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its affiliates. <span class="keyword">All</span> rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle <span class="keyword">is</span> a registered trademark <span class="keyword">of</span> Oracle Corporation <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its</span><br><span class="line">affiliates. Other names may be trademarks <span class="keyword">of</span> their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type <span class="string">&#x27;help;&#x27;</span> <span class="keyword">or</span> <span class="string">&#x27;\h&#x27;</span> <span class="keyword">for</span> help. Type <span class="string">&#x27;\c&#x27;</span> <span class="keyword">to</span> clear the <span class="keyword">current</span> input statement.</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> databses ;</span><br><span class="line">ERROR <span class="number">1064</span> (<span class="number">42000</span>): You have an error <span class="keyword">in</span> your <span class="keyword">SQL</span> syntax; <span class="keyword">check</span> the manual that corresponds <span class="keyword">to</span> your MySQL server version <span class="keyword">for</span> the <span class="keyword">right</span> syntax <span class="keyword">to</span> use near <span class="string">&#x27;databses&#x27;</span> <span class="keyword">at</span> line <span class="number">1</span></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> databases ;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> Database           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> information_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mysql              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> performance_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> test               <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="number">4</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.05</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> use mysql ;</span><br><span class="line">Reading <span class="keyword">table</span> information <span class="keyword">for</span> completion <span class="keyword">of</span> <span class="keyword">table</span> <span class="keyword">and</span> <span class="keyword">column</span> names</span><br><span class="line">You can turn off this feature <span class="keyword">to</span> <span class="keyword">get</span> a quicker startup <span class="keyword">with</span> <span class="operator">-</span>A</span><br><span class="line"></span><br><span class="line">Database changed</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> tables ;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="operator">|</span> Tables_in_mysql           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="operator">|</span> columns_priv              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> db                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> event                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> func                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> general_log               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_category             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_keyword              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_relation             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_topic                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> innodb_index_stats        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> innodb_table_stats        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> ndb_binlog_index          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> plugin                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> proc                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> procs_priv                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> proxies_priv              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> servers                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_master_info         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_relay_log_info      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_worker_info         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slow_log                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tables_priv               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_leap_second     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_name            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_transition      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_transition_type <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">user</span>                      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="number">28</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">desc</span> <span class="keyword">user</span> ；</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">desc</span> <span class="keyword">user</span> ;</span><br><span class="line">ERROR <span class="number">1064</span> (<span class="number">42000</span>): You have an error <span class="keyword">in</span> your <span class="keyword">SQL</span> syntax; <span class="keyword">check</span> the manual that corresponds <span class="keyword">to</span> your MySQL server version <span class="keyword">for</span> the <span class="keyword">right</span> syntax <span class="keyword">to</span> use near <span class="string">&#x27;desc user&#x27;</span> <span class="keyword">at</span> line <span class="number">3</span></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">desc</span> <span class="keyword">user</span></span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">desc</span> <span class="keyword">user</span>;</span><br><span class="line">ERROR <span class="number">1064</span> (<span class="number">42000</span>): You have an error <span class="keyword">in</span> your <span class="keyword">SQL</span> syntax; <span class="keyword">check</span> the manual that corresponds <span class="keyword">to</span> your MySQL server version <span class="keyword">for</span> the <span class="keyword">right</span> syntax <span class="keyword">to</span> use near <span class="string">&#x27;desc user&#x27;</span> <span class="keyword">at</span> line <span class="number">2</span></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">desc</span> <span class="keyword">user</span> ;</span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------+-----------------------------------+------+-----+-----------------------+-------+</span></span><br><span class="line"><span class="operator">|</span> Field                  <span class="operator">|</span> Type                              <span class="operator">|</span> <span class="keyword">Null</span> <span class="operator">|</span> Key <span class="operator">|</span> <span class="keyword">Default</span>               <span class="operator">|</span> Extra <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------+-----------------------------------+------+-----+-----------------------+-------+</span></span><br><span class="line"><span class="operator">|</span> Host                   <span class="operator">|</span> <span class="type">char</span>(<span class="number">60</span>)                          <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span> PRI <span class="operator">|</span>                       <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">User</span>                   <span class="operator">|</span> <span class="type">char</span>(<span class="number">16</span>)                          <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span> PRI <span class="operator">|</span>                       <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Password               <span class="operator">|</span> <span class="type">char</span>(<span class="number">41</span>)                          <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span>                       <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Select_priv            <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Insert_priv            <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Update_priv            <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Delete_priv            <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Create_priv            <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Drop_priv              <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Reload_priv            <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Shutdown_priv          <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Process_priv           <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> File_priv              <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Grant_priv             <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> References_priv        <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Index_priv             <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Alter_priv             <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Show_db_priv           <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Super_priv             <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Create_tmp_table_priv  <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Lock_tables_priv       <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Execute_priv           <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Repl_slave_priv        <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Repl_client_priv       <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Create_view_priv       <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Show_view_priv         <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Create_routine_priv    <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Alter_routine_priv     <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Create_user_priv       <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Event_priv             <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Trigger_priv           <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Create_tablespace_priv <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> ssl_type               <span class="operator">|</span> enum(<span class="string">&#x27;&#x27;</span>,<span class="string">&#x27;ANY&#x27;</span>,<span class="string">&#x27;X509&#x27;</span>,<span class="string">&#x27;SPECIFIED&#x27;</span>) <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span>                       <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> ssl_cipher             <span class="operator">|</span> <span class="type">blob</span>                              <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> x509_issuer            <span class="operator">|</span> <span class="type">blob</span>                              <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> x509_subject           <span class="operator">|</span> <span class="type">blob</span>                              <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> max_questions          <span class="operator">|</span> <span class="type">int</span>(<span class="number">11</span>) unsigned                  <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="number">0</span>                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> max_updates            <span class="operator">|</span> <span class="type">int</span>(<span class="number">11</span>) unsigned                  <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="number">0</span>                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> max_connections        <span class="operator">|</span> <span class="type">int</span>(<span class="number">11</span>) unsigned                  <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="number">0</span>                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> max_user_connections   <span class="operator">|</span> <span class="type">int</span>(<span class="number">11</span>) unsigned                  <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="number">0</span>                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> plugin                 <span class="operator">|</span> <span class="type">char</span>(<span class="number">64</span>)                          <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> mysql_native_password <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> authentication_string  <span class="operator">|</span> text                              <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> password_expired       <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------+-----------------------------------+------+-----+-----------------------+-------+</span></span><br><span class="line"><span class="number">43</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.21</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> host,<span class="keyword">user</span>,password <span class="keyword">from</span> <span class="keyword">user</span> ;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------+-------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> host      <span class="operator">|</span> <span class="keyword">user</span> <span class="operator">|</span> password                                  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------+-------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> localhost <span class="operator">|</span> root  <span class="operator">|</span> <span class="operator">*</span><span class="number">0877</span>ABF89C5BE70C33A37E82834E8A9D03060F71 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hadoop102 <span class="operator">|</span> root <span class="operator">|</span> <span class="operator">*</span>FD17D48A578685F9749FDF93CACE07FE9D22C41D <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> <span class="operator">|</span> root  <span class="operator">|</span> <span class="operator">*</span>FD17D48A578685F9749FDF93CACE07FE9D22C41D <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> ::<span class="number">1</span>       <span class="operator">|</span> root <span class="operator">|</span> <span class="operator">*</span>FD17D48A578685F9749FDF93CACE07FE9D22C41D <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------+-------------------------------------------+</span></span><br><span class="line">可以看到只有这几台机器能连上,所以我们要修改,我们只需要第一个,其他几个都不需要,因为只有第一个我们修改了密码是:shangbaishuyao,其他都不是</span><br><span class="line"><span class="number">4</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> host <span class="operator">!=</span>&quot;localhost&quot;;</span><br><span class="line">Query OK, <span class="number">3</span> <span class="keyword">rows</span> affected (<span class="number">0.06</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> host,<span class="keyword">user</span>,password <span class="keyword">from</span> <span class="keyword">user</span> ;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------+-------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> host      <span class="operator">|</span> <span class="keyword">user</span> <span class="operator">|</span> password                                  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------+-------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> localhost <span class="operator">|</span> root <span class="operator">|</span> <span class="operator">*</span><span class="number">0877</span>ABF89C5BE70C33A37E82834E8A9D03060F71 <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------+-------------------------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> update <span class="keyword">user</span> <span class="keyword">set</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">where</span> host<span class="operator">=</span><span class="string">&#x27;localhost&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> host,<span class="keyword">user</span>,password <span class="keyword">from</span> <span class="keyword">user</span> ;</span><br><span class="line"><span class="operator">+</span><span class="comment">------+------+-------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> host <span class="operator">|</span> <span class="keyword">user</span> <span class="operator">|</span> password                                  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+------+-------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">%</span>    <span class="operator">|</span> root <span class="operator">|</span> <span class="operator">*</span><span class="number">0877</span>ABF89C5BE70C33A37E82834E8A9D03060F71 <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+------+-------------------------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> </span><br></pre></td></tr></table></figure><p>如图可以看到:任意机器都可以访问了</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_251.png" alt="blog: www.xubatian.cn"></p><p>一定要记得刷新权限:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_252.png" alt="blog: www.xubatian.cn"></p><p>配置只要是root用户+密码，在任何主机上都能登录MySQL数据库。<br>1．进入mysql</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# mysql -uroot -pshangbaishuyao</span><br></pre></td></tr></table></figure><p>2．显示数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;show databases;</span><br></pre></td></tr></table></figure><p>3．使用mysql数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;use mysql;</span><br></pre></td></tr></table></figure><p>4．展示mysql数据库中的所有表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;show tables;</span><br></pre></td></tr></table></figure><p>5．展示user表的结构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;desc user;</span><br></pre></td></tr></table></figure><p>6．查询user表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;select User, Host, Password from user;</span><br></pre></td></tr></table></figure><p>7．修改user表，把Host表内容修改为%</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;update user set host=&#x27;%&#x27; where host=&#x27;localhost&#x27;;</span><br></pre></td></tr></table></figure><p>8．删除root用户的其他host</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;delete from user where Host=&#x27;hadoop10&#x27;;</span><br><span class="line">mysql&gt;delete from user where Host=&#x27;127.0.0.1&#x27;;</span><br><span class="line">mysql&gt;delete from user where Host=&#x27;::1&#x27;;</span><br></pre></td></tr></table></figure><p>9．刷新</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;flush privileges;</span><br></pre></td></tr></table></figure><p>10．退出</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;quit;</span><br></pre></td></tr></table></figure><h2 id="Hive元数据配置到MySql"><a href="#Hive元数据配置到MySql" class="headerlink" title="Hive元数据配置到MySql"></a>Hive元数据配置到MySql</h2><h3 id="驱动拷贝"><a href="#驱动拷贝" class="headerlink" title="驱动拷贝"></a>驱动拷贝</h3><p>1．在/opt/software/mysql-libs目录下解压mysql-connector-java-5.1.27.tar.gz驱动包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# tar -zxvf mysql-connector-java-5.1.27.tar.gz</span><br></pre></td></tr></table></figure><p>2．拷贝/opt/software/mysql-libs/mysql-connector-java-5.1.27目录下的mysql-connector-java-5.1.27-bin.jar到/opt/module/hive/lib/</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-connector-java-5.1.27]# cp mysql-connector-java-5.1.27-bin.jar  /opt/module/hive/lib/</span><br></pre></td></tr></table></figure><p>2.5.2 配置Metastore到MySql<br>1．在/opt/module/hive/conf目录下创建一个hive-site.xml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ touch hive-site.xml</span><br><span class="line">[shangbaishuyao@hadoop102 conf]$ vi hive-site.xml</span><br></pre></td></tr></table></figure><p>2．根据官方文档配置参数，拷贝数据到hive-site.xml文件中</p><p><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration">https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!--使用JDBC连接那个库 ,当发现不存在直接创建出来--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;000000&lt;/value&gt;      这个密码一定要改为:shangbaishuyao</span><br><span class="line">  &lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>3．配置完毕后，如果启动hive异常，可以重新启动虚拟机。（重启后，别忘了启动hadoop集群）</p><h3 id="多窗口启动Hive测试"><a href="#多窗口启动Hive测试" class="headerlink" title="多窗口启动Hive测试"></a>多窗口启动Hive测试</h3><p>1．先启动MySQL</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao<span class="variable">@hadoop102</span> mysql<span class="operator">-</span>libs]$ mysql <span class="operator">-</span>uroot <span class="operator">-</span>p000000</span><br><span class="line">查看有几个数据库</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> Database           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> information_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mysql             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> performance_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> test               <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br></pre></td></tr></table></figure><p>2．再次打开多个窗口，分别启动hive</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/hive</span><br></pre></td></tr></table></figure><p>3．启动hive后，回到MySQL窗口查看数据库，显示增加了metastore数据库</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> Database           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> information_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> metastore          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mysql             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> performance_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> test               <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br></pre></td></tr></table></figure><h2 id="HiveJDBC访问"><a href="#HiveJDBC访问" class="headerlink" title="HiveJDBC访问"></a>HiveJDBC访问</h2><p>为什么使用hiveJDBC形式连接呢?</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_253.png" alt="blog: www.xubatian.cn"></p><p>准备1:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_254.png" alt="blog: www.xubatian.cn"></p><p>准备2:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_255.png" alt="blog: www.xubatian.cn"></p><h3 id="启动hiveserver2服务-CDH中不用启动这个"><a href="#启动hiveserver2服务-CDH中不用启动这个" class="headerlink" title="启动hiveserver2服务,CDH中不用启动这个"></a>启动hiveserver2服务,CDH中不用启动这个</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/hiveserver2</span><br></pre></td></tr></table></figure><h3 id="启动beeline-在CDH中任何目录下直接beeline"><a href="#启动beeline-在CDH中任何目录下直接beeline" class="headerlink" title="启动beeline,在CDH中任何目录下直接beeline"></a>启动beeline,在CDH中任何目录下直接beeline</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/beeline</span><br><span class="line">Beeline version 1.2.1 by Apache Hive</span><br><span class="line"><span class="meta">beeline&gt;</span><span class="bash"></span></span><br><span class="line"><span class="bash">2.6.3 连接hiveserver2</span></span><br><span class="line"><span class="meta">beeline&gt;</span><span class="bash"> !connect jdbc:hive2://hadoop102:10000（回车）</span></span><br><span class="line">Connecting to jdbc:hive2://hadoop102:10000</span><br><span class="line">Enter username for jdbc:hive2://hadoop102:10000: shangbaishuyao（回车）</span><br><span class="line">Enter password for jdbc:hive2://hadoop102:10000: （直接回车）</span><br><span class="line">Connected to: Apache Hive (version 1.2.1)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://hadoop102:10000&gt; show databases;</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default        |</span><br><span class="line">| hive_db2       |</span><br><span class="line">+----------------+--+</span><br></pre></td></tr></table></figure><p>上白书妖补充完成图片:<br>Hive账号密码:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_256.png" alt="blog: www.xubatian.cn"></p><h3 id="Hive常用交互命令"><a href="#Hive常用交互命令" class="headerlink" title="Hive常用交互命令"></a>Hive常用交互命令</h3><p>他不是连到hive客户端里面用的,他是连接到</p><p>正常情况下,我们要连接到hive客户端里面才能操作.如下:</p><p>1．“-e”不进入hive的交互窗口执行sql语句</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/hive -e &quot;select id from student;&quot;</span><br></pre></td></tr></table></figure><p>2．“-f”执行脚本中sql语句<br>    （1）在/opt/module/datas目录下创建hivef.sql文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 datas]$ touch hivef.sql</span><br></pre></td></tr></table></figure><p>​        文件中写入正确的sql语句<br>​        select *from student;<br>​    （2）执行文件中的sql语句</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql</span><br></pre></td></tr></table></figure><p>（3）执行文件中的sql语句并将结果写入文件中</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql  &gt; /opt/module/datas/hive_result.txt</span><br></pre></td></tr></table></figure><h3 id="Hive其他命令操作"><a href="#Hive其他命令操作" class="headerlink" title="Hive其他命令操作"></a>Hive其他命令操作</h3><p>1．退出hive窗口：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash"><span class="built_in">exit</span>;</span></span><br><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">quit;</span></span><br></pre></td></tr></table></figure><p>在新版的hive中没区别了，在以前的版本是有的：<br>exit:先隐性提交数据，再退出；<br>quit:不提交数据，退出；<br>2．在hive cli命令窗口中如何查看hdfs文件系统</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;dfs -ls /;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_257.png" alt="blog: www.xubatian.cn"></p><p>3．在hive cli命令窗口中如何查看本地文件系统</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;! ls /opt/module/datas;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_258.png" alt="blog: www.xubatian.cn"></p><p>4．查看在hive中输入的所有历史命令<br>    （1）进入到当前用户的根目录/root或/home/shangbaishuyao<br>    （2）查看. hivehistory文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 ~]$ cat .hivehistory</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_259.png" alt="blog: www.xubatian.cn"></p><h2 id="Hive常见属性配置"><a href="#Hive常见属性配置" class="headerlink" title="Hive常见属性配置"></a>Hive常见属性配置</h2><h3 id="Hive数据仓库位置配置"><a href="#Hive数据仓库位置配置" class="headerlink" title="Hive数据仓库位置配置"></a>Hive数据仓库位置配置</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_260.png" alt="blog: www.xubatian.cn"></p><p>因为他就是我们默认default这个库的最原始位置.<br>    1）Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse路径下。<br>    2）在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹。<br>    3）修改default数据仓库原始位置（将hive-default.xml.template如下配置信息拷贝到hive-site.xml文件中）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;location of default database for the warehouse&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>配置同组用户有执行权限</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure><h3 id="查询后信息显示配置"><a href="#查询后信息显示配置" class="headerlink" title="查询后信息显示配置"></a>查询后信息显示配置</h3><p>1）在hive-site.xml文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.cli.print.header&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--当前是哪一个databases--&gt; </span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>上白书妖补充效果图:<br>这种方式只是在当时有用,退出重进之后就没用了,所以我们要进行上面配置<br>文件的配置</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_261.png" alt="blog: www.xubatian.cn"></p><p>2）重新启动hive，对比配置前后差异。<br>（1）配置前，如图所示</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_262.png" alt="blog: www.xubatian.cn"></p><p>（2）配置后，如图所示</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_263.png" alt="blog: www.xubatian.cn"></p><h3 id="Hive运行日志信息配置"><a href="#Hive运行日志信息配置" class="headerlink" title="Hive运行日志信息配置"></a>Hive运行日志信息配置</h3><p>1．Hive的log默认存放在/tmp/shangbaishuyao/hive.log目录下（当前用户名下）</p><p>因为放在tmp目录下,他是会定期清除的,所以要修改<br>2．修改hive的log存放日志到/opt/module/hive/logs<br>    （1）修改/opt/module/hive/conf/hive-log4j.properties.template文件名称为<br>hive-log4j.properties</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ pwd</span><br><span class="line">/opt/module/hive/conf</span><br><span class="line">[shangbaishuyao@hadoop102 conf]$ mv hive-log4j.properties.template hive-log4j.properties</span><br></pre></td></tr></table></figure><p>   （2）在hive-log4j.properties文件中修改log存放位置</p><p>hive.log.dir=/opt/module/hive/logs</p><h3 id="参数配置方式"><a href="#参数配置方式" class="headerlink" title="参数配置方式"></a>参数配置方式</h3><p>Hive的底层是hadoop,所以就逃不开hadoop四个默认的配置文件以及四个自定义文件如:core-site.xml等,所以说我们在启动hive的时候,这八个文件肯定会去加载的,这个我们hadoop的时候学过,所以我们一下配置就没加入,但是如果你hadoop都没有配置的话,你得先从hadoop开始配置</p><p>1．查看当前所有的配置信息<br>hive&gt;set;</p><h3 id="参数的配置三种方式"><a href="#参数的配置三种方式" class="headerlink" title="参数的配置三种方式"></a>参数的配置三种方式</h3><p>  三种方式中: 修改xml文件是永久生效,其他只是当前生效<br>（1）第一种:配置文件方式<br>默认配置文件：hive-default.xml<br>用户自定义配置文件：hive-site.xml<br>    注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。<br>（2）第二种:命令行参数方式<br>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。<br>例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;</span><br></pre></td></tr></table></figure><p>设置参数<br>注意: 如果只是set mapred.reduce.tasks;就是查看<br>               set mapred.reduce.tasks = 10; 这是设置成10个<br>注意：仅对本次hive启动有效</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_264.png" alt="blog: www.xubatian.cn"></p><p>查看参数设置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure><p>shangbaishuayo补充:默认是-1</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_265.png" alt="blog: www.xubatian.cn"></p><p>（3）第三种:参数声明方式<br>可以在HQL中使用SET关键字设定参数<br>例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks=100;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_266.png" alt="blog: www.xubatian.cn"></p><p>注意：仅对本次hive启动有效。<br>查看参数设置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure><p>上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。配置文件的方式是永久有效的</p><h1 id="Hive数据类型"><a href="#Hive数据类型" class="headerlink" title="Hive数据类型"></a>Hive数据类型</h1><h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><table><thead><tr><th>Hive数据类型</th><th>Java数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT  (tinyint)</td><td>byte</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT (smalint)</td><td>short</td><td>2byte有符号整数</td><td>20</td></tr><tr><td>INT    (int)</td><td>int</td><td>4byte有符号整数</td><td>20</td></tr><tr><td>BIGINT  (bigint)</td><td>long</td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN (boolean)</td><td>boolean</td><td>布尔类型，true或者false</td><td>TRUE  FALSE</td></tr><tr><td>FLOAT   (float)</td><td>float</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td>DOUBLE  (double)</td><td>double</td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td>STRING  (string)</td><td>string</td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP (timestamp)</td><td></td><td>时间类型</td><td></td></tr><tr><td>BINARY  (binary)</td><td></td><td>字节数组</td><td></td></tr></tbody></table><p>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</p><h2 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h2><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct()</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map()</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array()</td></tr></tbody></table><p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p><h2 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h2><p>1）假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;songsong&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;friends&quot;</span>: [<span class="string">&quot;bingbing&quot;</span> , <span class="string">&quot;lili&quot;</span>] ,       <span class="comment">//列表Array, </span></span><br><span class="line">    <span class="attr">&quot;children&quot;</span>: &#123;                      <span class="comment">//键值Map,</span></span><br><span class="line">        <span class="attr">&quot;xiao song&quot;</span>: <span class="number">18</span> ,</span><br><span class="line">        <span class="attr">&quot;xiaoxiao song&quot;</span>: <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">&quot;address&quot;</span>: &#123;                      <span class="comment">//结构Struct,</span></span><br><span class="line">        <span class="attr">&quot;street&quot;</span>: <span class="string">&quot;hui long guan&quot;</span> ,</span><br><span class="line">        <span class="attr">&quot;city&quot;</span>: <span class="string">&quot;beijing&quot;</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2）基于上述数据结构，我们在Hive里创建对应的表，并导入数据。<br>创建本地测试文件test.txt   在../datas/test.txt</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure><p>注意：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。<br>3）Hive上创建测试表test<br>_</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test(</span><br><span class="line">name string,</span><br><span class="line">friends <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>,</span><br><span class="line">children map<span class="operator">&lt;</span>string, <span class="type">int</span><span class="operator">&gt;</span>,</span><br><span class="line">address struct<span class="operator">&lt;</span>street:string, city:string<span class="operator">&gt;</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">collection items terminated <span class="keyword">by</span> <span class="string">&#x27;_&#x27;</span></span><br><span class="line">map keys terminated <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span></span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br></pre></td></tr></table></figure><p>字段解释：<br>row format delimited fields terminated by ‘,’  – 列分隔符<br>collection items terminated by ‘_’      –MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)<br>map keys terminated by ‘:’                – MAP中的key与value的分隔符<br>lines terminated by ‘\n’;                    – 行分隔符<br>4）导入文本数据到测试表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath ‘/opt/module/datas/test.txt’into table test</span><br></pre></td></tr></table></figure><p>5）访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> friends[<span class="number">1</span>],children[<span class="string">&#x27;xiao song&#x27;</span>],address.city <span class="keyword">from</span> test</span><br><span class="line"><span class="keyword">where</span> name<span class="operator">=</span>&quot;songsong&quot;;</span><br><span class="line">OK</span><br><span class="line">_c0     _c1     city</span><br><span class="line">lili    <span class="number">18</span>      beijing</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.076</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_267.png" alt="blog: www.xubatian.cn"></p><h2 id="类型转化"><a href="#类型转化" class="headerlink" title="类型转化"></a>类型转化</h2><p>Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p><h3 id="1．隐式类型转换规则如下"><a href="#1．隐式类型转换规则如下" class="headerlink" title="1．隐式类型转换规则如下"></a>1．隐式类型转换规则如下</h3><p>（1）任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。<br>（2）所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。<br>（3）TINYINT、SMALLINT、INT都可以转换为FLOAT。<br>（4）BOOLEAN类型不可以转换为任何其它的类型。</p><h3 id="2．可以使用CAST操作显示进行数据类型转换"><a href="#2．可以使用CAST操作显示进行数据类型转换" class="headerlink" title="2．可以使用CAST操作显示进行数据类型转换"></a>2．可以使用CAST操作显示进行数据类型转换</h3><p>例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;涓涓不塞，是为江河；源源不断，是为奋斗；生生不息，是为中国。——人民日报                &lt;/p&gt;
&lt;p&gt;​                                              &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="hive" scheme="http://xubatian.cn/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper简介</title>
    <link href="http://xubatian.cn/zookeeper%E7%AE%80%E4%BB%8B/"/>
    <id>http://xubatian.cn/zookeeper%E7%AE%80%E4%BB%8B/</id>
    <published>2022-01-18T06:51:38.000Z</published>
    <updated>2022-01-23T02:58:21.703Z</updated>
    
    <content type="html"><![CDATA[<p>我们经常在说命运，但我觉得，命是自己的，运却和整个国家相关联。历史的洪流滚起，即便微如沙粒，也能奔腾入海，也能汇成史诗。 ——人民日报                                              </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_215.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="Zookeeper入门"><a href="#Zookeeper入门" class="headerlink" title="Zookeeper入门"></a>Zookeeper入门</h1><h2 id="zookeeper概述"><a href="#zookeeper概述" class="headerlink" title="zookeeper概述"></a>zookeeper概述</h2><p>Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。多作为集群提供服务的中间件.<br>    分布式系统: 分布式系统指由很多台计算机组成的一个整体！这个整体一致对外,并且处理同一请求，系统对内透明，对外不透明！内部的每台计算机，都可以相互通信，例如使用RPC 或者是WebService！客户端向一个分布式系统发送的一次请求到接受到响应，有可能会经历多台计算机!<br>    Zookeeper从设计模式角度来理解，是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生了变化，Zookeeper就负责通知已经在Zookeeper上注册的那些观察者做出相应的反应.</p><p>Zookeeper = 文件系统 + 通知机制</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_216.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p>Zookeeper集群上每台存的数据都是一模一样的</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_220.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_219.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_221.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>域名通过DNS域名解析器解析成ip地址,所以在互联网行业,光有域名是没啥用的</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_222.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_223.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_224.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_225.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h2><h3 id="官网首页"><a href="#官网首页" class="headerlink" title="官网首页"></a>官网首页</h3><p><a href="https://zookeeper.apache.org/">https://zookeeper.apache.org/</a></p><h3 id="下载截图"><a href="#下载截图" class="headerlink" title="下载截图"></a>下载截图</h3><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_226.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_227.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="Zookeeper安装"><a href="#Zookeeper安装" class="headerlink" title="Zookeeper安装"></a>Zookeeper安装</h1><h2 id="本地模式安装部署"><a href="#本地模式安装部署" class="headerlink" title="本地模式安装部署"></a>本地模式安装部署</h2><p>1．安装前准备<br>（1）安装Jdk<br>（2）拷贝Zookeeper安装包到Linux系统下<br>（3）解压到指定目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>2．配置修改<br>    （1）将/opt/module/zookeeper-3.4.10/conf这个路径下的zoo_sample.cfg修改为zoo.cfg；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ mv zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure><p>​    （2）打开zoo.cfg文件，修改dataDir路径：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ vim zoo.cfg</span><br><span class="line">修改如下内容：</span><br><span class="line">dataDir=/opt/module/zookeeper-3.4.10/zkData</span><br></pre></td></tr></table></figure><p>​    （3）在/opt/module/zookeeper-3.4.10/这个目录上创建zkData文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ mkdir zkData</span><br></pre></td></tr></table></figure><p>3．操作Zookeeper<br>（1）启动Zookeeper</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start</span><br></pre></td></tr></table></figure><p>（2）查看进程是否启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ jps</span><br><span class="line">4020 Jps</span><br><span class="line">4001 QuorumPeerMain</span><br></pre></td></tr></table></figure><p>（3）查看状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: standalone</span><br></pre></td></tr></table></figure><p>（4）启动客户端：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ bin/zkCli.sh</span><br></pre></td></tr></table></figure><p>（5）退出客户端：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] quit</span><br></pre></td></tr></table></figure><p>注意:</p><p>ZookeeperMain是客户端进程<br>QuorumPeerMain是服务端进程  </p><p>如果通过close关闭的话,他并不会把客户端的进程杀掉,只是把你们间的连接对象给关掉了</p><p>   (6) 停止Zookeeper</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh stop</span><br></pre></td></tr></table></figure><h2 id="Zookeeper的四字命令"><a href="#Zookeeper的四字命令" class="headerlink" title="Zookeeper的四字命令"></a>Zookeeper的四字命令</h2><p>​    Zookeeper支持某些特定的四字命令(The Four Letter Words) 与其进行交互，它们大多是查询命令，用来获取Zookeeper服务的当前状态及相关信息，用户在客户端可以通过telnet<br>或nc 向Zookeeper提交相应的命令。<br>​    Zookeeper常用四字命令主要如下:</p><table><thead><tr><th>ruok</th><th>测试服务是否处于正确状态，如果确实如此，那么服务返回 imok ,否则不做任何响应。</th></tr></thead><tbody><tr><td>conf</td><td>3.3.0版本引入的，打印出服务相关配置的详细信息</td></tr><tr><td>cons</td><td>列出所有连接到这台服务器的客户端全部会话详细信息。包括 接收/发送的包数量，会话id，操作延迟、最后的操作执行等等信息</td></tr><tr><td>crst</td><td>重置所有连接的连接和会话统计信息</td></tr><tr><td>dump</td><td>列出那些比较重要的会话和临时节点。这个命令只能在leader节点上有用</td></tr><tr><td>envi</td><td>打印出服务环境的详细信息</td></tr></tbody></table><p>注: 使用之前，需要先安装nc，可以使用yum方式进行安装.</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_228.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_229.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="配置参数解读"><a href="#配置参数解读" class="headerlink" title="配置参数解读"></a>配置参数解读</h2><p>Zookeeper中的配置文件zoo.cfg中参数含义解读如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1．tickTime =2000：通信心跳数，Zookeeper服务器与客户端心跳时间，单位毫秒</span><br><span class="line">Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。</span><br><span class="line">它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime)</span><br><span class="line">2．initLimit =10：LF初始通信时限</span><br><span class="line">集群中的Follower跟随者服务器与Leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。</span><br><span class="line">3．syncLimit =5：LF同步通信时限</span><br><span class="line">集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。</span><br><span class="line">4．dataDir：数据文件目录+数据持久化路径</span><br><span class="line">主要用于保存Zookeeper中的数据。</span><br><span class="line">5．clientPort =2181：客户端连接端口</span><br><span class="line">监听客户端连接的端口。</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="Zookeeper内部原理"><a href="#Zookeeper内部原理" class="headerlink" title="Zookeeper内部原理"></a>Zookeeper内部原理</h1><h2 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h2><p>1）半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器。<br>2）Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。<br>3）以一个简单的例子来说明整个选举的过程。<br>假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么，如图所示。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_230.png" width = "" height = "" alt="xubatian的博客" align="center" /><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">（1）服务器1启动，此时只有它一台服务器启动了，它发出去的报文没有任何响应，所以它的选举状态一直是LOOKING状态。</span><br><span class="line">（2）服务器2启动，它与最开始启动的服务器1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以id值较大的服务器2胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3)，所以服务器1、2还是继续保持LOOKING状态。</span><br><span class="line">（3）服务器3启动，根据前面的理论分析，服务器3成为服务器1、2、3中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的Leader。</span><br><span class="line">（4）服务器4启动，根据前面的分析，理论上服务器4应该是服务器1、2、3、4中最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以它只能接收当小弟的命了。</span><br><span class="line">（5）服务器5启动，同4一样当小弟。</span><br></pre></td></tr></table></figure><p>xubatian补充解析:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">假如需要启动5台zookeeper, 第一台启动投票给自己, 第二台也是, 第三台也是. 到了第三台就已经是半数以上了. 如果你的myId大,则你就是Leader. 这个时里面没有数据,我们启动的情况. 但是假如里面有数据了, 但是我的Leader,即myid=3的挂掉了.那有如何重新选举呢? 原本5台zookeeper数据都是一样的. 但是最后leader,即myid=3的机器在挂掉的瞬间将数据给了myid=1的. 那这个时候其他的还没来得及给. 这个时候我们也是需要考虑谁的数据最全.即, 谁的数据最全, 谁就是大哥,即leader.</span><br><span class="line">    总结: zookeeper的选举机制, 最重要的就是myid和谁的数据量最全.</span><br></pre></td></tr></table></figure><h2 id="节点类型"><a href="#节点类型" class="headerlink" title="节点类型"></a>节点类型</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_231.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="Stat结构体"><a href="#Stat结构体" class="headerlink" title="Stat结构体"></a>Stat结构体</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_232.png" width = "" height = "" alt="xubatian的博客" align="center" /><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1）czxid-创建节点的事务zxid</span><br><span class="line">每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。</span><br><span class="line">事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。</span><br><span class="line">2）ctime - znode被创建的毫秒数(从1970年开始)</span><br><span class="line">3）mzxid - znode最后更新的事务zxid</span><br><span class="line">4）mtime - znode最后修改的毫秒数(从1970年开始)</span><br><span class="line">5）pZxid-znode最后更新的子节点zxid</span><br><span class="line">6）cversion - znode子节点变化号，znode子节点修改次数</span><br><span class="line">7）dataversion - znode数据变化号</span><br><span class="line">8）aclVersion - znode访问控制列表的变化号</span><br><span class="line">9）ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。</span><br><span class="line">10）dataLength- znode的数据长度</span><br><span class="line">11）numChildren - znode子节点数量</span><br></pre></td></tr></table></figure><h2 id="监听器原理"><a href="#监听器原理" class="headerlink" title="监听器原理"></a>监听器原理</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_233.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="写数据流程"><a href="#写数据流程" class="headerlink" title="写数据流程"></a>写数据流程</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_234.png" alt="blog: www.xubatian.cn"></p><h1 id="Zookeeper实战"><a href="#Zookeeper实战" class="headerlink" title="Zookeeper实战"></a>Zookeeper实战</h1><h2 id="分布式安装部署"><a href="#分布式安装部署" class="headerlink" title="分布式安装部署"></a>分布式安装部署</h2><p>1．集群规划<br>在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper。<br>2．解压安装<br>（1）解压Zookeeper安装包到/opt/module/目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>（2）同步/opt/module/zookeeper-3.4.10目录内容到hadoop103、hadoop104</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ xsync zookeeper-3.4.10/</span><br></pre></td></tr></table></figure><p>3．配置服务器编号<br>（1）在/opt/module/zookeeper-3.4.10/这个目录下创建zkData</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ mkdir -p zkData</span><br></pre></td></tr></table></figure><p>（2）在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zkData]$ touch myid</span><br></pre></td></tr></table></figure><p>添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码<br>（3）编辑myid文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zkData]$ vi myid</span><br></pre></td></tr></table></figure><p>​    在文件中添加与server对应的编号：(编号是选举的时候用)<br>2<br>（4）拷贝配置好的zookeeper到其他机器上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zkData]$ xsync myid</span><br></pre></td></tr></table></figure><p>并分别在hadoop102、hadoop103上修改myid文件中内容为3、4<br>4．配置zoo.cfg文件<br>（1）重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ mv zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure><p>（2）打开zoo.cfg文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vim zoo.cfg</span><br></pre></td></tr></table></figure><p>修改数据存储路径配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/opt/module/zookeeper-3.4.10/zkData</span><br><span class="line">增加如下配置</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">######################cluster##########################</span></span></span><br><span class="line">主机名  交换数据端口号  选举数据端口号  </span><br><span class="line"></span><br><span class="line">server.2=hadoop102:2888:3888</span><br><span class="line">server.3=hadoop103:2888:3888</span><br><span class="line">server.4=hadoop104:2888:3888</span><br></pre></td></tr></table></figure><p>（3）同步zoo.cfg配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ xsync zoo.cfg</span><br></pre></td></tr></table></figure><p>（4）配置参数解读</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">server.A=B:C:D。</span><br></pre></td></tr></table></figure><p>A是一个数字，表示这个是第几号服务器；<br>集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。<br>B是这个服务器的ip地址；<br>C是这个服务器与集群中的Leader服务器交换信息的端口；<br>D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</p><p>bin/zkServer.sh start 开启<br>bin/zkServer.sh status 查看状态<br>不能用,因为你已经是一个集群了,只是开了一个zookeeper是不能用的,但是他是起来了的,只不过是不能干活的    </p><p>4．集群操作<br>（1）分别启动Zookeeper</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start</span><br><span class="line">[shangbaishuyao@hadoop103 zookeeper-3.4.10]$ bin/zkServer.sh start</span><br><span class="line">[shangbaishuyao@hadoop104 zookeeper-3.4.10]$ bin/zkServer.sh start</span><br></pre></td></tr></table></figure><p>（2）查看状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh status</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line">[shangbaishuyao@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh status</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: leader</span><br><span class="line">[shangbaishuyao@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh status</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure><h2 id="客户端命令行操作"><a href="#客户端命令行操作" class="headerlink" title="客户端命令行操作"></a>客户端命令行操作</h2><table><thead><tr><th>命令基本语法</th><th>功能描述</th></tr></thead><tbody><tr><td>help</td><td>显示所有操作命令</td></tr><tr><td>ls path [watch]</td><td>使用 ls 命令来查看当前znode中所包含的内容</td></tr><tr><td>ls2 path [watch] 相当于  ls + stat</td><td>查看当前节点数据并能看到更新次数等数据</td></tr><tr><td>create 创建节点<img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_235.png" alt="img"></td><td>普通创建-s  含有序列-e  临时（重启或者超时消失） 上白书妖补充:<img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_236.png" alt="img"> 你在创建节点的时候要指定一下这个节点要存写什么数据,不然不给你创建节点,如下图:<img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_237.png" alt="img"></td></tr><tr><td>get path [watch]</td><td>获得节点的值</td></tr><tr><td>set</td><td>设置节点的具体值,修改节点的值</td></tr><tr><td>stat</td><td>查看节点状态</td></tr><tr><td>delete</td><td>删除节点</td></tr><tr><td>rmr</td><td>递归删除节点</td></tr></tbody></table><h2 id="连接客户端"><a href="#连接客户端" class="headerlink" title="连接客户端"></a>连接客户端</h2><p>1．启动客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop103 zookeeper-3.4.10]$ bin/zkCli.sh</span><br></pre></td></tr></table></figure><p>2．显示所有操作命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] help</span><br></pre></td></tr></table></figure><p>3．查看当前znode中所包含的内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /</span><br><span class="line">[zookeeper]</span><br></pre></td></tr></table></figure><p>4．查看当前节点详细数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls2 /</span><br><span class="line">[zookeeper]</span><br><span class="line">cZxid = 0x0</span><br><span class="line">ctime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">mZxid = 0x0</span><br><span class="line">mtime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">pZxid = 0x0</span><br><span class="line">cversion = -1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure><p>5．分别创建2个普通节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] create /sanguo &quot;jinlian&quot;</span><br><span class="line">Created /sanguo</span><br><span class="line">[zk: localhost:2181(CONNECTED) 4] create /sanguo/shuguo &quot;liubei&quot;</span><br><span class="line">Created /sanguo/shuguo</span><br></pre></td></tr></table></figure><p>6．获得节点的值</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 5] get /sanguo</span><br><span class="line">jinlian</span><br><span class="line">cZxid = 0x100000003</span><br><span class="line">ctime = Wed Aug 29 00:03:23 CST 2018</span><br><span class="line">mZxid = 0x100000003</span><br><span class="line">mtime = Wed Aug 29 00:03:23 CST 2018</span><br><span class="line">pZxid = 0x100000004</span><br><span class="line">cversion = 1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 7</span><br><span class="line">numChildren = 1</span><br><span class="line">[zk: localhost:2181(CONNECTED) 6]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 6] get /sanguo/shuguo</span><br><span class="line">liubei</span><br><span class="line">cZxid = 0x100000004</span><br><span class="line">ctime = Wed Aug 29 00:04:35 CST 2018</span><br><span class="line">mZxid = 0x100000004</span><br><span class="line">mtime = Wed Aug 29 00:04:35 CST 2018</span><br><span class="line">pZxid = 0x100000004</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 6</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure><p>7．创建短暂节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 7] create -e /sanguo/wuguo &quot;zhouyu&quot;</span><br><span class="line">Created /sanguo/wuguo</span><br></pre></td></tr></table></figure><p>（1）在当前客户端是能查看到的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] ls /sanguo </span><br><span class="line">[wuguo, shuguo]</span><br></pre></td></tr></table></figure><p>（2）退出当前客户端然后再重启客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 12] quit</span><br><span class="line">[shangbaishuyao@hadoop104 zookeeper-3.4.10]$ bin/zkCli.sh</span><br></pre></td></tr></table></figure><p>（3）再次查看根目录下短暂节点已经删除</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /sanguo</span><br><span class="line">[shuguo]</span><br></pre></td></tr></table></figure><p>8．创建带序号的节点<br>    （1）先创建一个普通的根节点/sanguo/weiguo</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] create /sanguo/weiguo &quot;caocao&quot;</span><br><span class="line">Created /sanguo/weiguo</span><br></pre></td></tr></table></figure><p>​    （2）创建带序号的节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] create -s /sanguo/weiguo/xiaoqiao &quot;jinlian&quot;</span><br><span class="line">Created /sanguo/weiguo/xiaoqiao0000000000</span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] create -s /sanguo/weiguo/daqiao &quot;jinlian&quot;</span><br><span class="line">Created /sanguo/weiguo/daqiao0000000001</span><br><span class="line">[zk: localhost:2181(CONNECTED) 4] create -s /sanguo/weiguo/diaocan &quot;jinlian&quot;</span><br><span class="line">Created /sanguo/weiguo/diaocan0000000002</span><br></pre></td></tr></table></figure><p>如果原来没有序号节点，序号从0开始依次递增。如果原节点下已有2个节点，则再排序时从2开始，以此类推。<br>9．修改节点数据值</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 6] set /sanguo/weiguo &quot;simayi&quot;</span><br></pre></td></tr></table></figure><p>10．节点的值变化监听<br>    （1）在hadoop104主机上注册监听/sanguo节点数据变化</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 26] [zk: localhost:2181(CONNECTED) 8] get /sanguo watch</span><br></pre></td></tr></table></figure><p>​    （2）在hadoop103主机上修改/sanguo节点的数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] set /sanguo &quot;xisi&quot;</span><br></pre></td></tr></table></figure><p>​    （3）观察hadoop104主机收到数据变化的监听</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected type:NodeDataChanged path:/sanguo</span><br></pre></td></tr></table></figure><p>11．节点的子节点变化监听（路径变化）<br>    （1）在hadoop104主机上注册监听/sanguo节点的子节点变化</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls /sanguo watch</span><br><span class="line">[aa0000000001, server101]</span><br></pre></td></tr></table></figure><p>​    （2）在hadoop103主机/sanguo节点上创建子节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] create /sanguo/jin &quot;simayi&quot;</span><br><span class="line">Created /sanguo/jin</span><br></pre></td></tr></table></figure><p>​    （3）观察hadoop104主机收到子节点变化的监听</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/sanguo</span><br></pre></td></tr></table></figure><p>12．删除节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 4] delete /sanguo/jin</span><br></pre></td></tr></table></figure><p>13．递归删除节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 15] rmr /sanguo/shuguo</span><br></pre></td></tr></table></figure><p>14．查看节点状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 17] stat /sanguo</span><br><span class="line">cZxid = 0x100000003</span><br><span class="line">ctime = Wed Aug 29 00:03:23 CST 2018</span><br><span class="line">mZxid = 0x100000011</span><br><span class="line">mtime = Wed Aug 29 00:21:23 CST 2018</span><br><span class="line">pZxid = 0x100000014</span><br><span class="line">cversion = 9</span><br><span class="line">dataVersion = 1</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 4</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure><h1 id="API应用"><a href="#API应用" class="headerlink" title="API应用"></a>API应用</h1><h2 id="案例代码"><a href="#案例代码" class="headerlink" title="案例代码"></a>案例代码</h2><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/zookeeper/TestZookeeperAPI/TestZookeeper.java">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/zookeeper/TestZookeeperAPI/TestZookeeper.java</a></p><h2 id="IDEA环境搭建"><a href="#IDEA环境搭建" class="headerlink" title="IDEA环境搭建"></a>IDEA环境搭建</h2><p>1．创建一个Maven工程<br>2．添加pom文件</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;RELEASE&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;log4j-core&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;<span class="number">2.8</span><span class="number">.2</span>&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- https:<span class="comment">//mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --&gt;</span></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;zookeeper&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;<span class="number">3.4</span><span class="number">.10</span>&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure><p>2．添加pom文件<br>3．拷贝log4j.properties文件到项目根目录<br>需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger=INFO, stdout  </span><br><span class="line">log4j.appender.stdout=org.apache.log4j.ConsoleAppender  </span><br><span class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout  </span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  </span><br><span class="line">log4j.appender.logfile=org.apache.log4j.FileAppender  </span><br><span class="line">log4j.appender.logfile.File=target/spring.log  </span><br><span class="line">log4j.appender.logfile.layout=org.apache.log4j.PatternLayout  </span><br><span class="line">log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n </span><br></pre></td></tr></table></figure><h2 id="创建ZooKeeper客户端"><a href="#创建ZooKeeper客户端" class="headerlink" title="创建ZooKeeper客户端"></a>创建ZooKeeper客户端</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String connectString =</span><br><span class="line"> <span class="string">&quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</span><br><span class="line"><span class="keyword">private</span> ZooKeeper zkClient = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">zkClient = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 收到事件通知后的回调函数（用户的业务逻辑）</span></span><br><span class="line">System.out.println(event.getType() + <span class="string">&quot;--&quot;</span> + event.getPath());</span><br><span class="line"></span><br><span class="line"><span class="comment">// 再次启动监听</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">zkClient.getChildren(<span class="string">&quot;/&quot;</span>, <span class="keyword">true</span>);</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="创建子节点"><a href="#创建子节点" class="headerlink" title="创建子节点"></a>创建子节点</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建子节点</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 参数1：要创建的节点的路径； 参数2：节点数据 ； 参数3：节点权限 ；参数4：节点的类型</span></span><br><span class="line">String nodeCreated = zkClient.create(<span class="string">&quot;/shangbaishuyao&quot;</span>, <span class="string">&quot;jinlian&quot;</span>.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="获取子节点并监听节点变化"><a href="#获取子节点并监听节点变化" class="headerlink" title="获取子节点并监听节点变化"></a>获取子节点并监听节点变化</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取子节点</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getChildren</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">List&lt;String&gt; children = zkClient.getChildren(<span class="string">&quot;/&quot;</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">System.out.println(child);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 延时阻塞</span></span><br><span class="line">Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="判断Znode是否存在"><a href="#判断Znode是否存在" class="headerlink" title="判断Znode是否存在"></a>判断Znode是否存在</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 判断znode是否存在</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">exist</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">Stat stat = zkClient.exists(<span class="string">&quot;/eclipse&quot;</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">System.out.println(stat == <span class="keyword">null</span> ? <span class="string">&quot;not exist&quot;</span> : <span class="string">&quot;exist&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="修改znode数据-获得znode数据-删除znode节点"><a href="#修改znode数据-获得znode数据-删除znode节点" class="headerlink" title="修改znode数据,获得znode数据,删除znode节点"></a>修改znode数据,获得znode数据,删除znode节点</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    获取Znode数据</span></span><br><span class="line"><span class="comment">    修改znode数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testznodeData</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">byte</span>[] getdatas = zkClient.getData(<span class="string">&quot;/shangbaishuyao&quot;</span>,<span class="keyword">false</span> , <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">    Stat setdatas = zkClient.setData(<span class="string">&quot;/shangbaishuyao&quot;</span>,<span class="string">&quot;xww&quot;</span>.getBytes(),<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    System.out.println(<span class="string">&quot;得到shangbaishuyao节点的数据&quot;</span>+<span class="keyword">new</span> String(getdatas));</span><br><span class="line">    System.out.println(<span class="string">&quot;修改shangbaishuyao节点的数据&quot;</span> + setdatas);</span><br><span class="line"></span><br><span class="line">zkClient.delete(<span class="string">&quot;/shangbaishuyao&quot;</span>,<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="监听服务器节点动态上下线案例"><a href="#监听服务器节点动态上下线案例" class="headerlink" title="监听服务器节点动态上下线案例"></a>监听服务器节点动态上下线案例</h2><p>1．需求<br>某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线。<br>2．需求分析，如图所示</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_238.png" alt="blog: www.xubatian.cn"></p><h2 id="案例代码-1"><a href="#案例代码-1" class="headerlink" title="案例代码"></a>案例代码</h2><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/zookeeper/zookeeperCase/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/zookeeper/zookeeperCase/</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;我们经常在说命运，但我觉得，命是自己的，运却和整个国家相关联。历史的洪流滚起，即便微如沙粒，也能奔腾入海，也能汇成史诗。 ——人民日报                                              &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="zookeeper" scheme="http://xubatian.cn/tags/zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>小目标</title>
    <link href="http://xubatian.cn/%E5%8A%A8%E6%80%81/"/>
    <id>http://xubatian.cn/%E5%8A%A8%E6%80%81/</id>
    <published>2022-01-18T05:08:58.000Z</published>
    <updated>2022-01-23T02:59:49.642Z</updated>
    
    <content type="html"><![CDATA[<p>2022年01月18日 博客框架基本搭建完成,后续完善则是在写文章的基础上进行定向更改. 首先立个flag. 争取在01月21日凌晨 将大数据笔记整理好. 不能耽误后续的目标达成, 时间紧任务重.生死看淡,不服就干.  加油!   —— 来自技术不好,却在努力挣扎的菜鸟</p><span id="more"></span>                                                                                                                <p>热爱生活,爱阳光</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_212.png" width = "" height = "" alt="xubatian的博客" align="center" />]]></content>
    
    
    <summary type="html">&lt;p&gt;2022年01月18日 博客框架基本搭建完成,后续完善则是在写文章的基础上进行定向更改. 首先立个flag. 争取在01月21日凌晨 将大数据笔记整理好. 不能耽误后续的目标达成, 时间紧任务重.生死看淡,不服就干.  加油!   —— 来自技术不好,却在努力挣扎的菜鸟&lt;/p&gt;</summary>
    
    
    
    <category term="动态" scheme="http://xubatian.cn/categories/%E5%8A%A8%E6%80%81/"/>
    
    
    <category term="动态" scheme="http://xubatian.cn/tags/%E5%8A%A8%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop生产调优手册</title>
    <link href="http://xubatian.cn/Hadoop%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C/"/>
    <id>http://xubatian.cn/Hadoop%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C/</id>
    <published>2022-01-17T13:57:58.000Z</published>
    <updated>2022-01-23T02:58:21.728Z</updated>
    
    <content type="html"><![CDATA[<p>无论何时，无论遇到何事，都要保持年轻的心态。放下过往陈旧的观念，打破固有的认知经验，去探索新鲜的事物，把更多时间放在修炼自我上。——人民日报                            </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_146.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="HDFS—核心参数"><a href="#HDFS—核心参数" class="headerlink" title="HDFS—核心参数"></a>HDFS—核心参数</h1><h2 id="NameNode内存生产配置"><a href="#NameNode内存生产配置" class="headerlink" title="NameNode内存生产配置"></a>NameNode内存生产配置</h2><p>1）NameNode内存计算<br>    每个文件块大概占用150byte，一台服务器128G内存为例，能存储多少文件块呢？<br>    128 * 1024 * 1024 * 1024  / 150Byte ≈  9.1亿<br>    G     MB    KB     Byte<br>2）Hadoop2.x系列，配置NameNode内存<br>    NameNode内存默认2000m，如果服务器内存4G，NameNode内存可以配置3g。在hadoop-env.sh文件中配置如下。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_NAMENODE_OPTS=-Xmx3072m</span><br></pre></td></tr></table></figure><p>3）Hadoop3.x系列，配置NameNode内存<br>    （1）hadoop-env.sh中描述Hadoop的内存是动态分配的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The maximum amount of heap to use (Java -Xmx).  If no unit</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> is provided, it will be converted to MB.  Daemons will</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> prefer any Xmx setting <span class="keyword">in</span> their respective _OPT variable.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> There is no default; the JVM will autoscale based upon machine</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> memory size.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">export</span> HADOOP_HEAPSIZE_MAX=</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The minimum amount of heap to use (Java -Xms).  If no unit</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> is provided, it will be converted to MB.  Daemons will</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> prefer any Xms setting <span class="keyword">in</span> their respective _OPT variable.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> There is no default; the JVM will autoscale based upon machine</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> memory size.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">export</span> HADOOP_HEAPSIZE_MIN=</span></span><br><span class="line">HADOOP_NAMENODE_OPTS=-Xmx102400m</span><br></pre></td></tr></table></figure><p>   （2）查看NameNode占用内存</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 ~]$ jps</span><br><span class="line">3088 NodeManager</span><br><span class="line">2611 NameNode</span><br><span class="line">3271 JobHistoryServer</span><br><span class="line">2744 DataNode</span><br><span class="line">3579 Jps</span><br><span class="line">[atguigu@hadoop102 ~]$ jmap -heap 2611</span><br><span class="line">Heap Configuration:</span><br><span class="line">   MaxHeapSize              = 1031798784 (984.0MB)</span><br></pre></td></tr></table></figure><p>  （3）查看DataNode占用内存</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 ~]$ jmap -heap 2744</span><br><span class="line">Heap Configuration:</span><br><span class="line">   MaxHeapSize              = 1031798784 (984.0MB)</span><br></pre></td></tr></table></figure><p>查看发现hadoop102上的NameNode和DataNode占用内存都是自动分配的，且相等。不是很合理。<br>经验参考：</p><p><a href="https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_hardware_requirements.html#concept_fzz_dq4_gbb">https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_hardware_requirements.html#concept_fzz_dq4_gbb</a></p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_147.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>具体修改：hadoop-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export HDFS_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=INFO,RFAS -Xmx1024m&quot;</span><br><span class="line"></span><br><span class="line">export HDFS_DATANODE_OPTS=&quot;-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m&quot;</span><br></pre></td></tr></table></figure><h2 id="NameNode心跳并发配置"><a href="#NameNode心跳并发配置" class="headerlink" title="NameNode心跳并发配置"></a>NameNode心跳并发配置</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_148.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>1）hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.</span><br><span class="line">NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。</span><br><span class="line">对于大集群或者有大量客户端的集群来说，通常需要增大该参数。默认值是10。</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;21&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>企业经验：dfs.namenode.handler.count=，比如集群规模（DataNode台数）为3台时，此参数设置为21。可通过简单的python代码计算该值，代码如下。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 ~]$ sudo yum install -y python</span><br><span class="line">[shangbaishuyao@hadoop102 ~]$ python</span><br><span class="line">Python 2.7.5 (default, Apr 11 2018, 07:36:10) </span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux2</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import math</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="built_in">print</span> int(20*math.log(3))</span></span><br><span class="line">21</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; quit()</span></span><br></pre></td></tr></table></figure><h2 id="开启回收站配置"><a href="#开启回收站配置" class="headerlink" title="开启回收站配置"></a>开启回收站配置</h2><p>开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用。</p><p>1）回收站工作机制</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_149.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>2）开启回收站功能参数说明<br>（1）默认值fs.trash.interval = 0，0表示禁用回收站；其他值表示设置文件的存活时间。<br>（2）默认值fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等。<br>（3）要求fs.trash.checkpoint.interval &lt;= fs.trash.interval。<br>3）启用回收站<br>修改core-site.xml，配置垃圾回收时间为1分钟。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>4）查看回收站<br>          回收站目录在HDFS集群中的路径：/user/shangbaishuyao/.Trash/….<br>5）注意：通过网页上直接删除的文件也不会走回收站。<br>6）通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Trash trash = New Trash(conf);</span><br><span class="line">trash.moveToTrash(path);</span><br></pre></td></tr></table></figure><p>7）只有在命令行利用hadoop fs -rm命令删除的文件才会走回收站。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop fs -rm -r /user/shangbaishuyao/input</span><br><span class="line">2021-07-14 16:13:42,643 INFO fs.TrashPolicyDefault: Moved: &#x27;hdfs://hadoop102:9820/user/shangbaishuyao/input&#x27; to trash at: hdfs://hadoop102:9820/user/shangbaishuyao/.Trash/Current/user/shangbaishuyao/input</span><br></pre></td></tr></table></figure><p>8）恢复回收站数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop fs -mv</span><br><span class="line">/user/shangbaishuyao/.Trash/Current/user/shangbaishuyao/input    /user/shangbaishuyao/input</span><br></pre></td></tr></table></figure><h1 id="HDFS—集群压测"><a href="#HDFS—集群压测" class="headerlink" title="HDFS—集群压测"></a>HDFS—集群压测</h1><p>在企业中非常关心每天从Java后台拉取过来的数据，需要多久能上传到集群？消费者关心多久能从HDFS上拉取需要的数据？<br>为了搞清楚HDFS的读写性能，生产环境上非常需要对集群进行压测。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_150.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>HDFS的读写性能主要受网络和磁盘影响比较大。为了方便测试，将hadoop102、hadoop103、hadoop104虚拟机网络都设置为100mbps。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_151.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>100Mbps单位是bit；10M/s单位是byte ; 1byte=8bit，100Mbps/8=12.5M/s。<br>测试网速：来到hadoop102的/opt/module目录，创建一个</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 software]$ python -m SimpleHTTPServer</span><br></pre></td></tr></table></figure><h2 id="测试HDFS写性能"><a href="#测试HDFS写性能" class="headerlink" title="测试HDFS写性能"></a>测试HDFS写性能</h2><p>0）写测试底层原理</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_152.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>1）测试内容：向HDFS集群写10个128M的文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB</span><br><span class="line"></span><br><span class="line">2021-02-09 10:43:16,853 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:             Date &amp; time: Tue Feb 09 10:43:16 CST 2021</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:         Number of files: 10</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:  Total MBytes processed: 1280</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:       Throughput mb/sec: 1.61</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:  Average IO rate mb/sec: 1.9</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:   IO rate std deviation: 0.76</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:      Test exec time sec: 133.05</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:</span><br></pre></td></tr></table></figure><p>注意：nrFiles n为生成mapTask的数量，生产环境一般可通过hadoop103:8088查看CPU核数，设置为（CPU核数 - 1）</p><p>Ø Number of files：生成mapTask数量，一般是集群中（CPU核数-1），我们测试虚拟机就按照实际的物理内存-1分配即可</p><p>Ø Total MBytes processed：单个map处理的文件大小</p><p>Ø Throughput mb/sec:单个mapTak的吞吐量 </p><p>​            计算方式：处理的总文件大小/每一个mapTask写数据的时间累加</p><p>​            集群整体吞吐量：生成mapTask数量*单个mapTak的吞吐量</p><p>Ø Average IO rate mb/sec::平均mapTak的吞吐量</p><p>​            计算方式：每个mapTask处理文件大小/每一个mapTask写数据的时间全部相加除以task数量</p><p>Ø IO rate std deviation:方差、反映各个mapTask处理的差值，越小越均衡</p><p>2）注意：如果测试过程中，出现异常</p><p>（1）可以在yarn-site.xml中设置虚拟内存检测为false</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（2）分发配置并重启Yarn集群</p><p>3）测试结果分析</p><p>​    （1）由于副本1就在本地，所以该副本不参与测试</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_153.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>一共参与测试的文件：10个文件 * 2个副本 = 20个<br>压测后的速度：1.61<br>实测速度：1.61M/s * 20个文件 ≈ 32M/s<br>三台服务器的带宽：12.5 + 12.5 + 12.5 ≈ 30m/s<br>所有网络资源都已经用满。<br><strong>如果实测速度远远小于网络，并且实测速度不能满足工作需求，可以考虑采用固态硬盘或者增加磁盘个数。</strong></p><p>（2）如果客户端不在集群节点，那就三个副本都参与计算</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_154.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><h2 id="测试HDFS读性能"><a href="#测试HDFS读性能" class="headerlink" title="测试HDFS读性能"></a>测试HDFS读性能</h2><p>1）测试内容：读取HDFS集群10个128M的文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB</span><br><span class="line"></span><br><span class="line">2021-02-09 11:34:15,847 INFO fs.TestDFSIO: ----- TestDFSIO ----- : read</span><br><span class="line">2021-02-09 11:34:15,847 INFO fs.TestDFSIO:             Date &amp; time: Tue Feb 09 11:34:15 CST 2021</span><br><span class="line">2021-02-09 11:34:15,847 INFO fs.TestDFSIO:         Number of files: 10</span><br><span class="line">2021-02-09 11:34:15,847 INFO fs.TestDFSIO:  Total MBytes processed: 1280</span><br><span class="line">2021-02-09 11:34:15,848 INFO fs.TestDFSIO:       Throughput mb/sec: 200.28</span><br><span class="line">2021-02-09 11:34:15,848 INFO fs.TestDFSIO:  Average IO rate mb/sec: 266.74</span><br><span class="line">2021-02-09 11:34:15,848 INFO fs.TestDFSIO:   IO rate std deviation: 143.12</span><br><span class="line">2021-02-09 11:34:15,848 INFO fs.TestDFSIO:      Test exec time sec: 20.83</span><br></pre></td></tr></table></figure><p>2）删除测试生成数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -clean</span><br></pre></td></tr></table></figure><p>3）测试结果分析：为什么读取文件速度大于网络带宽？由于目前只有三台服务器，且有三个副本，数据读取就近原则，相当于都是读取的本地磁盘数据，没有走网络。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_155.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><h1 id="HDFS—多目录"><a href="#HDFS—多目录" class="headerlink" title="HDFS—多目录"></a>HDFS—多目录</h1><h2 id="NameNode多目录配置"><a href="#NameNode多目录配置" class="headerlink" title="NameNode多目录配置"></a>NameNode多目录配置</h2><p>1）NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_156.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>2）具体配置如下</p><p>（1）在hdfs-site.xml文件中添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;file://$&#123;hadoop.tmp.dir&#125;/dfs/name1,file://$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>注意：因为每台服务器节点的磁盘情况不同，所以这个配置配完之后，可以选择不分发</p><p>（2）停止集群，删除三台节点的data和logs中所有数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ rm -rf data/ logs/</span><br><span class="line">[shangbaishuyao@hadoop103 hadoop-3.1.3]$ rm -rf data/ logs/</span><br><span class="line">[shangbaishuyao@hadoop104 hadoop-3.1.3]$ rm -rf data/ logs/</span><br></pre></td></tr></table></figure><p>（3）格式化集群并启动。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ bin/hdfs namenode -format</span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p>3）查看结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 dfs]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">drwx------. 3 shangbaishuyao shangbaishuyao 4096 12月 11 08:03 data</span><br><span class="line">drwxrwxr-x. 3 shangbaishuyao shangbaishuyao 4096 12月 11 08:03 name1</span><br><span class="line">drwxrwxr-x. 3 shangbaishuyao shangbaishuyao 4096 12月 11 08:03 name2</span><br></pre></td></tr></table></figure><p><strong>检查name1和name2里面的内容，发现一模一样。</strong></p><h2 id="DataNode多目录配置"><a href="#DataNode多目录配置" class="headerlink" title="DataNode多目录配置"></a>DataNode多目录配置</h2><p>1）DataNode可以配置成多个目录，每个目录存储的数据不一样（数据不是副本）</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_157.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>2）具体配置如下</p><p>在hdfs-site.xml文件中添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;file://$&#123;hadoop.tmp.dir&#125;/dfs/data1,file://$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>3）查看结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 dfs]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">drwx------. 3 shangbaishuyao shangbaishuyao 4096 4月   4 14:22 data1</span><br><span class="line">drwx------. 3 shangbaishuyao shangbaishuyao 4096 4月   4 14:22 data2</span><br><span class="line">drwxrwxr-x. 3 shangbaishuyao shangbaishuyao 4096 12月 11 08:03 name1</span><br><span class="line">drwxrwxr-x. 3 shangbaishuyao shangbaishuyao 4096 12月 11 08:03 name2</span><br></pre></td></tr></table></figure><p>4）向集群上传一个文件，再次观察两个文件夹里面的内容发现不一致（一个有数一个没有）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop fs -put wcinput/word.txt /</span><br></pre></td></tr></table></figure><h2 id="集群数据均衡之磁盘间数据均衡"><a href="#集群数据均衡之磁盘间数据均衡" class="headerlink" title="集群数据均衡之磁盘间数据均衡"></a>集群数据均衡之磁盘间数据均衡</h2><p>生产环境，由于硬盘空间不足，往往需要增加一块硬盘。刚加载的硬盘没有数据时，可以执行磁盘数据均衡命令。（Hadoop3.x新特性）</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_158.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>（1）生成均衡计划（我们只有一块磁盘，不会生成计划）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs diskbalancer -plan hadoop103</span><br></pre></td></tr></table></figure><p>（2）执行均衡计划</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs diskbalancer -execute hadoop103.plan.json</span><br></pre></td></tr></table></figure><p>（3）查看当前均衡任务的执行情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs diskbalancer -query hadoop103</span><br></pre></td></tr></table></figure><p>（4）取消均衡任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs diskbalancer -cancel hadoop103.plan.json</span><br></pre></td></tr></table></figure><h1 id="HDFS—集群扩容及缩容"><a href="#HDFS—集群扩容及缩容" class="headerlink" title="HDFS—集群扩容及缩容"></a>HDFS—集群扩容及缩容</h1><h2 id="添加白名单"><a href="#添加白名单" class="headerlink" title="添加白名单"></a>添加白名单</h2><p>白名单：表示在白名单的主机IP地址可以，用来存储数据。<br>企业中：配置白名单，可以尽量防止黑客恶意访问攻击。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_159.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>配置白名单步骤如下：</p><p>1）在NameNode节点的/opt/module/hadoop-3.1.3/etc/hadoop目录下分别创建whitelist 和blacklist文件<br>（1）创建白名单</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ vim whitelist</span><br></pre></td></tr></table></figure><p>在whitelist中添加如下主机名称，假如集群正常工作的节点为102 103 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br></pre></td></tr></table></figure><p>（2）创建黑名单</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ touch blacklist</span><br></pre></td></tr></table></figure><p>​    保持空的就可以<br>2）在hdfs-site.xml配置文件中增加dfs.hosts配置参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 白名单 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.hosts&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/whitelist&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 黑名单 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/blacklist&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>3）分发配置文件whitelist，hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop104 hadoop]$ xsync hdfs-site.xml whitelist</span><br></pre></td></tr></table></figure><p>4）第一次添加白名单必须重启集群，不是第一次，只需要刷新NameNode节点即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ myhadoop.sh stop</span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ myhadoop.sh start</span><br></pre></td></tr></table></figure><p>5）在web浏览器上查看DN，<a href="http://hadoop102:9870/dfshealth.html#tab-datanode">http://hadoop102:9870/dfshealth.html#tab-datanode</a>   注意: hadoop102位服务器IP地址</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_160.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>6）在hadoop104上执行上传数据数据失败</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop104 hadoop-3.1.3]$ hadoop fs -put NOTICE.txt /</span><br></pre></td></tr></table></figure><p>7）二次修改白名单，增加hadoop104</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ vim whitelist</span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p>8）刷新NameNode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs dfsadmin -refreshNodes</span><br><span class="line">Refresh nodes successful</span><br></pre></td></tr></table></figure><p>9）在web浏览器上查看DN，<a href="http://hadoop102:9870/dfshealth.html#tab-datanode">http://hadoop102:9870/dfshealth.html#tab-datanode</a></p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_161.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><h2 id="服役新服务器"><a href="#服役新服务器" class="headerlink" title="服役新服务器"></a>服役新服务器</h2><p>1）需求<br>随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点。<br>2）环境准备<br>（1）在hadoop100主机上再克隆一台hadoop105主机<br>（2）修改IP地址和主机名称</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop105 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br><span class="line">[root@hadoop105 ~]# vim /etc/hostname</span><br></pre></td></tr></table></figure><p>（3）拷贝hadoop102的/opt/module目录和/etc/profile.d/my_env.sh到hadoop105</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 opt]$ scp -r module/* shangbaishuyao@hadoop105:/opt/module/</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@hadoop102 opt]$ sudo scp /etc/profile.d/my_env.sh root@hadoop105:/etc/profile.d/my_env.sh</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ source /etc/profile</span><br></pre></td></tr></table></figure><p>（4）删除hadoop105上Hadoop的历史数据，data和log数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ rm -rf data/ logs/</span><br></pre></td></tr></table></figure><p>（5）配置hadoop102和hadoop103到hadoop105的ssh无密登录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 .ssh]$ ssh-copy-id hadoop105</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@hadoop103 .ssh]$ ssh-copy-id hadoop105</span><br></pre></td></tr></table></figure><p>3）服役新节点具体步骤<br>（1）直接启动DataNode，即可关联到集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ hdfs --daemon start datanode</span><br><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ yarn --daemon start nodemanager</span><br></pre></td></tr></table></figure><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_162.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>4）在白名单中增加新服役的服务器<br>（1）在白名单whitelist中增加hadoop104、hadoop105，并重启集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ vim whitelist</span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br><span class="line">hadoop105</span><br></pre></td></tr></table></figure><p>（2）分发</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ xsync whitelist</span><br></pre></td></tr></table></figure><p>（3）刷新NameNode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs dfsadmin -refreshNodes</span><br><span class="line">Refresh nodes successful</span><br></pre></td></tr></table></figure><p>5）在hadoop105上上传文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ hadoop fs -put /opt/module/hadoop-3.1.3/LICENSE.txt /</span><br></pre></td></tr></table></figure><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_163.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p><strong>思考：如果数据不均衡（hadoop105数据少，其他节点数据多），怎么处理？</strong></p><h2 id="服务器间数据均衡"><a href="#服务器间数据均衡" class="headerlink" title="服务器间数据均衡"></a>服务器间数据均衡</h2><p>1）企业经验：<br>在企业开发中，如果经常在hadoop102和hadoop104上提交任务，且副本数为2，由于数据本地性原则，就会导致hadoop102和hadoop104数据过多，hadoop103存储的数据量小。<br>另一种情况，就是新服役的服务器数据量比较少，需要执行集群均衡命令。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_164.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>2）开启数据均衡命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ sbin/start-balancer.sh -threshold 10</span><br></pre></td></tr></table></figure><p>对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%，可根据实际情况进行调整。<br>3）停止数据均衡命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ sbin/stop-balancer.sh</span><br></pre></td></tr></table></figure><p>注意：由于HDFS需要启动单独的Rebalance Server来执行Rebalance操作，所以尽量不要在NameNode上执行start-balancer.sh，而是找一台比较空闲的机器。</p><h2 id="黑名单退役服务器"><a href="#黑名单退役服务器" class="headerlink" title="黑名单退役服务器"></a>黑名单退役服务器</h2><p>黑名单：表示在黑名单的主机IP地址不可以，用来存储数据。<br>企业中：配置黑名单，用来退役服务器。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_165.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>黑名单配置步骤如下：<br>1）编辑/opt/module/hadoop-3.1.3/etc/hadoop目录下的blacklist文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop] vim blacklist</span><br></pre></td></tr></table></figure><p>添加如下主机名称（要退役的节点）<br>hadoop105<br>注意：如果白名单中没有配置，需要在hdfs-site.xml配置文件中增加dfs.hosts配置参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 黑名单 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/blacklist&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>2）分发配置文件blacklist，hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop104 hadoop]$ xsync hdfs-site.xml blacklist</span><br></pre></td></tr></table></figure><p>3）第一次添加黑名单必须重启集群，不是第一次，只需要刷新NameNode节点即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs dfsadmin -refreshNodes</span><br><span class="line">Refresh nodes successful</span><br></pre></td></tr></table></figure><p>4）检查Web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_166.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>5）等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_167.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ hdfs --daemon stop datanode</span><br></pre></td></tr></table></figure><p>stopping datanode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ yarn --daemon stop nodemanager</span><br></pre></td></tr></table></figure><p>stopping nodemanager</p><p>6）如果数据不均衡，可以用命令实现集群的再平衡</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ sbin/start-balancer.sh -threshold 10</span><br></pre></td></tr></table></figure><h1 id="HDFS—存储优化"><a href="#HDFS—存储优化" class="headerlink" title="HDFS—存储优化"></a>HDFS—存储优化</h1><p><strong>注：演示纠删码和异构存储需要一共5台虚拟机。尽量拿另外一套集群。提前准备5台服务器的集群。</strong></p><h2 id="纠删码"><a href="#纠删码" class="headerlink" title="纠删码"></a>纠删码</h2><h3 id="纠删码原理"><a href="#纠删码原理" class="headerlink" title="纠删码原理"></a>纠删码原理</h3><p>HDFS默认情况下，一个文件有3个副本，这样提高了数据的可靠性，但也带来了2倍的冗余开销。Hadoop3.x引入了纠删码，采用计算的方式，可以节省约50％左右的存储空间。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_168.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>1）纠删码操作相关的命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs ec</span><br><span class="line">Usage: bin/hdfs ec [COMMAND]</span><br><span class="line">          [-listPolicies]</span><br><span class="line">          [-addPolicies -policyFile &lt;file&gt;]</span><br><span class="line">          [-getPolicy -path &lt;path&gt;]</span><br><span class="line">          [-removePolicy -policy &lt;policy&gt;]</span><br><span class="line">          [-setPolicy -path &lt;path&gt; [-policy &lt;policy&gt;] [-replicate]]</span><br><span class="line">          [-unsetPolicy -path &lt;path&gt;]</span><br><span class="line">          [-listCodecs]</span><br><span class="line">          [-enablePolicy -policy &lt;policy&gt;]</span><br><span class="line">          [-disablePolicy -policy &lt;policy&gt;]</span><br><span class="line">          [-help &lt;command-name&gt;].</span><br></pre></td></tr></table></figure><p>2）查看当前支持的纠删码策略</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3] hdfs ec -listPolicies</span><br><span class="line"></span><br><span class="line">Erasure Coding Policies:</span><br><span class="line">ErasureCodingPolicy=[Name=RS-10-4-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=10, numParityUnits=4]], CellSize=1048576, Id=5], State=DISABLED</span><br><span class="line"></span><br><span class="line">ErasureCodingPolicy=[Name=RS-3-2-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=3, numParityUnits=2]], CellSize=1048576, Id=2], State=DISABLED</span><br><span class="line"></span><br><span class="line">ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1], State=ENABLED</span><br><span class="line"></span><br><span class="line">ErasureCodingPolicy=[Name=RS-LEGACY-6-3-1024k, Schema=[ECSchema=[Codec=rs-legacy, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=3], State=DISABLED</span><br><span class="line"></span><br><span class="line">ErasureCodingPolicy=[Name=XOR-2-1-1024k, Schema=[ECSchema=[Codec=xor, numDataUnits=2, numParityUnits=1]], CellSize=1048576, Id=4], State=DISABLED</span><br></pre></td></tr></table></figure><p>3）纠删码策略解释:<br>RS-3-2-1024k：使用RS编码，每3个数据单元，生成2个校验单元，共5个单元，也就是说：这5个单元中，只要有任意的3个单元存在（不管是数据单元还是校验单元，只要总数=3），就可以得到原始数据。每个单元的大小是1024k=1024*1024=1048576。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_169.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p><strong>RS-10-4-1024k</strong>：使用RS编码，每10个数据单元（cell），生成4个校验单元，共14个单元，也就是说：这14个单元中，只要有任意的10个单元存在（不管是数据单元还是校验单元，只要总数=10），就可以得到原始数据。每个单元的大小是1024k=1024<em>1024=1048576。</em></p><p><strong>RS-6-3-1024k</strong>：使用RS编码，每6个数据单元，生成3个校验单元，共9个单元，也就是说：这9个单元中，只要有任意的6个单元存在（不管是数据单元还是校验单元，只要总数=6），就可以得到原始数据。每个单元的大小是1024k=1024*1024=1048576。</p><p><strong>RS-LEGACY-6-3-1024k</strong>：策略和上面的RS-6-3-1024k一样，只是编码的算法用的是rs-legacy。 </p><p><strong>XOR-2-1-1024k</strong>：使用XOR编码（速度比RS编码快），每2个数据单元，生成1个校验单元，共3个单元，也就是说：这3个单元中，只要有任意的2个单元存在（不管是数据单元还是校验单元，只要总数= 2），就可以得到原始数据。每个单元的大小是1024k=1024*1024=1048576。</p><h3 id="纠删码案例实操"><a href="#纠删码案例实操" class="headerlink" title="纠删码案例实操"></a>纠删码案例实操</h3><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_170.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>纠删码策略是给具体一个路径设置。所有往此路径下存储的文件，都会执行此策略。<br>默认只开启对RS-6-3-1024k策略的支持，如要使用别的策略需要提前启用。</p><p>1）需求：将/input目录设置为RS-3-2-1024k策略<br>2）具体步骤<br>（1）开启对RS-3-2-1024k策略的支持</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$  hdfs ec -enablePolicy  -policy RS-3-2-1024k</span><br><span class="line">Erasure coding policy RS-3-2-1024k is enabled</span><br></pre></td></tr></table></figure><p>（2）在HDFS创建目录，并设置RS-3-2-1024k策略</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102  hadoop-3.1.3]$  hdfs dfs -mkdir /input</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@hadoop202 hadoop-3.1.3]$ hdfs ec -setPolicy -path /input -policy RS-3-2-1024k</span><br></pre></td></tr></table></figure><p>（3）上传文件，并查看文件编码后的存储情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs dfs -put web.log /input</span><br></pre></td></tr></table></figure><p>注：你所上传的文件需要大于2M才能看出效果。（低于2M，只有一个数据单元和两个校验单元）<br>（4）查看存储路径的数据单元和校验单元，并作破坏实验</p><h2 id="异构存储（冷热数据分离）"><a href="#异构存储（冷热数据分离）" class="headerlink" title="异构存储（冷热数据分离）"></a>异构存储（冷热数据分离）</h2><p>异构存储主要解决，不同的数据，存储在不同类型的硬盘中，达到最佳性能的问题。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_171.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_172.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><h3 id="异构存储Shell操作"><a href="#异构存储Shell操作" class="headerlink" title="异构存储Shell操作"></a>异构存储Shell操作</h3><p>（1）查看当前有哪些存储策略可以用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -listPolicies</span><br></pre></td></tr></table></figure><p>（2）为指定路径（数据存储目录）设置指定的存储策略</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs storagepolicies -setStoragePolicy -path xxx -policy xxx</span><br></pre></td></tr></table></figure><p>（3）获取指定路径（数据存储目录或文件）的存储策略</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs storagepolicies -getStoragePolicy -path xxx</span><br></pre></td></tr></table></figure><p>（4）取消存储策略；执行改命令之后该目录或者文件，以其上级的目录为准，如果是根目录，那么就是HOT</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs storagepolicies -unsetStoragePolicy -path xxx</span><br></pre></td></tr></table></figure><p>（5）查看文件块的分布</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs fsck xxx -files -blocks -locations</span><br></pre></td></tr></table></figure><p>（6）查看集群节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop dfsadmin -report</span><br></pre></td></tr></table></figure><h3 id="测试环境准备"><a href="#测试环境准备" class="headerlink" title="测试环境准备"></a>测试环境准备</h3><p>1）测试环境描述<br>服务器规模：5台<br>集群配置：副本数为2，创建好带有存储类型的目录（提前创建）<br>集群规划：</p><table><thead><tr><th>节点</th><th>存储类型分配</th></tr></thead><tbody><tr><td>hadoop102</td><td>RAM_DISK，SSD</td></tr><tr><td>hadoop103</td><td>SSD，DISK</td></tr><tr><td>hadoop104</td><td>DISK，RAM_DISK</td></tr><tr><td>hadoop105</td><td>ARCHIVE</td></tr><tr><td>hadoop106</td><td>ARCHIVE</td></tr></tbody></table><p>2）配置文件信息<br>（1）为hadoop102节点的hdfs-site.xml添加如下信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.storage.policy.enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; </span><br><span class="line">&lt;value&gt;[SSD]file:///opt/module/hadoop-3.1.3/hdfsdata/ssd,[RAM_DISK]file:///opt/module/hadoop-3.1.3/hdfsdata/ram_disk&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（2）为hadoop103节点的hdfs-site.xml添加如下信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.storage.policy.enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;[SSD]file:///opt/module/hadoop-3.1.3/hdfsdata/ssd,[DISK]file:///opt/module/hadoop-3.1.3/hdfsdata/disk&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（3）为hadoop104节点的hdfs-site.xml添加如下信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.storage.policy.enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;[RAM_DISK]file:///opt/module/hdfsdata/ram_disk,[DISK]file:///opt/module/hadoop-3.1.3/hdfsdata/disk&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（4）为hadoop105节点的hdfs-site.xml添加如下信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.storage.policy.enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;[ARCHIVE]file:///opt/module/hadoop-3.1.3/hdfsdata/archive&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（5）为hadoop106节点的hdfs-site.xml添加如下信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.storage.policy.enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;[ARCHIVE]file:///opt/module/hadoop-3.1.3/hdfsdata/archive&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>3）数据准备<br>（1）启动集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs namenode -format</span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ myhadoop.sh start</span><br></pre></td></tr></table></figure><p>（1）并在HDFS上创建文件目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop fs -mkdir /hdfsdata</span><br></pre></td></tr></table></figure><p>（2）并将文件资料上传</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop fs -put /opt/module/hadoop-3.1.3/NOTICE.txt /hdfsdata</span><br></pre></td></tr></table></figure><h3 id="HOT存储策略案例"><a href="#HOT存储策略案例" class="headerlink" title="HOT存储策略案例"></a>HOT存储策略案例</h3><p>（1）最开始我们未设置存储策略的情况下，我们获取该目录的存储策略</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -getStoragePolicy -path /hdfsdata</span><br></pre></td></tr></table></figure><p>（2）我们查看上传的文件块分布</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs fsck /hdfsdata -files -blocks -locations</span><br><span class="line"></span><br><span class="line">[DatanodeInfoWithStorage[192.168.10.104:9866,DS-0b133854-7f9e-48df-939b-5ca6482c5afb,DISK], DatanodeInfoWithStorage[192.168.10.103:9866,DS-ca1bd3b9-d9a5-4101-9f92-3da5f1baa28b,DISK]]</span><br></pre></td></tr></table></figure><p>未设置存储策略，所有文件块都存储在DISK下。所以，默认存储策略为HOT。</p><h3 id="WARM存储策略测试"><a href="#WARM存储策略测试" class="headerlink" title="WARM存储策略测试"></a>WARM存储策略测试</h3><p>（1）接下来我们为数据降温</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy WARM</span><br></pre></td></tr></table></figure><p>（2）再次查看文件块分布，我们可以看到文件块依然放在原处。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs fsck /hdfsdata -files -blocks -locations</span><br></pre></td></tr></table></figure><p>（3）我们需要让他HDFS按照存储策略自行移动文件块</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs mover /hdfsdata</span><br></pre></td></tr></table></figure><p>（4）再次查看文件块分布，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs fsck /hdfsdata -files -blocks -locations</span><br><span class="line"></span><br><span class="line">[DatanodeInfoWithStorage[192.168.10.105:9866,DS-d46d08e1-80c6-4fca-b0a2-4a3dd7ec7459,ARCHIVE], DatanodeInfoWithStorage[192.168.10.103:9866,DS-ca1bd3b9-d9a5-4101-9f92-3da5f1baa28b,DISK]]</span><br></pre></td></tr></table></figure><p>文件块一半在DISK，一半在ARCHIVE，符合我们设置的WARM策略</p><h3 id="COLD策略测试"><a href="#COLD策略测试" class="headerlink" title="COLD策略测试"></a>COLD策略测试</h3><p>（1）我们继续将数据降温为cold</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy COLD</span><br></pre></td></tr></table></figure><p>注意：当我们将目录设置为COLD并且我们未配置ARCHIVE存储目录的情况下，不可以向该目录直接上传文件，会报出异常。<br>（2）手动转移</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs mover /hdfsdata</span><br></pre></td></tr></table></figure><p>（3）检查文件块的分布</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ bin/hdfs fsck /hdfsdata -files -blocks -locations</span><br><span class="line"></span><br><span class="line">[DatanodeInfoWithStorage[192.168.10.105:9866,DS-d46d08e1-80c6-4fca-b0a2-4a3dd7ec7459,ARCHIVE], DatanodeInfoWithStorage[192.168.10.106:9866,DS-827b3f8b-84d7-47c6-8a14-0166096f919d,ARCHIVE]]</span><br></pre></td></tr></table></figure><p>所有文件块都在ARCHIVE，符合COLD存储策略。</p><h3 id="ONE-SSD策略测试"><a href="#ONE-SSD策略测试" class="headerlink" title="ONE_SSD策略测试"></a>ONE_SSD策略测试</h3><p>（1）接下来我们将存储策略从默认的HOT更改为One_SSD</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy One_SSD</span><br></pre></td></tr></table></figure><p>（2）手动转移文件块</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs mover /hdfsdata</span><br></pre></td></tr></table></figure><p>（3）转移完成后，我们查看文件块分布，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ bin/hdfs fsck /hdfsdata -files -blocks -locations</span><br><span class="line"></span><br><span class="line">[DatanodeInfoWithStorage[192.168.10.104:9866,DS-0b133854-7f9e-48df-939b-5ca6482c5afb,DISK], DatanodeInfoWithStorage[192.168.10.103:9866,DS-2481a204-59dd-46c0-9f87-ec4647ad429a,SSD]]</span><br></pre></td></tr></table></figure><p>文件块分布为一半在SSD，一半在DISK，符合One_SSD存储策略。</p><h3 id="ALL-SSD策略测试"><a href="#ALL-SSD策略测试" class="headerlink" title="ALL_SSD策略测试"></a>ALL_SSD策略测试</h3><p>（1）接下来，我们再将存储策略更改为All_SSD</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy All_SSD</span><br></pre></td></tr></table></figure><p>（2）手动转移文件块</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs mover /hdfsdata</span><br></pre></td></tr></table></figure><p>（3）查看文件块分布，我们可以看到，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ bin/hdfs fsck /hdfsdata -files -blocks -locations</span><br><span class="line"></span><br><span class="line">[DatanodeInfoWithStorage[192.168.10.102:9866,DS-c997cfb4-16dc-4e69-a0c4-9411a1b0c1eb,SSD], DatanodeInfoWithStorage[192.168.10.103:9866,DS-2481a204-59dd-46c0-9f87-ec4647ad429a,SSD]]</span><br></pre></td></tr></table></figure><p>所有的文件块都存储在SSD，符合All_SSD存储策略。</p><h3 id="LAZY-PERSIST策略测试"><a href="#LAZY-PERSIST策略测试" class="headerlink" title="LAZY_PERSIST策略测试"></a>LAZY_PERSIST策略测试</h3><p>（1）继续改变策略，将存储策略改为lazy_persist</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy lazy_persist</span><br></pre></td></tr></table></figure><p>（2）手动转移文件块</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs mover /hdfsdata</span><br></pre></td></tr></table></figure><p>（3）查看文件块分布</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs fsck /hdfsdata -files -blocks -locations</span><br><span class="line"></span><br><span class="line">[DatanodeInfoWithStorage[192.168.10.104:9866,DS-0b133854-7f9e-48df-939b-5ca6482c5afb,DISK], DatanodeInfoWithStorage[192.168.10.103:9866,DS-ca1bd3b9-d9a5-4101-9f92-3da5f1baa28b,DISK]]</span><br></pre></td></tr></table></figure><p>这里我们发现所有的文件块都是存储在DISK，按照理论一个副本存储在RAM_DISK，其他副本存储在DISK中，这是因为，我们还需要配置“dfs.datanode.max.locked.memory”，“dfs.block.size”参数。<br>那么出现存储策略为LAZY_PERSIST时，文件块副本都存储在DISK上的原因有如下两点：<br>（1）当客户端所在的DataNode节点没有RAM_DISK时，则会写入客户端所在的DataNode节点的DISK磁盘，其余副本会写入其他节点的DISK磁盘。<br>（2）当客户端所在的DataNode有RAM_DISK，但“dfs.datanode.max.locked.memory”参数值未设置或者设置过小（小于“dfs.block.size”参数值）时，则会写入客户端所在的DataNode节点的DISK磁盘，其余副本会写入其他节点的DISK磁盘。<br>但是由于虚拟机的“max locked memory”为64KB，所以，如果参数配置过大，还会报出错误：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain</span><br><span class="line">java.lang.RuntimeException: Cannot start datanode because the configured max locked memory size (dfs.datanode.max.locked.memory) of 209715200 bytes is more than the datanode&#x27;s available RLIMIT_MEMLOCK ulimit of 65536 bytes.</span><br></pre></td></tr></table></figure><p>我们可以通过该命令查询此参数的内存</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ ulimit -a</span><br><span class="line"></span><br><span class="line">max locked memory       (kbytes, -l) 64</span><br></pre></td></tr></table></figure><h1 id="HDFS—故障排除"><a href="#HDFS—故障排除" class="headerlink" title="HDFS—故障排除"></a>HDFS—故障排除</h1><p>注意：采用三台服务器即可，恢复到Yarn开始的服务器快照。</p><h2 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_173.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>1）需求：<br>NameNode进程挂了并且存储的数据也丢失了，如何恢复NameNode<br>2）故障模拟<br>（1）kill -9 NameNode进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 current]$ kill -9 19886</span><br></pre></td></tr></table></figure><p>（2）删除NameNode存储的数据（/opt/module/hadoop-3.1.3/data/tmp/dfs/name）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ rm -rf /opt/module/hadoop-3.1.3/data/dfs/name/*</span><br></pre></td></tr></table></figure><p>3）问题解决<br>（1）拷贝SecondaryNameNode中数据到原NameNode存储数据目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 dfs]$ scp -r shangbaishuyao@hadoop104:/opt/module/hadoop-3.1.3/data/dfs/namesecondary/* ./name/</span><br></pre></td></tr></table></figure><p>（2）重新启动NameNode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs --daemon start namenode</span><br></pre></td></tr></table></figure><p>（3）向集群上传一个文件</p><h2 id="集群安全模式-amp-磁盘修复"><a href="#集群安全模式-amp-磁盘修复" class="headerlink" title="集群安全模式&amp;磁盘修复"></a>集群安全模式&amp;磁盘修复</h2><p>1）安全模式：文件系统只接受读数据请求，而不接受删除、修改等变更请求<br>2）进入安全模式场景</p><p>Ø NameNode在加载镜像文件和编辑日志期间处于安全模式；</p><p>Ø NameNode再接收DataNode注册时，处于安全模式</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_174.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>3）退出安全模式条件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dfs.namenode.safemode.min.datanodes:最小可用datanode数量，默认0</span><br><span class="line">dfs.namenode.safemode.threshold-pct:副本数达到最小要求的block占系统总block数的百分比，默认0.999f。（只允许丢一个块）</span><br><span class="line">dfs.namenode.safemode.extension:稳定时间，默认值30000毫秒，即30秒</span><br></pre></td></tr></table></figure><p>4）基本语法<br>集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（1）bin/hdfs dfsadmin -safemode get（功能描述：查看安全模式状态）</span><br><span class="line">（2）bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态）</span><br><span class="line">（3）bin/hdfs dfsadmin -safemode leave（功能描述：离开安全模式状态）</span><br><span class="line">（4）bin/hdfs dfsadmin -safemode wait（功能描述：等待安全模式状态）</span><br></pre></td></tr></table></figure><p>5）案例1：启动集群进入安全模式<br>    （1）重新启动集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 subdir0]$ myhadoop.sh stop</span><br><span class="line">[shangbaishuyao@hadoop102 subdir0]$ myhadoop.sh start</span><br></pre></td></tr></table></figure><p>​    （2）集群启动后，立即来到集群上删除数据，提示集群处于安全模式</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_176.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>案例2：磁盘修复<br>    需求：数据块损坏，进入安全模式，如何处理<br>    （1）分别进入hadoop102、hadoop103、hadoop104的/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1015489500-192.168.10.102-1611909480872/current/finalized/subdir0/subdir0目录，统一删除某2个块信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 subdir0]$ pwd</span><br><span class="line">/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1015489500-192.168.10.102-1611909480872/current/finalized/subdir0/subdir0</span><br><span class="line">[shangbaishuyao@hadoop102 subdir0]$ rm -rf blk_1073741847 blk_1073741847_1023.meta</span><br><span class="line">[shangbaishuyao@hadoop102 subdir0]$ rm -rf blk_1073741865 blk_1073741865_1042.meta</span><br></pre></td></tr></table></figure><p>说明：hadoop103/hadoop104重复执行以上命令<br>    （2）重新启动集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 subdir0]$ myhadoop.sh stop</span><br><span class="line">[shangbaishuyao@hadoop102 subdir0]$ myhadoop.sh start</span><br></pre></td></tr></table></figure><p>​    （3）观察<a href="http://hadoop102:9870/dfshealth.html#tab-overview">http://hadoop102:9870/dfshealth.html#tab-overview</a></p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_177.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>说明：安全模式已经打开，块的数量没有达到要求。<br>    （4）离开安全模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 subdir0]$ hdfs dfsadmin -safemode get</span><br><span class="line">Safe mode is ON</span><br><span class="line">[shangbaishuyao@hadoop102 subdir0]$ hdfs dfsadmin -safemode leave</span><br><span class="line">Safe mode is OFF</span><br></pre></td></tr></table></figure><p>​    （5）观察<a href="http://hadoop102:9870/dfshealth.html#tab-overview">http://hadoop102:9870/dfshealth.html#tab-overview</a></p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_178.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>​    （6）将元数据删除</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_179.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>（7）观察<a href="http://hadoop102:9870/dfshealth.html#tab-overview%EF%BC%8C%E9%9B%86%E7%BE%A4%E5%B7%B2%E7%BB%8F%E6%AD%A3%E5%B8%B8">http://hadoop102:9870/dfshealth.html#tab-overview，集群已经正常</a></p><p>案例3：<br>    需求：模拟等待安全模式<br>（1）查看当前模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs dfsadmin -safemode get</span><br><span class="line">Safe mode is OFF</span><br></pre></td></tr></table></figure><p>（2）先进入安全模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ bin/hdfs dfsadmin -safemode enter</span><br></pre></td></tr></table></figure><p>（3）创建并执行下面的脚本<br>在/opt/module/hadoop-3.1.3路径上，编辑一个脚本safemode.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ vim safemode.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">hdfs dfsadmin -safemode wait</span><br><span class="line">hdfs dfs -put /opt/module/hadoop-3.1.3/README.txt /</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ chmod 777 safemode.sh</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ ./safemode.sh </span><br></pre></td></tr></table></figure><p>（4）再打开一个窗口，执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ bin/hdfs dfsadmin -safemode leave</span><br></pre></td></tr></table></figure><p>（5）再观察上一个窗口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Safe mode is OFF</span><br></pre></td></tr></table></figure><p>（6）HDFS集群上已经有上传的数据了</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_180.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><h2 id="慢磁盘监控"><a href="#慢磁盘监控" class="headerlink" title="慢磁盘监控"></a>慢磁盘监控</h2><p>“慢磁盘”指的时写入数据非常慢的一类磁盘。其实慢性磁盘并不少见，当机器运行时间长了，上面跑的任务多了，磁盘的读写性能自然会退化，严重时就会出现写入数据延时的问题。<br>如何发现慢磁盘？<br>正常在HDFS上创建一个目录，只需要不到1s的时间。如果你发现创建目录超过1分钟及以上，而且这个现象并不是每次都有。只是偶尔慢了一下，就很有可能存在慢磁盘。<br>可以采用如下方法找出是哪块磁盘慢：<br><strong>1）通过心跳未联系时间。</strong><br>一般出现慢磁盘现象，会影响到DataNode与NameNode之间的心跳。正常情况心跳时间间隔是3s。超过3s说明有异常。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_181.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p><strong>2）fio命令，测试磁盘的读写性能</strong><br>（1）顺序读测试</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 ~]# sudo yum install -y fio</span><br><span class="line">[shangbaishuyao@hadoop102 ~]# sudo fio -filename=/home/shangbaishuyao/test.log -direct=1 -iodepth 1 -thread -rw=read -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_r</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: bw=360MiB/s (378MB/s), 360MiB/s-360MiB/s (378MB/s-378MB/s), io=20.0GiB (21.5GB), run=56885-56885msec</span><br></pre></td></tr></table></figure><p>​          结果显示，磁盘的总体顺序读速度为360MiB/s。<br>（2）顺序写测试</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 ~]# sudo fio -filename=/home/shangbaishuyao/test.log -direct=1 -iodepth 1 -thread -rw=write -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: bw=341MiB/s (357MB/s), 341MiB/s-341MiB/s (357MB/s-357MB/s), io=19.0GiB (21.4GB), run=60001-60001msec</span><br></pre></td></tr></table></figure><p>​          结果显示，磁盘的总体顺序写速度为341MiB/s。<br>（3）随机写测试</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 ~]# sudo fio -filename=/home/shangbaishuyao/test.log -direct=1 -iodepth 1 -thread -rw=randwrite -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_randw</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: bw=309MiB/s (324MB/s), 309MiB/s-309MiB/s (324MB/s-324MB/s), io=18.1GiB (19.4GB), run=60001-60001msec</span><br></pre></td></tr></table></figure><p>​           结果显示，磁盘的总体随机写速度为309MiB/s。<br>（4）混合随机读写：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 ~]# sudo fio -filename=/home/shangbaishuyao/test.log -direct=1 -iodepth 1 -thread -rw=randrw -rwmixread=70 -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_r_w -ioscheduler=noop</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: bw=220MiB/s (231MB/s), 220MiB/s-220MiB/s (231MB/s-231MB/s), io=12.9GiB (13.9GB), run=60001-60001msec</span><br><span class="line">  WRITE: bw=94.6MiB/s (99.2MB/s), 94.6MiB/s-94.6MiB/s (99.2MB/s-99.2MB/s), io=5674MiB (5950MB), run=60001-60001msec</span><br></pre></td></tr></table></figure><p>结果显示，磁盘的总体混合随机读写，读速度为220MiB/s，写速度94.6MiB/s。</p><h2 id="小文件归档"><a href="#小文件归档" class="headerlink" title="小文件归档"></a>小文件归档</h2><p>1）HDFS存储小文件弊端</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_182.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB。</p><p>2）解决存储小文件办法之一<br>HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_183.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>3）案例实操<br>（1）需要启动YARN进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ start-yarn.sh</span><br></pre></td></tr></table></figure><p>（2）归档文件<br>    把/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/output路径下。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop archive -archiveName input.har -p  /input   /output</span><br></pre></td></tr></table></figure><p>（3）查看归档</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop fs -ls /output/input.har</span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop fs -ls har:///output/input.har</span><br></pre></td></tr></table></figure><p>（4）解归档文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop fs -cp har:///output/input.har/*    /</span><br></pre></td></tr></table></figure><h1 id="HDFS—集群迁移"><a href="#HDFS—集群迁移" class="headerlink" title="HDFS—集群迁移"></a>HDFS—集群迁移</h1><h2 id="Apache和Apache集群间数据拷贝"><a href="#Apache和Apache集群间数据拷贝" class="headerlink" title="Apache和Apache集群间数据拷贝"></a>Apache和Apache集群间数据拷贝</h2><p>1）scp实现两个远程主机之间的文件复制<br>    scp -r hello.txt root@hadoop103:/user/shangbaishuyao/hello.txt        // 推 push<br>    scp -r root@hadoop103:/user/shangbaishuyao/hello.txt  hello.txt        // 拉 pull<br>    scp -r root@hadoop103:/user/shangbaishuyao/hello.txt root@hadoop104:/user/shangbaishuyao   //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。<br>2）采用distcp命令实现两个Hadoop集群之间的递归数据复制</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$  bin/hadoop distcp hdfs://hadoop102:8020/user/shangbaishuyao/hello.txt hdfs://hadoop105:8020/user/shangbaishuyao/hello.txt</span><br></pre></td></tr></table></figure><h2 id="Apache和CDH集群间数据拷贝"><a href="#Apache和CDH集群间数据拷贝" class="headerlink" title="Apache和CDH集群间数据拷贝"></a>Apache和CDH集群间数据拷贝</h2><p>迁移数据</p><p>1）准备两套集群，我这使用apache集群和CDH集群。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_184.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>2）启动集群</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_185.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_186.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>3）启动完毕后，将apache集群中，hive库里dwd，dws，ads三个库的数据迁移到CDH集群</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_187.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_188.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>4）在apache集群里hosts加上CDH Namenode对应域名并分发给各机器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# vim /etc/hosts</span><br></pre></td></tr></table></figure><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_189.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# scp /etc/hosts hadoop102:/etc/                                                                                              </span><br><span class="line">[root@hadoop101 ~]# scp /etc/hosts hadoop103:/etc/</span><br></pre></td></tr></table></figure><p>5）因为集群都是HA模式，所以需要在apache集群上配置CDH集群,让distcp能识别出CDH的nameservice</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop]# vim /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml </span><br><span class="line">&lt;!--配置nameservice--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;mycluster,nameservice1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--指定本地服务--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.internal.nameservices&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;mycluster,nameservice1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!--配置多NamenNode--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;nn1,nn2,nn3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hadoop101:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hadoop102:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn3&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hadoop103:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!--配置nameservice1的namenode服务--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.namenodes.nameservice1&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;namenode30,namenode37&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.rpc-address.nameservice1.namenode30&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop104:8020&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.rpc-address.nameservice1.namenode37&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop106:8020&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address.nameservice1.namenode30&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop104:9870&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address.nameservice1.namenode37&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop106:9870&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.failover.proxy.provider.nameservice1&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;!--为NamneNode设置HTTP服务监听--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hadoop101:9870&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hadoop102:9870&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.http-address.mycluster.nn3&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hadoop103:9870&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!--配置HDFS客户端联系Active NameNode节点的Java类--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>6）修改CDH hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# vim /etc/hosts</span><br></pre></td></tr></table></figure><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_190.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>7）进行分发，这里的hadoop104，hadoop105，hadoop106分别对应apache的hadoop101，hadoop102，hadoop103</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# scp /etc/hosts hadoop102:/etc/</span><br><span class="line">[root@hadoop101 ~]# scp /etc/hosts hadoop103:/etc/</span><br></pre></td></tr></table></figure><p>8）同样修改CDH集群配置，在所有hdfs-site.xml文件里修改配置</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_191.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">&lt;value&gt;mycluster,nameservice1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.internal.nameservices&lt;/name&gt;</span><br><span class="line">&lt;value&gt;nameservice1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;</span><br><span class="line">&lt;value&gt;nn1,nn2,nn3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop104:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop105:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.rpc-address.mycluster.nn3&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop106:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop104:9870&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop105:9870&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.http-address.mycluster.nn3&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop106:9870&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;</span><br><span class="line">&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>9）最后注意：重点由于我的Apahce集群和CDH集群3台集群都是hadoop101，hadoop102，hadoop103所以要关闭域名访问，使用IP访问<br>CDH把钩去了</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_192.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>10）apache设置为false</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_193.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>11）再使用hadoop distcp命令进行迁移，-Dmapred.job.queue.name指定队列，默认是default队列。上面配置集群都配了的话，那么在CDH和apache集群下都可以执行这个命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop]# hadoop distcp -Dmapred.job.queue.name=hive  webhdfs://mycluster:9070/user/hive/warehouse/dwd.db/  hdfs://nameservice1/user/hive/warehouse</span><br><span class="line"></span><br></pre></td></tr></table></figure><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_194.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>12）会启动一个MR任务，正在迁移</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_195.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>13）查看cdh 9870 http地址</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_196.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_197.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_198.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>14）数据已经成功迁移。数据迁移成功之后，接下来迁移hive表结构，编写shell脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 module]# vim exportHive.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">hive -e &quot;use dwd;show tables&quot;&gt;tables.txt</span><br><span class="line">cat tables.txt |while read eachline</span><br><span class="line">do</span><br><span class="line">hive -e &quot;use dwd;show create table $eachline&quot;&gt;&gt;tablesDDL.txt</span><br><span class="line">echo &quot;;&quot; &gt;&gt; tablesDDL.txt</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>15）执行脚本后将tablesDDL.txt文件分发到CDH集群下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 module]# scp tablesDDL.txt  hadoop104:/opt/module/</span><br></pre></td></tr></table></figure><p>16）然后CDH下导入此表结构，先进到CDH的hive里创建dwd库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 module]# hive</span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create database dwd;</span></span><br></pre></td></tr></table></figure><p>17）创建数据库后，边界tablesDDL.txt在最上方加上use dwd;</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_199.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>18）并且将createtab_stmt都替换成空格</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 module]# sed -i s&quot;#createtab_stmt# #g&quot; tablesDDL.txt</span><br></pre></td></tr></table></figure><p>19）最后执行hive -f命令将表结构导入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 module]# hive -f tablesDDL.txt </span><br></pre></td></tr></table></figure><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_200.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>20）最后将表的分区重新刷新下，只有刷新分区才能把数据读出来，编写脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 module]# vim msckPartition.sh</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">hive -e &quot;use dwd;show tables&quot;&gt;tables.txt</span><br><span class="line">cat tables.txt |while read eachline</span><br><span class="line">do</span><br><span class="line">hive -e &quot;use dwd;MSCK REPAIR TABLE $eachline&quot;</span><br><span class="line">done</span><br><span class="line">[root@hadoop101 module]# chmod +777 msckPartition.sh </span><br><span class="line">[root@hadoop101 module]# ./msckPartition.sh </span><br></pre></td></tr></table></figure><p>21）刷完分区后，查询表数据</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_201.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><h1 id="MapReduce生产经验"><a href="#MapReduce生产经验" class="headerlink" title="MapReduce生产经验"></a>MapReduce生产经验</h1><h2 id="MapReduce跑的慢的原因"><a href="#MapReduce跑的慢的原因" class="headerlink" title="MapReduce跑的慢的原因"></a>MapReduce跑的慢的原因</h2><p>MapReduce程序效率的瓶颈在于两点：</p><p><strong>1）计算机性能</strong><br>CPU、内存、磁盘、网络</p><p><strong>2）I/O操作优化</strong><br>（1）数据倾斜<br>（2）Map运行时间太长，导致Reduce等待过久<br>（3）小文件过多</p><h2 id="MapReduce常用调优参数"><a href="#MapReduce常用调优参数" class="headerlink" title="MapReduce常用调优参数"></a>MapReduce常用调优参数</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_202.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_203.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><h2 id="MapReduce数据倾斜问题"><a href="#MapReduce数据倾斜问题" class="headerlink" title="MapReduce数据倾斜问题"></a>MapReduce数据倾斜问题</h2><p>1）数据倾斜现象<br>数据频率倾斜——某一个区域的数据量要远远大于其他区域。<br>数据大小倾斜——部分记录的大小远远大于平均值。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_204.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>2）减少数据倾斜的方法<br>（1）首先检查是否空值过多造成的数据倾斜<br>          生产环境，可以直接过滤掉空值；如果想保留空值，就自定义分区，将空值加随机数打散。最后再二次聚合。<br>（2）能在map阶段提前处理，最好先在Map阶段处理。如：Combiner、MapJoin<br>（3）设置多个reduce个数</p><h1 id="Hadoop-Yarn生产经验"><a href="#Hadoop-Yarn生产经验" class="headerlink" title="Hadoop-Yarn生产经验"></a>Hadoop-Yarn生产经验</h1><h2 id="常用的调优参数"><a href="#常用的调优参数" class="headerlink" title="常用的调优参数"></a>常用的调优参数</h2><p>1）调优参数列表<br>（1）Resourcemanager相关</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yarn.resourcemanager.scheduler.client.thread-countResourceManager处理调度器请求的线程数量</span><br><span class="line">yarn.resourcemanager.scheduler.class配置调度器</span><br></pre></td></tr></table></figure><p>（2）Nodemanager相关</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">yarn.nodemanager.resource.memory-mb              NodeManager使用内存数</span><br><span class="line">yarn.nodemanager.resource.system-reserved-memory-mb  NodeManager为系统保留多少内存，和上一个参数二者取一即可</span><br><span class="line"></span><br><span class="line">yarn.nodemanager.resource.cpu-vcoresNodeManager使用CPU核数</span><br><span class="line">yarn.nodemanager.resource.count-logical-processors-as-cores是否将虚拟核数当作CPU核数</span><br><span class="line">yarn.nodemanager.resource.pcores-vcores-multiplier虚拟核数和物理核数乘数，例如：4核8线程，该参数就应设为2</span><br><span class="line">yarn.nodemanager.resource.detect-hardware-capabilities是否让yarn自己检测硬件进行配置</span><br><span class="line"></span><br><span class="line">yarn.nodemanager.pmem-check-enabled是否开启物理内存检查限制container</span><br><span class="line">yarn.nodemanager.vmem-check-enabled是否开启虚拟内存检查限制container</span><br><span class="line">yarn.nodemanager.vmem-pmem-ratio        虚拟内存物理内存比例</span><br></pre></td></tr></table></figure><p>（3）Container容器相关<br>yarn.scheduler.minimum-allocation-mb         容器最小内存</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yarn.scheduler.maximum-allocation-mb     容器最大内存</span><br><span class="line">yarn.scheduler.minimum-allocation-vcores 容器最小核数</span><br><span class="line">yarn.scheduler.maximum-allocation-vcores 容器最大核数</span><br></pre></td></tr></table></figure><h1 id="Hadoop综合调优"><a href="#Hadoop综合调优" class="headerlink" title="Hadoop综合调优"></a>Hadoop综合调优</h1><h2 id="Hadoop小文件优化方法"><a href="#Hadoop小文件优化方法" class="headerlink" title="Hadoop小文件优化方法"></a>Hadoop小文件优化方法</h2><h2 id="Hadoop小文件弊端"><a href="#Hadoop小文件弊端" class="headerlink" title="Hadoop小文件弊端"></a>Hadoop小文件弊端</h2><p>HDFS上每个文件都要在NameNode上创建对应的元数据，这个元数据的大小约为150byte，这样当小文件比较多的时候，就会产生很多的元数据文件，一方面会大量占用NameNode的内存空间，另一方面就是元数据文件过多，使得寻址索引速度变慢。<br>小文件过多，在进行MR计算时，会生成过多切片，需要启动过多的MapTask。每个MapTask处理的数据量小，导致MapTask的处理时间比启动时间还小，白白消耗资源。</p><h2 id="Hadoop小文件解决方案"><a href="#Hadoop小文件解决方案" class="headerlink" title="Hadoop小文件解决方案"></a>Hadoop小文件解决方案</h2><p>1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS（数据源头）<br>2）Hadoop Archive（存储方向）<br>      是一个高效的将小文件放入HDFS块中的文件存档工具，能够将多个小文件打包成一个HAR文件，从而达到减少NameNode的内存使用<br>3）CombineTextInputFormat（计算方向）<br>CombineTextInputFormat用于将多个小文件在切片过程中生成一个单独的切片或者少量的切片。<br>4）开启uber模式，实现JVM重用（计算方向）<br>默认情况下，每个Task任务都需要启动一个JVM来运行，如果Task任务计算的数据量很小，我们可以让同一个Job的多个Task运行在一个JVM中，不必为每个Task都开启一个JVM。<br>    （1）未开启uber模式，在/input路径上上传多个小文件并执行wordcount程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output2</span><br></pre></td></tr></table></figure><p>​    （2）观察控制台<br>2021-02-14 16:13:50,607 INFO mapreduce.Job: Job job_1613281510851_0002 running in uber mode : false<br>（3）观察<a href="http://hadoop103:8088/cluster">http://hadoop103:8088/cluster</a></p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_205.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>（4）开启uber模式，在mapred-site.xml中添加如下配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--  开启uber模式，默认关闭 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.ubertask.enable&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- uber模式中最大的mapTask数量，可向下修改  --&gt; </span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.ubertask.maxmaps&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;9&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- uber模式中最大的reduce数量，可向下修改 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.ubertask.maxreduces&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- uber模式中最大的输入数据量，默认使用dfs.blocksize 的值，可向下修改 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.ubertask.maxbytes&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（5）分发配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ xsync mapred-site.xml</span><br></pre></td></tr></table></figure><p>（6）再次执行wordcount程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output2</span><br></pre></td></tr></table></figure><p>​    （7）观察控制台</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2021-02-14 16:28:36,198 INFO mapreduce.Job: Job job_1613281510851_0003 running in uber mode : true</span><br></pre></td></tr></table></figure><p>（8）观察<a href="http://hadoop103:8088/cluster">http://hadoop103:8088/cluster</a></p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_206.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><h2 id="测试MapReduce计算性能"><a href="#测试MapReduce计算性能" class="headerlink" title="测试MapReduce计算性能"></a>测试MapReduce计算性能</h2><p>使用Sort程序评测MapReduce<br>注：一个虚拟机不超过150G磁盘尽量不要执行这段代码<br>（1）使用RandomWriter来产生随机数，每个节点运行10个Map任务，每个Map产生大约1G大小的二进制随机数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar randomwriter random-data</span><br></pre></td></tr></table></figure><p>（2）执行Sort程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar sort random-data sorted-data</span><br></pre></td></tr></table></figure><p>（3）验证数据是否真正排好序了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 mapreduce]$ </span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar testmapredsort -sortInput random-data -sortOutput sorted-data</span><br></pre></td></tr></table></figure><h1 id="企业开发场景案例"><a href="#企业开发场景案例" class="headerlink" title="企业开发场景案例"></a>企业开发场景案例</h1><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>（1）需求：从1G数据中，统计每个单词出现次数。服务器3台，每台配置4G内存，4核CPU，4线程。<br>（2）需求分析：<br>          1G / 128m = 8个MapTask；1个ReduceTask；1个mrAppMaster<br>  平均每个节点运行10个 / 3台 ≈ 3个任务（4    3    3）</p><h2 id="HDFS参数调优"><a href="#HDFS参数调优" class="headerlink" title="HDFS参数调优"></a>HDFS参数调优</h2><p>（1）修改：hadoop-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export HDFS_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=INFO,RFAS -Xmx1024m&quot;</span><br><span class="line"></span><br><span class="line">export HDFS_DATANODE_OPTS=&quot;-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m&quot;</span><br></pre></td></tr></table></figure><p>（2）修改hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- NameNode有一个工作线程池，默认值是10 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;21&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">（3）修改core-site.xml</span><br><span class="line">&lt;!-- 配置垃圾回收时间为60分钟 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;60&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（4）分发配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ xsync hadoop-env.sh hdfs-site.xml core-site.xml</span><br></pre></td></tr></table></figure><h2 id="MapReduce参数调优"><a href="#MapReduce参数调优" class="headerlink" title="MapReduce参数调优"></a>MapReduce参数调优</h2><p>（1）修改mapred-site.xml</p><!-- 环形缓冲区大小，默认100m --><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.task.io.sort.mb&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;100&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 环形缓冲区溢写阈值，默认0.8 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.sort.spill.percent&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;0.80&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- merge合并次数，默认10个 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.task.io.sort.factor&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;10&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- maptask内存，默认1g； maptask堆内存大小默认和该值大小一致mapreduce.map.java.opts --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;-1&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;The amount of memory to request from the scheduler for each    map task. If this is not specified or is non-positive, it is inferred from mapreduce.map.java.opts and mapreduce.job.heap.memory-mb.ratio. If java-opts are also not specified, we set it to 1024.</span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- matask的CPU核数，默认1个 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.cpu.vcores&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- matask异常重试次数，默认4次 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.maxattempts&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 每个Reduce去Map中拉取数据的并行数。默认值是5 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.shuffle.parallelcopies&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;5&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Buffer大小占Reduce可用内存的比例，默认值0.7 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.shuffle.input.buffer.percent&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;0.70&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Buffer中的数据达到多少比例开始写入磁盘，默认值0.66。 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.shuffle.merge.percent&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;0.66&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- reducetask内存，默认1g；reducetask堆内存大小默认和该值大小一致mapreduce.reduce.java.opts --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;-1&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;The amount of memory to request from the scheduler for each    reduce task. If this is not specified or is non-positive, it is inferred</span><br><span class="line">    from mapreduce.reduce.java.opts and mapreduce.job.heap.memory-mb.ratio.</span><br><span class="line">    If java-opts are also not specified, we set it to 1024.</span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- reducetask的CPU核数，默认1个 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.cpu.vcores&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- reducetask失败重试次数，默认4次 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.maxattempts&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 当MapTask完成的比例达到该值后才会为ReduceTask申请资源。默认是0.05 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.reduce.slowstart.completedmaps&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;0.05&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 如果程序在规定的默认10分钟内没有读到数据，将强制超时退出 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.task.timeout&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;600000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（2）分发配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ xsync mapred-site.xml</span><br></pre></td></tr></table></figure><h2 id="Yarn参数调优"><a href="#Yarn参数调优" class="headerlink" title="Yarn参数调优"></a>Yarn参数调优</h2><p>（1）修改yarn-site.xml配置参数如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 选择调度器，默认容量 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;</span><br><span class="line">&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- ResourceManager处理调度器请求的线程数量,默认50；如果提交的任务数大于50，可以增加该值，但是不能超过3台 * 4线程 = 12线程（去除其他应用程序实际不能超过8） --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;Number of threads to handle scheduler interface.&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.scheduler.client.thread-count&lt;/name&gt;</span><br><span class="line">&lt;value&gt;8&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 是否让yarn自动检测硬件进行配置，默认是false，如果该节点有很多其他应用程序，建议手动配置。如果该节点没有其他应用程序，可以采用自动 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;Enable auto-detection of node capabilities such as</span><br><span class="line">memory and CPU.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.resource.detect-hardware-capabilities&lt;/name&gt;</span><br><span class="line">&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 是否将虚拟核数当作CPU核数，默认是false，采用物理CPU核数 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;Flag to determine if logical processors(such as</span><br><span class="line">hyperthreads) should be counted as cores. Only applicable on Linux</span><br><span class="line">when yarn.nodemanager.resource.cpu-vcores is set to -1 and</span><br><span class="line">yarn.nodemanager.resource.detect-hardware-capabilities is true.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.resource.count-logical-processors-as-cores&lt;/name&gt;</span><br><span class="line">&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 虚拟核数和物理核数乘数，默认是1.0 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;Multiplier to determine how to convert phyiscal cores to</span><br><span class="line">vcores. This value is used if yarn.nodemanager.resource.cpu-vcores</span><br><span class="line">is set to -1(which implies auto-calculate vcores) and</span><br><span class="line">yarn.nodemanager.resource.detect-hardware-capabilities is set to true. Thenumber of vcores will be calculated asnumber of CPUs * multiplier.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.resource.pcores-vcores-multiplier&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- NodeManager使用内存数，默认8G，修改为4G内存 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;Amount of physical memory, in MB, that can be allocated </span><br><span class="line">for containers. If set to -1 and</span><br><span class="line">yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">automatically calculated(in case of Windows and Linux).</span><br><span class="line">In other cases, the default is 8192MB.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">&lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- nodemanager的CPU核数，不按照硬件环境自动设定时默认是8个，修改为4个 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;Number of vcores that can be allocated</span><br><span class="line">for containers. This is used by the RM scheduler when allocating</span><br><span class="line">resources for containers. This is not used to limit the number of</span><br><span class="line">CPUs used by YARN containers. If it is set to -1 and</span><br><span class="line">yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">automatically determined from the hardware in case of Windows and Linux.</span><br><span class="line">In other cases, number of vcores is 8 by default.&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;</span><br><span class="line">&lt;value&gt;4&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 容器最小内存，默认1G --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;The minimum allocation for every container request at the RMin MBs. Memory requests lower than this will be set to the value of thisproperty. Additionally, a node manager that is configured to have less memorythan this value will be shut down by the resource manager.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 容器最大内存，默认8G，修改为2G --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;The maximum allocation for every container request at the RMin MBs. Memory requests higher than this will throw anInvalidResourceRequestException.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2048&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 容器最小CPU核数，默认1个 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;The minimum allocation for every container request at the RMin terms of virtual CPU cores. Requests lower than this will be set to thevalue of this property. Additionally, a node manager that is configured tohave fewer virtual cores than this value will be shut down by the resourcemanager.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 容器最大CPU核数，默认4个，修改为2个 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;The maximum allocation for every container request at the RMin terms of virtual CPU cores. Requests higher than this will throw an</span><br><span class="line">InvalidResourceRequestException.&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 虚拟内存检查，默认打开，修改为关闭 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;Whether virtual memory limits will be enforced for</span><br><span class="line">containers.&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 虚拟内存和物理内存设置比例,默认2.1 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;Ratio between virtual memory to physical memory whensetting memory limits for containers. Container allocations areexpressed in terms of physical memory, and virtual memory usageis allowed to exceed this allocation by this ratio.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2.1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（2）分发配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ xsync yarn-site.xml</span><br></pre></td></tr></table></figure><h2 id="执行程序"><a href="#执行程序" class="headerlink" title="执行程序"></a>执行程序</h2><p>（1）重启集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ sbin/stop-yarn.sh</span><br><span class="line">[shangbaishuyao@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><p>（2）执行WordCount程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</span><br></pre></td></tr></table></figure><p>（3）观察Yarn任务执行页面<br><a href="http://hadoop103:8088/cluster/apps">http://hadoop103:8088/cluster/apps</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;无论何时，无论遇到何事，都要保持年轻的心态。放下过往陈旧的观念，打破固有的认知经验，去探索新鲜的事物，把更多时间放在修炼自我上。——人民日报                            &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="Hadoop企业级优化" scheme="http://xubatian.cn/tags/Hadoop%E4%BC%81%E4%B8%9A%E7%BA%A7%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop企业优化</title>
    <link href="http://xubatian.cn/Hadoop%E4%BC%81%E4%B8%9A%E4%BC%98%E5%8C%96/"/>
    <id>http://xubatian.cn/Hadoop%E4%BC%81%E4%B8%9A%E4%BC%98%E5%8C%96/</id>
    <published>2022-01-16T16:08:45.000Z</published>
    <updated>2022-01-23T02:58:21.755Z</updated>
    
    <content type="html"><![CDATA[<p>跳出原有的格局，用积极的心态，去学习新的思维方式，许多困难都能迎刃而解。——人民日报                              </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_137.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><p>前言</p><p>mapreduce 是一个hadoop的计算引擎,是hadoop的几个模块之一. 他是可插拔的.就是说mapreduce是可以换的. 因为mapreduce计算的太慢了. 所以后期我们会将mapreduce换成hive,spark,Flink. 因需求而定.此处了解.</p><h1 id="MapReduce-跑的慢的原因"><a href="#MapReduce-跑的慢的原因" class="headerlink" title="MapReduce 跑的慢的原因"></a>MapReduce 跑的慢的原因</h1><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">MapReduce 程序效率的瓶颈在于两点：</span><br><span class="line">1.计算机性能</span><br><span class="line">       CPU、内存、磁盘健康、网络</span><br><span class="line">2．I/O操作优化</span><br><span class="line">      （1）数据倾斜</span><br><span class="line">      （2）Map和Reduce数设置不合理</span><br><span class="line">      （3）Map运行时间太长，导致Reduce等待过久</span><br><span class="line">      （4）小文件过多</span><br><span class="line">      （5）大量的不可分块的超大文件</span><br><span class="line">      （6）Spill次数过多</span><br><span class="line">      （7）Merge次数过多等。</span><br></pre></td></tr></table></figure><h1 id="MapReduce优化方法"><a href="#MapReduce优化方法" class="headerlink" title="MapReduce优化方法"></a>MapReduce优化方法</h1><p>MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数。</p><h2 id="数据输入"><a href="#数据输入" class="headerlink" title="数据输入"></a>数据输入</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(1) 合并小文件：在执行MR任务前将小文件进行合并，大量的小文件会产生大量的Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢。</span><br><span class="line">(2) 采用CombineTextlnputFormat来作为输入，解决输入端大量小文件场景。</span><br></pre></td></tr></table></figure><h2 id="Map阶段"><a href="#Map阶段" class="headerlink" title="Map阶段"></a>Map阶段</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">（1）减少溢写（Spill）次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO。</span><br><span class="line">（2）减少合并（Merge）次数：通过调整io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间。</span><br><span class="line">（3）在Map之后，不影响业务逻辑前提下，先进行Combine处理，减少I/O </span><br></pre></td></tr></table></figure><h2 id="Reduce阶段"><a href="#Reduce阶段" class="headerlink" title="Reduce阶段"></a>Reduce阶段</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（1）合理设置Map和Reduce数：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map、Reduce任务间  竞争资源，造成处理超时等错误。</span><br><span class="line">（2）设置Map、Reduce共存：调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间。</span><br><span class="line">（3）规避使用Reduce：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。</span><br><span class="line">（4）合理设置Reduce端的Buffer：默认情况下，数据达到一个阈值的时候，Buffer中的数据就会写入磁盘，然后Reduce会从磁盘中获得所有的数据。也就是说，Buffer和Reduce是没有直接关联的，中间多次写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得Buffer中的一部分数据可以直接输送到Reduce，从而减少IO开销：mapreduce.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读Buffer中的数据直接拿给Reduce使用。这样一来，设置Buffer需要内存，读取数据需要内存，Reduce计算也要内存，所以要根据作业的运行情况进行调整。</span><br></pre></td></tr></table></figure><h2 id="I-O传输"><a href="#I-O传输" class="headerlink" title="I/O传输"></a>I/O传输</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(1)采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZO压缩编码器。</span><br><span class="line">(2)使用SequenceFile二进制文件。</span><br></pre></td></tr></table></figure><h2 id="数据倾斜问题"><a href="#数据倾斜问题" class="headerlink" title="数据倾斜问题"></a>数据倾斜问题</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1. 数据倾斜现象</span><br><span class="line">      数据频率倾斜------某一个区域的数据量要远远大于其他区域。</span><br><span class="line">      数据大小倾斜------部分记录的大小远远大于平均值。</span><br><span class="line">2. 减少数据倾斜的方法</span><br><span class="line">方法1: 抽样和范围分区可以通过对原始数据进行抽样得到的结果集来预设分区边界值。</span><br><span class="line">方法2：自定义分区基于输出键的背景知识进行自定义分区。例如，如果Map输出键的单词来源于一本书。且其中某几个专业词汇较多。那么就可以自定义分区将这这些专业词汇发送给固定的一部分Reduce实例。而将其他的都发送给剩余的Reduce实例。</span><br><span class="line">方法3：Combine使用Combine可以大量地减小数据倾斜。在可能的情况下，Combine的目的就是聚合并精简数据。</span><br><span class="line">方法4：采用Map Join，尽量避免Reduce Join 。</span><br></pre></td></tr></table></figure><h2 id="常用的调优参数"><a href="#常用的调优参数" class="headerlink" title="常用的调优参数"></a>常用的调优参数</h2><h3 id="1．资源相关参数"><a href="#1．资源相关参数" class="headerlink" title="1．资源相关参数"></a>1．资源相关参数</h3><p>（1）以下参数是在用户自己的MR应用程序中配置就可以生效（mapred-default.xml）</p><table><thead><tr><th>配置参数</th><th>参数说明</th></tr></thead><tbody><tr><td>mapreduce.map.memory.mb</td><td>一个MapTask可使用的资源上限（单位:MB），默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死。</td></tr><tr><td>mapreduce.reduce.memory.mb</td><td>一个ReduceTask可使用的资源上限（单位:MB），默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死。</td></tr><tr><td>mapreduce.map.cpu.vcores</td><td>每个MapTask可使用的最多cpu core数目，默认值: 1</td></tr><tr><td>mapreduce.reduce.cpu.vcores</td><td>每个ReduceTask可使用的最多cpu core数目，默认值: 1</td></tr><tr><td>mapreduce.reduce.shuffle.parallelcopies</td><td>每个Reduce去Map中取数据的并行数。默认值是5</td></tr><tr><td>mapreduce.reduce.shuffle.merge.percent</td><td>Buffer中的数据达到多少比例开始写入磁盘。默认值0.66</td></tr><tr><td>mapreduce.reduce.shuffle.input.buffer.percent</td><td>Buffer大小占Reduce可用内存的比例。默认值0.7</td></tr><tr><td>mapreduce.reduce.input.buffer.percent</td><td>指定多少比例的内存用来存放Buffer中的数据，默认值是0.0</td></tr></tbody></table><p>(2)  应该在YARN启动之前就配置在服务器的配置文件中才能生效（yarn-default.xml）</p><table><thead><tr><th>配置参数</th><th>参数说明</th></tr></thead><tbody><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>给应用程序Container分配的最小内存，默认值：1024</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>给应用程序Container分配的最大内存，默认值：8192</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>每个Container申请的最小CPU核数，默认值：1</td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>每个Container申请的最大CPU核数，默认值：32</td></tr><tr><td>yarn.nodemanager.resource.memory-mb</td><td>给Containers分配的最大物理内存，默认值：8192</td></tr></tbody></table><p>（3）Shuffle性能优化的关键参数，应在YARN启动之前就配置好（mapred-default.xml）</p><table><thead><tr><th>配置参数</th><th>参数说明</th></tr></thead><tbody><tr><td>mapreduce.task.io.sort.mb</td><td>Shuffle的环形缓冲区大小，默认100m</td></tr><tr><td>mapreduce.map.sort.spill.percent</td><td>环形缓冲区溢出的阈值，默认80%</td></tr></tbody></table><h3 id="2．容错相关参数-MapReduce性能优化"><a href="#2．容错相关参数-MapReduce性能优化" class="headerlink" title="2．容错相关参数(MapReduce性能优化)"></a>2．容错相关参数(MapReduce性能优化)</h3><table><thead><tr><th>配置参数</th><th>参数说明</th></tr></thead><tbody><tr><td>mapreduce.map.maxattempts</td><td>每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。</td></tr><tr><td>mapreduce.reduce.maxattempts</td><td>每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。</td></tr><tr><td>mapreduce.task.timeout</td><td>Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是600000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。</td></tr></tbody></table><h2 id="HDFS小文件优化方法"><a href="#HDFS小文件优化方法" class="headerlink" title="HDFS小文件优化方法"></a>HDFS小文件优化方法</h2><h3 id="HDFS小文件弊端"><a href="#HDFS小文件弊端" class="headerlink" title="HDFS小文件弊端"></a>HDFS小文件弊端</h3><p>HDFS上每个文件都要在NameNode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢。</p><h3 id="HDFS小文件解决方案"><a href="#HDFS小文件解决方案" class="headerlink" title="HDFS小文件解决方案"></a>HDFS小文件解决方案</h3><p>小文件的优化无非以下几种方式：</p><p>（1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS。</p><p>（2）在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并。</p><p>（3）在MapReduce处理时，可采用CombineTextInputFormat提高效率。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_140.png" width = "" height = "" alt="xubatian的博客" align="center" /><p><strong>重要:开启JVM重(chong)用效果是非常显著的.</strong></p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_141.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="MapReduce扩展案例"><a href="#MapReduce扩展案例" class="headerlink" title="MapReduce扩展案例"></a>MapReduce扩展案例</h1><h2 id="倒排索引案例（多job串联）"><a href="#倒排索引案例（多job串联）" class="headerlink" title="倒排索引案例（多job串联）"></a>倒排索引案例（多job串联）</h2><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><p>有大量的文本（文档、网页），需要建立搜索索引</p><h3 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h3><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_143.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="第一次处理案例代码"><a href="#第一次处理案例代码" class="headerlink" title="第一次处理案例代码"></a>第一次处理案例代码</h3><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/InvertedIndex/FirstTreatment/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/InvertedIndex/FirstTreatment/</a></p><h3 id="第二次处理案例代码"><a href="#第二次处理案例代码" class="headerlink" title="第二次处理案例代码"></a>第二次处理案例代码</h3><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/InvertedIndex/SecondTreatment/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/InvertedIndex/SecondTreatment/</a></p><h2 id="TopN案例"><a href="#TopN案例" class="headerlink" title="TopN案例"></a>TopN案例</h2><h3 id="需求-1"><a href="#需求-1" class="headerlink" title="需求"></a>需求</h3><p>对需求输出结果进行加工，输出流量使用量在前10的用户信息</p><h3 id="需求分析-1"><a href="#需求分析-1" class="headerlink" title="需求分析"></a>需求分析</h3><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_144.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="案例代码"><a href="#案例代码" class="headerlink" title="案例代码"></a>案例代码</h3><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/TopN/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/TopN/</a></p><h2 id="找博客共同好友案例"><a href="#找博客共同好友案例" class="headerlink" title="找博客共同好友案例"></a>找博客共同好友案例</h2><h3 id="需求-2"><a href="#需求-2" class="headerlink" title="需求"></a>需求</h3><p>以下是博客的好友列表数据，冒号前是一个用户，冒号后是该用户的所有好友（数据中的好友关系是单向的）<br>求出哪些人两两之间有共同好友，及他俩的共同好友都有谁？</p><p>数据输入:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">A:B,C,D,F,E,O</span><br><span class="line"></span><br><span class="line">B:A,C,E,K</span><br><span class="line"></span><br><span class="line">C:F,A,D,I</span><br><span class="line"></span><br><span class="line">D:A,E,F,L</span><br><span class="line"></span><br><span class="line">E:B,C,D,M,L</span><br><span class="line"></span><br><span class="line">F:A,B,C,D,E,O,M</span><br><span class="line"></span><br><span class="line">G:A,C,D,E,F</span><br><span class="line"></span><br><span class="line">H:A,C,D,E,O</span><br><span class="line"></span><br><span class="line">I:A,O</span><br><span class="line"></span><br><span class="line">J:B,O</span><br><span class="line"></span><br><span class="line">K:A,C,D</span><br><span class="line"></span><br><span class="line">L:D,E,F</span><br><span class="line"></span><br><span class="line">M:E,F,G</span><br><span class="line"></span><br><span class="line">O:A,H,I,J</span><br></pre></td></tr></table></figure><h3 id="需求分析-2"><a href="#需求分析-2" class="headerlink" title="需求分析"></a>需求分析</h3><p>先求出A、B、C、….等是谁的好友</p><p>第一次输出结果</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">AI,K,C,B,G,F,H,O,D,</span><br><span class="line"></span><br><span class="line">BA,F,J,E,</span><br><span class="line"></span><br><span class="line">CA,E,B,H,F,G,K,</span><br><span class="line"></span><br><span class="line">DG,C,K,A,L,F,E,H,</span><br><span class="line"></span><br><span class="line">EG,M,L,H,A,F,B,D,</span><br><span class="line"></span><br><span class="line">FL,M,D,C,G,A,</span><br><span class="line"></span><br><span class="line">GM,</span><br><span class="line"></span><br><span class="line">HO,</span><br><span class="line"></span><br><span class="line">IO,C,</span><br><span class="line"></span><br><span class="line">JO,</span><br><span class="line"></span><br><span class="line">KB,</span><br><span class="line"></span><br><span class="line">LD,E,</span><br><span class="line"></span><br><span class="line">ME,F,</span><br><span class="line"></span><br><span class="line">OA,H,I,J,F,</span><br></pre></td></tr></table></figure><p>第二次输出结果</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line">A-BE C </span><br><span class="line"></span><br><span class="line">A-CD F </span><br><span class="line"></span><br><span class="line">A-DE F </span><br><span class="line"></span><br><span class="line">A-ED B C </span><br><span class="line"></span><br><span class="line">A-FO B C D E </span><br><span class="line"></span><br><span class="line">A-GF E C D </span><br><span class="line"></span><br><span class="line">A-HE C D O </span><br><span class="line"></span><br><span class="line">A-IO </span><br><span class="line"></span><br><span class="line">A-JO B </span><br><span class="line"></span><br><span class="line">A-KD C </span><br><span class="line"></span><br><span class="line">A-LF E D </span><br><span class="line"></span><br><span class="line">A-ME F </span><br><span class="line"></span><br><span class="line">B-CA </span><br><span class="line"></span><br><span class="line">B-DA E </span><br><span class="line"></span><br><span class="line">B-EC </span><br><span class="line"></span><br><span class="line">B-FE A C </span><br><span class="line"></span><br><span class="line">B-GC E A </span><br><span class="line"></span><br><span class="line">B-HA E C </span><br><span class="line"></span><br><span class="line">B-IA </span><br><span class="line"></span><br><span class="line">B-KC A </span><br><span class="line"></span><br><span class="line">B-LE </span><br><span class="line"></span><br><span class="line">B-ME </span><br><span class="line"></span><br><span class="line">B-OA </span><br><span class="line"></span><br><span class="line">C-DA F </span><br><span class="line"></span><br><span class="line">C-ED </span><br><span class="line"></span><br><span class="line">C-FD A </span><br><span class="line"></span><br><span class="line">C-GD F A </span><br><span class="line"></span><br><span class="line">C-HD A </span><br><span class="line"></span><br><span class="line">C-IA </span><br><span class="line"></span><br><span class="line">C-KA D </span><br><span class="line"></span><br><span class="line">C-LD F </span><br><span class="line"></span><br><span class="line">C-MF </span><br><span class="line"></span><br><span class="line">C-OI A </span><br><span class="line"></span><br><span class="line">D-EL </span><br><span class="line"></span><br><span class="line">D-FA E </span><br><span class="line"></span><br><span class="line">D-GE A F </span><br><span class="line"></span><br><span class="line">D-HA E </span><br><span class="line"></span><br><span class="line">D-IA </span><br><span class="line"></span><br><span class="line">D-KA </span><br><span class="line"></span><br><span class="line">D-LE F </span><br><span class="line"></span><br><span class="line">D-MF E </span><br><span class="line"></span><br><span class="line">D-OA </span><br><span class="line"></span><br><span class="line">E-FD M C B </span><br><span class="line"></span><br><span class="line">E-GC D </span><br><span class="line"></span><br><span class="line">E-HC D </span><br><span class="line"></span><br><span class="line">E-JB </span><br><span class="line"></span><br><span class="line">E-KC D </span><br><span class="line"></span><br><span class="line">E-LD </span><br><span class="line"></span><br><span class="line">F-GD C A E </span><br><span class="line"></span><br><span class="line">F-HA D O E C </span><br><span class="line"></span><br><span class="line">F-IO A </span><br><span class="line"></span><br><span class="line">F-JB O </span><br><span class="line"></span><br><span class="line">F-KD C A </span><br><span class="line"></span><br><span class="line">F-LE D </span><br><span class="line"></span><br><span class="line">F-ME </span><br><span class="line"></span><br><span class="line">F-OA </span><br><span class="line"></span><br><span class="line">G-HD C E A </span><br><span class="line"></span><br><span class="line">G-IA </span><br><span class="line"></span><br><span class="line">G-KD A C </span><br><span class="line"></span><br><span class="line">G-LD F E </span><br><span class="line"></span><br><span class="line">G-ME F </span><br><span class="line"></span><br><span class="line">G-OA </span><br><span class="line"></span><br><span class="line">H-IO A </span><br><span class="line"></span><br><span class="line">H-JO </span><br><span class="line"></span><br><span class="line">H-KA C D </span><br><span class="line"></span><br><span class="line">H-LD E </span><br><span class="line"></span><br><span class="line">H-ME </span><br><span class="line"></span><br><span class="line">H-OA </span><br><span class="line"></span><br><span class="line">I-JO </span><br><span class="line"></span><br><span class="line">I-KA </span><br><span class="line"></span><br><span class="line">I-OA </span><br><span class="line"></span><br><span class="line">K-LD </span><br><span class="line"></span><br><span class="line">K-OA </span><br><span class="line"></span><br><span class="line">L-ME F</span><br></pre></td></tr></table></figure><h3 id="案例代码-1"><a href="#案例代码-1" class="headerlink" title="案例代码"></a>案例代码</h3><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/FindBlogFriends/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/FindBlogFriends/</a></p><h1 id="常见错误及解决方案"><a href="#常见错误及解决方案" class="headerlink" title="常见错误及解决方案"></a>常见错误及解决方案</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>）导包容易出错。尤其Text和CombineTextInputFormat。</span><br><span class="line"><span class="number">2</span>）Mapper中第一个输入的参数必须是LongWritable或者NullWritable，不可以是IntWritable.  报的错误是类型转换异常。</span><br><span class="line"><span class="number">3</span>）java.lang.Exception: java.io.IOException: Illegal partition <span class="keyword">for</span> <span class="number">13926435656</span> (<span class="number">4</span>)，说明Partition和ReduceTask个数没对上，调整ReduceTask个数。</span><br><span class="line"><span class="number">4</span>）如果分区数不是<span class="number">1</span>，但是reducetask为<span class="number">1</span>，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于<span class="number">1</span>。不大于<span class="number">1</span>肯定不执行。</span><br><span class="line"><span class="number">5</span>）在Windows环境编译的jar包导入到Linux环境中运行，</span><br><span class="line">hadoop jar wc.jar com.atguigu.mapreduce.wordcount.WordCountDriver /user/atguigu/ /user/atguigu/output</span><br><span class="line">报如下错误：</span><br><span class="line">Exception in thread <span class="string">&quot;main&quot;</span> java.lang.UnsupportedClassVersionError: com/atguigu/mapreduce/wordcount/WordCountDriver : Unsupported major.minor version <span class="number">52.0</span></span><br><span class="line">原因是Windows环境用的jdk1<span class="number">.7</span>，Linux环境用的jdk1<span class="number">.8</span>。</span><br><span class="line">解决方案：统一jdk版本。</span><br><span class="line"><span class="number">6</span>）缓存pd.txt小文件案例中，报找不到pd.txt文件</span><br><span class="line">原因：大部分为路径书写错误。还有就是要检查pd.txt.txt的问题。还有个别电脑写相对路径找不到pd.txt，可以修改为绝对路径。</span><br><span class="line"><span class="number">7</span>）报类型转换异常。</span><br><span class="line">通常都是在驱动函数中设置Map输出和最终输出时编写错误。</span><br><span class="line">Map输出的key如果没有排序，也会报类型转换异常。</span><br><span class="line"><span class="number">8</span>）集群中运行wc.jar时出现了无法获得输入文件。</span><br><span class="line">原因：WordCount案例的输入文件不能放用HDFS集群的根目录。</span><br><span class="line"><span class="number">9</span>）出现了如下相关异常</span><br><span class="line">Exception in thread <span class="string">&quot;main&quot;</span> java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z</span><br><span class="line">at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)</span><br><span class="line">at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:<span class="number">609</span>)</span><br><span class="line">at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:<span class="number">977</span>)</span><br><span class="line">java.io.IOException: Could not locate executable <span class="keyword">null</span>\bin\winutils.exe in the Hadoop binaries.</span><br><span class="line">at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:<span class="number">356</span>)</span><br><span class="line">at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:<span class="number">371</span>)</span><br><span class="line">at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:<span class="number">364</span>)</span><br><span class="line">解决方案：拷贝hadoop.dll文件到Windows目录C:\Windows\System32。个别同学电脑还需要修改Hadoop源码。</span><br><span class="line">方案二：创建如下包名，并将NativeIO.java拷贝到该包名下</span><br></pre></td></tr></table></figure><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_145.png" width = "" height = "" alt="xubatian的博客" align="center" /><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span>）自定义Outputformat时，注意在RecordWirter中的close方法必须关闭流资源。否则输出的文件内容中数据为空。</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (atguigufos != <span class="keyword">null</span>) &#123;</span><br><span class="line">atguigufos.close();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (otherfos != <span class="keyword">null</span>) &#123;</span><br><span class="line">otherfos.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>NativelO.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br><span class="line">924</span><br><span class="line">925</span><br><span class="line">926</span><br><span class="line">927</span><br><span class="line">928</span><br><span class="line">929</span><br><span class="line">930</span><br><span class="line">931</span><br><span class="line">932</span><br><span class="line">933</span><br><span class="line">934</span><br><span class="line">935</span><br><span class="line">936</span><br><span class="line">937</span><br><span class="line">938</span><br><span class="line">939</span><br><span class="line">940</span><br><span class="line">941</span><br><span class="line">942</span><br><span class="line">943</span><br><span class="line">944</span><br><span class="line">945</span><br><span class="line">946</span><br><span class="line">947</span><br><span class="line">948</span><br><span class="line">949</span><br><span class="line">950</span><br><span class="line">951</span><br><span class="line">952</span><br><span class="line">953</span><br><span class="line">954</span><br><span class="line">955</span><br><span class="line">956</span><br><span class="line">957</span><br><span class="line">958</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment"> * or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment"> * distributed with this work for additional information</span></span><br><span class="line"><span class="comment"> * regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment"> * to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment"> * &quot;License&quot;); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment"> * with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">package</span> org.apache.hadoop.io.nativeio;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileDescriptor;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.RandomAccessFile;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Field;</span><br><span class="line"><span class="keyword">import</span> java.nio.ByteBuffer;</span><br><span class="line"><span class="keyword">import</span> java.nio.MappedByteBuffer;</span><br><span class="line"><span class="keyword">import</span> java.nio.channels.FileChannel;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ConcurrentHashMap;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.classification.InterfaceAudience;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.classification.InterfaceStability;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.CommonConfigurationKeys;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.HardLink;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.SecureIOUtils.AlreadyExistsException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.NativeCodeLoader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Shell;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.PerformanceAdvisory;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.logging.Log;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.logging.LogFactory;</span><br><span class="line"><span class="keyword">import</span> sun.misc.Unsafe;</span><br><span class="line"><span class="keyword">import</span> com.google.common.annotations.VisibleForTesting;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * JNI wrappers for various native IO-related calls not available in Java. These</span></span><br><span class="line"><span class="comment"> * functions should generally be used alongside a fallback to another more</span></span><br><span class="line"><span class="comment"> * portable mechanism.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@InterfaceAudience</span>.Private</span><br><span class="line"><span class="meta">@InterfaceStability</span>.Unstable</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NativeIO</span> </span>&#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">POSIX</span> </span>&#123;</span><br><span class="line"><span class="comment">// Flags for open() call from bits/fcntl.h</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_RDONLY = <span class="number">00</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_WRONLY = <span class="number">01</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_RDWR = <span class="number">02</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_CREAT = <span class="number">0100</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_EXCL = <span class="number">0200</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_NOCTTY = <span class="number">0400</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_TRUNC = <span class="number">01000</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_APPEND = <span class="number">02000</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_NONBLOCK = <span class="number">04000</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_SYNC = <span class="number">010000</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_ASYNC = <span class="number">020000</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_FSYNC = O_SYNC;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_NDELAY = O_NONBLOCK;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Flags for posix_fadvise() from bits/fcntl.h</span></span><br><span class="line"><span class="comment">/* No further special treatment. */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> POSIX_FADV_NORMAL = <span class="number">0</span>;</span><br><span class="line"><span class="comment">/* Expect random page references. */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> POSIX_FADV_RANDOM = <span class="number">1</span>;</span><br><span class="line"><span class="comment">/* Expect sequential page references. */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> POSIX_FADV_SEQUENTIAL = <span class="number">2</span>;</span><br><span class="line"><span class="comment">/* Will need these pages. */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> POSIX_FADV_WILLNEED = <span class="number">3</span>;</span><br><span class="line"><span class="comment">/* Don&#x27;t need these pages. */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> POSIX_FADV_DONTNEED = <span class="number">4</span>;</span><br><span class="line"><span class="comment">/* Data will be accessed once. */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> POSIX_FADV_NOREUSE = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Wait upon writeout of all pages in the range before performing the write.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SYNC_FILE_RANGE_WAIT_BEFORE = <span class="number">1</span>;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Initiate writeout of all those dirty pages in the range which are not</span></span><br><span class="line"><span class="comment"> * presently under writeback.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SYNC_FILE_RANGE_WRITE = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Wait upon writeout of all pages in the range after performing the write.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SYNC_FILE_RANGE_WAIT_AFTER = <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Log LOG = LogFactory.getLog(NativeIO.class);</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> nativeLoaded = <span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> fadvisePossible = <span class="keyword">true</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> syncFileRangePossible = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> String WORKAROUND_NON_THREADSAFE_CALLS_KEY = <span class="string">&quot;hadoop.workaround.non.threadsafe.getpwuid&quot;</span>;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">boolean</span> WORKAROUND_NON_THREADSAFE_CALLS_DEFAULT = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> cacheTimeout = -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> CacheManipulator cacheManipulator = <span class="keyword">new</span> CacheManipulator();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> CacheManipulator <span class="title">getCacheManipulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> cacheManipulator;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setCacheManipulator</span><span class="params">(CacheManipulator cacheManipulator)</span> </span>&#123;</span><br><span class="line">POSIX.cacheManipulator = cacheManipulator;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Used to manipulate the operating system cache.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@VisibleForTesting</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CacheManipulator</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mlock</span><span class="params">(String identifier, ByteBuffer buffer, <span class="keyword">long</span> len)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">POSIX.mlock(buffer, len);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getMemlockLimit</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> NativeIO.getMemlockLimit();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getOperatingSystemPageSize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> NativeIO.getOperatingSystemPageSize();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">posixFadviseIfPossible</span><span class="params">(String identifier, FileDescriptor fd, <span class="keyword">long</span> offset, <span class="keyword">long</span> len, <span class="keyword">int</span> flags)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> NativeIOException </span>&#123;</span><br><span class="line">NativeIO.POSIX.posixFadviseIfPossible(identifier, fd, offset, len, flags);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">verifyCanMlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> NativeIO.isAvailable();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A CacheManipulator used for testing which does not actually call mlock. This</span></span><br><span class="line"><span class="comment"> * allows many tests to be run even when the operating system does not allow</span></span><br><span class="line"><span class="comment"> * mlock, or only allows limited mlocking.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@VisibleForTesting</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">NoMlockCacheManipulator</span> <span class="keyword">extends</span> <span class="title">CacheManipulator</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mlock</span><span class="params">(String identifier, ByteBuffer buffer, <span class="keyword">long</span> len)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">LOG.info(<span class="string">&quot;mlocking &quot;</span> + identifier);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getMemlockLimit</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="number">1125899906842624L</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getOperatingSystemPageSize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="number">4096</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">verifyCanMlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (NativeCodeLoader.isNativeCodeLoaded()) &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">workaroundNonThreadSafePasswdCalls = conf.getBoolean(WORKAROUND_NON_THREADSAFE_CALLS_KEY,</span><br><span class="line">WORKAROUND_NON_THREADSAFE_CALLS_DEFAULT);</span><br><span class="line"></span><br><span class="line">initNative();</span><br><span class="line">nativeLoaded = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">cacheTimeout = conf.getLong(CommonConfigurationKeys.HADOOP_SECURITY_UID_NAME_CACHE_TIMEOUT_KEY,</span><br><span class="line">CommonConfigurationKeys.HADOOP_SECURITY_UID_NAME_CACHE_TIMEOUT_DEFAULT) * <span class="number">1000</span>;</span><br><span class="line">LOG.debug(<span class="string">&quot;Initialized cache for IDs to User/Group mapping with a &quot;</span> + <span class="string">&quot; cache timeout of &quot;</span></span><br><span class="line">+ cacheTimeout / <span class="number">1000</span> + <span class="string">&quot; seconds.&quot;</span>);</span><br><span class="line"></span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line"><span class="comment">// This can happen if the user has an older version of libhadoop.so</span></span><br><span class="line"><span class="comment">// installed - in this case we can continue without native IO</span></span><br><span class="line"><span class="comment">// after warning</span></span><br><span class="line">PerformanceAdvisory.LOG.debug(<span class="string">&quot;Unable to initialize NativeIO libraries&quot;</span>, t);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return true if the JNI-based native IO extensions are available.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isAvailable</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> NativeCodeLoader.isNativeCodeLoaded() &amp;&amp; nativeLoaded;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">assertCodeLoaded</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!isAvailable()) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;NativeIO was not loaded&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Wrapper around open(2) */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> FileDescriptor <span class="title">open</span><span class="params">(String path, <span class="keyword">int</span> flags, <span class="keyword">int</span> mode)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Wrapper around fstat(2) */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> Stat <span class="title">fstat</span><span class="params">(FileDescriptor fd)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Native chmod implementation. On UNIX, it is a wrapper around chmod(2) */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">chmodImpl</span><span class="params">(String path, <span class="keyword">int</span> mode)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">chmod</span><span class="params">(String path, <span class="keyword">int</span> mode)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!Shell.WINDOWS) &#123;</span><br><span class="line">chmodImpl(path, mode);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">chmodImpl(path, mode);</span><br><span class="line">&#125; <span class="keyword">catch</span> (NativeIOException nioe) &#123;</span><br><span class="line"><span class="keyword">if</span> (nioe.getErrorCode() == <span class="number">3</span>) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> NativeIOException(<span class="string">&quot;No such file or directory&quot;</span>, Errno.ENOENT);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">LOG.warn(</span><br><span class="line">String.format(<span class="string">&quot;NativeIO.chmod error (%d): %s&quot;</span>, nioe.getErrorCode(), nioe.getMessage()));</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> NativeIOException(<span class="string">&quot;Unknown error&quot;</span>, Errno.UNKNOWN);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Wrapper around posix_fadvise(2) */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">posix_fadvise</span><span class="params">(FileDescriptor fd, <span class="keyword">long</span> offset, <span class="keyword">long</span> len, <span class="keyword">int</span> flags)</span> <span class="keyword">throws</span> NativeIOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Wrapper around sync_file_range(2) */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">sync_file_range</span><span class="params">(FileDescriptor fd, <span class="keyword">long</span> offset, <span class="keyword">long</span> nbytes, <span class="keyword">int</span> flags)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> NativeIOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Call posix_fadvise on the given file descriptor. See the manpage for this</span></span><br><span class="line"><span class="comment"> * syscall for more information. On systems where this call is not available,</span></span><br><span class="line"><span class="comment"> * does nothing.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> NativeIOException</span></span><br><span class="line"><span class="comment"> *             if there is an error with the syscall</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">posixFadviseIfPossible</span><span class="params">(String identifier, FileDescriptor fd, <span class="keyword">long</span> offset, <span class="keyword">long</span> len, <span class="keyword">int</span> flags)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> NativeIOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (nativeLoaded &amp;&amp; fadvisePossible) &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">posix_fadvise(fd, offset, len, flags);</span><br><span class="line">&#125; <span class="keyword">catch</span> (UnsupportedOperationException uoe) &#123;</span><br><span class="line">fadvisePossible = <span class="keyword">false</span>;</span><br><span class="line">&#125; <span class="keyword">catch</span> (UnsatisfiedLinkError ule) &#123;</span><br><span class="line">fadvisePossible = <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Call sync_file_range on the given file descriptor. See the manpage for this</span></span><br><span class="line"><span class="comment"> * syscall for more information. On systems where this call is not available,</span></span><br><span class="line"><span class="comment"> * does nothing.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> NativeIOException</span></span><br><span class="line"><span class="comment"> *             if there is an error with the syscall</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">syncFileRangeIfPossible</span><span class="params">(FileDescriptor fd, <span class="keyword">long</span> offset, <span class="keyword">long</span> nbytes, <span class="keyword">int</span> flags)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> NativeIOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (nativeLoaded &amp;&amp; syncFileRangePossible) &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">sync_file_range(fd, offset, nbytes, flags);</span><br><span class="line">&#125; <span class="keyword">catch</span> (UnsupportedOperationException uoe) &#123;</span><br><span class="line">syncFileRangePossible = <span class="keyword">false</span>;</span><br><span class="line">&#125; <span class="keyword">catch</span> (UnsatisfiedLinkError ule) &#123;</span><br><span class="line">syncFileRangePossible = <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">mlock_native</span><span class="params">(ByteBuffer buffer, <span class="keyword">long</span> len)</span> <span class="keyword">throws</span> NativeIOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Locks the provided direct ByteBuffer into memory, preventing it from swapping</span></span><br><span class="line"><span class="comment"> * out. After a buffer is locked, future accesses will not incur a page fault.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * See the mlock(2) man page for more information.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> NativeIOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">mlock</span><span class="params">(ByteBuffer buffer, <span class="keyword">long</span> len)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">assertCodeLoaded();</span><br><span class="line"><span class="keyword">if</span> (!buffer.isDirect()) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;Cannot mlock a non-direct ByteBuffer&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">mlock_native(buffer, len);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Unmaps the block from memory. See munmap(2).</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * There isn&#x27;t any portable way to unmap a memory region in Java. So we use the</span></span><br><span class="line"><span class="comment"> * sun.nio method here. Note that unmapping a memory region could cause crashes</span></span><br><span class="line"><span class="comment"> * if code continues to reference the unmapped code. However, if we don&#x27;t</span></span><br><span class="line"><span class="comment"> * manually unmap the memory, we are dependent on the finalizer to do it, and we</span></span><br><span class="line"><span class="comment"> * have no idea when the finalizer will run.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> buffer</span></span><br><span class="line"><span class="comment"> *            The buffer to unmap.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">munmap</span><span class="params">(MappedByteBuffer buffer)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (buffer <span class="keyword">instanceof</span> sun.nio.ch.DirectBuffer) &#123;</span><br><span class="line">sun.misc.Cleaner cleaner = ((sun.nio.ch.DirectBuffer) buffer).cleaner();</span><br><span class="line">cleaner.clean();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Linux only methods used for getOwner() implementation */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">long</span> <span class="title">getUIDforFDOwnerforOwner</span><span class="params">(FileDescriptor fd)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> String <span class="title">getUserName</span><span class="params">(<span class="keyword">long</span> uid)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Result type of the fstat call</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Stat</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> ownerId, groupId;</span><br><span class="line"><span class="keyword">private</span> String owner, group;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> mode;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Mode constants</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFMT = <span class="number">0170000</span>; <span class="comment">/* type of file */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFIFO = <span class="number">0010000</span>; <span class="comment">/* named pipe (fifo) */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFCHR = <span class="number">0020000</span>; <span class="comment">/* character special */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFDIR = <span class="number">0040000</span>; <span class="comment">/* directory */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFBLK = <span class="number">0060000</span>; <span class="comment">/* block special */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFREG = <span class="number">0100000</span>; <span class="comment">/* regular */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFLNK = <span class="number">0120000</span>; <span class="comment">/* symbolic link */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFSOCK = <span class="number">0140000</span>; <span class="comment">/* socket */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFWHT = <span class="number">0160000</span>; <span class="comment">/* whiteout */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_ISUID = <span class="number">0004000</span>; <span class="comment">/* set user id on execution */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_ISGID = <span class="number">0002000</span>; <span class="comment">/* set group id on execution */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_ISVTX = <span class="number">0001000</span>; <span class="comment">/* save swapped text even after use */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IRUSR = <span class="number">0000400</span>; <span class="comment">/* read permission, owner */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IWUSR = <span class="number">0000200</span>; <span class="comment">/* write permission, owner */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IXUSR = <span class="number">0000100</span>; <span class="comment">/* execute/search permission, owner */</span></span><br><span class="line"></span><br><span class="line">Stat(<span class="keyword">int</span> ownerId, <span class="keyword">int</span> groupId, <span class="keyword">int</span> mode) &#123;</span><br><span class="line"><span class="keyword">this</span>.ownerId = ownerId;</span><br><span class="line"><span class="keyword">this</span>.groupId = groupId;</span><br><span class="line"><span class="keyword">this</span>.mode = mode;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Stat(String owner, String group, <span class="keyword">int</span> mode) &#123;</span><br><span class="line"><span class="keyword">if</span> (!Shell.WINDOWS) &#123;</span><br><span class="line"><span class="keyword">this</span>.owner = owner;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">this</span>.owner = stripDomain(owner);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (!Shell.WINDOWS) &#123;</span><br><span class="line"><span class="keyword">this</span>.group = group;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">this</span>.group = stripDomain(group);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">this</span>.mode = mode;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">&quot;Stat(owner=&#x27;&quot;</span> + owner + <span class="string">&quot;&#x27;, group=&#x27;&quot;</span> + group + <span class="string">&quot;&#x27;&quot;</span> + <span class="string">&quot;, mode=&quot;</span> + mode + <span class="string">&quot;)&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getOwner</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> owner;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getGroup</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> group;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getMode</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> mode;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns the file stat for a file descriptor.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> fd</span></span><br><span class="line"><span class="comment"> *            file descriptor.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the file descriptor file stat.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> *             thrown if there was an IO error while obtaining the file stat.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Stat <span class="title">getFstat</span><span class="params">(FileDescriptor fd)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">Stat stat = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">if</span> (!Shell.WINDOWS) &#123;</span><br><span class="line">stat = fstat(fd);</span><br><span class="line">stat.owner = getName(IdCache.USER, stat.ownerId);</span><br><span class="line">stat.group = getName(IdCache.GROUP, stat.groupId);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">stat = fstat(fd);</span><br><span class="line">&#125; <span class="keyword">catch</span> (NativeIOException nioe) &#123;</span><br><span class="line"><span class="keyword">if</span> (nioe.getErrorCode() == <span class="number">6</span>) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> NativeIOException(<span class="string">&quot;The handle is invalid.&quot;</span>, Errno.EBADF);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">LOG.warn(String.format(<span class="string">&quot;NativeIO.getFstat error (%d): %s&quot;</span>, nioe.getErrorCode(),</span><br><span class="line">nioe.getMessage()));</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> NativeIOException(<span class="string">&quot;Unknown error&quot;</span>, Errno.UNKNOWN);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> stat;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> String <span class="title">getName</span><span class="params">(IdCache domain, <span class="keyword">int</span> id)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">Map&lt;Integer, CachedName&gt; idNameCache = (domain == IdCache.USER) ? USER_ID_NAME_CACHE : GROUP_ID_NAME_CACHE;</span><br><span class="line">String name;</span><br><span class="line">CachedName cachedName = idNameCache.get(id);</span><br><span class="line"><span class="keyword">long</span> now = System.currentTimeMillis();</span><br><span class="line"><span class="keyword">if</span> (cachedName != <span class="keyword">null</span> &amp;&amp; (cachedName.timestamp + cacheTimeout) &gt; now) &#123;</span><br><span class="line">name = cachedName.name;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">name = (domain == IdCache.USER) ? getUserName(id) : getGroupName(id);</span><br><span class="line"><span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">String type = (domain == IdCache.USER) ? <span class="string">&quot;UserName&quot;</span> : <span class="string">&quot;GroupName&quot;</span>;</span><br><span class="line">LOG.debug(<span class="string">&quot;Got &quot;</span> + type + <span class="string">&quot; &quot;</span> + name + <span class="string">&quot; for ID &quot;</span> + id + <span class="string">&quot; from the native implementation&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">cachedName = <span class="keyword">new</span> CachedName(name, now);</span><br><span class="line">idNameCache.put(id, cachedName);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> name;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">native</span> String <span class="title">getUserName</span><span class="params">(<span class="keyword">int</span> uid)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">native</span> String <span class="title">getGroupName</span><span class="params">(<span class="keyword">int</span> uid)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CachedName</span> </span>&#123;</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">long</span> timestamp;</span><br><span class="line"><span class="keyword">final</span> String name;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">CachedName</span><span class="params">(String name, <span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.name = name;</span><br><span class="line"><span class="keyword">this</span>.timestamp = timestamp;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Map&lt;Integer, CachedName&gt; USER_ID_NAME_CACHE = <span class="keyword">new</span> ConcurrentHashMap&lt;Integer, CachedName&gt;();</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Map&lt;Integer, CachedName&gt; GROUP_ID_NAME_CACHE = <span class="keyword">new</span> ConcurrentHashMap&lt;Integer, CachedName&gt;();</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">enum</span> <span class="title">IdCache</span> </span>&#123;</span><br><span class="line">USER, GROUP</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> MMAP_PROT_READ = <span class="number">0x1</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> MMAP_PROT_WRITE = <span class="number">0x2</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> MMAP_PROT_EXEC = <span class="number">0x4</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">long</span> <span class="title">mmap</span><span class="params">(FileDescriptor fd, <span class="keyword">int</span> prot, <span class="keyword">boolean</span> shared, <span class="keyword">long</span> length)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">munmap</span><span class="params">(<span class="keyword">long</span> addr, <span class="keyword">long</span> length)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> workaroundNonThreadSafePasswdCalls = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Windows</span> </span>&#123;</span><br><span class="line"><span class="comment">// Flags for CreateFile() call on Windows</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> GENERIC_READ = <span class="number">0x80000000L</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> GENERIC_WRITE = <span class="number">0x40000000L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> FILE_SHARE_READ = <span class="number">0x00000001L</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> FILE_SHARE_WRITE = <span class="number">0x00000002L</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> FILE_SHARE_DELETE = <span class="number">0x00000004L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> CREATE_NEW = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> CREATE_ALWAYS = <span class="number">2</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> OPEN_EXISTING = <span class="number">3</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> OPEN_ALWAYS = <span class="number">4</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> TRUNCATE_EXISTING = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> FILE_BEGIN = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> FILE_CURRENT = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> FILE_END = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> FILE_ATTRIBUTE_NORMAL = <span class="number">0x00000080L</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a directory with permissions set to the specified mode. By setting</span></span><br><span class="line"><span class="comment"> * permissions at creation time, we avoid issues related to the user lacking</span></span><br><span class="line"><span class="comment"> * WRITE_DAC rights on subsequent chmod calls. One example where this can occur</span></span><br><span class="line"><span class="comment"> * is writing to an SMB share where the user does not have Full Control rights,</span></span><br><span class="line"><span class="comment"> * and therefore WRITE_DAC is denied.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> path</span></span><br><span class="line"><span class="comment"> *            directory to create</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> mode</span></span><br><span class="line"><span class="comment"> *            permissions of new directory</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> *             if there is an I/O error</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createDirectoryWithMode</span><span class="params">(File path, <span class="keyword">int</span> mode)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">createDirectoryWithMode0(path.getAbsolutePath(), mode);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Wrapper around CreateDirectory() on Windows */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">createDirectoryWithMode0</span><span class="params">(String path, <span class="keyword">int</span> mode)</span> <span class="keyword">throws</span> NativeIOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Wrapper around CreateFile() on Windows */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> FileDescriptor <span class="title">createFile</span><span class="params">(String path, <span class="keyword">long</span> desiredAccess, <span class="keyword">long</span> shareMode,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">long</span> creationDisposition)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a file for write with permissions set to the specified mode. By</span></span><br><span class="line"><span class="comment"> * setting permissions at creation time, we avoid issues related to the user</span></span><br><span class="line"><span class="comment"> * lacking WRITE_DAC rights on subsequent chmod calls. One example where this</span></span><br><span class="line"><span class="comment"> * can occur is writing to an SMB share where the user does not have Full</span></span><br><span class="line"><span class="comment"> * Control rights, and therefore WRITE_DAC is denied.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * This method mimics the semantics implemented by the JDK in</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@link</span> java.io.FileOutputStream&#125;. The file is opened for truncate or append,</span></span><br><span class="line"><span class="comment"> * the sharing mode allows other readers and writers, and paths longer than</span></span><br><span class="line"><span class="comment"> * MAX_PATH are supported. (See io_util_md.c in the JDK.)</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> path</span></span><br><span class="line"><span class="comment"> *            file to create</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> append</span></span><br><span class="line"><span class="comment"> *            if true, then open file for append</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> mode</span></span><br><span class="line"><span class="comment"> *            permissions of new directory</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> FileOutputStream of opened file</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> *             if there is an I/O error</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> FileOutputStream <span class="title">createFileOutputStreamWithMode</span><span class="params">(File path, <span class="keyword">boolean</span> append, <span class="keyword">int</span> mode)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">long</span> desiredAccess = GENERIC_WRITE;</span><br><span class="line"><span class="keyword">long</span> shareMode = FILE_SHARE_READ | FILE_SHARE_WRITE;</span><br><span class="line"><span class="keyword">long</span> creationDisposition = append ? OPEN_ALWAYS : CREATE_ALWAYS;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> FileOutputStream(</span><br><span class="line">createFileWithMode0(path.getAbsolutePath(), desiredAccess, shareMode, creationDisposition, mode));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Wrapper around CreateFile() with security descriptor on Windows */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> FileDescriptor <span class="title">createFileWithMode0</span><span class="params">(String path, <span class="keyword">long</span> desiredAccess, <span class="keyword">long</span> shareMode,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">long</span> creationDisposition, <span class="keyword">int</span> mode)</span> <span class="keyword">throws</span> NativeIOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Wrapper around SetFilePointer() on Windows */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">long</span> <span class="title">setFilePointer</span><span class="params">(FileDescriptor fd, <span class="keyword">long</span> distanceToMove, <span class="keyword">long</span> moveMethod)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Windows only methods used for getOwner() implementation */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> String <span class="title">getOwner</span><span class="params">(FileDescriptor fd)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Supported list of Windows access right flags */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">enum</span> <span class="title">AccessRight</span> </span>&#123;</span><br><span class="line">ACCESS_READ(<span class="number">0x0001</span>), <span class="comment">// FILE_READ_DATA</span></span><br><span class="line">ACCESS_WRITE(<span class="number">0x0002</span>), <span class="comment">// FILE_WRITE_DATA</span></span><br><span class="line">ACCESS_EXECUTE(<span class="number">0x0020</span>); <span class="comment">// FILE_EXECUTE</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> accessRight;</span><br><span class="line"></span><br><span class="line">AccessRight(<span class="keyword">int</span> access) &#123;</span><br><span class="line">accessRight = access;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">accessRight</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> accessRight;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Windows only method used to check if the current process has requested access</span></span><br><span class="line"><span class="comment"> * rights on the given path.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">boolean</span> <span class="title">access0</span><span class="params">(String path, <span class="keyword">int</span> requestedAccess)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Checks whether the current process has desired access rights on the given</span></span><br><span class="line"><span class="comment"> * path.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * Longer term this native function can be substituted with JDK7 function</span></span><br><span class="line"><span class="comment"> * Files#isReadable, isWritable, isExecutable.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> path</span></span><br><span class="line"><span class="comment"> *            input path</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> desiredAccess</span></span><br><span class="line"><span class="comment"> *            ACCESS_READ, ACCESS_WRITE or ACCESS_EXECUTE</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> true if access is allowed</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> *             I/O exception on error</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">access</span><span class="params">(String path, AccessRight desiredAccess)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"><span class="comment">// return access0(path, desiredAccess.accessRight());</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Extends both the minimum and maximum working set size of the current process.</span></span><br><span class="line"><span class="comment"> * This method gets the current minimum and maximum working set size, adds the</span></span><br><span class="line"><span class="comment"> * requested amount to each and then sets the minimum and maximum working set</span></span><br><span class="line"><span class="comment"> * size to the new values. Controlling the working set size of the process also</span></span><br><span class="line"><span class="comment"> * controls the amount of memory it can lock.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> delta</span></span><br><span class="line"><span class="comment"> *            amount to increment minimum and maximum working set size</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> *             for any error</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@see</span> POSIX#mlock(ByteBuffer, long)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">extendWorkingSetSize</span><span class="params">(<span class="keyword">long</span> delta)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (NativeCodeLoader.isNativeCodeLoaded()) &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">initNative();</span><br><span class="line">nativeLoaded = <span class="keyword">true</span>;</span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line"><span class="comment">// This can happen if the user has an older version of libhadoop.so</span></span><br><span class="line"><span class="comment">// installed - in this case we can continue without native IO</span></span><br><span class="line"><span class="comment">// after warning</span></span><br><span class="line">PerformanceAdvisory.LOG.debug(<span class="string">&quot;Unable to initialize NativeIO libraries&quot;</span>, t);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Log LOG = LogFactory.getLog(NativeIO.class);</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> nativeLoaded = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (NativeCodeLoader.isNativeCodeLoaded()) &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">initNative();</span><br><span class="line">nativeLoaded = <span class="keyword">true</span>;</span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line"><span class="comment">// This can happen if the user has an older version of libhadoop.so</span></span><br><span class="line"><span class="comment">// installed - in this case we can continue without native IO</span></span><br><span class="line"><span class="comment">// after warning</span></span><br><span class="line">PerformanceAdvisory.LOG.debug(<span class="string">&quot;Unable to initialize NativeIO libraries&quot;</span>, t);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return true if the JNI-based native IO extensions are available.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isAvailable</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> NativeCodeLoader.isNativeCodeLoaded() &amp;&amp; nativeLoaded;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Initialize the JNI method ID and class ID cache */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">initNative</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Get the maximum number of bytes that can be locked into memory at any given</span></span><br><span class="line"><span class="comment"> * point.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> 0 if no bytes can be locked into memory; Long.MAX_VALUE if there is</span></span><br><span class="line"><span class="comment"> *         no limit; The number of bytes that can be locked into memory</span></span><br><span class="line"><span class="comment"> *         otherwise.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">long</span> <span class="title">getMemlockLimit</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> isAvailable() ? getMemlockLimit0() : <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">long</span> <span class="title">getMemlockLimit0</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the operating system&#x27;s page size.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">long</span> <span class="title">getOperatingSystemPageSize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">Field f = Unsafe.class.getDeclaredField(<span class="string">&quot;theUnsafe&quot;</span>);</span><br><span class="line">f.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">Unsafe unsafe = (Unsafe) f.get(<span class="keyword">null</span>);</span><br><span class="line"><span class="keyword">return</span> unsafe.pageSize();</span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">LOG.warn(<span class="string">&quot;Unable to get operating system page size.  Guessing 4096.&quot;</span>, e);</span><br><span class="line"><span class="keyword">return</span> <span class="number">4096</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CachedUid</span> </span>&#123;</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">long</span> timestamp;</span><br><span class="line"><span class="keyword">final</span> String username;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">CachedUid</span><span class="params">(String username, <span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.timestamp = timestamp;</span><br><span class="line"><span class="keyword">this</span>.username = username;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Map&lt;Long, CachedUid&gt; uidCache = <span class="keyword">new</span> ConcurrentHashMap&lt;Long, CachedUid&gt;();</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> cacheTimeout;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> initialized = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The Windows logon name has two part, NetBIOS domain name and user account</span></span><br><span class="line"><span class="comment"> * name, of the format DOMAIN\UserName. This method will remove the domain part</span></span><br><span class="line"><span class="comment"> * of the full logon name.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> Fthe</span></span><br><span class="line"><span class="comment"> *            full principal name containing the domain</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> name with domain removed</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> String <span class="title">stripDomain</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> i = name.indexOf(<span class="string">&#x27;\\&#x27;</span>);</span><br><span class="line"><span class="keyword">if</span> (i != -<span class="number">1</span>)</span><br><span class="line">name = name.substring(i + <span class="number">1</span>);</span><br><span class="line"><span class="keyword">return</span> name;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getOwner</span><span class="params">(FileDescriptor fd)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">ensureInitialized();</span><br><span class="line"><span class="keyword">if</span> (Shell.WINDOWS) &#123;</span><br><span class="line">String owner = Windows.getOwner(fd);</span><br><span class="line">owner = stripDomain(owner);</span><br><span class="line"><span class="keyword">return</span> owner;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">long</span> uid = POSIX.getUIDforFDOwnerforOwner(fd);</span><br><span class="line">CachedUid cUid = uidCache.get(uid);</span><br><span class="line"><span class="keyword">long</span> now = System.currentTimeMillis();</span><br><span class="line"><span class="keyword">if</span> (cUid != <span class="keyword">null</span> &amp;&amp; (cUid.timestamp + cacheTimeout) &gt; now) &#123;</span><br><span class="line"><span class="keyword">return</span> cUid.username;</span><br><span class="line">&#125;</span><br><span class="line">String user = POSIX.getUserName(uid);</span><br><span class="line">LOG.info(<span class="string">&quot;Got UserName &quot;</span> + user + <span class="string">&quot; for UID &quot;</span> + uid + <span class="string">&quot; from the native implementation&quot;</span>);</span><br><span class="line">cUid = <span class="keyword">new</span> CachedUid(user, now);</span><br><span class="line">uidCache.put(uid, cUid);</span><br><span class="line"><span class="keyword">return</span> user;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a FileInputStream that shares delete permission on the file opened,</span></span><br><span class="line"><span class="comment"> * i.e. other process can delete the file the FileInputStream is reading. Only</span></span><br><span class="line"><span class="comment"> * Windows implementation uses the native interface.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> FileInputStream <span class="title">getShareDeleteFileInputStream</span><span class="params">(File f)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!Shell.WINDOWS) &#123;</span><br><span class="line"><span class="comment">// On Linux the default FileInputStream shares delete permission</span></span><br><span class="line"><span class="comment">// on the file opened.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> FileInputStream(f);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// Use Windows native interface to create a FileInputStream that</span></span><br><span class="line"><span class="comment">// shares delete permission on the file opened.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line">FileDescriptor fd = Windows.createFile(f.getAbsolutePath(), Windows.GENERIC_READ,</span><br><span class="line">Windows.FILE_SHARE_READ | Windows.FILE_SHARE_WRITE | Windows.FILE_SHARE_DELETE,</span><br><span class="line">Windows.OPEN_EXISTING);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> FileInputStream(fd);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a FileInputStream that shares delete permission on the file opened at</span></span><br><span class="line"><span class="comment"> * a given offset, i.e. other process can delete the file the FileInputStream is</span></span><br><span class="line"><span class="comment"> * reading. Only Windows implementation uses the native interface.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> FileInputStream <span class="title">getShareDeleteFileInputStream</span><span class="params">(File f, <span class="keyword">long</span> seekOffset)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!Shell.WINDOWS) &#123;</span><br><span class="line">RandomAccessFile rf = <span class="keyword">new</span> RandomAccessFile(f, <span class="string">&quot;r&quot;</span>);</span><br><span class="line"><span class="keyword">if</span> (seekOffset &gt; <span class="number">0</span>) &#123;</span><br><span class="line">rf.seek(seekOffset);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> FileInputStream(rf.getFD());</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// Use Windows native interface to create a FileInputStream that</span></span><br><span class="line"><span class="comment">// shares delete permission on the file opened, and set it to the</span></span><br><span class="line"><span class="comment">// given offset.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line">FileDescriptor fd = NativeIO.Windows.createFile(</span><br><span class="line">f.getAbsolutePath(), NativeIO.Windows.GENERIC_READ, NativeIO.Windows.FILE_SHARE_READ</span><br><span class="line">| NativeIO.Windows.FILE_SHARE_WRITE | NativeIO.Windows.FILE_SHARE_DELETE,</span><br><span class="line">NativeIO.Windows.OPEN_EXISTING);</span><br><span class="line"><span class="keyword">if</span> (seekOffset &gt; <span class="number">0</span>)</span><br><span class="line">NativeIO.Windows.setFilePointer(fd, seekOffset, NativeIO.Windows.FILE_BEGIN);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> FileInputStream(fd);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create the specified File for write access, ensuring that it does not exist.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> f</span></span><br><span class="line"><span class="comment"> *            the file that we want to create</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> permissions</span></span><br><span class="line"><span class="comment"> *            we want to have on the file (if security is enabled)</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> AlreadyExistsException</span></span><br><span class="line"><span class="comment"> *             if the file already exists</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> *             if any other error occurred</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> FileOutputStream <span class="title">getCreateForWriteFileOutputStream</span><span class="params">(File f, <span class="keyword">int</span> permissions)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!Shell.WINDOWS) &#123;</span><br><span class="line"><span class="comment">// Use the native wrapper around open(2)</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">FileDescriptor fd = NativeIO.POSIX.open(f.getAbsolutePath(),</span><br><span class="line">NativeIO.POSIX.O_WRONLY | NativeIO.POSIX.O_CREAT | NativeIO.POSIX.O_EXCL, permissions);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> FileOutputStream(fd);</span><br><span class="line">&#125; <span class="keyword">catch</span> (NativeIOException nioe) &#123;</span><br><span class="line"><span class="keyword">if</span> (nioe.getErrno() == Errno.EEXIST) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> AlreadyExistsException(nioe);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">throw</span> nioe;</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// Use the Windows native APIs to create equivalent FileOutputStream</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">FileDescriptor fd = NativeIO.Windows.createFile(</span><br><span class="line">f.getCanonicalPath(), NativeIO.Windows.GENERIC_WRITE, NativeIO.Windows.FILE_SHARE_DELETE</span><br><span class="line">| NativeIO.Windows.FILE_SHARE_READ | NativeIO.Windows.FILE_SHARE_WRITE,</span><br><span class="line">NativeIO.Windows.CREATE_NEW);</span><br><span class="line">NativeIO.POSIX.chmod(f.getCanonicalPath(), permissions);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> FileOutputStream(fd);</span><br><span class="line">&#125; <span class="keyword">catch</span> (NativeIOException nioe) &#123;</span><br><span class="line"><span class="keyword">if</span> (nioe.getErrorCode() == <span class="number">80</span>) &#123;</span><br><span class="line"><span class="comment">// ERROR_FILE_EXISTS</span></span><br><span class="line"><span class="comment">// 80 (0x50)</span></span><br><span class="line"><span class="comment">// The file exists</span></span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> AlreadyExistsException(nioe);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">throw</span> nioe;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">synchronized</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">ensureInitialized</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!initialized) &#123;</span><br><span class="line">cacheTimeout = <span class="keyword">new</span> Configuration().getLong(<span class="string">&quot;hadoop.security.uid.cache.secs&quot;</span>, <span class="number">4</span> * <span class="number">60</span> * <span class="number">60</span>) * <span class="number">1000</span>;</span><br><span class="line">LOG.info(<span class="string">&quot;Initialized cache for UID to User mapping with a cache&quot;</span> + <span class="string">&quot; timeout of &quot;</span> + cacheTimeout / <span class="number">1000</span></span><br><span class="line">+ <span class="string">&quot; seconds.&quot;</span>);</span><br><span class="line">initialized = <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A version of renameTo that throws a descriptive exception when it fails.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> src</span></span><br><span class="line"><span class="comment"> *            The source path</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> dst</span></span><br><span class="line"><span class="comment"> *            The destination path</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> NativeIOException</span></span><br><span class="line"><span class="comment"> *             On failure.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">renameTo</span><span class="params">(File src, File dst)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!nativeLoaded) &#123;</span><br><span class="line"><span class="keyword">if</span> (!src.renameTo(dst)) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;renameTo(src=&quot;</span> + src + <span class="string">&quot;, dst=&quot;</span> + dst + <span class="string">&quot;) failed.&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">renameTo0(src.getAbsolutePath(), dst.getAbsolutePath());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">link</span><span class="params">(File src, File dst)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!nativeLoaded) &#123;</span><br><span class="line">HardLink.createHardLink(src, dst);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">link0(src.getAbsolutePath(), dst.getAbsolutePath());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A version of renameTo that throws a descriptive exception when it fails.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> src</span></span><br><span class="line"><span class="comment"> *            The source path</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> dst</span></span><br><span class="line"><span class="comment"> *            The destination path</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> NativeIOException</span></span><br><span class="line"><span class="comment"> *             On failure.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">renameTo0</span><span class="params">(String src, String dst)</span> <span class="keyword">throws</span> NativeIOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">link0</span><span class="params">(String src, String dst)</span> <span class="keyword">throws</span> NativeIOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Unbuffered file copy from src to dst without tainting OS buffer cache</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * In POSIX platform: It uses FileChannel#transferTo() which internally attempts</span></span><br><span class="line"><span class="comment"> * unbuffered IO on OS with native sendfile64() support and falls back to</span></span><br><span class="line"><span class="comment"> * buffered IO otherwise.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * It minimizes the number of FileChannel#transferTo call by passing the the src</span></span><br><span class="line"><span class="comment"> * file size directly instead of a smaller size as the 3rd parameter. This saves</span></span><br><span class="line"><span class="comment"> * the number of sendfile64() system call when native sendfile64() is supported.</span></span><br><span class="line"><span class="comment"> * In the two fall back cases where sendfile is not supported,</span></span><br><span class="line"><span class="comment"> * FileChannle#transferTo already has its own batching of size 8 MB and 8 KB,</span></span><br><span class="line"><span class="comment"> * respectively.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * In Windows Platform: It uses its own native wrapper of CopyFileEx with</span></span><br><span class="line"><span class="comment"> * COPY_FILE_NO_BUFFERING flag, which is supported on Windows Server 2008 and</span></span><br><span class="line"><span class="comment"> * above.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Ideally, we should use FileChannel#transferTo() across both POSIX and Windows</span></span><br><span class="line"><span class="comment"> * platform. Unfortunately, the</span></span><br><span class="line"><span class="comment"> * wrapper(Java_sun_nio_ch_FileChannelImpl_transferTo0) used by</span></span><br><span class="line"><span class="comment"> * FileChannel#transferTo for unbuffered IO is not implemented on Windows. Based</span></span><br><span class="line"><span class="comment"> * on OpenJDK 6/7/8 source code, Java_sun_nio_ch_FileChannelImpl_transferTo0 on</span></span><br><span class="line"><span class="comment"> * Windows simply returns IOS_UNSUPPORTED.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Note: This simple native wrapper does minimal parameter checking before copy</span></span><br><span class="line"><span class="comment"> * and consistency check (e.g., size) after copy. It is recommended to use</span></span><br><span class="line"><span class="comment"> * wrapper function like the Storage#nativeCopyFileUnbuffered() function in</span></span><br><span class="line"><span class="comment"> * hadoop-hdfs with pre/post copy checks.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> src</span></span><br><span class="line"><span class="comment"> *            The source path</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> dst</span></span><br><span class="line"><span class="comment"> *            The destination path</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">copyFileUnbuffered</span><span class="params">(File src, File dst)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (nativeLoaded &amp;&amp; Shell.WINDOWS) &#123;</span><br><span class="line">copyFileUnbuffered0(src.getAbsolutePath(), dst.getAbsolutePath());</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">FileInputStream fis = <span class="keyword">null</span>;</span><br><span class="line">FileOutputStream fos = <span class="keyword">null</span>;</span><br><span class="line">FileChannel input = <span class="keyword">null</span>;</span><br><span class="line">FileChannel output = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">fis = <span class="keyword">new</span> FileInputStream(src);</span><br><span class="line">fos = <span class="keyword">new</span> FileOutputStream(dst);</span><br><span class="line">input = fis.getChannel();</span><br><span class="line">output = fos.getChannel();</span><br><span class="line"><span class="keyword">long</span> remaining = input.size();</span><br><span class="line"><span class="keyword">long</span> position = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">long</span> transferred = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (remaining &gt; <span class="number">0</span>) &#123;</span><br><span class="line">transferred = input.transferTo(position, remaining, output);</span><br><span class="line">remaining -= transferred;</span><br><span class="line">position += transferred;</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">IOUtils.cleanup(LOG, output);</span><br><span class="line">IOUtils.cleanup(LOG, fos);</span><br><span class="line">IOUtils.cleanup(LOG, input);</span><br><span class="line">IOUtils.cleanup(LOG, fis);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">copyFileUnbuffered0</span><span class="params">(String src, String dst)</span> <span class="keyword">throws</span> NativeIOException</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;跳出原有的格局，用积极的心态，去学习新的思维方式，许多困难都能迎刃而解。——人民日报                              &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="Hadoop企业级优化" scheme="http://xubatian.cn/tags/Hadoop%E4%BC%81%E4%B8%9A%E7%BA%A7%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>hadoop组成模块之Yarn资源调度器</title>
    <link href="http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BYarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    <id>http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BYarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%99%A8/</id>
    <published>2022-01-16T13:22:00.000Z</published>
    <updated>2022-01-23T02:58:21.728Z</updated>
    
    <content type="html"><![CDATA[<p>经历时间沉淀，才能看清历史的坐标；站在高山之巅，更能领略河流的奔腾。——人民日报                                                               </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_119.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><p>前言</p><p> 你可以将yarn就理解为是一个操作系统. 他为你将来在yarn上面运行的n多个job去负责资源的管理.</p><p>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。</p><p>思考：<br>1）如何管理集群资源？<br>2）如何给任务合理分配资源？</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_213.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。</p><h1 id="Yarn基本架构"><a href="#Yarn基本架构" class="headerlink" title="Yarn基本架构"></a>Yarn基本架构</h1><p>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。</p><p>YARN主要由ResourceManager(负责整个集群资源管理)、NodeManager(负责单个节点的管理)、ApplicationMaster(负责资源的申请,容错,监控等)和Container(整个集群资源的封装,算是一个容器. 将来每个job都会跑在容器中.而container容器封装了job所要的一些资源)等组件构成,如图所示:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_135.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="Yarn工作机制"><a href="#Yarn工作机制" class="headerlink" title="Yarn工作机制"></a>Yarn工作机制</h1><p>Yarn运行机制,如图:  mapTask就是一个线程</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_122.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>工作机制详解:</p><p>​    （1）MR程序提交到客户端所在的节点。</p><p>​    （2）YarnRunner向ResourceManager申请一个Application。</p><p>​    （3）RM将该应用程序的资源路径返回给YarnRunner。</p><p>​    （4）该程序将运行所需资源提交到HDFS上。</p><p>​    （5）程序资源提交完毕后，申请运行mrAppMaster。</p><p>​    （6）RM将用户的请求初始化成一个Task。</p><p>​    （7）其中一个NodeManager领取到Task任务。</p><p>​    （8）该NodeManager创建容器Container，并产生MRAppmaster。</p><p>​    （9）Container从HDFS上拷贝资源到本地。</p><p>​    （10）MRAppmaster向RM 申请运行MapTask资源。</p><p>​    （11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。</p><p>​    （12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。</p><p>​    （13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。</p><p>​    （14）ReduceTask向MapTask获取相应分区的数据。</p><p>​    （15）程序运行完毕后，MR会向RM申请注销自己。</p><h1 id="作业提交全过程"><a href="#作业提交全过程" class="headerlink" title="作业提交全过程"></a>作业提交全过程</h1><h2 id="作业提交过程之YARN"><a href="#作业提交过程之YARN" class="headerlink" title="作业提交过程之YARN"></a>作业提交过程之YARN</h2><p>如图:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_123.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>作业提交全过程详解<br>（1）作业提交<br>第1步：Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。<br>第2步：Client向RM申请一个作业id。<br>第3步：RM给Client返回该job资源的提交路径和作业id。<br>第4步：Client提交jar包、切片信息和配置文件到指定的资源提交路径。<br>第5步：Client提交完资源后，向RM申请运行MrAppMaster。<br>（2）作业初始化<br>第6步：当RM收到Client的请求后，将该job添加到容量调度器中。<br>第7步：某一个空闲的NM领取到该Job。<br>第8步：该NM创建Container，并产生MRAppmaster。<br>第9步：下载Client提交的资源到本地。<br>（3）任务分配<br>第10步：MrAppMaster向RM申请运行多个MapTask任务资源。<br>第11步：RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。<br>（4）任务运行<br>第12步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。<br>第13步：MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。<br>第14步：ReduceTask向MapTask获取相应分区的数据。<br>第15步：程序运行完毕后，MR会向RM申请注销自己。<br>（5）进度和状态更新<br>YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。<br>（6）作业完成<br>除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</p><h2 id="作业提交过程之MapReduce"><a href="#作业提交过程之MapReduce" class="headerlink" title="作业提交过程之MapReduce"></a>作业提交过程之MapReduce</h2><p>如图所示:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_124.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="资源调度器-FIFO调度器-表示先进先出"><a href="#资源调度器-FIFO调度器-表示先进先出" class="headerlink" title="资源调度器(FIFO调度器 表示先进先出)"></a>资源调度器(FIFO调度器 表示先进先出)</h1><p>目前，Hadoop作业调度器主要有三种：FIFO Scheduler、Capacity Scheduler和Fair Scheduler。Hadoop2.7.2默认的资源调度器是Capacity Scheduler。<br>具体设置详见：yarn-default.xml文件</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;description&gt;The <span class="class"><span class="keyword">class</span> <span class="title">to</span> <span class="title">use</span> <span class="title">as</span> <span class="title">the</span> <span class="title">resource</span> <span class="title">scheduler</span>.&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">name</span>&gt;<span class="title">yarn</span>.<span class="title">resourcemanager</span>.<span class="title">scheduler</span>.<span class="title">class</span>&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">value</span>&gt;<span class="title">org</span>.<span class="title">apache</span>.<span class="title">hadoop</span>.<span class="title">yarn</span>.<span class="title">server</span>.<span class="title">resourcemanager</span>.<span class="title">scheduler</span>.<span class="title">capacity</span>.<span class="title">CapacityScheduler</span>&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="class">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br></pre></td></tr></table></figure><h2 id="1．先进先出调度器（FIFO）"><a href="#1．先进先出调度器（FIFO）" class="headerlink" title="1．先进先出调度器（FIFO）"></a>1．先进先出调度器（FIFO）</h2><p> FIFO调度器:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_125.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>Hadoop最初设计目的是支持大数据批处理作业，如日志挖掘、Web索引等作业，</p><p>为此，Hadoop仅提供了一个非常简单的调度机制：FIFO，即先来先服务，在该调度机制下，所有作业被统一提交到一个队列中，Hadoop按照提交顺序依次运行这些作业。</p><p>但随着Hadoop的普及，单个Hadoop集群的用户量越来越大，不同用户提交的应用程序往往具有不同的服务质量要求，典型的应用有以下几种：</p><p>批处理作业：这种作业往往耗时较长，对时间完成一般没有严格要求，如数据挖掘、机器学习等方面的应用程序。</p><p>交互式作业：这种作业期望能及时返回结果，如SQL查询（Hive）等。</p><p>生产性作业：这种作业要求有一定量的资源保证，如统计值计算、垃圾数据分析等。</p><p>此外，这些应用程序对硬件资源需求量也是不同的，如过滤、统计类作业一般为CPU密集型作业，而数据挖掘、机器学习作业一般为I/O密集型作业。因此，简单的FIFO调度策略不仅不能满足多样化需求，也不能充分利用硬件资源。</p><h2 id="2．容量调度器（Capacity-Scheduler）"><a href="#2．容量调度器（Capacity-Scheduler）" class="headerlink" title="2．容量调度器（Capacity Scheduler）"></a>2．容量调度器（Capacity Scheduler）</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_126.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>Capacity Scheduler Capacity Scheduler 是Yahoo开发的多用户调度器，它以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限，同时，每个用户也可设定一定的资源使用上限以防止资源滥用。而当一个队列的资源有剩余时，可暂时将剩余资源共享给其他队列。</p><p>​    总之，Capacity Scheduler 主要有以下几个特点：</p><p>①容量保证。管理员可为每个队列设置资源最低保证和资源使用上限，而所有提交到该队列的应用程序共享这些资源。</p><p>②灵活性，如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列释放的资源会归还给该队列。这种资源灵活分配的方式可明显提高资源利用率。</p><p>③多重租赁。支持多用户共享集群和多应用程序同时运行。为防止单个应用程序、用户或者队列独占集群中的资源，管理员可为之增加多重约束（比如单个应用程序同时运行的任务数等）。</p><p>④安全保证。每个队列有严格的ACL列表规定它的访问用户，每个用户可指定哪些用户允许查看自己应用程序的运行状态或者控制应用程序（比如杀死应用程序）。此外，管理员可指定队列管理员和集群系统管理员。</p><p>⑤动态更新配置文件。管理员可根据需要动态修改各种配置参数，以实现在线集群管理。</p><h2 id="3．公平调度器（Fair-Scheduler）"><a href="#3．公平调度器（Fair-Scheduler）" class="headerlink" title="3．公平调度器（Fair Scheduler）"></a>3．公平调度器（Fair Scheduler）</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_127.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_128.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_129.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>Fair Scheduler Fair Schedulere是Facebook开发的多用户调度器。<br>    <strong>公平调度器的目的是让所有的作业随着时间的推移，都能平均地获取等同的共享资源！</strong>当一个作业在运行时，它会使用整个集群但是如果有其他作业提交上来，系统会将空闲的资源分配给新的作业！每个任务大致上会获取平等数量的资源！和传统的调度策略不同的是<br>它会让小的任务在合理的时间完成，同时不会让需要长时间运行的耗费大量资源的应用挨饿！<br>    同Capacity Scheduler类似，它以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限，同时，每个用户也可设定一定的资源使用上限以防止资源滥用；当一个队列的资源有剩余时，可暂时将剩余资源共享给其他队列。<br>    当然，Fair Scheduler也存在很多与Capacity Scheduler不同之处，这主要体现在以下几个方面：<br>①资源公平共享。在每个队列中，Fair Scheduler 可选择按照FIFO、Fair或DRF策略为应用程序分配资源。其中，Fair 策略(默认)是一种基于最大最小公平算法实现的资源多路复用方式，默认情况下，每个队列内部采用该方式分配资源。这意味着，如果一个队列中有两个应用程序同时运行，则每个应用程序可得到1/2的资源；如果三个应用程序同时运行，则每个应用程序可得到1/3的资源。<br>②支持资源抢占。当某个队列中有剩余资源时，调度器会将这些资源共享给其他队列，而当该队列中有新的应用程序提交时，调度器要为它回收资源。为了尽可能降低不必要的计算浪费，调度器采用了先等待再强制回收的策略，即如果等待一段时间后尚有未归还的资源，则会进行资源抢占：从那些超额使用资源的队列中杀死一部分任务，进而释放资源。<br>③负载均衡。Fair Scheduler提供了一个基于任务数目的负载均衡机制，该机制尽可能将系统中的任务均匀分配到各个节点上。此外，用户也可以根据自己的需要设计负载均衡机制。<br>④调度策略配置灵活。Fair Scheduler允许管理员为每个队列单独设置调度策略（当前支持FIFO、Fair或DRF三种）。<br>⑤提高小应用程序响应时间。由于采用了最大最小公平算法，小作业可以快速获取资源并运行完成</p><h1 id="容量调度器多队列提交案例"><a href="#容量调度器多队列提交案例" class="headerlink" title="容量调度器多队列提交案例"></a>容量调度器多队列提交案例</h1><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>​    Yarn默认的容量调度器是一条单队列的调度器，在实际使用中会出现单个任务阻塞整个队列的情况。同时，随着业务的增长，公司需要分业务限制集群使用率。这就需要我们按照业务种类配置多条任务队列。</p><h2 id="配置多队列的容量调度器"><a href="#配置多队列的容量调度器" class="headerlink" title="配置多队列的容量调度器"></a>配置多队列的容量调度器</h2><p>默认Yarn的配置下，容量调度器只有一条Default队列。在capacity-scheduler.xml中可以配置多条队列，并降低default队列资源占比：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="keyword">default</span>,hive&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;</span><br><span class="line">    <span class="function">The queues at the <span class="keyword">this</span> <span class="title">level</span> <span class="params">(root is the root queue)</span>.</span></span><br><span class="line"><span class="function">  &lt;/description&gt;</span></span><br><span class="line"><span class="function">&lt;/property&gt;</span></span><br><span class="line"><span class="function">&lt;property&gt;</span></span><br><span class="line"><span class="function">  &lt;name&gt;yarn.scheduler.capacity.root.<span class="keyword">default</span>.capacity&lt;/name&gt;</span></span><br><span class="line"><span class="function">  &lt;value&gt;40&lt;/value&gt;</span></span><br><span class="line"><span class="function">&lt;/property&gt;</span></span><br></pre></td></tr></table></figure><p>同时为新加队列添加必要属性：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--队列目标资源百分比，所有队列相加必须等于<span class="number">100</span>--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.capacity&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="number">60</span>&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;!--队列最大资源百分比--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.maximum-capacity&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="number">100</span>&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;!—单用户可用队列资源占比--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.user-limit-factor&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="number">1</span>&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;!--队列状态（RUNNING或STOPPING）--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.state&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;RUNNING&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;!—队列允许哪些用户提交--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.acl_submit_applications&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;!—队列允许哪些用户管理--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.acl_administer_queue&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br></pre></td></tr></table></figure><p>在配置完成后，重启Yarn，就可以看到两条队列：</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_130.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_131.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_132.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="向Hive队列提交任务"><a href="#向Hive队列提交任务" class="headerlink" title="向Hive队列提交任务"></a>向Hive队列提交任务</h2><p>默认的任务提交都是提交到default队列的。如果希望向其他队列提交任务，需要在Driver中声明：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WcDrvier</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">        configuration.set(<span class="string">&quot;mapred.job.queue.name&quot;</span>, <span class="string">&quot;hive&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1. 获取一个Job实例</span></span><br><span class="line">        Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 设置类路径</span></span><br><span class="line">        job.setJarByClass(WcDrvier.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 设置Mapper和Reducer</span></span><br><span class="line">        job.setMapperClass(WcMapper.class);</span><br><span class="line">        job.setReducerClass(WcReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 设置Mapper和Reducer的输出类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setCombinerClass(WcReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5. 设置输入输出文件</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//6. 提交Job</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>默认的任务提交都是提交到default队列的。如果希望向其他队列提交任务，需要在Driver中声明：</p><p>这样，这个任务在集群提交时，就会提交到hive队列：</p><p>图为公平调度器</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_133.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="任务的推测执行"><a href="#任务的推测执行" class="headerlink" title="任务的推测执行"></a>任务的推测执行</h2><h3 id="1．作业完成时间取决于最慢的任务完成时间"><a href="#1．作业完成时间取决于最慢的任务完成时间" class="headerlink" title="1．作业完成时间取决于最慢的任务完成时间"></a>1．作业完成时间取决于最慢的任务完成时间</h3><p>一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。</p><p>思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？</p><h3 id="2．推测执行机制"><a href="#2．推测执行机制" class="headerlink" title="2．推测执行机制"></a>2．推测执行机制</h3><p>发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。</p><h3 id="3．执行推测任务的前提条件"><a href="#3．执行推测任务的前提条件" class="headerlink" title="3．执行推测任务的前提条件"></a>3．执行推测任务的前提条件</h3><p>（1）每个Task只能有一个备份任务</p><p>（2）当前Job已完成的Task必须不小于0.05（5%）</p><p>（3）开启推测执行参数设置。mapred-site.xml文件中默认是打开的。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.speculative&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="keyword">true</span>&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;If <span class="keyword">true</span>, then multiple instances of some map tasks may be executed in parallel.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="keyword">true</span>&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;If <span class="keyword">true</span>, then multiple instances of some reduce tasks may be executed in parallel.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h3 id="4．不能启用推测执行机制情况"><a href="#4．不能启用推测执行机制情况" class="headerlink" title="4．不能启用推测执行机制情况"></a>4．不能启用推测执行机制情况</h3><p>  （1）任务间存在严重的负载倾斜；</p><p>  （2）特殊任务，比如任务向数据库中写数据。</p><h3 id="5．算法原理，如图所示"><a href="#5．算法原理，如图所示" class="headerlink" title="5．算法原理，如图所示"></a>5．算法原理，如图所示</h3><p>推测执行算法原理</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_134.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="Yarn常用命令"><a href="#Yarn常用命令" class="headerlink" title="Yarn常用命令"></a>Yarn常用命令</h1><p>Yarn状态的查询，除了可以在hadoop103:8088页面查看外，还可以通过命令操作。常见的命令操作如下所示：<br>需求：执行WordCount案例，并用Yarn命令查看任务运行情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ myhadoop.sh start</span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</span><br></pre></td></tr></table></figure><h2 id="yarn-application查看任务"><a href="#yarn-application查看任务" class="headerlink" title="yarn application查看任务"></a>yarn application查看任务</h2><p>（1）列出所有Application：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn application -list</span><br><span class="line">2021-02-06 10:21:19,238 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):0</span><br><span class="line">                Application-Id    Application-Name    Application-Type      User     Queue             State       Final-State       Progress                       Tracking-URL</span><br></pre></td></tr></table></figure><p>（2）根据Application状态过滤：yarn application -list -appStates （所有状态：ALL、NEW、NEW_SAVING、SUBMITTED、ACCEPTED、RUNNING、FINISHED、FAILED、KILLED）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn application -list -appStates FINISHED</span><br><span class="line">2021-02-06 10:22:20,029 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Total number of applications (application-types: [], states: [FINISHED] and tags: []):1</span><br><span class="line">                Application-Id    Application-Name    Application-Type      User     Queue             State       Final-State       Progress                       Tracking-URL</span><br><span class="line">application_1612577921195_0001          word count           MAPREDUCE   shangbaishuyao   default          FINISHED         SUCCEEDED           100%http://hadoop102:19888/jobhistory/job/job_1612577921195_0001</span><br></pre></td></tr></table></figure><p>（3）Kill掉Application：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn application -kill application_1612577921195_0001</span><br><span class="line">2021-02-06 10:23:48,530 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Application application_1612577921195_0001 has already finished</span><br></pre></td></tr></table></figure><h2 id="yarn-logs查看日志"><a href="#yarn-logs查看日志" class="headerlink" title="yarn logs查看日志"></a>yarn logs查看日志</h2><p>（1）查询Application日志：yarn logs -applicationId <ApplicationId></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn logs -applicationId application_1612577921195_0001</span><br></pre></td></tr></table></figure><p>（2）查询Container日志：yarn logs -applicationId <ApplicationId> -containerId <ContainerId> </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn logs -applicationId application_1612577921195_0001 -containerId container_1612577921195_0001_01_000001</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="yarn-applicationattempt查看尝试运行的任务"><a href="#yarn-applicationattempt查看尝试运行的任务" class="headerlink" title="yarn applicationattempt查看尝试运行的任务"></a>yarn applicationattempt查看尝试运行的任务</h2><p>（1）列出所有Application尝试的列表：yarn applicationattempt -list <ApplicationId></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn applicationattempt -list application_1612577921195_0001</span><br><span class="line">2021-02-06 10:26:54,195 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Total number of application attempts :1</span><br><span class="line">         ApplicationAttempt-Id               State                    AM-Container-Id                       Tracking-URL</span><br><span class="line">appattempt_1612577921195_0001_000001            FINISHEDcontainer_1612577921195_0001_01_000001http://hadoop103:8088/proxy/application_1612577921195_0001/</span><br></pre></td></tr></table></figure><p>（2）打印ApplicationAttemp状态：yarn applicationattempt -status <ApplicationAttemptId></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn applicationattempt -status appattempt_1612577921195_0001_000001</span><br><span class="line">2021-02-06 10:27:55,896 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Application Attempt Report : </span><br><span class="line">ApplicationAttempt-Id : appattempt_1612577921195_0001_000001</span><br><span class="line">State : FINISHED</span><br><span class="line">AMContainer : container_1612577921195_0001_01_000001</span><br><span class="line">Tracking-URL : http://hadoop103:8088/proxy/application_1612577921195_0001/</span><br><span class="line">RPC Port : 34756</span><br><span class="line">AM Host : hadoop104</span><br><span class="line">Diagnostics :</span><br></pre></td></tr></table></figure><h2 id="yarn-container查看容器"><a href="#yarn-container查看容器" class="headerlink" title="yarn container查看容器"></a>yarn container查看容器</h2><p>（1）列出所有Container：yarn container -list <ApplicationAttemptId></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn container -list appattempt_1612577921195_0001_000001</span><br><span class="line">2021-02-06 10:28:41,396 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Total number of containers :0</span><br><span class="line">                  Container-Id          Start Time         Finish Time               State                Host   Node Http Address</span><br></pre></td></tr></table></figure><p>​    （2）打印Container状态：    yarn container -status <ContainerId></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn container -status container_1612577921195_0001_01_000001</span><br><span class="line">2021-02-06 10:29:58,554 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Container with id &#x27;container_1612577921195_0001_01_000001&#x27; doesn&#x27;t exist in RM or Timeline Server.</span><br></pre></td></tr></table></figure><p>​    注：只有在任务跑的途中才能看到container的状态</p><h2 id="yarn-node查看节点状态"><a href="#yarn-node查看节点状态" class="headerlink" title="yarn node查看节点状态"></a>yarn node查看节点状态</h2><p>​    列出所有节点：yarn node -list -all</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn node -list -all</span><br><span class="line">2021-02-06 10:31:36,962 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Total Nodes:3</span><br><span class="line">         Node-Id     Node-StateNode-Http-AddressNumber-of-Running-Containers</span><br><span class="line"> hadoop103:38168        RUNNING   hadoop103:8042                           0</span><br><span class="line"> hadoop102:42012        RUNNING   hadoop102:8042                           0</span><br><span class="line"> hadoop104:39702        RUNNING   hadoop104:8042                           0</span><br></pre></td></tr></table></figure><h2 id="yarn-rmadmin更新配置"><a href="#yarn-rmadmin更新配置" class="headerlink" title="yarn rmadmin更新配置"></a>yarn rmadmin更新配置</h2><p>加载队列配置：yarn rmadmin -refreshQueues</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn rmadmin -refreshQueues</span><br><span class="line">2021-02-06 10:32:03,331 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8033</span><br></pre></td></tr></table></figure><h2 id="yarn-queue查看队列"><a href="#yarn-queue查看队列" class="headerlink" title="yarn queue查看队列"></a>yarn queue查看队列</h2><p>​    打印队列信息：yarn queue -status <QueueName></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn queue -status default</span><br><span class="line">2021-02-06 10:32:33,403 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Queue Information : </span><br><span class="line">Queue Name : default</span><br><span class="line">State : RUNNING</span><br><span class="line">Capacity : 100.0%</span><br><span class="line">Current Capacity : .0%</span><br><span class="line">Maximum Capacity : 100.0%</span><br><span class="line">Default Node Label expression : &lt;DEFAULT_PARTITION&gt;</span><br><span class="line">Accessible Node Labels : *</span><br><span class="line">Preemption : disabled</span><br><span class="line">Intra-queue Preemption : disabled</span><br></pre></td></tr></table></figure><h1 id="Yarn生产环境核心参数"><a href="#Yarn生产环境核心参数" class="headerlink" title="Yarn生产环境核心参数"></a>Yarn生产环境核心参数</h1><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_214.png" width = "" height = "" alt="xubatian的博客" align="center" />]]></content>
    
    
    <summary type="html">&lt;p&gt;经历时间沉淀，才能看清历史的坐标；站在高山之巅，更能领略河流的奔腾。——人民日报                                                               &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="Yarn" scheme="http://xubatian.cn/tags/Yarn/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop数据压缩</title>
    <link href="http://xubatian.cn/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/"/>
    <id>http://xubatian.cn/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/</id>
    <published>2022-01-16T12:38:22.000Z</published>
    <updated>2022-01-23T02:58:21.728Z</updated>
    
    <content type="html"><![CDATA[<p>不要小瞧心态的改变，那会让你重获新生。——人民日报                                            </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_112.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><p>前言</p><p>虽然hadoop的HDFS理论上可以存储海量的数据.即使磁盘不够也是可以通过增加机器的方式扩展存储空间. 但是加机器是要花钱买的呀.所以,那群无聊的人就使用到了压缩的方式. 但是压缩的方式有很多种. 咱们mapreduce是需要切片的. 所以有哪些压缩是可以支持切片的呢?那些压缩格式的压缩速度更快呢? 那些压缩能够将数据压缩的更小呢? … </p><h1 id="压缩概述"><a href="#压缩概述" class="headerlink" title="压缩概述"></a>压缩概述</h1><p>压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时，I/O操作、网络数据传输、Shuffle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此，使用数据压缩显得非常重要。</p><p>鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源,最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启用压缩。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价。</p><h1 id="压缩策略和原则"><a href="#压缩策略和原则" class="headerlink" title="压缩策略和原则"></a>压缩策略和原则</h1><p>压缩是提高Hadoop运行效率的一种优化策略。<br>通过对Mapper、Reducer运行过程的数据进行压缩,以减少磁盘IO,提高MR程序运行速度。<br>注意：采用压缩技术减少了磁盘IO，但同时增加了CPU运算负担.所以，压缩特性运用得当能提高性能,但是运用不当也可能降低性能.</p><p>压缩基本原则：<br>  (1) 运算密集型的job, 少用压缩<br>  (2) IO密集型的job, 多用压缩</p><h1 id="MR支持的压缩编码"><a href="#MR支持的压缩编码" class="headerlink" title="MR支持的压缩编码"></a>MR支持的压缩编码</h1><p>Hadoop默认使用的是DEFLATE压缩格式 , bzip2 hadoop自带的</p><table><thead><tr><th>压缩格式</th><th>hadoop自带？</th><th>算法</th><th>文件扩展名</th><th>是否可切分</th><th>换成压缩格式后，原来的程序是否需要修改</th></tr></thead><tbody><tr><td>DEFLATE</td><td>是，直接使用</td><td>DEFLATE</td><td>.deflate</td><td>否</td><td>和文本处理一样，不需要修改</td></tr><tr><td>Gzip</td><td>是，直接使用</td><td>DEFLATE</td><td>.gz</td><td>否</td><td>和文本处理一样，不需要修改</td></tr><tr><td>bzip2</td><td>是，直接使用</td><td>bzip2</td><td>.bz2</td><td>是</td><td>和文本处理一样，不需要修改</td></tr><tr><td>LZO</td><td>否，需要安装</td><td>LZO</td><td>.lzo</td><td>是</td><td>需要建索引，还需要指定输入格式</td></tr><tr><td>Snappy</td><td>否，需要安装</td><td>Snappy</td><td>.snappy</td><td>否</td><td>和文本处理一样，不需要修改</td></tr></tbody></table><p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示。</p><table><thead><tr><th>压缩格式</th><th>对应的编码/解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><p>压缩性能的比较</p><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr></tbody></table><p><a href="http://google.github.io/snappy/">http://google.github.io/snappy/</a><br>On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.</p><h1 id="压缩方式选择"><a href="#压缩方式选择" class="headerlink" title="压缩方式选择"></a>压缩方式选择</h1><h2 id="Gzip压缩"><a href="#Gzip压缩" class="headerlink" title="Gzip压缩"></a>Gzip压缩</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_113.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="Bzip2压缩"><a href="#Bzip2压缩" class="headerlink" title="Bzip2压缩"></a>Bzip2压缩</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_114.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="Lzo压缩"><a href="#Lzo压缩" class="headerlink" title="Lzo压缩"></a>Lzo压缩</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_115.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="Snappy压缩"><a href="#Snappy压缩" class="headerlink" title="Snappy压缩"></a>Snappy压缩</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_116.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="压缩位置选择"><a href="#压缩位置选择" class="headerlink" title="压缩位置选择"></a>压缩位置选择</h1><p>压缩可以在MapReduce作用的任意阶段启用,如图:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_117.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="压缩参数配置"><a href="#压缩参数配置" class="headerlink" title="压缩参数配置"></a>压缩参数配置</h1><p>要在Hadoop中启用压缩，可以配置如下参数：表4-10 配置参数</p><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs  （在core-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress（在mapred-site.xml中配置）</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec（在mapred-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）</td><td>org.apache.hadoop.io.compress. DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置）</td><td>RECORD</td><td>reducer输出</td><td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td></tr></tbody></table><h1 id="压缩实操案例"><a href="#压缩实操案例" class="headerlink" title="压缩实操案例"></a>压缩实操案例</h1><h2 id="数据流的压缩和解压缩"><a href="#数据流的压缩和解压缩" class="headerlink" title="数据流的压缩和解压缩"></a>数据流的压缩和解压缩</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_118.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>测试一下如下压缩方式: 表4-11</p><table><thead><tr><th>DEFLATE</th><th>org.apache.hadoop.io.compress.DefaultCodec</th></tr></thead><tbody><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr></tbody></table><h2 id="案例代码"><a href="#案例代码" class="headerlink" title="案例代码"></a>案例代码</h2><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/test/TestCompress.java">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/test/TestCompress.java</a></p><h1 id="Map输出端采用压缩"><a href="#Map输出端采用压缩" class="headerlink" title="Map输出端采用压缩"></a>Map输出端采用压缩</h1><p>即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可，我们来看下代码怎么设置。</p><h2 id="1．给大家提供的Hadoop源码支持的压缩格式有：BZip2Codec-、DefaultCodec"><a href="#1．给大家提供的Hadoop源码支持的压缩格式有：BZip2Codec-、DefaultCodec" class="headerlink" title="1．给大家提供的Hadoop源码支持的压缩格式有：BZip2Codec 、DefaultCodec"></a>1．给大家提供的Hadoop源码支持的压缩格式有：BZip2Codec 、DefaultCodec</h2><p>代码案例:</p><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/WordCountDriver.java">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/WordCountDriver.java</a></p><h2 id="2．Mapper保持不变"><a href="#2．Mapper保持不变" class="headerlink" title="2．Mapper保持不变"></a>2．Mapper保持不变</h2><p>代码案例:</p><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/WordCountMapper.java">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/WordCountMapper.java</a></p><h2 id="3．Reducer保持不变"><a href="#3．Reducer保持不变" class="headerlink" title="3．Reducer保持不变"></a>3．Reducer保持不变</h2><p>代码案例:</p><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/WordCountReducer.java">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/WordCountReducer.java</a></p><h1 id="Reduce输出端采用压缩-Mapper和Reducer保持不变"><a href="#Reduce输出端采用压缩-Mapper和Reducer保持不变" class="headerlink" title="Reduce输出端采用压缩 Mapper和Reducer保持不变"></a>Reduce输出端采用压缩 Mapper和Reducer保持不变</h1><p>基于Map输出端采用压缩案例处理。代码案例:</p><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/WordCountDriver.java">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/WordCountDriver.java</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;不要小瞧心态的改变，那会让你重获新生。——人民日报                                            &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="hadoop压缩" scheme="http://xubatian.cn/tags/hadoop%E5%8E%8B%E7%BC%A9/"/>
    
  </entry>
  
  <entry>
    <title>hadoop的计数器应用和数据清洗</title>
    <link href="http://xubatian.cn/hadoop%E7%9A%84%E8%AE%A1%E6%95%B0%E5%99%A8%E5%BA%94%E7%94%A8%E5%92%8C%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/"/>
    <id>http://xubatian.cn/hadoop%E7%9A%84%E8%AE%A1%E6%95%B0%E5%99%A8%E5%BA%94%E7%94%A8%E5%92%8C%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/</id>
    <published>2022-01-16T10:58:45.000Z</published>
    <updated>2022-01-23T02:58:21.660Z</updated>
    
    <content type="html"><![CDATA[<p>真正的勇敢，不是只知道进，不知道退；而是在应进的时候，不退缩；该退的时候，不怕人嘲笑，敢于退让。——人民日报                                            </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.baidu.cn_109.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="计数器应用"><a href="#计数器应用" class="headerlink" title="计数器应用"></a>计数器应用</h1><p>Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。<br>1.计数器API<br>（1）采用枚举的方式统计计数<br>            enum MyCounter{MALFORORMED,NORMAL}<br>            //对枚举定义的自定义计数器加1<br>            context.getCounter(MyCounter.MALFORORMED).increment(1);<br>（2）采用计数器组、计数器名称的方式统计<br>            context.getCounter(“counterGroup”, “counter”).increment(1);<br>            组名和计数器名称随便起，但最好有意义。<br>（3）计数结果在程序运行后的控制台上查看。</p><ol start="2"><li>计数器案例实操<br>详见数据清洗案例。 案例代码: <a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/mapJoin/MapJoinMapper.java">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/mapJoin/MapJoinMapper.java</a></li></ol><p><strong>计数器, 看看这个方法被调用了多少次</strong></p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_108.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_109.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_110.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="数据清洗（ETL）"><a href="#数据清洗（ETL）" class="headerlink" title="数据清洗（ETL）"></a>数据清洗（ETL）</h1><p>概念</p><p>在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序。</p><h2 id="数据清洗案例实操-简单解析版"><a href="#数据清洗案例实操-简单解析版" class="headerlink" title="数据清洗案例实操-简单解析版"></a>数据清洗案例实操-简单解析版</h2><p>需求<br>去除日志中字段个数小于等于11的日志</p><p>需求分析<br>    需要在Map阶段对输入的数据根据规则进行过滤清洗。</p><p>案例代码</p><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/ETL/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/ETL/</a></p><h2 id="数据清洗案例实操-复杂解析版"><a href="#数据清洗案例实操-复杂解析版" class="headerlink" title="数据清洗案例实操-复杂解析版"></a>数据清洗案例实操-复杂解析版</h2><p>需求<br>对Web访问日志中的各字段识别切分，去除日志中不合法的记录。根据清洗规则，输出过滤后的数据。</p><p>需求分析<br>    需要在Map阶段对输入的数据根据规则进行过滤清洗。</p><p>案例代码</p><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/ETL2/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/ETL2/</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;真正的勇敢，不是只知道进，不知道退；而是在应进的时候，不退缩；该退的时候，不怕人嘲笑，敢于退让。——人民日报                                            &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="hadoop计数器应用" scheme="http://xubatian.cn/tags/hadoop%E8%AE%A1%E6%95%B0%E5%99%A8%E5%BA%94%E7%94%A8/"/>
    
    <category term="hadoop数据清洗" scheme="http://xubatian.cn/tags/hadoop%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/"/>
    
  </entry>
  
  <entry>
    <title>hadoop模块组成之Join多种应用</title>
    <link href="http://xubatian.cn/hadoop%E6%A8%A1%E5%9D%97%E7%BB%84%E6%88%90%E4%B9%8BJoin%E5%A4%9A%E7%A7%8D%E5%BA%94%E7%94%A8/"/>
    <id>http://xubatian.cn/hadoop%E6%A8%A1%E5%9D%97%E7%BB%84%E6%88%90%E4%B9%8BJoin%E5%A4%9A%E7%A7%8D%E5%BA%94%E7%94%A8/</id>
    <published>2022-01-15T16:50:47.000Z</published>
    <updated>2022-01-23T02:58:21.708Z</updated>
    
    <content type="html"><![CDATA[<p> 在你坚持不住的时候，记得告诉自己：想一千次，不如去做一次。华丽地跌倒，胜过无谓地徘徊。任何时候你不放弃，一切都还有可能。只要你满怀希望，就会所向披靡。——人民日报                  </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.baidu.cn_108.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="Join多种应用"><a href="#Join多种应用" class="headerlink" title="Join多种应用"></a>Join多种应用</h3><h4 id="Reduce-Join"><a href="#Reduce-Join" class="headerlink" title="Reduce Join"></a>Reduce Join</h4><h5 id="Reduce-Join工作原理"><a href="#Reduce-Join工作原理" class="headerlink" title="Reduce Join工作原理"></a>Reduce Join工作原理</h5><p>Map端的主要工作：</p><p>​        为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key,其余部分和新加的标志作为value，最后进行输出。</p><p>Reduce端的主要工作：</p><p>​        在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录(在Map阶段已经打标<br>志)分开，最后进行合并就ok了。</p><h5 id="Reduce-Join案例实操及代码"><a href="#Reduce-Join案例实操及代码" class="headerlink" title="Reduce Join案例实操及代码"></a>Reduce Join案例实操及代码</h5><p>通过将关联条件作为Map输出的key，将两表满足Join条件的数据并携带数据所来源的文件信息，发往同一个ReduceTask，在Reduce中进行数据的串联，如图所示:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_106.png" width = "" height = "" alt="xubatian的博客" align="center" /><p><strong>案例代码</strong>: <a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/reduceJoin/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/reduceJoin/</a></p><h5 id="Reduce-Join的缺陷及解决方案"><a href="#Reduce-Join的缺陷及解决方案" class="headerlink" title="Reduce Join的缺陷及解决方案"></a>Reduce Join的缺陷及解决方案</h5><p>缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜。</p><p> 解决方案: Map端实现数据合并</p><h4 id="Map-Join"><a href="#Map-Join" class="headerlink" title="Map Join"></a>Map Join</h4><h5 id="mapJoin概念"><a href="#mapJoin概念" class="headerlink" title="mapJoin概念"></a>mapJoin概念</h5><p>1．使用场景<br>Map Join适用于一张表十分小、一张表很大的场景。<br>2．优点<br>思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？<br>在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。<br>3．具体办法：采用DistributedCache<br>    （1）在Mapper的setup阶段，将文件读取到缓存集合中。<br>    （2）在驱动函数中加载缓存。<br>// 缓存普通文件到Task运行节点。<br>job.addCacheFile(new URI(“file://e:/cache/pd.txt”));</p><h5 id="Map-Join案例实操及代码"><a href="#Map-Join案例实操及代码" class="headerlink" title="Map Join案例实操及代码"></a>Map Join案例实操及代码</h5><p><strong>案例代码</strong>:<a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/mapJoin/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/mapJoin/</a></p><p>需求分析</p><p>MapJoin适用于关联表中有小表的情形</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_107.png" width = "" height = "" alt="xubatian的博客" align="center" />]]></content>
    
    
    <summary type="html">&lt;p&gt; 在你坚持不住的时候，记得告诉自己：想一千次，不如去做一次。华丽地跌倒，胜过无谓地徘徊。任何时候你不放弃，一切都还有可能。只要你满怀希望，就会所向披靡。——人民日报                  &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="mapJoin" scheme="http://xubatian.cn/tags/mapJoin/"/>
    
    <category term="ReduceJoin" scheme="http://xubatian.cn/tags/ReduceJoin/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce框架原理之InputFormat数据输入</title>
    <link href="http://xubatian.cn/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86%E4%B9%8BInputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5/"/>
    <id>http://xubatian.cn/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86%E4%B9%8BInputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5/</id>
    <published>2022-01-14T16:20:06.000Z</published>
    <updated>2022-01-23T02:58:21.703Z</updated>
    
    <content type="html"><![CDATA[<p>骑假马练不出真演技。流量转瞬即逝，常量才能长青。 ——人民日报                             </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_94.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><p>前言</p><p>上文 对mapreduce的概述 简单了解什么是MapReduce? </p><p>MapReduce是hadoop解决大数据计算问题的而落地的一个计算引擎. 他是一个计算框架.</p><p>mapreduce如何做到海量数据的计算的呢?</p><p>mapreduce的流程是什么样子的呢?</p><p>mapreduce的有哪些部分组成的的?</p><p>图示:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/hadoop/mm2.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>如上图所示, 这就是完整的mapreduce 的数据流图.</p><p>首先我们知道了hadoop的hdfs是将海量的数据进行了存储.  而hadoop的mapreduce是将存储的海量数据进行计算得出我们想要的结果.</p><p>那么他要计算这些数据.  首先他得将数据输入到我们的计算引擎mapreduce当中, 即使用Inputformat将数据导入到mapreduce里面来, 然后通过mapTask将数据打散,即map任务, 再通过reduceTask任务将数据聚合,即reduce任务. 最后通过outputFormat将计算的结果输出到某个文件或者某个数据库中.</p><p>那么由此可知. 整个mapreduce是由. Inputformart输入数据.  MapTask 打散数据. ReduceTask聚合数据. Outputformart输出结果.这几个部分组成的. 所以,要知道mapreduce的框架原理.需要学习组成mapreduce的四个环节.</p><p>那么这四个环节分别叫什么呢?</p><p><strong>InputFormat数据输入—–&gt;MapTask工作机制,Shuffle工作机制,ReduceTask工作机制—&gt;OutputFormat数据输出.</strong></p><p>着整个过程成为 mapreduce过程. 内容分这几个部分. 当然,这几部分也是很笼统的. 里面设计具体的很多概念.</p><h1 id="MapReduce框架原理之InputFormat数据输入"><a href="#MapReduce框架原理之InputFormat数据输入" class="headerlink" title="MapReduce框架原理之InputFormat数据输入"></a>MapReduce框架原理之InputFormat数据输入</h1><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/hadoop/mm.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>InputFormat, 这个是整个MapReduce里面非常关键的一个对象.</p><p>什么是InputFormat呢? 就是你输入到这个mapper里面的数据其实就是由InputFormat来负责的. 比如我怎么从文件里面去读这个数据. 这里还涉及到是否要切片这个概念.</p><p>MapReduce数据流是比较简单的.一个是mapper阶段. 一个是reduce阶段. 在mapper阶段前面就有一个InputFormat帮我们去读数据. 在Reducer后面有一个OutPutFormat帮我们去写数据.在mapper与reducer中间就是我们讲的shuffle过程.大部分工作都是在shuffle里面做的.</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/hadoop/mm2.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>那么 InputFormat他又是如何拿数据呢? 这涉及到了切片机制. 那么什么是切片呢?</p><p>我数据是按照块(block)的的方式去存到HDFS上的. 那么既然他是按照块存储的. 假设配置文件设置128M为一块,一个300M文件分成了三个块.可能存到不同的服务器上. 那么我mapreduce也是按照块为单位去拿数据吗? 不是的, 他是将这128M的块, 按照我们配置的配置文件的切片大小去获取数据. 比如 我们设置100M 为一个切片. 那么这个128M的块, 就会有两个切片.即100M 和 28M. mapreduce启动job任务去拿数据他是并行的, 所以一个mapTask只会获取一个切片,所以,他会启动两个mapTask同时去拿这两个切片.</p><p>注意: 切片不是物理上将块切开的,而是逻辑上切开的. </p><p>总结: 切片和InputFormat有啥联系呢?是在切片信息的时候涉及到. 要知道我们的切片是由我们InputFormat来负责的.</p><p>既然 切片是由Inputformat负责的,所以需要了解什么是切片.</p><h3 id="切片与MapTask并行度决定机制"><a href="#切片与MapTask并行度决定机制" class="headerlink" title="切片与MapTask并行度决定机制"></a>切片与MapTask并行度决定机制</h3><p>切片不从物理上切开,比如说,我有一个两百兆的文件.我上传到HDFS以后. 假如说的我一块的大小是128M. 那么这个200M的文件上传到HDFS以后被切成了两块了.<br>第一块128M,第二块72M. 到此为止只是块的概念.<br>接下来我就要通过map来分析你这个数据了.正常情况下,其实你的一个块的数据要交给mapTask去处理.但是严格意义上并不是这样的.而是一个切片要交给一个mapTask来处理.<br>那么这个切片是怎么来的呢?<br>切片和块的大小有点关系,但是我们可以自己去设置有多大. 比如假设我设置我的切片大小是64M. 那么对于128M块大小的数据在真正进入到map要处理的时候.会把这个128M的块的数据从逻辑上切成两片.<br>第一片交给一个mapTask去处理.<br>第二片也交给一个mapTask去处理.<br>所以说,严格来说不是一个块的数据交给mapTask.而是一个切片的数据交给一个mapTask.</p><p>72M的也是切成两片.<br>这里我们所说的切只是逻辑上的,这个块还是整个的一个块,128M.他是不会从物理上给你切开的. 只是逻辑上的.<br> 一个切片,一个mapTask.  注意: 默认情况下切片的大小和块的大小是一样的.<br>也就意味着不自己设置的情况下.他的一个切片的大小也是128M. 正好我的默认情况下块的大小是一样的.</p><p>那么问题来了, 我是不是一个切片越大,mapTask越多,我并行能力越强,是否我的性能就越高呢?</p><p><strong>问题引出</strong></p><p>MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度。<br>思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？</p><p><strong>MapTask并行度决定机制</strong></p><p>数据块：Block是HDFS物理上把数据分成一块一块。<br>数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。(逻辑上对数据块进行划分,让多个mapTask并行处理数据块的不同切片部分)</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/hadoop/mm4.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>假如我有300M的数据.假设切片大小我设置为100M. 块大小设置为128M. 那么这个300M的文件将来我存到dataNode上面的时候,如上图. 128M一块 ,128M一块, 44M一块.<br>块大小还是按照128M. 但是我的切片大小是100M. 那么将来我在处理我128M的数据的时候,那我就要从逻辑上将我的128M块 切片分成100M 和 28M.  其中这100M我交给mapTask去处理. 那么我剩下的28M怎么办呢? 他不是在往后面找72M数据合起来. 他有一个切片的概念. 切片他是不考虑数据的整体性. 正常情况下他是以块来作为一个单位来切的.  所以hadoop他默认就是按照128M来切的.这样就不用了跨机器读你的数据了.</p><p>记住: 你有多少个切片,将来你就有多少个mapTask.  每一个切片都分配一个mapTask来进行并行处理.   默认情况下切片大小就是你的块大小.</p><h3 id="通过源码查看切片机制"><a href="#通过源码查看切片机制" class="headerlink" title="通过源码查看切片机制"></a>通过源码查看切片机制</h3><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/hadoop/mm5.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="数据倾斜问题"><a href="#数据倾斜问题" class="headerlink" title="数据倾斜问题"></a>数据倾斜问题</h3><p>我切片,剩余的长度除以切片大小是大于1.1的,那我就接着去切. 如果是小于等于1.1就不切了,就是一块了. 假如我默认切片大小是128M,但是我的数据块是129M,这样我就不切了吗? 这就要看他是否大于默认切片大小(128M)的1.1倍了.  但是为什么不大于1.1我就不切了呢? 比如说我默认切片大小是32M,那么我32*1.1=35.2M   那就是只要我切片大小&lt;=35.3 我就不切了.  比如说我有一个35.2M的数据,如果说没有这个切片大小&lt;= 默认切片大小的1.1倍就不切的这个公式的话. 而是完全按照32M来切.那我就切出来一个32和一个3.2M的数据.这就是两个切片了. 两个切片将来就需要交给两个mapTask去处理. 那么负责处理32M切片的mapTask是负责3.2M切片的mapTask的数据的10倍. 这就导致一个mapTask负责数据量比较大,另一个负责数据量比较小.这就出现了数据倾斜的问题.</p><h3 id="Job提交流程源码和切片源码详解"><a href="#Job提交流程源码和切片源码详解" class="headerlink" title="Job提交流程源码和切片源码详解"></a>Job提交流程源码和切片源码详解</h3><p>所谓的Job提交流程就是当我们这个job提交之后,它后续做了哪些事情?</p><p>即,我们提交job之后,MapTask执行之前.我们程序都做了什么.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">Job提交流程(MapTask执行之前):</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> 在驱动类中job.waitForCompletion(<span class="keyword">true</span>) 提交Job</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> 执行submit()方法</span><br><span class="line">   [<span class="number">1</span>]. ensureState(JobState.DEFINE);  确认Job的状态</span><br><span class="line">   [<span class="number">2</span>]. setUseNewAPI(); 设置使用新API</span><br><span class="line">   [<span class="number">3</span>]. connect();</span><br><span class="line">   ① <span class="keyword">return</span> <span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line">   ② 调用构造器里面的重要方法initialize(jobTrackAddr, conf);</span><br><span class="line">      (<span class="number">1</span>). clientProtocol = provider.create(conf); 获取Job的运行方式(本地 LocalJobRunner / yarn YARNRunner)</span><br><span class="line"></span><br><span class="line">   [<span class="number">4</span>]. 获取到JobSubmitter submitter , 然后提交Job submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">   ① checkSpecs(job); 校验输出路径是否存在. 如果存在，直接抛出异常.</span><br><span class="line">   ②  Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line">       获取Job的临时的工作目录，例如: D:/tmp/hadoop-Administrator/mapred/staging/Administrator780971275/.staging </span><br><span class="line">   ③  JobID jobId = submitClient.getNewJobID(); 生成JobId</span><br><span class="line">   ④  Path submitJobDir = <span class="keyword">new</span> Path(jobStagingArea, jobId.toString()); </span><br><span class="line">       获取到提交Job的目录 ，实际就是   jobStagingArea + jobId 。</span><br><span class="line">   ⑤  copyAndConfigureFiles(job, submitJobDir); 复制并配置一些文件</span><br><span class="line">       (<span class="number">1</span>).  rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line">       (<span class="number">2</span>).  submitJobDir = jtFs.makeQualified(submitJobDir); 获取到提交job的全路径</span><br><span class="line">       (<span class="number">3</span>).  FileSystem.mkdirs(jtFs, submitJobDir, mapredSysPerms); 在文件系统创建Job提交的相关目录</span><br><span class="line">   ⑥  <span class="keyword">int</span> maps = writeSplits(job, submitJobDir); 生成切片信息，并将 job.split 切片信息的文件写到job提交目录中  (下面着重讲如何生成切片信息的)</span><br><span class="line">       (<span class="number">1</span>). maps = writeNewSplits(job, jobSubmitDir); </span><br><span class="line">       (<span class="number">2</span>). InputFormat&lt;?, ?&gt; input = ReflectionUtils.newInstance(job.getInputFormatClass(), conf);</span><br><span class="line">            切片信息是否InputFormat来生成的.  默认的的InputFormat是 TextInputFormat</span><br><span class="line">       (<span class="number">3</span>). List&lt;InputSplit&gt; splits = input.getSplits(job); 通过InputFormat来获取切片信息</span><br><span class="line">    a.  <span class="keyword">long</span> minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job)); ==&gt;<span class="number">1</span></span><br><span class="line">    b.  <span class="keyword">long</span> maxSize = getMaxSplitSize(job);  ==&gt; Long.MAX_VALUE</span><br><span class="line">    c.  <span class="keyword">long</span> blockSize = file.getBlockSize(); 获取块大小，本地默认的块大小32M.</span><br><span class="line">    d.  <span class="keyword">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize); 计算切片大小</span><br><span class="line">             ****** Math.max(minSize, Math.min(maxSize, blockSize));假如我想要切片大小变大我需要将minSize调大就可以了.如果我想要切片大小变的更小我就需要调整maxSize了.</span><br><span class="line">    e.   bytesRemaining)/splitSize &gt; SPLIT_SLOP(<span class="number">1.1</span>) 如果剩余文件的大小超过切片</span><br><span class="line">         大小的<span class="number">1.1</span>倍,继续切片，反之, 不切片.</span><br><span class="line"></span><br><span class="line">   ⑦  conf.setInt(MRJobConfig.NUM_MAPS, maps); 设置MapTask的个数</span><br><span class="line">   ⑧   writeConf(conf, submitJobFile); 将job相关的 job.xml 配置信息等写到job提交目录中</span><br><span class="line">   ⑨  status = submitClient.submitJob(</span><br><span class="line">             jobId, submitJobDir.toString(), job.getCredentials()); 真正提交Job，提交了job之后就需要执行我的mapTask了</span><br><span class="line">   ⑩  jtFs.delete(submitJobDir, <span class="keyword">true</span>); 等Job执行结束后，删除临时目录.</span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">----------------------------------------------------------------------------------------------------------------          </span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">MapTask:   如何生成切片信息的 (⑥  <span class="keyword">int</span> maps = writeSplits(job, submitJobDir); 生成切片信息，并将 job.split 切片信息的文件写到job提交目录中)</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span>Job job = <span class="keyword">new</span> Job(JobID.downgrade(jobid), jobSubmitDir); </span><br><span class="line">   创建一个真正执行的job. </span><br><span class="line">   Job: org.apache.hadoop.mapred.LocalJobRunner.Job</span><br><span class="line"><span class="number">2.</span> <span class="keyword">this</span>.start();   执行的是LocalJobRunner.Job 中的run方法. </span><br><span class="line"><span class="number">3.</span> 在LocalJobRunner.Job的run方法中， 执行runTasks(mapRunnables, mapService, <span class="string">&quot;map&quot;</span>);</span><br><span class="line"><span class="number">4.</span> 在runTasks中迭代出所有的MapTaskRunnable,并开始让线程执行。</span><br><span class="line"><span class="number">5.</span> 执行MapTaskRunnable的run方法. </span><br><span class="line"><span class="number">6.</span> 创建MapTask对象   MapTask map = <span class="keyword">new</span> MapTask(systemJobFile.toString(), mapId, taskId,info.getSplitIndex(), <span class="number">1</span>);</span><br><span class="line"><span class="number">7.</span> 执行MapTask的run方法</span><br><span class="line"><span class="number">8.</span> 执行runNewMapper(job, splitMetaInfo, umbilical, reporter);</span><br><span class="line"><span class="number">9.</span> 获取context， mapper ， inputFormat  ，recordReader 等</span><br><span class="line"><span class="number">10.</span> 获取Map端负责输出的对象 :</span><br><span class="line">output = <span class="keyword">new</span> NewOutputCollector(taskContext, job, umbilical, reporter);</span><br><span class="line"><span class="number">11.</span> mapper.run(mapperContext);</span><br><span class="line"><span class="number">12.</span> 执行Mapper中的map方法。</span><br></pre></td></tr></table></figure><p>&emsp;     </p><h3 id="Job提交流程源码详解"><a href="#Job提交流程源码详解" class="headerlink" title="Job提交流程源码详解"></a>Job提交流程源码详解</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">waitForCompletion()</span><br><span class="line"></span><br><span class="line">submit();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1建立连接</span></span><br><span class="line">connect();</span><br><span class="line"><span class="comment">// 1）创建提交Job的代理</span></span><br><span class="line"><span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line"><span class="comment">// （1）判断是本地yarn还是远程</span></span><br><span class="line">initialize(jobTrackAddr, conf); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 提交job</span></span><br><span class="line">submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster)</span><br><span class="line"><span class="comment">// 1）创建给集群提交数据的Stag路径</span></span><br><span class="line">Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2）获取jobid ，并创建Job路径</span></span><br><span class="line">JobID jobId = submitClient.getNewJobID();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3）拷贝jar包到集群</span></span><br><span class="line"></span><br><span class="line">copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4）计算切片，生成切片规划文件</span></span><br><span class="line">writeSplits(job, submitJobDir);</span><br><span class="line">maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">input.getSplits(job);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5）向Stag路径写XML配置文件</span></span><br><span class="line">writeConf(conf, submitJobFile);</span><br><span class="line">conf.writeXml(out);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6）提交Job,返回提交状态</span></span><br><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/hadoop/job1.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="FileInputFormat切片源码解析-input-getSplits-job"><a href="#FileInputFormat切片源码解析-input-getSplits-job" class="headerlink" title="FileInputFormat切片源码解析(input.getSplits(job))"></a>FileInputFormat切片源码解析(input.getSplits(job))</h3><p>（1）程序先找到你数据存储的目录。<br>（2）开始遍历处理（规划切片）目录下的每一个文件<br>（3）遍历第一个文件ss.txt<br>        a）获取文件大小fs.sizeOf(ss.txt)<br>        b）计算切片大小<br>               computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M<br>        c）默认情况下，切片大小=blocksize<br>        d）开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M<br>                （每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片）<br>        e）将切片信息写到一个切片规划文件中<br>        f）整个切片的核心过程在getSplit()方法中完成<br>        g）InputSplit只记录了切片的元数据信息，比如起始位置、长度以及所在的节点列表等。<br>（4）提交切片规划文件到YARN上，YARN上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数。</p><h3 id="FileInputFormat切片机制"><a href="#FileInputFormat切片机制" class="headerlink" title="FileInputFormat切片机制"></a>FileInputFormat切片机制</h3><p>​     1、切片机制<br>​       （1）简单地按照文件的内容长度进行切片<br>​       （2）切片大小，默认等于Block大小<br>​       （3）切片时不考虑数据集整体，一个文件单独切片<br>​    2、案例分析<br>​      （1）输入数据有两个文件：<br>​                    filel.txt 320M<br>​                    file2.txt  10M<br>​          (2) 经过FileInputFormat的切片机制运算后，形成的切片信息如下：<br>​                    filel.txt.split2–     128<del>256<br>​                    file1.txt.split3–    256</del>320<br>​                    file2.txt.split1-      0~10M</p><h3 id="FileInputFormat切片大小的参数配置"><a href="#FileInputFormat切片大小的参数配置" class="headerlink" title="FileInputFormat切片大小的参数配置"></a>FileInputFormat切片大小的参数配置</h3><p>（1）源码中计算切片大小的公式<br>            Math.max(minSize,Math.min(maxSize,blockSize));<br>            mapreduce.input.fileinputformat.split.minsize=1 默认值为1<br>            mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue 默认值Long.MAXValue<br>            因此，默认情况下，切片大小=blocksize。<br>（2）切片大小设置<br>                maxsize（切片最大值）：参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值。<br>                minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blockSize还大。<br>(3)获取切片信息API<br>            // 获取切片的文件名称<br>            String name = inputSplit.getPath().getName();<br>            // 根据文件类型获取切片信息<br>            FileSplit inputSplit = (FileSplit) context.getInputSplit();</p><h3 id="有哪些形式的InputFormat"><a href="#有哪些形式的InputFormat" class="headerlink" title="有哪些形式的InputFormat"></a>有哪些形式的InputFormat</h3><p>思考：在运行MapReduce程序时，输入的文件格式包括：基于行的日志文件二进制格式文件、数据库表等。那么，针对不同的数据类型，MapReduce是如何读取这些数据的呢？</p><p>针对不容类型,不同场景,我们做一些实现类,自己写代码.</p><p>这里有最常见的,写好的实现类.</p><p>FileInputFormat 常 见 的 接 口 实 现 类 包 括:<br>TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat等。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_100.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_101.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_102.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="KeyValueTextInputFormat使用案例"><a href="#KeyValueTextInputFormat使用案例" class="headerlink" title="KeyValueTextInputFormat使用案例"></a>KeyValueTextInputFormat使用案例</h3><p><strong>案例代码</strong>: <a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/keyValueTextInputFormat/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/keyValueTextInputFormat/</a></p><p>keyValueTextInputFormat他读取文件的时候还是按照行来读取. 但是他读取进来时候如何去切分他, 他只会将文件切分成两份,切完后,左边是key, 右边就是value.</p><p>所以我们得带驱动类Driver中设置我们的分割符, 那些是key ,哪些是value.</p><h3 id="NLineInputFormat使用案例"><a href="#NLineInputFormat使用案例" class="headerlink" title="NLineInputFormat使用案例"></a>NLineInputFormat使用案例</h3><p><strong>案例代码</strong>:<a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/NLineInputFormat">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/NLineInputFormat</a></p><p>NLineInputFormat 表什么呢? N表示数字. Line表示行.  表达的意思是n行的InputFormat.  他是按照你指定的行号进行切片的. 比如说我有一个文件. 我希望你将来对我文件进行处理的时候是每三行, 或者说每四行进行处理. 这个N 是你具体指定的. 就是按照行数来进行切片.<br>所以NLineInputFormat改变的是你的切片的规则, 比如说我有三十一行数据,n为3表示每三行生成一个切片,最终我三十一行生成11个切片.</p><h3 id="CombineTextInputFormat切片机制"><a href="#CombineTextInputFormat切片机制" class="headerlink" title="CombineTextInputFormat切片机制"></a>CombineTextInputFormat切片机制</h3><p><strong>案例代码</strong>:<a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/CombineTextInputFormat/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/CombineTextInputFormat/</a></p><p>如果说我们使用框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。</p><p>比如说我有5个文件, 第一个是1M,第二个是2M,3M,4M,5M. 因为他们都小于我们的块大小128M.  所以说按照单个文件来去切的话, 每个文件都是一个切片, 最终生成5个切片. 且我的每一个切片都是对应一个mapTask, 但是这么小的切片对应我的mapTask不是杀鸡用牛刀吗?它都不值得我去启动一次mapTask.因为你数据量太小了.所以对于小文件来说我就不能使用TextInputFormat了, 我必须对你做一个处理.</p><p>比如接下来说的CombineTextInputFormat.</p><p>他是用的场景就是小文件过多的情况下.他的思想是什么呢?<br>首先我们使用CombineTextInputFormat的时候得先设置虚拟存储切片的最大值. 具体设置多大按照实际情况来定. 假如说我把虚拟存储最大值设置为4M    ,那么他会根据这个4M进行判断,我这个文件到底要不要去切. 然后最终我怎么去帮你生成切片.所以CombineTextInputFormat也是可以避免数据倾斜问题.</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/hadoop/job6.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>如上图所示:</p><p>我们设置虚拟存储的文件大小(setMaxInputSplitSize)为4M. 比如说我们有几个文件(如上图) 这些小文件.  因为我们设置虚拟存储大小是4M.  在虚拟存储过程中有一个算法,就是如果你的文件的大小小于你虚拟存储的最大值(即4M). 那么对于你这个文件来说就将你划分成一块.这一块是虚拟的.不是真正划分的. 但是如果你实际文件大小是大于你设置的文件最大值的(4M) , 但是他又小于两倍的最大值. 即大于4M 小于8M.  在这种情况下我就将你这个文件一分为2. 如上面的两块的2.55M .最后生成最终存储的文件即虚拟文件. 接下来他就按照这个规划帮你进行切片. 生成切片的过程中他会判断你虚拟存储的文件大小是否大于我们设置的值(4M). 如果大于则单独形成一个切片.  如果你虚拟存储的大小不大于4M. 那么他就会和你下一个虚拟存储文件进行合并, 共同形成一个切片.  如上图的1.7M+2.55M 形成一个切片.  最终形成3个切片. 三个切片不会差太多.这就规避了数据倾斜问题.</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/hadoop/job7.png" width = "" height = "" alt="xubatian的博客" align="center" /><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/hadoop/job8.png" width = "" height = "" alt="xubatian的博客" align="center" />“ </p><p>1、应用场景：</p><p>CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。</p><p>2、虚拟存储切片最大值设置</p><p>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m<br>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</p><p>3、切片机制</p><p>生成切片过程包括：虚拟存储过程和切片过程二部分。</p><h3 id="自定义InputFormat"><a href="#自定义InputFormat" class="headerlink" title="自定义InputFormat"></a>自定义InputFormat</h3><p>在企业开发中，Hadoop框架自带的InputFormat类型不能满足所有应用场景，需要自定义InputFormat来解决实际问题。<br>自定义InputFormat步骤如下：<br>（1）自定义一个类继承FileInputFormat。<br>（2）改写RecordReader，实现一次读取一个完整文件封装为KV。<br>（3）在输出时使用SequenceFileOutPutFormat输出合并文件。</p><p>无论HDFS还是MapReduce，在处理小文件时效率都非常低，但又难免面临处理大量小文件的场景，此时，就需要有相应解决方案。可以自定义InputFormat实现小文件的合并。</p><p>比如说我有n多个小文件,我想将他存到HDFS中,之前的一种方式是使用har文件.<br>但是现在我想把我的n多个小文件最终存到一个文件里面.就是死将这n多个小文件里面的内容拿出来存到一个文件里面,存成一个稍微大一点的文件. 但是我不能简单存成一个txt文件. 因为这样的话就会乱了.就揉到一起了.  我还是希望你存到这大文件里面以后他们之间还是有一个很明确的划分的. 所以这一次我会用到一个SequenceFlie.  SequenceFile就是一个序列文件. 这个文件的格式比较特殊. 虽然是一个文件, 但是它里面也是以k,v 的形式来存的. k 可以是原来文件的路径.v是文件的内容 (D:/one.txt  , 文件内容  ) . 虽然说我将多个文件存到一个文件当中, 但是这个文件里面他也是有明确的划分的.  将来我们去读的时候也是可以通过 k, v 的形式去读出来. 而且这种格式存起来特别的紧凑.  比如原先我n个小文件存起来是5M, 但是通过 SequenceFile这种方式存的话可能就 3M. 因为他是二进制的存储方式.  sequenceFile这种文件的格式hadoop是支持的 , 但是我们得自己操作, 将小文件里面的内容读出来,以 k ,v 的形式给他写进去.这个过程需要我们自己做.所以我在读我们每一个小文件的时候希望一次性将文件里面的内容都读出来然后向sequenceFile中去放. 比如说k 就是我们小文件的路径加名字. v就是我文件的内容. 我一次性将这个k,v 写到sequenceFile中.  但是我们上面的InputFormat没有一次性读整个文件的. 都是一行一行读的. 所以这就需要我们自定义了.</p><p>具体案例: 略.</p><h1 id="MapReduce工作流程"><a href="#MapReduce工作流程" class="headerlink" title="MapReduce工作流程"></a>MapReduce工作流程</h1><p>1.MapReduce工作流程:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/hadoop/map1.png" width = "" height = "" alt="xubatian的博客" align="center" />“ </p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/hadoop/map2.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>2.流程详解:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">上面的流程是整个MapReduce最全工作流程，但是Shuffle过程只是从第<span class="number">7</span>步开始到第<span class="number">16</span>步结束，具体Shuffle过程详解，如下：</span><br><span class="line"></span><br><span class="line"><span class="number">1</span>）MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>）多个溢出文件会被合并成大的溢出文件</span><br><span class="line"></span><br><span class="line"><span class="number">4</span>）在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序</span><br><span class="line"></span><br><span class="line"><span class="number">5</span>）ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据</span><br><span class="line"></span><br><span class="line"><span class="number">6</span>）ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）</span><br><span class="line"></span><br><span class="line"><span class="number">7</span>）合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>．注意</span><br><span class="line">Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。</span><br><span class="line">缓冲区的大小可以通过参数调整，参数：io.sort.mb默认100M。</span><br><span class="line"></span><br><span class="line"><span class="number">4</span>．源码解析流程</span><br><span class="line">context.write(k, NullWritable.get());</span><br><span class="line">output.write(key, value);</span><br><span class="line">collector.collect(key, value,partitioner.getPartition(key, value, partitions));</span><br><span class="line">HashPartitioner();</span><br><span class="line">collect()</span><br><span class="line">close()</span><br><span class="line">collect.flush()</span><br><span class="line">          sortAndSpill()</span><br><span class="line">         sort()  </span><br><span class="line">          <span class="function">QuickSort</span></span><br><span class="line"><span class="function"><span class="title">mergeParts</span><span class="params">()</span></span>;</span><br><span class="line">          file.out;</span><br><span class="line">file.out.index;</span><br><span class="line">collector.close();</span><br></pre></td></tr></table></figure><p>整个mapreduce分为:</p><p><strong>InputFormat数据输入—–&gt;MapTask工作机制,Shuffle工作机制,ReduceTask工作机制—&gt;OutputFormat数据输出.</strong></p><p>此篇文章讲解了InputFormat数据输入, 下一篇就是 MapTask工作机制,Shuffle工作机制,ReduceTask工作机制.</p><p>Shuffle工作机制比较重要,单独记录. 下一篇是: hadoop组成模块之mapreduce的MapTask机制和reduceTask机制</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;骑假马练不出真演技。流量转瞬即逝，常量才能长青。 ——人民日报                             &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="mapreduce" scheme="http://xubatian.cn/tags/mapreduce/"/>
    
    <category term="InputFormat" scheme="http://xubatian.cn/tags/InputFormat/"/>
    
  </entry>
  
</feed>
