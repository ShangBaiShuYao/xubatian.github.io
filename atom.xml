<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>我的梦想是星辰大海</title>
  
  <subtitle>知识源于积累,登峰造极源于自律</subtitle>
  <link href="http://xubatian.cn/atom.xml" rel="self"/>
  
  <link href="http://xubatian.cn/"/>
  <updated>2022-02-16T17:20:05.097Z</updated>
  <id>http://xubatian.cn/</id>
  
  <author>
    <name>xubatian</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ClickHouse表引擎</title>
    <link href="http://xubatian.cn/ClickHouse%E8%A1%A8%E5%BC%95%E6%93%8E/"/>
    <id>http://xubatian.cn/ClickHouse%E8%A1%A8%E5%BC%95%E6%93%8E/</id>
    <published>2022-02-16T16:46:52.000Z</published>
    <updated>2022-02-16T17:20:05.097Z</updated>
    
    <content type="html"><![CDATA[<h3 id="表引擎的使用"><a href="#表引擎的使用" class="headerlink" title="表引擎的使用"></a>表引擎的使用</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217004854.png"></p><span id="more"></span><h3 id="TinyLog"><a href="#TinyLog" class="headerlink" title="TinyLog"></a>TinyLog</h3><p>​            以列文件的形式保存在磁盘上，不支持索引，没有并发控制。一般保存少量数据的小表，生产环境上作用有限。可以用于<strong>平时练习测试</strong>用。如：</p><pre><code>create table t_tinylog ( id String, name String) engine=TinyLog;</code></pre><h3 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h3><p>​            内存引擎，数据以未压缩的原始形式直接保存在内存当中，服务器重启数据就会消失。读写操作不会相互阻塞，不支持索引。简单查询下有非常非常高的性能表现（超过10G/s）。<br>​            一般用到它的地方不多，除了用来测试，就是在需要非常高的性能，同时数据量又不太大（上限大概 1 亿行）的场景。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据保存在内存中,但是内存数据不稳定,一旦服务器重启,数据消失.除非你对查询的性能有非常高的要求.但是这种情况很少.</span><br></pre></td></tr></table></figure><h3 id="MergeTree-最强大的表引擎"><a href="#MergeTree-最强大的表引擎" class="headerlink" title="MergeTree(最强大的表引擎)"></a>MergeTree(最强大的表引擎)</h3><p>​            <strong>ClickHouse中最强大的表引擎当属MergeTree（合并树）引擎及该系列（MergeTree）中的其他引擎，支持索引和分区， 地位可以相当于innodb之于Mysql</strong>。 而且基于MergeTree，还衍生除了很多小弟，也是非常有特色的引擎。</p><p><strong>建表语句</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table t_order_mt(</span><br><span class="line">    id UInt32,</span><br><span class="line">    sku_id String,</span><br><span class="line">    total_amount Decimal(16,2),</span><br><span class="line">    create_time  Datetime                      </span><br><span class="line"> ) engine =MergeTree</span><br><span class="line">   partition by toYYYYMMDD(create_time)      </span><br><span class="line">   primary key (id)</span><br><span class="line">   order by (id,sku_id);</span><br></pre></td></tr></table></figure><p><strong>Partition</strong>:  分区,假如我Datetime设置的是2020-12-12 12:12 但是我的分区是按照2020-12-12来分区的转换成年月日的形式.<br><strong>primary key</strong>:  主键,以前的主键是数据看看表中字段的唯一标记. 现在在clickhouse中主键没有唯一标记.它主要做的就是提升我们的查询效率.他其实就是为了建我们的查询索引的.<br><strong>order by</strong>:  排序,在clickhouse中,order by不仅仅是排序,去重等也是依靠order by来做的</p><p><strong>插入数据</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">insert into  t_order_mt values</span><br><span class="line">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;);</span><br></pre></td></tr></table></figure><p><strong>MergeTree其实还有很多参数(绝大多数用默认值即可)，但是三个参数是更加重要的，也涉及了关于MergeTree的很多概念。</strong></p><h4 id="partition-by-分区-（可选项-不填的话数据在一个分区）"><a href="#partition-by-分区-（可选项-不填的话数据在一个分区）" class="headerlink" title="partition by 分区 （可选项,不填的话数据在一个分区）"></a>partition by 分区 （可选项,不填的话数据在一个分区）</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005428.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005448.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005514.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimize table xxxx final;</span><br></pre></td></tr></table></figure><p>Ø 例如</p><p>再次执行上面的插入操作</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">insert into  t_order_mt values</span><br><span class="line">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;);</span><br></pre></td></tr></table></figure><p>查看数据并没有纳入任何分区</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005604.png"></p><p>看到分成了四块,按照create_time分区.</p><p>手动optimize之后</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop202 :) optimize table t_order_mt final;</span><br></pre></td></tr></table></figure><p>再次查询</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005655.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005736.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">插入数据不会立马合并,要不然就是你插入的数据量足够大,要不然就是你插入的数据时间足够长,要不然就是你手动合并.</span><br></pre></td></tr></table></figure><h4 id="primary-key主键-可选"><a href="#primary-key主键-可选" class="headerlink" title="primary key主键(可选)"></a>primary key主键(可选)</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005829.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005847.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005857.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005917.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217005939.png"></p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010017.png" style="zoom:200%;" /><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010059.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">稀疏索引的好处就是可以用很少的索引数据，定位更多的数据，代价就是只能定位到索引粒度的第一行，然后再进行进行一点扫描。</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010125.png"></p><h4 id="order-by（必选-作用-分区内排序）"><a href="#order-by（必选-作用-分区内排序）" class="headerlink" title="order by（必选,作用:分区内排序）"></a>order by（必选,作用:分区内排序）</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010152.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010213.png"></p><h3 id="二级索引"><a href="#二级索引" class="headerlink" title="二级索引"></a>二级索引</h3><p>​        目前在ClickHouse的官网上二级索引的功能是被标注为实验性的。即不稳定,还有很大的发展空间.</p><h4 id="1-使用二级索引前需要增加设置"><a href="#1-使用二级索引前需要增加设置" class="headerlink" title="(1)使用二级索引前需要增加设置"></a>(1)使用二级索引前需要增加设置</h4><p>是否允许使用实验性的二级索引,开启二级索引:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set allow_experimental_data_skipping_indices=1;</span><br></pre></td></tr></table></figure><h4 id="2-创建测试表"><a href="#2-创建测试表" class="headerlink" title="(2)创建测试表"></a>(2)创建测试表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table t_order_mt2(</span><br><span class="line">    id UInt32,</span><br><span class="line">    sku_id String,</span><br><span class="line">    total_amount Decimal(16,2),</span><br><span class="line">    create_time  Datetime,</span><br><span class="line">INDEX a total_amount TYPE minmax GRANULARITY 5</span><br><span class="line"> ) engine =MergeTree</span><br><span class="line">   partition by toYYYYMMDD(create_time)</span><br><span class="line">   primary key (id)</span><br><span class="line">   order by (id, sku_id);</span><br></pre></td></tr></table></figure><p>其中<strong>GRANULARITY N</strong> 是设定二级索引对于一级索引粒度的粒度。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010335.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010442.png"><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010500.png"></p><h4 id="3-插入数据"><a href="#3-插入数据" class="headerlink" title="(3)插入数据"></a>(3)插入数据</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">insert into  t_order_mt2 values</span><br><span class="line">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;);</span><br></pre></td></tr></table></figure><h4 id="4-对比效果"><a href="#4-对比效果" class="headerlink" title="(4)对比效果"></a>(4)对比效果</h4><p>那么在使用下面语句进行测试，可以看出二级索引能够为非主键字段的查询发挥作用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop202 lib]$ clickhouse-client  --send_logs_level=trace &lt;&lt;&lt; &#x27;select * from t_order_mt2  where total_amount &gt; toDecimal32(900., 2)&#x27;;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010727.png"></p><h3 id="数据TTL"><a href="#数据TTL" class="headerlink" title="数据TTL"></a>数据TTL</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010904.png"></p><h4 id="1-列级别TTL（执行整个列的失效时间）"><a href="#1-列级别TTL（执行整个列的失效时间）" class="headerlink" title="(1)列级别TTL（执行整个列的失效时间）"></a>(1)列级别TTL（执行整个列的失效时间）</h4><p><strong>创建测试表</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table t_order_mt3(</span><br><span class="line">    id UInt32,</span><br><span class="line">    sku_id String,</span><br><span class="line">    total_amount Decimal(16,2)  TTL create_time+interval 10 SECOND, //表示当前这一列数值的存活时间.就是在你创建</span><br><span class="line">    create_time  Datetime                                           //时间过了十秒之后,这一列数据就失效了</span><br><span class="line"> ) engine =MergeTree</span><br><span class="line"> partition by toYYYYMMDD(create_time)</span><br><span class="line">   primary key (id)</span><br><span class="line">   order by (id, sku_id);</span><br></pre></td></tr></table></figure><p><strong>插入数据</strong>（注意：根据实际时间改变）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">insert into  t_order_mt3 values</span><br><span class="line">(106,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-12 22:52:30&#x27;),</span><br><span class="line">(107,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-12 22:52:30&#x27;),</span><br><span class="line">(110,&#x27;sku_003&#x27;,600.00,&#x27;2020-06-13 12:00:00&#x27;);</span><br></pre></td></tr></table></figure><p><strong>手动合并，查看效果  到期后，指定的字段数据归0</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011106.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217010933.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">其实这个TTL这个手动合并，其实不是帮你解决业务上的合并的。 其实他是从本身优化的角度来考虑的。 比如： 加入我有些数据，数据量比较大， 但是这些数据我执行完成后就不需要了， 那我可以设置一个失效时间。 在长时间后数据失效。 设置完失效时间后，虽然不能到了指定时间后，马上将这个字段重置为0. 但是他会过一段时间后，后台看到这个标记，后台会自动帮你进行合并。这样就相当于把空间释放掉了。这是Clickhouse自己做的一个优化。</span><br></pre></td></tr></table></figure><h4 id="2-表级TTL（指定整个表的失效时间）"><a href="#2-表级TTL（指定整个表的失效时间）" class="headerlink" title="(2)表级TTL（指定整个表的失效时间）"></a>(2)表级TTL（指定整个表的失效时间）</h4><p>下面的这条语句是数据会在create_time 之后10秒丢失</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table t_order_mt3 MODIFY TTL create_time + INTERVAL 10 SECOND;</span><br></pre></td></tr></table></figure><p>涉及判断的字段必须是Date或者Datetime类型，推荐使用分区的日期字段。<br>能够使用的时间周期：</p><ul><li>SECOND</li><li>MINUTE</li><li>HOUR</li><li>DAY</li><li>WEEK</li><li>MONTH</li><li>QUARTER</li><li>YEAR </li></ul><h2 id="ReplacingMergeTree"><a href="#ReplacingMergeTree" class="headerlink" title="ReplacingMergeTree"></a>ReplacingMergeTree</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011322.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">去重不是业务上的去重,而是Clickhouse本身为了去优化做的去重.所以说,clickhouse是可以去重,但是什么时间去重,我们不确定. 所以说你给Clickhouse的数据应该是在你在前台已经去重的数据这更合适</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011346.png"></p><p><strong>案例演示</strong></p><p><strong>创建表</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table t_order_rmt(</span><br><span class="line">    id UInt32,</span><br><span class="line">    sku_id String,</span><br><span class="line">    total_amount Decimal(16,2) ,</span><br><span class="line">    create_time  Datetime </span><br><span class="line"> ) engine =ReplacingMergeTree(create_time)</span><br><span class="line">   partition by toYYYYMMDD(create_time)</span><br><span class="line">   primary key (id)</span><br><span class="line">   order by (id, sku_id);</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011431.png"></p><p><strong>向表中插入数据</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">insert into  t_order_rmt values</span><br><span class="line">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;);</span><br></pre></td></tr></table></figure><p><strong>执行第一次查询</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop202 :) select * from t_order_rmt;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011516.png"></p><p><strong>手动合并</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OPTIMIZE TABLE t_order_rmt FINAL;</span><br></pre></td></tr></table></figure><p><strong>再执行一次查询</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop202 :) select * from t_order_rmt;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011601.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011624.png"></p><h2 id="SummingMergeTree-提供预聚合的作用"><a href="#SummingMergeTree-提供预聚合的作用" class="headerlink" title="SummingMergeTree(提供预聚合的作用)"></a>SummingMergeTree(提供预聚合的作用)</h2><pre><code>    对于不查询明细，只关心以维度进行汇总聚合结果的场景。如果只使用普通的MergeTree的话，无论是存储空间的开销，还是查询时临时聚合的开销都比较大。    ClickHouse 为了这种场景，提供了一种能够“预聚合”的引擎SummingMergeTree</code></pre><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011728.png"></p><p><strong>案例演示</strong></p><p><strong>创建表</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table t_order_smt(</span><br><span class="line">    id UInt32,</span><br><span class="line">    sku_id String,</span><br><span class="line">    total_amount Decimal(16,2) ,</span><br><span class="line">    create_time  Datetime </span><br><span class="line"> ) engine =SummingMergeTree(total_amount)</span><br><span class="line">   partition by toYYYYMMDD(create_time)</span><br><span class="line">   primary key (id)</span><br><span class="line">   order by (id,sku_id );</span><br></pre></td></tr></table></figure><p><strong>插入数据</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">insert into  t_order_smt values</span><br><span class="line">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;);</span><br></pre></td></tr></table></figure><p><strong>执行第一次查询</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop202 :) select * from t_order_smt;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011833.png"></p><p><strong>手动合并</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OPTIMIZE TABLE t_order_smt FINAL;</span><br></pre></td></tr></table></figure><p><strong>再执行一次查询</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop202 :) select * from t_order_smt;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011907.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217011935.png"></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;表引擎的使用&quot;&gt;&lt;a href=&quot;#表引擎的使用&quot; class=&quot;headerlink&quot; title=&quot;表引擎的使用&quot;&gt;&lt;/a&gt;表引擎的使用&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217004854.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="clickhouse" scheme="http://xubatian.cn/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse数据类型</title>
    <link href="http://xubatian.cn/ClickHouse%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
    <id>http://xubatian.cn/ClickHouse%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</id>
    <published>2022-02-16T16:34:28.000Z</published>
    <updated>2022-02-16T16:46:30.504Z</updated>
    
    <content type="html"><![CDATA[<p>官方文档：<a href="https://clickhouse.yandex/docs/zh/data_types/">https://clickhouse.yandex/docs/zh/data_types/</a></p><h3 id="整型-主要用到"><a href="#整型-主要用到" class="headerlink" title="整型(主要用到)"></a>整型(主要用到)</h3><p>固定长度的整型，包括有符号整型或无符号整型。</p><p>整型范围（-2n-1~2n-1-1）：</p><p>Int8 - [-128 : 127]</p><p>Int16 - [-32768 : 32767]</p><p>Int32 - [-2147483648 : 2147483647]</p><p>Int64 - [-9223372036854775808 : 9223372036854775807]</p><span id="more"></span><p>无符号整型范围（0~2n-1）：</p><p>UInt8 - [0 : 255]</p><p>UInt16 - [0 : 65535]</p><p>UInt32 - [0 : 4294967295]</p><p>UInt64 - [0 : 18446744073709551615]</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用场景： 个数、数量、也可以存储型id</span><br></pre></td></tr></table></figure><h3 id="浮点型"><a href="#浮点型" class="headerlink" title="浮点型"></a>浮点型</h3><p>Float32 - float</p><p>Float64 – double</p><p>建议尽可能以整数形式存储数据。例如，将固定精度的数字转换为整数值，如时间用毫秒为单位表示，因为浮点型进行计算时可能引起四舍五入的误差。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217003813.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用场景：一般数据值比较小，不涉及大量的统计计算，精度要求不高的时候。比如保存商品的重量。</span><br></pre></td></tr></table></figure><h3 id="布尔型"><a href="#布尔型" class="headerlink" title="布尔型"></a>布尔型</h3><p>​        没有单独的类型来存储布尔值。可以使用 UInt8 类型，取值限制为 0 或 1。</p><h3 id="Decimal-型-主要用到-对精度要求高用Decimal"><a href="#Decimal-型-主要用到-对精度要求高用Decimal" class="headerlink" title="Decimal 型((主要用到)对精度要求高用Decimal)"></a>Decimal 型((主要用到)对精度要求高用Decimal)</h3><p>​        有符号的浮点数，可在加、减和乘法运算过程中保持精度。对于除法，最低有效数字会被丢弃（不舍入）。        </p><p>有三种声明：(32,64,128表示分配的空间大小)(表示有效位数是9,s是小数位)</p><ul><li>​            Decimal32(s)，相当于Decimal(9-s,s)，有效位数为1~9</li><li>​            Decimal64(s)，相当于Decimal(18-s,s)，有效位数为1~18</li><li>​            Decimal128(s)，相当于Decimal(38-s,s)，有效位数为1~38</li></ul><p>s标识小数位</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用场景： 一般金额字段、汇率、利率等字段为了保证小数点精度，都使用Decimal进行存储。</span><br></pre></td></tr></table></figure><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><ul><li><p>​    String</p><p>​        字符串可以任意长度的。它可以包含任意的字节集，包含空字节。</p></li><li><p>​    FixedString(N)</p></li></ul><p>​                固定长度 N 的字符串，N 必须是严格的正自然数。当服务端读取长度小于 N 的字符串时候，通过在字符串末尾添加空字节来达到 N 字节长度。 当服务端读取长度大于 N 的字符串时候，将返回错误消息。<br>​          与String相比，极少会使用FixedString，因为使用起来不是很方便。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用场景：名称、文字描述、字符型编码。 固定长度的可以保存一些定长的内容，比如一些编码，性别等但是考虑到一定的变化风险，带来收益不够明显，所以定长字符串使用意义有限。</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217004107.png"></p><h3 id="枚举类型-枚举-一个类它创建对象的个数是固定的叫枚举-比如季节-星期"><a href="#枚举类型-枚举-一个类它创建对象的个数是固定的叫枚举-比如季节-星期" class="headerlink" title="枚举类型(枚举: 一个类它创建对象的个数是固定的叫枚举,比如季节,星期)"></a>枚举类型(枚举: 一个类它创建对象的个数是固定的叫枚举,比如季节,星期)</h3><p>包括 Enum8 和 Enum16 类型。Enum 保存 ‘string’= integer 的对应关系。</p><p>Enum8 用 ‘String’= Int8 对描述。</p><p>Enum16 用 ‘String’= Int16 对描述。</p><p><strong>用法演示</strong></p><p>创建一个带有一个枚举 Enum8(‘hello’ = 1, ‘world’ = 2) 类型的列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE t_enum</span><br><span class="line">(</span><br><span class="line">    x Enum8(&#x27;hello&#x27; = 1, &#x27;world&#x27; = 2)</span><br><span class="line">)</span><br><span class="line">ENGINE = TinyLog;  //这个是引擎</span><br></pre></td></tr></table></figure><p>这个 x 列只能存储类型定义中列出的值：’hello’或’world’</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop202 :) INSERT INTO t_enum VALUES (&#x27;hello&#x27;), (&#x27;world&#x27;), (&#x27;hello&#x27;);</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217004225.png"></p><p>如果尝试保存任何其他值，ClickHouse 抛出异常</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop202 :) insert into t_enum values(&#x27;a&#x27;)</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217004247.png"></p><p>如果需要看到对应行的数值，则必须将 Enum 值转换为整数类型</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop202 :) SELECT CAST(x, &#x27;Int8&#x27;) FROM t_enum;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217004318.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用场景：对一些状态、类型的字段算是一种空间优化，也算是一种数据约束。但是实际使用中往往因为一些数据内容的变化增加一定的维护成本，甚至是数据丢失问题。所以谨慎使用。</span><br></pre></td></tr></table></figure><h3 id="时间类型-主要用到"><a href="#时间类型-主要用到" class="headerlink" title="时间类型(主要用到)"></a>时间类型(主要用到)</h3><p><strong>目前ClickHouse 有三种时间类型</strong><br>    Date接受年-月-日的字符串比如 ‘2019-12-16’<br>    Datetime接受年-月-日 时:分:秒的字符串比如 ‘2019-12-16 20:50:10’<br>    Datetime64接受年-月-日 时:分:秒.亚秒的字符串比如‘2019-12-16 20:50:10.66’</p><p>日期类型，用两个字节存储，表示从 1970-01-01 (无符号) 到当前的日期值。</p><p>还有很多数据结构，可以参考官方文档：<a href="https://clickhouse.yandex/docs/zh/data_types/">https://clickhouse.yandex/docs/zh/data_types/</a></p><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><p>​        Array(T)：由 T 类型元素组成的数组。<br>​        T 可以是任意类型，包含数组类型。 但不推荐使用多维数组，ClickHouse 对多维数组的支持有限。例如，不能在MergeTree表中存储多维数组。<br><strong>创建数组方式1，使用array函数</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array(T)</span><br><span class="line">hadoop202 :) SELECT array(1, 2) AS x, toTypeName(x) ;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217004531.png"></p><p><strong>创建数组方式2：使用方括号</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[]</span><br><span class="line">hadoop202 :) SELECT [1, 2] AS x, toTypeName(x);</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217004600.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;官方文档：&lt;a href=&quot;https://clickhouse.yandex/docs/zh/data_types/&quot;&gt;https://clickhouse.yandex/docs/zh/data_types/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;整型-主要用到&quot;&gt;&lt;a href=&quot;#整型-主要用到&quot; class=&quot;headerlink&quot; title=&quot;整型(主要用到)&quot;&gt;&lt;/a&gt;整型(主要用到)&lt;/h3&gt;&lt;p&gt;固定长度的整型，包括有符号整型或无符号整型。&lt;/p&gt;
&lt;p&gt;整型范围（-2n-1~2n-1-1）：&lt;/p&gt;
&lt;p&gt;Int8 - [-128 : 127]&lt;/p&gt;
&lt;p&gt;Int16 - [-32768 : 32767]&lt;/p&gt;
&lt;p&gt;Int32 - [-2147483648 : 2147483647]&lt;/p&gt;
&lt;p&gt;Int64 - [-9223372036854775808 : 9223372036854775807]&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="clickhouse" scheme="http://xubatian.cn/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse入门</title>
    <link href="http://xubatian.cn/ClickHouse%E5%85%A5%E9%97%A8/"/>
    <id>http://xubatian.cn/ClickHouse%E5%85%A5%E9%97%A8/</id>
    <published>2022-02-16T16:19:36.000Z</published>
    <updated>2022-02-16T16:44:32.002Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ClickHouse入门"><a href="#ClickHouse入门" class="headerlink" title="ClickHouse入门"></a>ClickHouse入门</h1><p>​        ClickHouse 是俄罗斯的Yandex于2016年开源的列式存储数据库（DBMS），使用C++语言编写，主要用于<strong>在线分析处理查询（OLAP），能够使用SQL查询实时生成分析数据报告。</strong>  </p><p>官方文档：<a href="https://clickhouse.yandex/docs/zh/data_types/">https://clickhouse.yandex/docs/zh/data_types/</a></p><span id="more"></span><h2 id="ClickHouse的特点"><a href="#ClickHouse的特点" class="headerlink" title="ClickHouse的特点"></a>ClickHouse的特点</h2><h3 id="列式存储"><a href="#列式存储" class="headerlink" title="列式存储"></a>列式存储</h3><p>以下面的表为例：</p><table><thead><tr><th><strong>Id</strong></th><th><strong>Name</strong></th><th><strong>Age</strong></th></tr></thead><tbody><tr><td>1</td><td>张三</td><td>18</td></tr><tr><td>2</td><td>李四</td><td>22</td></tr><tr><td>3</td><td>王五</td><td>34</td></tr></tbody></table><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217002324.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217002355.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217002441.png"></p><p><strong>OALP: 可以做查询,插入,但是不擅长删除和修改操作. 其实这个也可以理解. 因为我大数据拿到数据,会有对数据进行删除和修改的场景吗? 基本没有.</strong></p><h3 id="DBMS的功能"><a href="#DBMS的功能" class="headerlink" title="DBMS的功能"></a>DBMS的功能</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">几乎覆盖了标准SQL的大部分语法，包括 DDL和 DML(crud)，以及配套的各种函数，用户管理及权限管理，数据的备份与恢复</span><br></pre></td></tr></table></figure><h3 id="多样化引擎"><a href="#多样化引擎" class="headerlink" title="多样化引擎"></a>多样化引擎</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ClickHouse和MySQL类似，把表级的存储引擎插件化，根据表的不同需求可以设定不同的存储引擎。目前包括合并树、日志、接口和其他四大类20多种引擎。</span><br></pre></td></tr></table></figure><p><strong>表级的存储引擎插件化:</strong><br>        就是,这张表使用则引擎, 那张表使用那个引擎.<br>        可以根据本表的需求使用不同的引擎.<br>        目前包括树的,日志的,接口的,和其他四大类20多种引擎.<br>        Mysql有InnoDB和myISAM,InnoDB支持事务,myISQM不支持事务.</p><h3 id="高吞吐写入能力"><a href="#高吞吐写入能力" class="headerlink" title="高吞吐写入能力"></a>高吞吐写入能力</h3><p>​        ClickHouse采用类LSM Tree的结构，数据写入后定期在后台Compaction。通过类LSM tree的结构，<strong>ClickHouse在数据导入时全部是顺序append写，写入后数据段不可更改，在后台compaction(合并)时也是多个段merge sort后顺序写回磁盘</strong>。顺序写的特性，充分利用了磁盘的吞吐能力，即便在HDD上也有着优异的写入性能。<br>​        官方公开benchmark测试显示能够达到50MB-200MB/s的写入吞吐能力，按照每行100Byte估算，大约相当于50W-200W条/s的写入速度。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217002815.png"></p><h3 id="数据分区与线程级并行"><a href="#数据分区与线程级并行" class="headerlink" title="数据分区与线程级并行"></a>数据分区与线程级并行</h3><p>​        ClickHouse将数据划分为多个partition，每个partition再进一步划分为多个index granularity(索引力度)，然后通过多个CPU核心分别处理其中的一部分来实现并行数据处理。在这种设计下，单条Query就能利用整机所有CPU。极致的并行处理能力，极大的降低了查询延时。<br>​        所以，ClickHouse即使对于大量数据的查询也能够化整为零平行处理。但是有一个弊端就是对于单条查询使用多cpu，就不利于同时并发多条查询。所以对于高qps的查询业务，ClickHouse并不是强项。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217002917.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217003005.png"></p><h3 id="性能对比"><a href="#性能对比" class="headerlink" title="性能对比"></a>性能对比</h3><p>某网站精华帖，中对几款数据库做了性能对比。</p><h4 id="单表查询"><a href="#单表查询" class="headerlink" title="单表查询"></a>单表查询</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217003054.png"></p><h4 id="关联查询"><a href="#关联查询" class="headerlink" title="关联查询"></a>关联查询</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217003115.png"></p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>​     ClickHouse像很多OLAP数据库一样，单表查询速度由于关联查询，而且ClickHouse的两者差距更为明显。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220217003219.png"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;ClickHouse入门&quot;&gt;&lt;a href=&quot;#ClickHouse入门&quot; class=&quot;headerlink&quot; title=&quot;ClickHouse入门&quot;&gt;&lt;/a&gt;ClickHouse入门&lt;/h1&gt;&lt;p&gt;​        ClickHouse 是俄罗斯的Yandex于2016年开源的列式存储数据库（DBMS），使用C++语言编写，主要用于&lt;strong&gt;在线分析处理查询（OLAP），能够使用SQL查询实时生成分析数据报告。&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;官方文档：&lt;a href=&quot;https://clickhouse.yandex/docs/zh/data_types/&quot;&gt;https://clickhouse.yandex/docs/zh/data_types/&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="clickhouse" scheme="http://xubatian.cn/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>常用的shell脚本</title>
    <link href="http://xubatian.cn/%E5%B8%B8%E7%94%A8%E7%9A%84shell%E8%84%9A%E6%9C%AC/"/>
    <id>http://xubatian.cn/%E5%B8%B8%E7%94%A8%E7%9A%84shell%E8%84%9A%E6%9C%AC/</id>
    <published>2022-02-16T14:17:52.000Z</published>
    <updated>2022-02-16T14:58:13.787Z</updated>
    
    <content type="html"><![CDATA[<h1 id="myhadoop-sh"><a href="#myhadoop-sh" class="headerlink" title="myhadoop.sh"></a>myhadoop.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 hadoop集群 ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; --------------- 启动 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/sbin/start-dfs.sh&quot;</span><br><span class="line">        echo &quot; --------------- 启动 yarn ---------------&quot;</span><br><span class="line">        ssh hadoop103 &quot;/opt/module/hadoop-3.1.3/sbin/start-yarn.sh&quot;</span><br><span class="line">        echo &quot; --------------- 启动 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 hadoop集群 ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; --------------- 关闭 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 yarn ---------------&quot;</span><br><span class="line">        ssh hadoop103 &quot;/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><span id="more"></span><h1 id="myzookeeper-sh"><a href="#myzookeeper-sh" class="headerlink" title="myzookeeper.sh"></a>myzookeeper.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">for i in hadoop100 hadoop101 hadoop102 </span><br><span class="line">do</span><br><span class="line">        echo ---------- zookeeper $i 启动 ------------</span><br><span class="line">ssh $i &quot;/opt/module/zookeeper-3.5.7/bin/zkServer.sh start&quot;</span><br><span class="line">done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">for i in hadoop100 hadoop101 hadoop102 </span><br><span class="line">do</span><br><span class="line">        echo ---------- zookeeper $i 停止 ------------    </span><br><span class="line">ssh $i &quot;/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop&quot;</span><br><span class="line">done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;status&quot;)&#123;</span><br><span class="line">for i in hadoop100 hadoop101 hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">        echo ---------- zookeeper $i 状态 ------------    </span><br><span class="line">ssh $i &quot;/opt/module/zookeeper-3.5.7/bin/zkServer.sh status&quot;</span><br><span class="line">done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="mykafka-sh"><a href="#mykafka-sh" class="headerlink" title="mykafka.sh"></a>mykafka.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">    for i in hadoop100 hadoop101 hadoop102</span><br><span class="line">    do</span><br><span class="line">        echo &quot; --------启动 $i Kafka-------&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/kafka_2.11-2.4.1/bin/kafka-server-start.sh -daemon /opt/module/kafka_2.11-2.4.1/config/server.properties&quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">    for i in hadoop100 hadoop101 hadoop102</span><br><span class="line">    do</span><br><span class="line">        echo &quot; --------停止 $i Kafka-------&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/kafka_2.11-2.4.1/bin/kafka-server-stop.sh stop&quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="mykafka-producer-sh"><a href="#mykafka-producer-sh" class="headerlink" title="mykafka_producer.sh"></a>mykafka_producer.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">topic=$1</span><br><span class="line"></span><br><span class="line">for i in hadoop102</span><br><span class="line">do</span><br><span class="line">    echo &quot;------------ 启动 kafka 生产者 , 生产者主题为: $1 ---------------&quot;</span><br><span class="line">    ssh $i &quot;/opt/module/kafka_2.11-2.4.1/bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic $topic&quot;</span><br><span class="line">done</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="mykafka-listTopic-sh"><a href="#mykafka-listTopic-sh" class="headerlink" title="mykafka_listTopic.sh"></a>mykafka_listTopic.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">for i in hadoop102</span><br><span class="line">do</span><br><span class="line">    echo &quot; ----------查看 kafka 主题--------&quot;</span><br><span class="line">    ssh $i &quot;/opt/module/kafka_2.11-2.4.1/bin/kafka-topics.sh --zookeeper hadoop102:2181/kafka_2.11-2.4.1 --list&quot;</span><br><span class="line">done</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="mykafka-deleteTopic-sh"><a href="#mykafka-deleteTopic-sh" class="headerlink" title="mykafka_deleteTopic.sh"></a>mykafka_deleteTopic.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">delete_topic=$1</span><br><span class="line"></span><br><span class="line">for i in hadoop102</span><br><span class="line">do</span><br><span class="line">    echo &quot;------------ 删除 kafka $1 主题 ---------------&quot;</span><br><span class="line">    ssh $i &quot;/opt/module/kafka_2.11-2.4.1/bin/kafka-topics.sh --delete --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka_2.11-2.4.1 --topic $delete_topic&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h1 id="mykafka-createTopic-sh"><a href="#mykafka-createTopic-sh" class="headerlink" title="mykafka_createTopic.sh"></a>mykafka_createTopic.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">#$1: 脚本的第一个参数 做非空判断</span><br><span class="line">if [ -n &quot;$1&quot; ] ;then</span><br><span class="line">   one=$1</span><br><span class="line">else </span><br><span class="line">   echo &quot;请传入副本数&quot;</span><br><span class="line">   exit</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#$2: 脚本的第二个参数 做非空判断</span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">   two=$2</span><br><span class="line">else </span><br><span class="line">   echo &quot;请传入分区数&quot;</span><br><span class="line">   exit</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#$3: 脚本的第三个参数 做非空判断</span><br><span class="line">if [ -n &quot;$3&quot; ] ;then</span><br><span class="line">   topic=$3</span><br><span class="line">else </span><br><span class="line">   echo &quot;请传入需要创建的主题名称&quot;</span><br><span class="line">   exit</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for i in hadoop102</span><br><span class="line">do</span><br><span class="line">    echo &quot;------------ 创建 kafka 主题 , 主题为: $topic ---------------&quot;</span><br><span class="line">    ssh $i &quot;/opt/module/kafka_2.11-2.4.1/bin/kafka-topics.sh --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka_2.11-2.4.1  --create --replication-factor $one --partitions $two --topic $topic&quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># bin/kafka-topics.sh --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka_2.11-2.4.1  --create --replication-factor 1 --partitions 1 --topic topic_log</span><br></pre></td></tr></table></figure><h1 id="mykafka-consumer-sh"><a href="#mykafka-consumer-sh" class="headerlink" title="mykafka_consumer.sh"></a>mykafka_consumer.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">topic=$1</span><br><span class="line"></span><br><span class="line">for i in hadoop102</span><br><span class="line">do</span><br><span class="line">    echo &quot;------------ 启动 kafka 消费者 , 消费者主题为: $1 ---------------&quot;</span><br><span class="line">    ssh $i &quot;/opt/module/kafka_2.11-2.4.1/bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic $topic&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h1 id="mykylin-sh"><a href="#mykylin-sh" class="headerlink" title="mykylin.sh"></a>mykylin.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">    </span><br><span class="line">    for i in hadoop102</span><br><span class="line">    do</span><br><span class="line">        echo &quot; =================== 启动 kylin ===================&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/kylin-3.0.2/bin/kylin.sh start&quot;</span><br><span class="line">    done</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        for i in hadoop102</span><br><span class="line">    do</span><br><span class="line">        echo &quot; =================== 关闭 kylin ===================&quot;</span><br><span class="line"></span><br><span class="line">        ssh $i &quot;/opt/module/kylin-3.0.2/bin/kylin.sh stop&quot;</span><br><span class="line">    done</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="kylin-build-cube-sh"><a href="#kylin-build-cube-sh" class="headerlink" title="kylin_build_cube.sh"></a>kylin_build_cube.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">cube_name=order_cube</span><br><span class="line">do_date=`date -d &#x27;-1 day&#x27; +%F`</span><br><span class="line"></span><br><span class="line">#获取00:00时间戳</span><br><span class="line">start_date_unix=`date -d &quot;$do_date 08:00:00&quot; +%s`</span><br><span class="line">start_date=$(($start_date_unix*1000))</span><br><span class="line"></span><br><span class="line">#获取24:00的时间戳</span><br><span class="line">stop_date=$(($start_date+86400000))</span><br><span class="line"></span><br><span class="line">curl -X PUT -H &quot;Authorization: Basic QURNSU46S1lMSU4=&quot; -H &#x27;Content-Type: application/json&#x27; -d &#x27;&#123;&quot;startTime&quot;:&#x27;$start_date&#x27;, &quot;endTime&quot;:&#x27;$stop_date&#x27;, &quot;buildType&quot;:&quot;BUILD&quot;&#125;&#x27; http://hadoop102:7070/kylin/api/cubes/$cube_name/build</span><br></pre></td></tr></table></figure><h1 id="mysolr-sh"><a href="#mysolr-sh" class="headerlink" title="mysolr.sh"></a>mysolr.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">    for i in hadoop100 hadoop101 hadoop102 hadoop103 hadoop104</span><br><span class="line">    do</span><br><span class="line">        echo &quot; --------启动 $i solr-------&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/solr-5.2.1/bin/solr start&quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">    for i in hadoop100 hadoop101 hadoop102 hadoop103 hadoop104</span><br><span class="line">    do</span><br><span class="line">        echo &quot; --------停止 $i solr-------&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/solr-5.2.1/bin/solr stop&quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="myzabbix-sh"><a href="#myzabbix-sh" class="headerlink" title="myzabbix.sh"></a>myzabbix.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 Zabbix ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo ===hadoop102==</span><br><span class="line"></span><br><span class="line">        ssh hadoop102 &quot;sudo systemctl start zabbix-server zabbix-agent httpd rh-php72-php-fpm&quot;</span><br><span class="line"></span><br><span class="line">        # sleep 5</span><br><span class="line"></span><br><span class="line">        # ssh hadoop102 &quot;sudo systemctl enable zabbix-server zabbix-agent httpd rh-php72-php-fpm&quot;</span><br><span class="line"></span><br><span class="line">        echo ===hadoop100==</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop100 &quot;sudo systemctl start zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop100 &quot;sudo systemctl enable zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        echo ===hadoop101==</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop101 &quot;sudo systemctl start zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop101 &quot;sudo systemctl enable zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        echo ===hadoop103==</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop103 &quot;sudo systemctl start zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop103 &quot;sudo systemctl enable zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        echo ===hadoop104==</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop104 &quot;sudo systemctl start zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop104 &quot;sudo systemctl enable zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 Zabbix ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo ===hadoop102==</span><br><span class="line"></span><br><span class="line">        ssh hadoop102 &quot;sudo systemctl stop zabbix-server zabbix-agent httpd rh-php72-php-fpm&quot;</span><br><span class="line"></span><br><span class="line">        # sleep 5</span><br><span class="line"></span><br><span class="line">        # ssh hadoop102 &quot;sudo systemctl disable zabbix-server zabbix-agent httpd rh-php72-php-fpm&quot;</span><br><span class="line"></span><br><span class="line">        echo ===hadoop100==</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop100 &quot;sudo systemctl stop zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop100 &quot;sudo systemctl disable zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        echo ===hadoop101==</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop101 &quot;sudo systemctl stop zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop101 &quot;sudo systemctl disable zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        echo ===hadoop103==</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop103 &quot;sudo systemctl stop zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop103 &quot;sudo systemctl disable zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        echo ===hadoop104==</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop104 &quot;sudo systemctl stop zabbix-agent&quot;</span><br><span class="line"></span><br><span class="line">        sleep 5</span><br><span class="line"></span><br><span class="line">        ssh hadoop104 &quot;sudo systemctl disable zabbix-agent&quot;</span><br><span class="line">       </span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="ps-ef-sh"><a href="#ps-ef-sh" class="headerlink" title="ps-ef.sh"></a>ps-ef.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">echo ================== 查看本机所有进程,为zabbix监控所需: ps -ef | grep $1==================</span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...select All : ps -ef&quot;</span><br><span class="line">    ps -ef </span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line">ps -ef | grep $1</span><br></pre></td></tr></table></figure><h1 id="rpm-qa-sh"><a href="#rpm-qa-sh" class="headerlink" title="rpm-qa.sh"></a>rpm-qa.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">echo ================== 查询系统当中是否有这个安装包:&quot;sudo rpm qa | grep&quot; $1 ==================</span><br><span class="line">sudo rpm -qa | grep $1</span><br></pre></td></tr></table></figure><h1 id="superset-sh"><a href="#superset-sh" class="headerlink" title="superset.sh"></a>superset.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">echo === 启动superset命令: gunicorn --workers 5 --timeout 120 --bind 0.0.0.0:8787  &#x27;&quot;superset.app:create_app()&quot;&#x27; --daemon  ===</span><br><span class="line">echo === 停止superset命令: &quot;ps -ef | awk &#x27;/superset/ &amp;&amp; !/awk/&#123;print $2&#125;&#x27; | xargs kill -9&quot; ====</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">superset_status()&#123;</span><br><span class="line">    result=`ps -ef | awk &#x27;/gunicorn/ &amp;&amp; !/awk/&#123;print $2&#125;&#x27; | wc -l`</span><br><span class="line">    if [[ $result -eq 0 ]]; then</span><br><span class="line">        return 0</span><br><span class="line">    else</span><br><span class="line">        return 1</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line">superset_start()&#123;</span><br><span class="line">        source ~/.bashrc</span><br><span class="line">        superset_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">        if [[ $? -eq 0 ]]; then</span><br><span class="line">            conda activate superset ; gunicorn --workers 5 --timeout 120 --bind hadoop102:8787 --daemon &#x27;superset.app:create_app()&#x27;</span><br><span class="line">        else</span><br><span class="line">            echo &quot;superset正在运行&quot;</span><br><span class="line">        fi</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">superset_stop()&#123;</span><br><span class="line">    source ~/.bashrc</span><br><span class="line">    superset_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">    if [[ $? -eq 0 ]]; then</span><br><span class="line">        echo &quot;superset未在运行&quot;</span><br><span class="line">    else</span><br><span class="line">        ps -ef | awk &#x27;/gunicorn/ &amp;&amp; !/awk/&#123;print $2&#125;&#x27; | xargs kill -9 ; conda deactivate</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    start )</span><br><span class="line">        echo &quot;启动Superset&quot;</span><br><span class="line">        superset_start</span><br><span class="line">    ;;</span><br><span class="line">    stop )</span><br><span class="line">        echo &quot;停止Superset&quot;</span><br><span class="line">        superset_stop</span><br><span class="line">    ;;</span><br><span class="line">    restart )</span><br><span class="line">        echo &quot;重启Superset&quot;</span><br><span class="line">        superset_stop</span><br><span class="line">        superset_start</span><br><span class="line">    ;;</span><br><span class="line">    status )</span><br><span class="line">        superset_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">        if [[ $? -eq 0 ]]; then</span><br><span class="line">            echo &quot;superset未在运行&quot;</span><br><span class="line">        else</span><br><span class="line">            echo &quot;superset正在运行&quot;</span><br><span class="line">        fi</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="conda-superset-sh"><a href="#conda-superset-sh" class="headerlink" title="conda-superset.sh"></a>conda-superset.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">echo === 使用conda的python环境管理更改节点的python环境为: Python 3.7.11  进入命令是:conda activate superset 退出命令是:conda deactivate ,因为superset仅支持Python 3.7.11及以上的版本 ===</span><br><span class="line"></span><br><span class="line"># if [ $# -lt 1 ]</span><br><span class="line"># then</span><br><span class="line">#     echo &quot;No Args Input...&quot;</span><br><span class="line">#     exit ;</span><br><span class="line"># fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># conda_start()&#123;</span><br><span class="line">#         source ~/.bashrc</span><br><span class="line">#         # superset_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">#         if [[ $? -eq 0 ]]; then</span><br><span class="line">#             conda activate superset</span><br><span class="line">#         else</span><br><span class="line">#             echo &quot;conda_superset正在运行&quot;</span><br><span class="line">#         fi</span><br><span class="line"></span><br><span class="line"># &#125;</span><br><span class="line"></span><br><span class="line"># conda_stop()&#123;</span><br><span class="line">#     # superset_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">#     if [[ $? -eq 0 ]]; then</span><br><span class="line">#         conda deactivate</span><br><span class="line">#     else</span><br><span class="line">#         conda deactivate</span><br><span class="line">#     fi</span><br><span class="line"># &#125;</span><br><span class="line"></span><br><span class="line"># case $1 in</span><br><span class="line">#     start )</span><br><span class="line">#         echo &quot;启动conda-superset&quot;</span><br><span class="line">#         conda_start</span><br><span class="line">#     ;;</span><br><span class="line">#     stop )</span><br><span class="line">#         echo &quot;停止conda-superset&quot;</span><br><span class="line">#         conda_stop</span><br><span class="line">#     ;;</span><br><span class="line"># esac</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="datax-import-config-py"><a href="#datax-import-config-py" class="headerlink" title="datax_import_config.py"></a>datax_import_config.py</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"># coding=utf-8</span><br><span class="line">import json</span><br><span class="line">import getopt</span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">import MySQLdb</span><br><span class="line"></span><br><span class="line">#MySQL相关配置，需根据实际情况作出修改</span><br><span class="line">mysql_host = &quot;hadoop102&quot;</span><br><span class="line">mysql_port = &quot;3306&quot;</span><br><span class="line">mysql_user = &quot;root&quot;</span><br><span class="line">mysql_passwd = &quot;shangbaishuyao&quot;</span><br><span class="line"></span><br><span class="line">#HDFS NameNode相关配置，需根据实际情况作出修改</span><br><span class="line">hdfs_nn_host = &quot;hadoop102&quot;</span><br><span class="line">hdfs_nn_port = &quot;8020&quot;</span><br><span class="line"></span><br><span class="line">#生成配置文件的目标路径，可根据实际情况作出修改</span><br><span class="line">output_path = &quot;/opt/module/datax/job/mysql_import_hdfs_table_json_files&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_connection():</span><br><span class="line">    return MySQLdb.connect(host=mysql_host, port=int(mysql_port), user=mysql_user, passwd=mysql_passwd)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_mysql_meta(database, table):</span><br><span class="line">    connection = get_connection()</span><br><span class="line">    cursor = connection.cursor()</span><br><span class="line">    sql = &quot;SELECT COLUMN_NAME,DATA_TYPE from information_schema.COLUMNS WHERE TABLE_SCHEMA=%s AND TABLE_NAME=%s ORDER BY ORDINAL_POSITION&quot;</span><br><span class="line">    cursor.execute(sql, [database, table])</span><br><span class="line">    fetchall = cursor.fetchall()</span><br><span class="line">    cursor.close()</span><br><span class="line">    connection.close()</span><br><span class="line">    return fetchall</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_mysql_columns(database, table):</span><br><span class="line">    return map(lambda x: x[0], get_mysql_meta(database, table))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_hive_columns(database, table):</span><br><span class="line">    def type_mapping(mysql_type):</span><br><span class="line">        mappings = &#123;</span><br><span class="line">            &quot;bigint&quot;: &quot;bigint&quot;,</span><br><span class="line">            &quot;int&quot;: &quot;bigint&quot;,</span><br><span class="line">            &quot;smallint&quot;: &quot;bigint&quot;,</span><br><span class="line">            &quot;tinyint&quot;: &quot;bigint&quot;,</span><br><span class="line">            &quot;decimal&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;double&quot;: &quot;double&quot;,</span><br><span class="line">            &quot;float&quot;: &quot;float&quot;,</span><br><span class="line">            &quot;binary&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;char&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;varchar&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;datetime&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;time&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;timestamp&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;date&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;text&quot;: &quot;string&quot;</span><br><span class="line">        &#125;</span><br><span class="line">        return mappings[mysql_type]</span><br><span class="line"></span><br><span class="line">    meta = get_mysql_meta(database, table)</span><br><span class="line">    return map(lambda x: &#123;&quot;name&quot;: x[0], &quot;type&quot;: type_mapping(x[1].lower())&#125;, meta)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generate_json(source_database, source_table):</span><br><span class="line">    job = &#123;</span><br><span class="line">        &quot;job&quot;: &#123;</span><br><span class="line">            &quot;setting&quot;: &#123;</span><br><span class="line">                &quot;speed&quot;: &#123;</span><br><span class="line">                    &quot;channel&quot;: 3</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;errorLimit&quot;: &#123;</span><br><span class="line">                    &quot;record&quot;: 0,</span><br><span class="line">                    &quot;percentage&quot;: 0.02</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;content&quot;: [&#123;</span><br><span class="line">                &quot;reader&quot;: &#123;</span><br><span class="line">                    &quot;name&quot;: &quot;mysqlreader&quot;,</span><br><span class="line">                    &quot;parameter&quot;: &#123;</span><br><span class="line">                        &quot;username&quot;: mysql_user,</span><br><span class="line">                        &quot;password&quot;: mysql_passwd,</span><br><span class="line">                        &quot;column&quot;: get_mysql_columns(source_database, source_table),</span><br><span class="line">                        &quot;splitPk&quot;: &quot;&quot;,</span><br><span class="line">                        &quot;connection&quot;: [&#123;</span><br><span class="line">                            &quot;table&quot;: [source_table],</span><br><span class="line">                            &quot;jdbcUrl&quot;: [&quot;jdbc:mysql://&quot; + mysql_host + &quot;:&quot; + mysql_port + &quot;/&quot; + source_database]</span><br><span class="line">                        &#125;]</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;writer&quot;: &#123;</span><br><span class="line">                    &quot;name&quot;: &quot;hdfswriter&quot;,</span><br><span class="line">                    &quot;parameter&quot;: &#123;</span><br><span class="line">                        &quot;defaultFS&quot;: &quot;hdfs://&quot; + hdfs_nn_host + &quot;:&quot; + hdfs_nn_port,</span><br><span class="line">                        &quot;fileType&quot;: &quot;text&quot;,</span><br><span class="line">                        &quot;path&quot;: &quot;$&#123;targetdir&#125;&quot;,</span><br><span class="line">                        &quot;fileName&quot;: source_table,</span><br><span class="line">                        &quot;column&quot;: get_hive_columns(source_database, source_table),</span><br><span class="line">                        &quot;writeMode&quot;: &quot;append&quot;,</span><br><span class="line">                        &quot;fieldDelimiter&quot;: &quot;\t&quot;,</span><br><span class="line">                        &quot;compress&quot;: &quot;gzip&quot;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    if not os.path.exists(output_path):</span><br><span class="line">        os.makedirs(output_path)</span><br><span class="line">    with open(os.path.join(output_path, &quot;.&quot;.join([source_database, source_table, &quot;json&quot;])), &quot;w&quot;) as f:</span><br><span class="line">        json.dump(job, f)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main(args):</span><br><span class="line">    source_database = &quot;&quot;</span><br><span class="line">    source_table = &quot;&quot;</span><br><span class="line"></span><br><span class="line">    options, arguments = getopt.getopt(args, &#x27;-d:-t:&#x27;, [&#x27;sourcedb=&#x27;, &#x27;sourcetbl=&#x27;])</span><br><span class="line">    for opt_name, opt_value in options:</span><br><span class="line">        if opt_name in (&#x27;-d&#x27;, &#x27;--sourcedb&#x27;):</span><br><span class="line">            source_database = opt_value</span><br><span class="line">        if opt_name in (&#x27;-t&#x27;, &#x27;--sourcetbl&#x27;):</span><br><span class="line">            source_table = opt_value</span><br><span class="line"></span><br><span class="line">    generate_json(source_database, source_table)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    main(sys.argv[1:])</span><br></pre></td></tr></table></figure><h1 id="datax-import-config-sh"><a href="#datax-import-config-sh" class="headerlink" title="datax_import_config.sh"></a>datax_import_config.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t activity_info</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t activity_rule</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t base_category1</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t base_category2</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t base_category3</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t base_dic</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t base_province</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t base_region</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t base_trademark</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t cart_info</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t coupon_info</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t sku_attr_value</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t sku_info</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t sku_sale_attr_value</span><br><span class="line">python ~/bin/datax_import_config.py -d gmall -t spu_info</span><br></pre></td></tr></table></figure><h1 id="datax-import-config-files-sh"><a href="#datax-import-config-files-sh" class="headerlink" title="datax_import_config_files.sh"></a>datax_import_config_files.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">#全量表数据同步脚本</span><br><span class="line"></span><br><span class="line">DATAX_HOME=/opt/module/datax</span><br><span class="line"></span><br><span class="line"># 如果传入日期则do_date等于传入的日期，否则等于前一天日期</span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else</span><br><span class="line">    do_date=`date -d &quot;-1 day&quot; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#处理目标路径，此处的处理逻辑是，如果目标路径不存在，则创建；若存在，则清空，目的是保证同步任务可重复执行</span><br><span class="line">handle_targetdir() &#123;</span><br><span class="line">  hadoop fs -test -e $1</span><br><span class="line">  if [[ $? -eq 1 ]]; then</span><br><span class="line">    echo &quot;路径$1不存在，正在创建......&quot;</span><br><span class="line">    hadoop fs -mkdir -p $1</span><br><span class="line">  else</span><br><span class="line">    echo &quot;路径$1已经存在&quot;</span><br><span class="line">    fs_count=$(hadoop fs -count $1)</span><br><span class="line">    content_size=$(echo $fs_count | awk &#x27;&#123;print $3&#125;&#x27;)</span><br><span class="line">    if [[ $content_size -eq 0 ]]; then</span><br><span class="line">      echo &quot;路径$1为空&quot;</span><br><span class="line">    else</span><br><span class="line">      echo &quot;路径$1不为空，正在清空......&quot;</span><br><span class="line">      hadoop fs -rm -r -f $1/*</span><br><span class="line">    fi</span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#数据同步</span><br><span class="line">import_data() &#123;</span><br><span class="line">  datax_config=$1</span><br><span class="line">  target_dir=$2</span><br><span class="line"></span><br><span class="line">  handle_targetdir $target_dir</span><br><span class="line">  python $DATAX_HOME/bin/datax.py -p&quot;-Dtargetdir=$target_dir&quot; $datax_config</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;activity_info&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.activity_info.json /origin_data/gmall/db/ods_activity_info/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;activity_rule&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.activity_rule.json /origin_data/gmall/db/ods_activity_rule/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;base_category1&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_category1.json /origin_data/gmall/db/ods_base_category1/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;base_category2&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_category2.json /origin_data/gmall/db/ods_base_category2/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;base_category3&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_category3.json /origin_data/gmall/db/ods_base_category3/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;base_dic&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_dic.json /origin_data/gmall/db/ods_base_dic/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;base_province&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_province.json /origin_data/gmall/db/ods_base_province/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;base_region&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_region.json /origin_data/gmall/db/ods_base_region/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;base_trademark&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_trademark.json /origin_data/gmall/db/ods_base_trademark/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;cart_info&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.cart_info.json /origin_data/gmall/db/ods_cart_info/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;coupon_info&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.coupon_info.json /origin_data/gmall/db/ods_coupon_info/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;sku_attr_value&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.sku_attr_value.json /origin_data/gmall/db/ods_sku_attr_value/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;sku_info&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.sku_info.json /origin_data/gmall/db/ods_sku_info/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;sku_sale_attr_value&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.sku_sale_attr_value.json /origin_data/gmall/db/ods_sku_sale_attr_value/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;spu_info&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.spu_info.json /origin_data/gmall/db/ods_spu_info/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;all&quot;)</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.activity_info.json /origin_data/gmall/db/ods_activity_info/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.activity_rule.json /origin_data/gmall/db/ods_activity_rule/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_category1.json /origin_data/gmall/db/ods_base_category1/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_category2.json /origin_data/gmall/db/ods_base_category2/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_category3.json /origin_data/gmall/db/ods_base_category3/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_dic.json /origin_data/gmall/db/ods_base_dic/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_province.json /origin_data/gmall/db/ods_base_province/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_region.json /origin_data/gmall/db/ods_base_region/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.base_trademark.json /origin_data/gmall/db/ods_base_trademark/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.cart_info.json /origin_data/gmall/db/ods_cart_info/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.coupon_info.json /origin_data/gmall/db/ods_coupon_info/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.sku_attr_value.json /origin_data/gmall/db/ods_sku_attr_value/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.sku_info.json /origin_data/gmall/db/ods_sku_info/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.sku_sale_attr_value.json /origin_data/gmall/db/ods_sku_sale_attr_value/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/mysql_import_hdfs_table_json_files/gmall.spu_info.json /origin_data/gmall/db/ods_spu_info/$do_date</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="db-maxwell-kafka-flume-hdfs-sh"><a href="#db-maxwell-kafka-flume-hdfs-sh" class="headerlink" title="db_maxwell_kafka_flume_hdfs.sh"></a>db_maxwell_kafka_flume_hdfs.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; --------启动 hadoop101上的flume采集maxwell传到kafka上的业务数据-------&quot;</span><br><span class="line">        ssh hadoop100 &quot;nohup /opt/module/flume-1.9.0/bin/flume-ng agent -n a1 -c /opt/module/flume-1.9.0/conf -f /opt/module/flume-1.9.0/db_maxwell_kafka_topic/db_maxwell_kafka_topic.conf &gt;/dev/null 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line"></span><br><span class="line">        echo &quot; --------停止 hadoop101上的flume采集maxwell传到kafka上的业务数据--------&quot;</span><br><span class="line">        ssh hadoop100 &quot;ps -ef | grep db_maxwell_kafka | grep -v grep |awk &#x27;&#123;print \$2&#125;&#x27; | xargs -n1 kill&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#说明1：nohup，该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。nohup就是不挂起的意思，不挂断地运行命令。</span><br><span class="line">#说明2：awk 默认分隔符为空格</span><br><span class="line">#说明3：$2是在“”双引号内部会被解析为脚本的第二个参数，但是这里面想表达的含义是awk的第二个值，所以需要将他转义，用\$2表示。</span><br><span class="line">#说明4：xargs 表示取出前面命令运行的结果，作为后面命令的输入参数。</span><br></pre></td></tr></table></figure><h1 id="hdfs-to-ods-db-every-day-sh"><a href="#hdfs-to-ods-db-every-day-sh" class="headerlink" title="hdfs_to_ods_db_every_day.sh"></a>hdfs_to_ods_db_every_day.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">APP=gmall</span><br><span class="line"></span><br><span class="line"># 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天</span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else </span><br><span class="line">    do_date=`date -d &quot;-1 day&quot; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">ods_order_info=&quot; </span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_info partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_order_detail=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_detail/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_detail partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_sku_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/sku_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_sku_info partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_user_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/user_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_user_info partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_payment_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/payment_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_payment_info partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_base_category1=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_category1/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_category1 partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_base_category2=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_category2/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_category2 partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_base_category3=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_category3/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_category3 partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_base_trademark=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_trademark/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_trademark partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_activity_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/activity_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_activity_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_cart_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/cart_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_cart_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_comment_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/comment_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_comment_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_coupon_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/coupon_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_coupon_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_coupon_use=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/coupon_use/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_coupon_use partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_favor_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/favor_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_favor_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_order_refund_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_refund_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_refund_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_order_status_log=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_status_log/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_status_log partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_spu_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/spu_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_spu_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_activity_rule=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/activity_rule/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_activity_rule partition(dt=&#x27;$do_date&#x27;);&quot; </span><br><span class="line"></span><br><span class="line">ods_base_dic=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_dic/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_dic partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_order_detail_activity=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_detail_activity/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_detail_activity partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_order_detail_coupon=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_detail_coupon/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_detail_coupon partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_refund_payment=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/refund_payment/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_refund_payment partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_sku_attr_value=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/sku_attr_value/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_sku_attr_value partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_sku_sale_attr_value=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/sku_sale_attr_value/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_sku_sale_attr_value partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_base_province=&quot; </span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_province/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_province;&quot;</span><br><span class="line"></span><br><span class="line">ods_base_region=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_region/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_region;&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    &quot;ods_order_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_detail&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_detail&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_sku_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_sku_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_user_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_user_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_payment_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_payment_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_category1&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_category1&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_category2&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_category2&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_category3&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_category3&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_trademark&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_trademark&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_activity_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_activity_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_cart_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_cart_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_comment_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_comment_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_coupon_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_coupon_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_coupon_use&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_coupon_use&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_favor_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_favor_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_refund_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_refund_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_status_log&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_status_log&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_spu_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_spu_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_activity_rule&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_activity_rule&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_dic&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_dic&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_detail_activity&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_detail_activity&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_detail_coupon&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_detail_coupon&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_refund_payment&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_refund_payment&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_sku_attr_value&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_sku_attr_value&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_sku_sale_attr_value&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_sku_sale_attr_value&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;all&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_info$ods_order_detail$ods_sku_info$ods_user_info$ods_payment_info$ods_base_category1$ods_base_category2$ods_base_category3$ods_base_trademark$ods_activity_info$ods_cart_info$ods_comment_info$ods_coupon_info$ods_coupon_use$ods_favor_info$ods_order_refund_info$ods_order_status_log$ods_spu_info$ods_activity_rule$ods_base_dic$ods_order_detail_activity$ods_order_detail_coupon$ods_refund_payment$ods_sku_attr_value$ods_sku_sale_attr_value&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="hdfs-to-ods-db-first-day-sh"><a href="#hdfs-to-ods-db-first-day-sh" class="headerlink" title="hdfs_to_ods_db_first_day.sh"></a>hdfs_to_ods_db_first_day.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">APP=gmall</span><br><span class="line"></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">   do_date=$2</span><br><span class="line">else </span><br><span class="line">   echo &quot;请传入日期参数&quot;</span><br><span class="line">   exit</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line">ods_order_info=&quot; </span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_info partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_order_detail=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_detail/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_detail partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_sku_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/sku_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_sku_info partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_user_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/user_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_user_info partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_payment_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/payment_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_payment_info partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_base_category1=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_category1/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_category1 partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_base_category2=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_category2/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_category2 partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">ods_base_category3=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_category3/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_category3 partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_base_trademark=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_trademark/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_trademark partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_activity_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/activity_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_activity_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_cart_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/cart_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_cart_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_comment_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/comment_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_comment_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_coupon_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/coupon_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_coupon_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_coupon_use=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/coupon_use/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_coupon_use partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_favor_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/favor_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_favor_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_order_refund_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_refund_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_refund_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_order_status_log=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_status_log/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_status_log partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_spu_info=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/spu_info/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_spu_info partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_activity_rule=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/activity_rule/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_activity_rule partition(dt=&#x27;$do_date&#x27;);&quot; </span><br><span class="line"></span><br><span class="line">ods_base_dic=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_dic/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_dic partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_order_detail_activity=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_detail_activity/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_detail_activity partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_order_detail_coupon=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/order_detail_coupon/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_order_detail_coupon partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_refund_payment=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/refund_payment/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_refund_payment partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_sku_attr_value=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/sku_attr_value/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_sku_attr_value partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_sku_sale_attr_value=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/sku_sale_attr_value/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_sku_sale_attr_value partition(dt=&#x27;$do_date&#x27;); &quot;</span><br><span class="line"></span><br><span class="line">ods_base_province=&quot; </span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_province/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_province;&quot;</span><br><span class="line"></span><br><span class="line">ods_base_region=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/db/base_region/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.ods_base_region;&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    &quot;ods_order_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_detail&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_detail&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_sku_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_sku_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_user_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_user_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_payment_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_payment_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_category1&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_category1&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_category2&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_category2&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_category3&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_category3&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_trademark&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_trademark&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_activity_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_activity_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_cart_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_cart_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_comment_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_comment_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_coupon_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_coupon_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_coupon_use&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_coupon_use&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_favor_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_favor_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_refund_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_refund_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_status_log&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_status_log&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_spu_info&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_spu_info&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_activity_rule&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_activity_rule&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_dic&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_dic&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_detail_activity&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_detail_activity&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_order_detail_coupon&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_detail_coupon&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_refund_payment&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_refund_payment&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_sku_attr_value&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_sku_attr_value&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_sku_sale_attr_value&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_sku_sale_attr_value&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_province&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_province&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;ods_base_region&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_base_region&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">    &quot;all&quot;)&#123;</span><br><span class="line">        hive -e &quot;$ods_order_info$ods_order_detail$ods_sku_info$ods_user_info$ods_payment_info$ods_base_category1$ods_base_category2$ods_base_category3$ods_base_trademark$ods_activity_info$ods_cart_info$ods_comment_info$ods_coupon_info$ods_coupon_use$ods_favor_info$ods_order_refund_info$ods_order_status_log$ods_spu_info$ods_activity_rule$ods_base_dic$ods_order_detail_activity$ods_order_detail_coupon$ods_refund_payment$ods_sku_attr_value$ods_sku_sale_attr_value$ods_base_province$ods_base_region&quot;</span><br><span class="line">    &#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="ods-to-dim-db-every-day-sh"><a href="#ods-to-dim-db-every-day-sh" class="headerlink" title="ods_to_dim_db_every_day.sh"></a>ods_to_dim_db_every_day.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">APP=gmall</span><br><span class="line"></span><br><span class="line"># 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天</span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else </span><br><span class="line">    do_date=`date -d &quot;-1 day&quot; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">dim_user_info=&quot;</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">with</span><br><span class="line">tmp as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        old.id old_id,</span><br><span class="line">        old.login_name old_login_name,</span><br><span class="line">        old.nick_name old_nick_name,</span><br><span class="line">        old.name old_name,</span><br><span class="line">        old.phone_num old_phone_num,</span><br><span class="line">        old.email old_email,</span><br><span class="line">        old.user_level old_user_level,</span><br><span class="line">        old.birthday old_birthday,</span><br><span class="line">        old.gender old_gender,</span><br><span class="line">        old.create_time old_create_time,</span><br><span class="line">        old.operate_time old_operate_time,</span><br><span class="line">        old.start_date old_start_date,</span><br><span class="line">        old.end_date old_end_date,</span><br><span class="line">        new.id new_id,</span><br><span class="line">        new.login_name new_login_name,</span><br><span class="line">        new.nick_name new_nick_name,</span><br><span class="line">        new.name new_name,</span><br><span class="line">        new.phone_num new_phone_num,</span><br><span class="line">        new.email new_email,</span><br><span class="line">        new.user_level new_user_level,</span><br><span class="line">        new.birthday new_birthday,</span><br><span class="line">        new.gender new_gender,</span><br><span class="line">        new.create_time new_create_time,</span><br><span class="line">        new.operate_time new_operate_time,</span><br><span class="line">        new.start_date new_start_date,</span><br><span class="line">        new.end_date new_end_date</span><br><span class="line">    from</span><br><span class="line">    (</span><br><span class="line">        select</span><br><span class="line">            id,</span><br><span class="line">            login_name,</span><br><span class="line">            nick_name,</span><br><span class="line">            name,</span><br><span class="line">            phone_num,</span><br><span class="line">            email,</span><br><span class="line">            user_level,</span><br><span class="line">            birthday,</span><br><span class="line">            gender,</span><br><span class="line">            create_time,</span><br><span class="line">            operate_time,</span><br><span class="line">            start_date,</span><br><span class="line">            end_date</span><br><span class="line">        from $&#123;APP&#125;.dim_user_info</span><br><span class="line">        where dt=&#x27;9999-99-99&#x27;</span><br><span class="line">        and start_date&lt;&#x27;$do_date&#x27;</span><br><span class="line">    )old</span><br><span class="line">    full outer join</span><br><span class="line">    (</span><br><span class="line">        select</span><br><span class="line">            id,</span><br><span class="line">            login_name,</span><br><span class="line">            nick_name,</span><br><span class="line">            md5(name) name,</span><br><span class="line">            md5(phone_num) phone_num,</span><br><span class="line">            md5(email) email,</span><br><span class="line">            user_level,</span><br><span class="line">            birthday,</span><br><span class="line">            gender,</span><br><span class="line">            create_time,</span><br><span class="line">            operate_time,</span><br><span class="line">            &#x27;$do_date&#x27; start_date,</span><br><span class="line">            &#x27;9999-99-99&#x27; end_date</span><br><span class="line">        from $&#123;APP&#125;.ods_user_info</span><br><span class="line">        where dt=&#x27;$do_date&#x27;</span><br><span class="line">    )new</span><br><span class="line">    on old.id=new.id</span><br><span class="line">)</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_user_info partition(dt)</span><br><span class="line">select</span><br><span class="line">    nvl(new_id,old_id),</span><br><span class="line">    nvl(new_login_name,old_login_name),</span><br><span class="line">    nvl(new_nick_name,old_nick_name),</span><br><span class="line">    nvl(new_name,old_name),</span><br><span class="line">    nvl(new_phone_num,old_phone_num),</span><br><span class="line">    nvl(new_email,old_email),</span><br><span class="line">    nvl(new_user_level,old_user_level),</span><br><span class="line">    nvl(new_birthday,old_birthday),</span><br><span class="line">    nvl(new_gender,old_gender),</span><br><span class="line">    nvl(new_create_time,old_create_time),</span><br><span class="line">    nvl(new_operate_time,old_operate_time),</span><br><span class="line">    nvl(new_start_date,old_start_date),</span><br><span class="line">    nvl(new_end_date,old_end_date),</span><br><span class="line">    nvl(new_end_date,old_end_date) dt</span><br><span class="line">from tmp</span><br><span class="line">union all</span><br><span class="line">select</span><br><span class="line">    old_id,</span><br><span class="line">    old_login_name,</span><br><span class="line">    old_nick_name,</span><br><span class="line">    old_name,</span><br><span class="line">    old_phone_num,</span><br><span class="line">    old_email,</span><br><span class="line">    old_user_level,</span><br><span class="line">    old_birthday,</span><br><span class="line">    old_gender,</span><br><span class="line">    old_create_time,</span><br><span class="line">    old_operate_time,</span><br><span class="line">    old_start_date,</span><br><span class="line">    cast(date_add(&#x27;$do_date&#x27;,-1) as string),</span><br><span class="line">    cast(date_add(&#x27;$do_date&#x27;,-1) as string) dt</span><br><span class="line">from tmp</span><br><span class="line">where new_id is not null and old_id is not null;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line">dim_sku_info=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">with</span><br><span class="line">sku as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        price,</span><br><span class="line">        sku_name,</span><br><span class="line">        sku_desc,</span><br><span class="line">        weight,</span><br><span class="line">        is_sale,</span><br><span class="line">        spu_id,</span><br><span class="line">        category3_id,</span><br><span class="line">        tm_id,</span><br><span class="line">        create_time</span><br><span class="line">    from $&#123;APP&#125;.ods_sku_info</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">spu as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        spu_name</span><br><span class="line">    from $&#123;APP&#125;.ods_spu_info</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">c3 as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        name,</span><br><span class="line">        category2_id</span><br><span class="line">    from $&#123;APP&#125;.ods_base_category3</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">c2 as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        name,</span><br><span class="line">        category1_id</span><br><span class="line">    from $&#123;APP&#125;.ods_base_category2</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">c1 as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        name</span><br><span class="line">    from $&#123;APP&#125;.ods_base_category1</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">tm as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        tm_name</span><br><span class="line">    from $&#123;APP&#125;.ods_base_trademark</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">attr as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        sku_id,</span><br><span class="line">        collect_set(named_struct(&#x27;attr_id&#x27;,attr_id,&#x27;value_id&#x27;,value_id,&#x27;attr_name&#x27;,attr_name,&#x27;value_name&#x27;,value_name)) attrs</span><br><span class="line">    from $&#123;APP&#125;.ods_sku_attr_value</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">    group by sku_id</span><br><span class="line">),</span><br><span class="line">sale_attr as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        sku_id,</span><br><span class="line">        collect_set(named_struct(&#x27;sale_attr_id&#x27;,sale_attr_id,&#x27;sale_attr_value_id&#x27;,sale_attr_value_id,&#x27;sale_attr_name&#x27;,sale_attr_name,&#x27;sale_attr_value_name&#x27;,sale_attr_value_name)) sale_attrs</span><br><span class="line">    from $&#123;APP&#125;.ods_sku_sale_attr_value</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">    group by sku_id</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_sku_info partition(dt=&#x27;$do_date&#x27;)</span><br><span class="line">select</span><br><span class="line">    sku.id,</span><br><span class="line">    sku.price,</span><br><span class="line">    sku.sku_name,</span><br><span class="line">    sku.sku_desc,</span><br><span class="line">    sku.weight,</span><br><span class="line">    sku.is_sale,</span><br><span class="line">    sku.spu_id,</span><br><span class="line">    spu.spu_name,</span><br><span class="line">    sku.category3_id,</span><br><span class="line">    c3.name,</span><br><span class="line">    c3.category2_id,</span><br><span class="line">    c2.name,</span><br><span class="line">    c2.category1_id,</span><br><span class="line">    c1.name,</span><br><span class="line">    sku.tm_id,</span><br><span class="line">    tm.tm_name,</span><br><span class="line">    attr.attrs,</span><br><span class="line">    sale_attr.sale_attrs,</span><br><span class="line">    sku.create_time</span><br><span class="line">from sku</span><br><span class="line">left join spu on sku.spu_id=spu.id</span><br><span class="line">left join c3 on sku.category3_id=c3.id</span><br><span class="line">left join c2 on c3.category2_id=c2.id</span><br><span class="line">left join c1 on c2.category1_id=c1.id</span><br><span class="line">left join tm on sku.tm_id=tm.id</span><br><span class="line">left join attr on sku.id=attr.sku_id</span><br><span class="line">left join sale_attr on sku.id=sale_attr.sku_id;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line">dim_base_province=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_base_province</span><br><span class="line">select</span><br><span class="line">    bp.id,</span><br><span class="line">    bp.name,</span><br><span class="line">    bp.area_code,</span><br><span class="line">    bp.iso_code,</span><br><span class="line">    bp.iso_3166_2,</span><br><span class="line">    bp.region_id,</span><br><span class="line">    bp.name</span><br><span class="line">from $&#123;APP&#125;.ods_base_province bp</span><br><span class="line">join $&#123;APP&#125;.ods_base_region br on bp.region_id = br.id;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line">dim_coupon_info=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_coupon_info partition(dt=&#x27;$do_date&#x27;)</span><br><span class="line">select</span><br><span class="line">    id,</span><br><span class="line">    coupon_name,</span><br><span class="line">    coupon_type,</span><br><span class="line">    condition_amount,</span><br><span class="line">    condition_num,</span><br><span class="line">    activity_id,</span><br><span class="line">    benefit_amount,</span><br><span class="line">    benefit_discount,</span><br><span class="line">    create_time,</span><br><span class="line">    range_type,</span><br><span class="line">    limit_num,</span><br><span class="line">    taken_count,</span><br><span class="line">    start_time,</span><br><span class="line">    end_time,</span><br><span class="line">    operate_time,</span><br><span class="line">    expire_time</span><br><span class="line">from $&#123;APP&#125;.ods_coupon_info</span><br><span class="line">where dt=&#x27;$do_date&#x27;;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line">dim_activity_rule_info=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_activity_rule_info partition(dt=&#x27;$do_date&#x27;)</span><br><span class="line">select</span><br><span class="line">    ar.id,</span><br><span class="line">    ar.activity_id,</span><br><span class="line">    ai.activity_name,</span><br><span class="line">    ar.activity_type,</span><br><span class="line">    ai.start_time,</span><br><span class="line">    ai.end_time,</span><br><span class="line">    ai.create_time,</span><br><span class="line">    ar.condition_amount,</span><br><span class="line">    ar.condition_num,</span><br><span class="line">    ar.benefit_amount,</span><br><span class="line">    ar.benefit_discount,</span><br><span class="line">    ar.benefit_level</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        activity_id,</span><br><span class="line">        activity_type,</span><br><span class="line">        condition_amount,</span><br><span class="line">        condition_num,</span><br><span class="line">        benefit_amount,</span><br><span class="line">        benefit_discount,</span><br><span class="line">        benefit_level</span><br><span class="line">    from $&#123;APP&#125;.ods_activity_rule</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">)ar</span><br><span class="line">left join</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        activity_name,</span><br><span class="line">        start_time,</span><br><span class="line">        end_time,</span><br><span class="line">        create_time</span><br><span class="line">    from $&#123;APP&#125;.ods_activity_info</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">)ai</span><br><span class="line">on ar.activity_id=ai.id;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;dim_user_info&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_user_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;dim_sku_info&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_sku_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;dim_base_province&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_base_province&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;dim_coupon_info&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_coupon_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;dim_activity_rule_info&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_activity_rule_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;all&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_user_info$dim_sku_info$dim_coupon_info$dim_activity_rule_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="ods-to-dim-db-first-time-sh"><a href="#ods-to-dim-db-first-time-sh" class="headerlink" title="ods_to_dim_db_first_time.sh"></a>ods_to_dim_db_first_time.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">APP=gmall</span><br><span class="line"></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">   do_date=$2</span><br><span class="line">else </span><br><span class="line">   echo &quot;请传入日期参数&quot;</span><br><span class="line">   exit</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line"># 用户维度表（拉链表）</span><br><span class="line">dim_user_info=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_user_info partition(dt=&#x27;9999-99-99&#x27;)</span><br><span class="line">select</span><br><span class="line">    id,</span><br><span class="line">    login_name,</span><br><span class="line">    nick_name,</span><br><span class="line">    md5(name),</span><br><span class="line">    md5(phone_num),</span><br><span class="line">    md5(email),</span><br><span class="line">    user_level,</span><br><span class="line">    birthday,</span><br><span class="line">    gender,</span><br><span class="line">    create_time,</span><br><span class="line">    operate_time,</span><br><span class="line">    &#x27;$do_date&#x27;,</span><br><span class="line">    &#x27;9999-99-99&#x27;</span><br><span class="line">from $&#123;APP&#125;.ods_user_info</span><br><span class="line">where dt=&#x27;$do_date&#x27;;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line"># 商品维度表（全量）</span><br><span class="line">dim_sku_info=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">with</span><br><span class="line">sku as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        price,</span><br><span class="line">        sku_name,</span><br><span class="line">        sku_desc,</span><br><span class="line">        weight,</span><br><span class="line">        is_sale,</span><br><span class="line">        spu_id,</span><br><span class="line">        category3_id,</span><br><span class="line">        tm_id,</span><br><span class="line">        create_time</span><br><span class="line">    from $&#123;APP&#125;.ods_sku_info</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">spu as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        spu_name</span><br><span class="line">    from $&#123;APP&#125;.ods_spu_info</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">c3 as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        name,</span><br><span class="line">        category2_id</span><br><span class="line">    from $&#123;APP&#125;.ods_base_category3</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">c2 as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        name,</span><br><span class="line">        category1_id</span><br><span class="line">    from $&#123;APP&#125;.ods_base_category2</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">c1 as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        name</span><br><span class="line">    from $&#123;APP&#125;.ods_base_category1</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">tm as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        tm_name</span><br><span class="line">    from $&#123;APP&#125;.ods_base_trademark</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">),</span><br><span class="line">attr as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        sku_id,</span><br><span class="line">        collect_set(named_struct(&#x27;attr_id&#x27;,attr_id,&#x27;value_id&#x27;,value_id,&#x27;attr_name&#x27;,attr_name,&#x27;value_name&#x27;,value_name)) attrs</span><br><span class="line">    from $&#123;APP&#125;.ods_sku_attr_value</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">    group by sku_id</span><br><span class="line">),</span><br><span class="line">sale_attr as</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        sku_id,</span><br><span class="line">        collect_set(named_struct(&#x27;sale_attr_id&#x27;,sale_attr_id,&#x27;sale_attr_value_id&#x27;,sale_attr_value_id,&#x27;sale_attr_name&#x27;,sale_attr_name,&#x27;sale_attr_value_name&#x27;,sale_attr_value_name)) sale_attrs</span><br><span class="line">    from $&#123;APP&#125;.ods_sku_sale_attr_value</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">    group by sku_id</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_sku_info partition(dt=&#x27;$do_date&#x27;)</span><br><span class="line">select</span><br><span class="line">    sku.id,</span><br><span class="line">    sku.price,</span><br><span class="line">    sku.sku_name,</span><br><span class="line">    sku.sku_desc,</span><br><span class="line">    sku.weight,</span><br><span class="line">    sku.is_sale,</span><br><span class="line">    sku.spu_id,</span><br><span class="line">    spu.spu_name,</span><br><span class="line">    sku.category3_id,</span><br><span class="line">    c3.name,</span><br><span class="line">    c3.category2_id,</span><br><span class="line">    c2.name,</span><br><span class="line">    c2.category1_id,</span><br><span class="line">    c1.name,</span><br><span class="line">    sku.tm_id,</span><br><span class="line">    tm.tm_name,</span><br><span class="line">    attr.attrs,</span><br><span class="line">    sale_attr.sale_attrs,</span><br><span class="line">    sku.create_time</span><br><span class="line">from sku</span><br><span class="line">left join spu on sku.spu_id=spu.id</span><br><span class="line">left join c3 on sku.category3_id=c3.id</span><br><span class="line">left join c2 on c3.category2_id=c2.id</span><br><span class="line">left join c1 on c2.category1_id=c1.id</span><br><span class="line">left join tm on sku.tm_id=tm.id</span><br><span class="line">left join attr on sku.id=attr.sku_id</span><br><span class="line">left join sale_attr on sku.id=sale_attr.sku_id;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line"># 地区维度表（特殊）</span><br><span class="line">dim_base_province=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_base_province</span><br><span class="line">select</span><br><span class="line">    bp.id,</span><br><span class="line">    bp.name,</span><br><span class="line">    bp.area_code,</span><br><span class="line">    bp.iso_code,</span><br><span class="line">    bp.iso_3166_2,</span><br><span class="line">    bp.region_id,</span><br><span class="line">    br.region_name</span><br><span class="line">from $&#123;APP&#125;.ods_base_province bp</span><br><span class="line">join $&#123;APP&#125;.ods_base_region br on bp.region_id = br.id;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line"># 优惠券维度表（全量）</span><br><span class="line">dim_coupon_info=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_coupon_info partition(dt=&#x27;$do_date&#x27;)</span><br><span class="line">select</span><br><span class="line">    id,</span><br><span class="line">    coupon_name,</span><br><span class="line">    coupon_type,</span><br><span class="line">    condition_amount,</span><br><span class="line">    condition_num,</span><br><span class="line">    activity_id,</span><br><span class="line">    benefit_amount,</span><br><span class="line">    benefit_discount,</span><br><span class="line">    create_time,</span><br><span class="line">    range_type,</span><br><span class="line">    limit_num,</span><br><span class="line">    taken_count,</span><br><span class="line">    start_time,</span><br><span class="line">    end_time,</span><br><span class="line">    operate_time,</span><br><span class="line">    expire_time</span><br><span class="line">from $&#123;APP&#125;.ods_coupon_info</span><br><span class="line">where dt=&#x27;$do_date&#x27;;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line"># 活动维度表（全量）</span><br><span class="line">dim_activity_rule_info=&quot;</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_activity_rule_info partition(dt=&#x27;$do_date&#x27;)</span><br><span class="line">select</span><br><span class="line">    ar.id,</span><br><span class="line">    ar.activity_id,</span><br><span class="line">    ai.activity_name,</span><br><span class="line">    ar.activity_type,</span><br><span class="line">    ai.start_time,</span><br><span class="line">    ai.end_time,</span><br><span class="line">    ai.create_time,</span><br><span class="line">    ar.condition_amount,</span><br><span class="line">    ar.condition_num,</span><br><span class="line">    ar.benefit_amount,</span><br><span class="line">    ar.benefit_discount,</span><br><span class="line">    ar.benefit_level</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        activity_id,</span><br><span class="line">        activity_type,</span><br><span class="line">        condition_amount,</span><br><span class="line">        condition_num,</span><br><span class="line">        benefit_amount,</span><br><span class="line">        benefit_discount,</span><br><span class="line">        benefit_level</span><br><span class="line">    from $&#123;APP&#125;.ods_activity_rule</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">)ar</span><br><span class="line">left join</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        activity_name,</span><br><span class="line">        start_time,</span><br><span class="line">        end_time,</span><br><span class="line">        create_time</span><br><span class="line">    from $&#123;APP&#125;.ods_activity_info</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">)ai</span><br><span class="line">on ar.activity_id=ai.id;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;dim_user_info&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_user_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;dim_sku_info&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_sku_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;dim_base_province&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_base_province&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;dim_coupon_info&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_coupon_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;dim_activity_rule_info&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_activity_rule_info&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;all&quot;)&#123;</span><br><span class="line">    hive -e &quot;$dim_user_info$dim_sku_info$dim_coupon_info$dim_activity_rule_info$dim_base_province&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="tail-f-sh"><a href="#tail-f-sh" class="headerlink" title="tail-f.sh"></a>tail-f.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">echo ================== 展示本地日志文件最后几行输出:tail -f ./$1 ==================</span><br><span class="line">tail -f ./$1</span><br></pre></td></tr></table></figure><h1 id="which-sh"><a href="#which-sh" class="headerlink" title="which.sh"></a>which.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">echo ================== 查看 $1 文件位置的命令 :which $1 ==================</span><br><span class="line">which $1</span><br></pre></td></tr></table></figure><h1 id="azkaban-sh"><a href="#azkaban-sh" class="headerlink" title="azkaban.sh"></a>azkaban.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">start-web()&#123;</span><br><span class="line">        for i in hadoop102</span><br><span class="line">        do</span><br><span class="line">                ssh $i &quot;cd /opt/module/azkaban/azkaban-web-server-3.84.4;bin/start-web.sh&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stop-web()&#123;</span><br><span class="line">        for i in hadoop102</span><br><span class="line">        do</span><br><span class="line">                ssh $i &quot;cd /opt/module/azkaban/azkaban-web-server-3.84.4;bin/shutdown-web.sh&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">start-exec()&#123;</span><br><span class="line">        for i in hadoop100 hadoop101 hadoop102</span><br><span class="line">        do</span><br><span class="line">                ssh $i &quot;cd /opt/module/azkaban/azkaban-exec-server-3.84.4;bin/start-exec.sh&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">active-exec()&#123;</span><br><span class="line">        for i in hadoop100 hadoop101 hadoop102</span><br><span class="line">        do</span><br><span class="line">                ssh $i &quot;curl -G &#x27;$i:12321/executor?action=activate&#x27; &amp;&amp; echo&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stop-exec()&#123;</span><br><span class="line">        for i in hadoop100 hadoop101 hadoop102</span><br><span class="line">        do</span><br><span class="line">                ssh $i &quot;cd /opt/module/azkaban/azkaban-exec-server-3.84.4;bin/shutdown-exec.sh&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">case $1 in   </span><br><span class="line">        start-exec)</span><br><span class="line">                start-exec</span><br><span class="line">        ;;</span><br><span class="line">        active-exec)</span><br><span class="line">                active-exec</span><br><span class="line">        ;;</span><br><span class="line">        stop-exec)</span><br><span class="line">                stop-exec</span><br><span class="line">        ;;</span><br><span class="line">        start-web)</span><br><span class="line">                start-web</span><br><span class="line">        ;;</span><br><span class="line">        stop-web)</span><br><span class="line">                stop-web</span><br><span class="line">        ;;</span><br><span class="line">        </span><br><span class="line">        start)</span><br><span class="line">        echo &quot;=============== 启动executor ================&quot;</span><br><span class="line">                start-exec</span><br><span class="line">                sleep 10</span><br><span class="line">        echo &quot;=============== 激活executor ================&quot;</span><br><span class="line">                active-exec</span><br><span class="line">                sleep 5</span><br><span class="line">        echo &quot;=============== 启动webserver ===============&quot;</span><br><span class="line">                start-web</span><br><span class="line">        ;;</span><br><span class="line"></span><br><span class="line">        stop)</span><br><span class="line">        echo &quot;=============== 关闭executor ================&quot;</span><br><span class="line">                stop-exec</span><br><span class="line">                sleep 5</span><br><span class="line">        echo &quot;=============== 关闭webserver ===============&quot;</span><br><span class="line">                stop-web</span><br><span class="line">        ;;</span><br><span class="line">        *)</span><br><span class="line">        echo &#x27;Input args error...&#x27;</span><br><span class="line">esac</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="cluster5-0-all-sh"><a href="#cluster5-0-all-sh" class="headerlink" title="cluster5.0_all.sh"></a>cluster5.0_all.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">        echo ================== 启动 离线集群 ==================</span><br><span class="line"></span><br><span class="line">        #启动 Zookeeper集群</span><br><span class="line">        myzookeeper.sh start</span><br><span class="line"></span><br><span class="line">        #启动 Hadoop集群</span><br><span class="line">        myhadoop.sh start</span><br><span class="line"></span><br><span class="line">        #启动 Kafka采集集群</span><br><span class="line">        mykafka.sh start</span><br><span class="line"></span><br><span class="line">        #启动 Flume采集集群</span><br><span class="line">        applog-flume-kafka-topic_log.sh start</span><br><span class="line"></span><br><span class="line">        #启动 Flume消费集群</span><br><span class="line">        kafka-topic_log-hdfs-origin_data-gmall-log-topic_log.sh start</span><br><span class="line"></span><br><span class="line">        #启动flume消费maxwell传到kafka的数据</span><br><span class="line">        db_maxwell_kafka_flume_hdfs.sh start</span><br><span class="line"></span><br><span class="line">        #启动hive的metastore元数据服务</span><br><span class="line">        hive_metastore.sh start</span><br><span class="line"></span><br><span class="line">        #启动maxwell</span><br><span class="line">        maxwell.sh start</span><br><span class="line"></span><br><span class="line">        #启动hive的hiveserver2服务</span><br><span class="line">        hive_hiveserver2.sh start</span><br><span class="line"></span><br><span class="line">        &#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">        echo ================== 停止 离线集群 ==================</span><br><span class="line"></span><br><span class="line">        #启动hive的hiveserver2服务</span><br><span class="line">        hive_hiveserver2.sh stop</span><br><span class="line"></span><br><span class="line">        #关闭maxwell</span><br><span class="line">        maxwell.sh stop</span><br><span class="line"></span><br><span class="line">        #启动hive的metastore元数据服务</span><br><span class="line">        hive_metastore.sh stop</span><br><span class="line"></span><br><span class="line">        #关闭flume消费maxwell传到kafka的数据</span><br><span class="line">        db_maxwell_kafka_flume_hdfs.sh stop</span><br><span class="line"></span><br><span class="line">        #停止 Flume消费集群</span><br><span class="line">        kafka-topic_log-hdfs-origin_data-gmall-log-topic_log.sh stop</span><br><span class="line"></span><br><span class="line">        #停止 Flume采集集群</span><br><span class="line">        applog-flume-kafka-topic_log.sh stop</span><br><span class="line"></span><br><span class="line">        #停止 Kafka采集集群</span><br><span class="line">        mykafka.sh stop</span><br><span class="line"></span><br><span class="line">        #停止 Hadoop集群</span><br><span class="line">        myhadoop.sh stop</span><br><span class="line"></span><br><span class="line">        #停止 Zookeeper集群</span><br><span class="line">        myzookeeper.sh stop</span><br><span class="line"></span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="xcall-sh"><a href="#xcall-sh" class="headerlink" title="xcall.sh"></a>xcall.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line">echo &quot;查看每台节点上内存使用情况的命令:  xcall.sh free -h &quot;</span><br><span class="line">echo &quot;jps -ml 显示更多进程的信息,防止重名进程傻傻分不清&quot;</span><br><span class="line">for i in hadoop100 hadoop101 hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">    echo --------- $i ----------</span><br><span class="line">    ssh $i &quot;$*&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h1 id="xsync"><a href="#xsync" class="headerlink" title="xsync"></a>xsync</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">#1. 判断参数个数</span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo Not Enough Arguement!</span><br><span class="line">    exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#2. 遍历集群所有机器</span><br><span class="line">for host in hadoop100 hadoop101 hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">    echo ====================  $host  ====================</span><br><span class="line">    #3. 遍历所有目录，挨个发送</span><br><span class="line"></span><br><span class="line">    for file in $@</span><br><span class="line">    do</span><br><span class="line">        #4. 判断文件是否存在</span><br><span class="line">        if [ -e $file ]</span><br><span class="line">            then</span><br><span class="line">                #5. 获取父目录</span><br><span class="line">                pdir=$(cd -P $(dirname $file); pwd)</span><br><span class="line"></span><br><span class="line">                #6. 获取当前文件的名称</span><br><span class="line">                fname=$(basename $file)</span><br><span class="line">                ssh $host &quot;mkdir -p $pdir&quot;</span><br><span class="line">                rsync -av $pdir/$fname $host:$pdir</span><br><span class="line">            else</span><br><span class="line">                echo $file does not exists!</span><br><span class="line">        fi</span><br><span class="line">    done</span><br><span class="line">done</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="myhbase-sh"><a href="#myhbase-sh" class="headerlink" title="myhbase.sh"></a>myhbase.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 hbase ===================&quot;</span><br><span class="line"></span><br><span class="line">        # echo &quot; --------------- 启动 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hbase-2.0.5/bin/start-hbase.sh&quot;</span><br><span class="line">        # sleep 5</span><br><span class="line">        # ssh hadoop102 &quot;/opt/module/hbase-2.0.5/bin/hbase-daemon.sh start regionserver&quot;</span><br><span class="line">        </span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 hbase ===================&quot;</span><br><span class="line"></span><br><span class="line">        # echo &quot; --------------- 关闭 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hbase-2.0.5/bin/stop-hbase.sh&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="hive-load-hdfsDB-to-hiveHdfs5-0-sh"><a href="#hive-load-hdfsDB-to-hiveHdfs5-0-sh" class="headerlink" title="hive_load_hdfsDB_to_hiveHdfs5.0.sh"></a>hive_load_hdfsDB_to_hiveHdfs5.0.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">APP=gmall</span><br><span class="line"></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">   do_date=$2</span><br><span class="line">else </span><br><span class="line">   do_date=`date -d &#x27;-1 day&#x27; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">load_data()&#123;</span><br><span class="line">    sql=&quot;&quot;</span><br><span class="line">    for i in $*; do</span><br><span class="line">        #判断路径是否存在</span><br><span class="line">        hadoop fs -test -e /origin_data/$APP/db/$&#123;i:4&#125;/$do_date</span><br><span class="line">        #路径存在方可装载数据</span><br><span class="line">        if [[ $? = 0 ]]; then</span><br><span class="line">            sql=$sql&quot;load data inpath &#x27;/origin_data/$APP/db/$&#123;i:4&#125;/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.$i partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line">        fi</span><br><span class="line">    done</span><br><span class="line">    hive -e &quot;$sql&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    &quot;ods_activity_info_full&quot;)</span><br><span class="line">        load_data &quot;ods_activity_info_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_activity_rule_full&quot;)</span><br><span class="line">        load_data &quot;ods_activity_rule_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_base_category1_full&quot;)</span><br><span class="line">        load_data &quot;ods_base_category1_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_base_category2_full&quot;)</span><br><span class="line">        load_data &quot;ods_base_category2_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_base_category3_full&quot;)</span><br><span class="line">        load_data &quot;ods_base_category3_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_base_dic_full&quot;)</span><br><span class="line">        load_data &quot;ods_base_dic_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_base_province_full&quot;)</span><br><span class="line">        load_data &quot;ods_base_province_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_base_region_full&quot;)</span><br><span class="line">        load_data &quot;ods_base_region_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_base_trademark_full&quot;)</span><br><span class="line">        load_data &quot;ods_base_trademark_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_cart_info_full&quot;)</span><br><span class="line">        load_data &quot;ods_cart_info_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_coupon_info_full&quot;)</span><br><span class="line">        load_data &quot;ods_coupon_info_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_sku_attr_value_full&quot;)</span><br><span class="line">        load_data &quot;ods_sku_attr_value_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_sku_info_full&quot;)</span><br><span class="line">        load_data &quot;ods_sku_info_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_sku_sale_attr_value_full&quot;)</span><br><span class="line">        load_data &quot;ods_sku_sale_attr_value_full&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_spu_info_full&quot;)</span><br><span class="line">        load_data &quot;ods_spu_info_full&quot;</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    &quot;ods_cart_info_inc&quot;)</span><br><span class="line">        load_data &quot;ods_cart_info_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_comment_info_inc&quot;)</span><br><span class="line">        load_data &quot;ods_comment_info_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_coupon_use_inc&quot;)</span><br><span class="line">        load_data &quot;ods_coupon_use_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_favor_info_inc&quot;)</span><br><span class="line">        load_data &quot;ods_favor_info_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_order_detail_inc&quot;)</span><br><span class="line">        load_data &quot;ods_order_detail_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_order_detail_activity_inc&quot;)</span><br><span class="line">        load_data &quot;ods_order_detail_activity_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_order_detail_coupon_inc&quot;)</span><br><span class="line">        load_data &quot;ods_order_detail_coupon_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_order_info_inc&quot;)</span><br><span class="line">        load_data &quot;ods_order_info_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_order_refund_info_inc&quot;)</span><br><span class="line">        load_data &quot;ods_order_refund_info_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_order_status_log_inc&quot;)</span><br><span class="line">        load_data &quot;ods_order_status_log_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_payment_info_inc&quot;)</span><br><span class="line">        load_data &quot;ods_payment_info_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_refund_payment_inc&quot;)</span><br><span class="line">        load_data &quot;ods_refund_payment_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;ods_user_info_inc&quot;)</span><br><span class="line">        load_data &quot;ods_user_info_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;all&quot;)</span><br><span class="line">        load_data &quot;ods_activity_info_full&quot; &quot;ods_activity_rule_full&quot; &quot;ods_base_category1_full&quot; &quot;ods_base_category2_full&quot; &quot;ods_base_category3_full&quot; &quot;ods_base_dic_full&quot; &quot;ods_base_province_full&quot; &quot;ods_base_region_full&quot; &quot;ods_base_trademark_full&quot; &quot;ods_cart_info_full&quot; &quot;ods_coupon_info_full&quot; &quot;ods_sku_attr_value_full&quot; &quot;ods_sku_info_full&quot; &quot;ods_sku_sale_attr_value_full&quot; &quot;ods_spu_info_full&quot; &quot;ods_cart_info_inc&quot; &quot;ods_comment_info_inc&quot; &quot;ods_coupon_use_inc&quot; &quot;ods_favor_info_inc&quot; &quot;ods_order_detail_inc&quot; &quot;ods_order_detail_activity_inc&quot; &quot;ods_order_detail_coupon_inc&quot; &quot;ods_order_info_inc&quot; &quot;ods_order_refund_info_inc&quot; &quot;ods_order_status_log_inc&quot; &quot;ods_payment_info_inc&quot; &quot;ods_refund_payment_inc&quot; &quot;ods_user_info_inc&quot;</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="hive-load-hdfsLog-to-hiveHdfs5-0-sh"><a href="#hive-load-hdfsLog-to-hiveHdfs5-0-sh" class="headerlink" title="hive_load_hdfsLog_to_hiveHdfs5.0.sh"></a>hive_load_hdfsLog_to_hiveHdfs5.0.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line"># 定义变量方便修改</span><br><span class="line">APP=gmall</span><br><span class="line"></span><br><span class="line"># 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天</span><br><span class="line">if [ -n &quot;$1&quot; ] ;then</span><br><span class="line">   do_date=$1</span><br><span class="line">else </span><br><span class="line">   do_date=`date -d &quot;-1 day&quot; +%F`</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line">echo ================== 日志日期为 $do_date ==================</span><br><span class="line">#/origin_data/gmall/log/topic_log/2021-12-02</span><br><span class="line">sql=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/$APP/log/topic_log/$do_date&#x27; into table $&#123;APP&#125;.ods_log_inc partition(dt=&#x27;$do_date&#x27;);</span><br><span class="line">&quot;</span><br><span class="line">#第一步: 执行load data 将hdfs上的数据加载进hive里面</span><br><span class="line">hive -e &quot;$sql&quot;</span><br><span class="line"></span><br><span class="line">#第二步: 创建lzop索引   ------&gt; 数仓5.0使用的是gzip,所以不用建立索引</span><br><span class="line"># hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/$APP/ods/ods_log/dt=$do_date</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="myjps-sh"><a href="#myjps-sh" class="headerlink" title="myjps.sh"></a>myjps.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">for host in hadoop100 hadoop101 hadoop102 hadoop103 hadoop104 </span><br><span class="line">do</span><br><span class="line">        echo =============== $host ===============</span><br><span class="line">        ssh $host jps </span><br><span class="line">done</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="hive-hiveserver2-sh"><a href="#hive-hiveserver2-sh" class="headerlink" title="hive_hiveserver2.sh"></a>hive_hiveserver2.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot;------------ 开启hive hiveserver2服务去连接mysql中的元数据,后台启动 ---------------&quot;</span><br><span class="line">        # echo &quot;------------ hive连接mysql中的元数据有2种方式: 1）直接连接：直接去mysql中连接metastore库；2）通过服务连：hive有2种服务分别是metastore和hiveserver2，hive通过metastore服务去连接mysql中的元数据。---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;nohup /opt/module/hive-3.1.2/bin/hiveserver2 1&gt;/dev/null 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; ----------- 关闭hive hiveserver2服务去连接mysql中的元数据,后台启动 --------&quot;</span><br><span class="line">        ssh hadoop102 &quot;ps -ef | grep hive_hiveserver2 | grep -v grep |awk &#x27;&#123;print \$2&#125;&#x27; | xargs -n1 kill&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="hive-metastore-sh"><a href="#hive-metastore-sh" class="headerlink" title="hive_metastore.sh"></a>hive_metastore.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot;------------ 开启hive metastore 服务去连接mysql中的元数据,后台启动 ---------------&quot;</span><br><span class="line">        # echo &quot;------------ hive连接mysql中的元数据有2种方式: 1）直接连接：直接去mysql中连接metastore库；2）通过服务连：hive有2种服务分别是metastore和hiveserver2，hive通过metastore服务去连接mysql中的元数据。---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;nohup /opt/module/hive-3.1.2/bin/hive --service metastore&gt;log.txt 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; ----------- 关闭hive metastore 服务去连接mysql中的元数据,后台启动 --------&quot;</span><br><span class="line">        ssh hadoop102 &quot;ps -ef | grep hive | grep -v grep |awk &#x27;&#123;print \$2&#125;&#x27; | xargs -n1 kill&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="ls-al-sh"><a href="#ls-al-sh" class="headerlink" title="ls-al.sh"></a>ls-al.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">echo ================== 展示隐藏文件,Linux命令: ls - al ==================</span><br><span class="line">ls -al</span><br></pre></td></tr></table></figure><h1 id="ls-grep-sh"><a href="#ls-grep-sh" class="headerlink" title="ls-grep.sh"></a>ls-grep.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">#$1: 脚本的第一个参数 做非空判断</span><br><span class="line">if [ -n &quot;$1&quot; ] ;then</span><br><span class="line">   name=$1</span><br><span class="line">else </span><br><span class="line">   echo &quot;请传入一个查询参数&quot;</span><br><span class="line">   exit</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line">echo ================== 查看文件夹下是否有包含 $name 名称的文件 ==================</span><br><span class="line">ls | grep $name</span><br></pre></td></tr></table></figure><h1 id="maxwell-sh"><a href="#maxwell-sh" class="headerlink" title="maxwell.sh"></a>maxwell.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">MAXWELL_HOME=/opt/module/maxwell-1.29.2</span><br><span class="line"></span><br><span class="line">status_maxwell()&#123;</span><br><span class="line">    result=`ps -ef | grep maxwell | grep -v grep | wc -l`</span><br><span class="line">    return $result</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">start_maxwell()&#123;</span><br><span class="line">    # status_maxwell</span><br><span class="line">    # if [[ $? -lt 1 ]]; then</span><br><span class="line">    #     echo &quot;启动Maxwell&quot;</span><br><span class="line">    #     $MAXWELL_HOME/bin/maxwell --config $MAXWELL_HOME/config.properties --daemon</span><br><span class="line">    # else</span><br><span class="line">    #     echo &quot;Maxwell正在运行&quot;</span><br><span class="line">    # fi</span><br><span class="line">    $MAXWELL_HOME/bin/maxwell --config $MAXWELL_HOME/config.properties --daemon</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">stop_maxwell()&#123;</span><br><span class="line">    status_maxwell</span><br><span class="line">    if [[ $? -gt 0 ]]; then</span><br><span class="line">        echo &quot;停止Maxwell&quot;</span><br><span class="line">        ps -ef | grep maxwell | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27; | xargs kill -9</span><br><span class="line">    else</span><br><span class="line">        echo &quot;Maxwell未在运行&quot;</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    start )</span><br><span class="line">        start_maxwell</span><br><span class="line">    ;;</span><br><span class="line">    stop )</span><br><span class="line">        stop_maxwell</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="mydolphinscheduler-sh"><a href="#mydolphinscheduler-sh" class="headerlink" title="mydolphinscheduler.sh"></a>mydolphinscheduler.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 dolphinscheduler 集群 ===================&quot;</span><br><span class="line">        echo &quot;</span><br><span class="line">                1）一键启停所有服务</span><br><span class="line">                ./bin/start-all.sh</span><br><span class="line">                ./bin/stop-all.sh</span><br><span class="line">                2）启停 Master</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start master-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop master-server</span><br><span class="line">                3）启停 Worker</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start worker-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop worker-server</span><br><span class="line">                4）启停 Api</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start api-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop api-server</span><br><span class="line">                5）启停 Logger</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start logger-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop logger-server</span><br><span class="line">                6）启停 Alert</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start alert-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop alert-server</span><br><span class="line">            &quot;</span><br><span class="line"></span><br><span class="line">        ssh hadoop102 &quot;/opt/module/dolphinscheduler-1.3.9/bin/start-all.sh&quot;</span><br><span class="line"></span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 dolphinscheduler 集群 ===================&quot;</span><br><span class="line">        echo &quot;</span><br><span class="line">                1）一键启停所有服务</span><br><span class="line">                ./bin/start-all.sh</span><br><span class="line">                ./bin/stop-all.sh</span><br><span class="line">                2）启停 Master</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start master-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop master-server</span><br><span class="line">                3）启停 Worker</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start worker-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop worker-server</span><br><span class="line">                4）启停 Api</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start api-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop api-server</span><br><span class="line">                5）启停 Logger</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start logger-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop logger-server</span><br><span class="line">                6）启停 Alert</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start alert-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop alert-server</span><br><span class="line">            &quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/dolphinscheduler-1.3.9/bin/stop-all.sh&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="mydolphinscheduler-standalone-sh"><a href="#mydolphinscheduler-standalone-sh" class="headerlink" title="mydolphinscheduler-standalone.sh"></a>mydolphinscheduler-standalone.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 dolphinscheduler 单机模式(standalone) 单机模式内置zookeeper,所以必须关闭zookeeper===================&quot;</span><br><span class="line">        echo &quot;</span><br><span class="line">                1）一键启停所有服务</span><br><span class="line">                ./bin/start-all.sh</span><br><span class="line">                ./bin/stop-all.sh</span><br><span class="line">                2）启停 Master</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start master-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop master-server</span><br><span class="line">                3）启停 Worker</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start worker-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop worker-server</span><br><span class="line">                4）启停 Api</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start api-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop api-server</span><br><span class="line">                5）启停 Logger</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start logger-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop logger-server</span><br><span class="line">                6）启停 Alert</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start alert-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop alert-server</span><br><span class="line">            &quot;</span><br><span class="line"></span><br><span class="line">        ssh hadoop102 &quot;/opt/module/dolphinscheduler-1.3.9/bin/dolphinscheduler-daemon.sh start standalone-server&quot;</span><br><span class="line"></span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 dolphinscheduler 单机模式(standalone) 单机模式内置zookeeper,所以必须关闭zookeeper===================&quot;</span><br><span class="line">        echo &quot;</span><br><span class="line">                1）一键启停所有服务</span><br><span class="line">                ./bin/start-all.sh</span><br><span class="line">                ./bin/stop-all.sh</span><br><span class="line">                2）启停 Master</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start master-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop master-server</span><br><span class="line">                3）启停 Worker</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start worker-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop worker-server</span><br><span class="line">                4）启停 Api</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start api-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop api-server</span><br><span class="line">                5）启停 Logger</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start logger-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop logger-server</span><br><span class="line">                6）启停 Alert</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh start alert-server</span><br><span class="line">                ./bin/dolphinscheduler-daemon.sh stop alert-server</span><br><span class="line">            &quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/dolphinscheduler-1.3.9/bin/dolphinscheduler-daemon.sh stop standalone-server&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="mygrafana-sh"><a href="#mygrafana-sh" class="headerlink" title="mygrafana.sh"></a>mygrafana.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 Grafana ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; sudo systemctl start grafana-server &quot;</span><br><span class="line"></span><br><span class="line">        ssh hadoop102 &quot;sudo systemctl start grafana-server&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 Grafana ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; sudo systemctl stop grafana-server &quot;</span><br><span class="line"></span><br><span class="line">        ssh hadoop102 &quot;sudo systemctl stop grafana-server&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       </span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="sqoop-import-mysql-to-hdfs-ods-every-day-sh"><a href="#sqoop-import-mysql-to-hdfs-ods-every-day-sh" class="headerlink" title="sqoop_import_mysql_to_hdfs_ods_every_day.sh"></a>sqoop_import_mysql_to_hdfs_ods_every_day.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line">#每日同步脚本 , 需要考虑到新增及变化的 增量同步</span><br><span class="line">APP=gmall</span><br><span class="line">sqoop=/opt/module/sqoop-1.4.6/bin/sqoop</span><br><span class="line"></span><br><span class="line">#每天一天的日期</span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else</span><br><span class="line">    do_date=`date -d &#x27;-1 day&#x27; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">import_data()&#123;</span><br><span class="line">$sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/$APP \</span><br><span class="line">--username root \</span><br><span class="line">--password shangbaishuyao \</span><br><span class="line">--target-dir /origin_data/$APP/db/$1/$do_date \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--query &quot;$2 and  \$CONDITIONS&quot; \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; \</span><br><span class="line">--compress \</span><br><span class="line">--compression-codec lzop \</span><br><span class="line">--null-string &#x27;\\N&#x27; \</span><br><span class="line">--null-non-string &#x27;\\N&#x27;</span><br><span class="line"></span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 每日同步策略是新增及变化的 sqoop增量导入</span><br><span class="line">import_order_info()&#123;</span><br><span class="line">  import_data order_info &quot;select</span><br><span class="line">                            id, </span><br><span class="line">                            total_amount, </span><br><span class="line">                            order_status, </span><br><span class="line">                            user_id, </span><br><span class="line">                            payment_way,</span><br><span class="line">                            delivery_address,</span><br><span class="line">                            out_trade_no, </span><br><span class="line">                            create_time, </span><br><span class="line">                            operate_time,</span><br><span class="line">                            expire_time,</span><br><span class="line">                            tracking_no,</span><br><span class="line">                            province_id,</span><br><span class="line">                            activity_reduce_amount,</span><br><span class="line">                            coupon_reduce_amount,                            </span><br><span class="line">                            original_total_amount,</span><br><span class="line">                            feight_fee,</span><br><span class="line">                            feight_fee_reduce      </span><br><span class="line">                        from order_info</span><br><span class="line">                        where </span><br><span class="line">                        (date_format(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27; </span><br><span class="line">                        or date_format(operate_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;)&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_coupon_use()&#123;</span><br><span class="line">  import_data coupon_use &quot;select</span><br><span class="line">                          id,</span><br><span class="line">                          coupon_id,</span><br><span class="line">                          user_id,</span><br><span class="line">                          order_id,</span><br><span class="line">                          coupon_status,</span><br><span class="line">                          get_time,</span><br><span class="line">                          using_time,</span><br><span class="line">                          used_time,</span><br><span class="line">                          expire_time</span><br><span class="line">                        from coupon_use</span><br><span class="line">                        where (date_format(get_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;</span><br><span class="line">                        or date_format(using_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;</span><br><span class="line">                        or date_format(used_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;</span><br><span class="line">                        or date_format(expire_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;)&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_order_status_log()&#123;</span><br><span class="line">  import_data order_status_log &quot;select</span><br><span class="line">                                  id,</span><br><span class="line">                                  order_id,</span><br><span class="line">                                  order_status,</span><br><span class="line">                                  operate_time</span><br><span class="line">                                from order_status_log</span><br><span class="line">                                where date_format(operate_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_user_info()&#123;</span><br><span class="line">  import_data &quot;user_info&quot; &quot;select </span><br><span class="line">                            id,</span><br><span class="line">                            login_name,</span><br><span class="line">                            nick_name,</span><br><span class="line">                            name,</span><br><span class="line">                            phone_num,</span><br><span class="line">                            email,</span><br><span class="line">                            user_level, </span><br><span class="line">                            birthday,</span><br><span class="line">                            gender,</span><br><span class="line">                            create_time,</span><br><span class="line">                            operate_time</span><br><span class="line">                          from user_info </span><br><span class="line">                          where (DATE_FORMAT(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27; </span><br><span class="line">                          or DATE_FORMAT(operate_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;)&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_order_detail()&#123;</span><br><span class="line">  import_data order_detail &quot;select </span><br><span class="line">                              id,</span><br><span class="line">                              order_id, </span><br><span class="line">                              sku_id,</span><br><span class="line">                              sku_name,</span><br><span class="line">                              order_price,</span><br><span class="line">                              sku_num, </span><br><span class="line">                              create_time,</span><br><span class="line">                              source_type,</span><br><span class="line">                              source_id,</span><br><span class="line">                              split_total_amount,</span><br><span class="line">                              split_activity_amount,</span><br><span class="line">                              split_coupon_amount</span><br><span class="line">                            from order_detail </span><br><span class="line">                            where DATE_FORMAT(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_payment_info()&#123;</span><br><span class="line">  import_data &quot;payment_info&quot;  &quot;select </span><br><span class="line">                                id,  </span><br><span class="line">                                out_trade_no, </span><br><span class="line">                                order_id, </span><br><span class="line">                                user_id, </span><br><span class="line">                                payment_type, </span><br><span class="line">                                trade_no, </span><br><span class="line">                                total_amount,  </span><br><span class="line">                                subject, </span><br><span class="line">                                payment_status,</span><br><span class="line">                                create_time,</span><br><span class="line">                                callback_time </span><br><span class="line">                              from payment_info </span><br><span class="line">                              where (DATE_FORMAT(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27; </span><br><span class="line">                              or DATE_FORMAT(callback_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;)&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 同步策略是增量同步 创建时间是每天的</span><br><span class="line">import_comment_info()&#123;</span><br><span class="line">  import_data comment_info &quot;select</span><br><span class="line">                              id,</span><br><span class="line">                              user_id,</span><br><span class="line">                              sku_id,</span><br><span class="line">                              spu_id,</span><br><span class="line">                              order_id,</span><br><span class="line">                              appraise,</span><br><span class="line">                              create_time</span><br><span class="line">                            from comment_info</span><br><span class="line">                            where date_format(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_order_refund_info()&#123;</span><br><span class="line">  import_data order_refund_info &quot;select</span><br><span class="line">                                id,</span><br><span class="line">                                user_id,</span><br><span class="line">                                order_id,</span><br><span class="line">                                sku_id,</span><br><span class="line">                                refund_type,</span><br><span class="line">                                refund_num,</span><br><span class="line">                                refund_amount,</span><br><span class="line">                                refund_reason_type,</span><br><span class="line">                                refund_status,</span><br><span class="line">                                create_time</span><br><span class="line">                              from order_refund_info</span><br><span class="line">                              where date_format(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 同步策略是全量同步,理论上where1=1可以不用写,但是import_data 这个函数 最终执行的sql是 --query &quot;$2 and  \$CONDITIONS&quot; \ 这个 我们为了满足sql的语法要求,因为前面有个and所以我们必须有where 1=1</span><br><span class="line">import_sku_info()&#123;</span><br><span class="line">  import_data sku_info &quot;select </span><br><span class="line">                          id,</span><br><span class="line">                          spu_id,</span><br><span class="line">                          price,</span><br><span class="line">                          sku_name,</span><br><span class="line">                          sku_desc,</span><br><span class="line">                          weight,</span><br><span class="line">                          tm_id,</span><br><span class="line">                          category3_id,</span><br><span class="line">                          is_sale,</span><br><span class="line">                          create_time</span><br><span class="line">                        from sku_info where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_category1()&#123;</span><br><span class="line">  import_data &quot;base_category1&quot; &quot;select </span><br><span class="line">                                  id,</span><br><span class="line">                                  name </span><br><span class="line">                                from base_category1 where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_category2()&#123;</span><br><span class="line">  import_data &quot;base_category2&quot; &quot;select</span><br><span class="line">                                  id,</span><br><span class="line">                                  name,</span><br><span class="line">                                  category1_id </span><br><span class="line">                                from base_category2 where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_category3()&#123;</span><br><span class="line">  import_data &quot;base_category3&quot; &quot;select</span><br><span class="line">                                  id,</span><br><span class="line">                                  name,</span><br><span class="line">                                  category2_id</span><br><span class="line">                                from base_category3 where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_province()&#123;</span><br><span class="line">  import_data base_province &quot;select</span><br><span class="line">                              id,</span><br><span class="line">                              name,</span><br><span class="line">                              region_id,</span><br><span class="line">                              area_code,</span><br><span class="line">                              iso_code,</span><br><span class="line">                              iso_3166_2</span><br><span class="line">                            from base_province</span><br><span class="line">                            where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_region()&#123;</span><br><span class="line">  import_data base_region &quot;select</span><br><span class="line">                              id,</span><br><span class="line">                              region_name</span><br><span class="line">                            from base_region</span><br><span class="line">                            where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_trademark()&#123;</span><br><span class="line">  import_data base_trademark &quot;select</span><br><span class="line">                                id,</span><br><span class="line">                                tm_name</span><br><span class="line">                              from base_trademark</span><br><span class="line">                              where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_spu_info()&#123;</span><br><span class="line">  import_data spu_info &quot;select</span><br><span class="line">                            id,</span><br><span class="line">                            spu_name,</span><br><span class="line">                            category3_id,</span><br><span class="line">                            tm_id</span><br><span class="line">                          from spu_info</span><br><span class="line">                          where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_favor_info()&#123;</span><br><span class="line">  import_data favor_info &quot;select</span><br><span class="line">                          id,</span><br><span class="line">                          user_id,</span><br><span class="line">                          sku_id,</span><br><span class="line">                          spu_id,</span><br><span class="line">                          is_cancel,</span><br><span class="line">                          create_time,</span><br><span class="line">                          cancel_time</span><br><span class="line">                        from favor_info</span><br><span class="line">                        where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_cart_info()&#123;</span><br><span class="line">  import_data cart_info &quot;select</span><br><span class="line">                        id,</span><br><span class="line">                        user_id,</span><br><span class="line">                        sku_id,</span><br><span class="line">                        cart_price,</span><br><span class="line">                        sku_num,</span><br><span class="line">                        sku_name,</span><br><span class="line">                        create_time,</span><br><span class="line">                        operate_time,</span><br><span class="line">                        is_ordered,</span><br><span class="line">                        order_time,</span><br><span class="line">                        source_type,</span><br><span class="line">                        source_id</span><br><span class="line">                      from cart_info</span><br><span class="line">                      where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_coupon_info()&#123;</span><br><span class="line">  import_data coupon_info &quot;select</span><br><span class="line">                          id,</span><br><span class="line">                          coupon_name,</span><br><span class="line">                          coupon_type,</span><br><span class="line">                          condition_amount,</span><br><span class="line">                          condition_num,</span><br><span class="line">                          activity_id,</span><br><span class="line">                          benefit_amount,</span><br><span class="line">                          benefit_discount,</span><br><span class="line">                          create_time,</span><br><span class="line">                          range_type,</span><br><span class="line">                          limit_num,</span><br><span class="line">                          taken_count,</span><br><span class="line">                          start_time,</span><br><span class="line">                          end_time,</span><br><span class="line">                          operate_time,</span><br><span class="line">                          expire_time</span><br><span class="line">                        from coupon_info</span><br><span class="line">                        where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_activity_info()&#123;</span><br><span class="line">  import_data activity_info &quot;select</span><br><span class="line">                              id,</span><br><span class="line">                              activity_name,</span><br><span class="line">                              activity_type,</span><br><span class="line">                              start_time,</span><br><span class="line">                              end_time,</span><br><span class="line">                              create_time</span><br><span class="line">                            from activity_info</span><br><span class="line">                            where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_activity_rule()&#123;</span><br><span class="line">    import_data activity_rule &quot;select</span><br><span class="line">                                    id,</span><br><span class="line">                                    activity_id,</span><br><span class="line">                                    activity_type,</span><br><span class="line">                                    condition_amount,</span><br><span class="line">                                    condition_num,</span><br><span class="line">                                    benefit_amount,</span><br><span class="line">                                    benefit_discount,</span><br><span class="line">                                    benefit_level</span><br><span class="line">                                from activity_rule</span><br><span class="line">                                where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_dic()&#123;</span><br><span class="line">    import_data base_dic &quot;select</span><br><span class="line">                            dic_code,</span><br><span class="line">                            dic_name,</span><br><span class="line">                            parent_code,</span><br><span class="line">                            create_time,</span><br><span class="line">                            operate_time</span><br><span class="line">                          from base_dic</span><br><span class="line">                          where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import_order_detail_activity()&#123;</span><br><span class="line">    import_data order_detail_activity &quot;select</span><br><span class="line">                                                                id,</span><br><span class="line">                                                                order_id,</span><br><span class="line">                                                                order_detail_id,</span><br><span class="line">                                                                activity_id,</span><br><span class="line">                                                                activity_rule_id,</span><br><span class="line">                                                                sku_id,</span><br><span class="line">                                                                create_time</span><br><span class="line">                                                            from order_detail_activity</span><br><span class="line">                                                            where date_format(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import_order_detail_coupon()&#123;</span><br><span class="line">    import_data order_detail_coupon &quot;select</span><br><span class="line">                                                                id,</span><br><span class="line">                                                order_id,</span><br><span class="line">                                                                order_detail_id,</span><br><span class="line">                                                                coupon_id,</span><br><span class="line">                                                                coupon_use_id,</span><br><span class="line">                                                                sku_id,</span><br><span class="line">                                                                create_time</span><br><span class="line">                                                            from order_detail_coupon</span><br><span class="line">                                                            where date_format(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import_refund_payment()&#123;</span><br><span class="line">    import_data refund_payment &quot;select</span><br><span class="line">                                                        id,</span><br><span class="line">                                                        out_trade_no,</span><br><span class="line">                                                        order_id,</span><br><span class="line">                                                        sku_id,</span><br><span class="line">                                                        payment_type,</span><br><span class="line">                                                        trade_no,</span><br><span class="line">                                                        total_amount,</span><br><span class="line">                                                        subject,</span><br><span class="line">                                                        refund_status,</span><br><span class="line">                                                        create_time,</span><br><span class="line">                                                        callback_time</span><br><span class="line">                                                    from refund_payment</span><br><span class="line">                                                    where (DATE_FORMAT(create_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27; </span><br><span class="line">                                                    or DATE_FORMAT(callback_time,&#x27;%Y-%m-%d&#x27;)=&#x27;$do_date&#x27;)&quot;                                                    </span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_sku_attr_value()&#123;</span><br><span class="line">    import_data sku_attr_value &quot;select</span><br><span class="line">                                                    id,</span><br><span class="line">                                                    attr_id,</span><br><span class="line">                                                    value_id,</span><br><span class="line">                                                    sku_id,</span><br><span class="line">                                                    attr_name,</span><br><span class="line">                                                    value_name</span><br><span class="line">                                                from sku_attr_value</span><br><span class="line">                                                where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import_sku_sale_attr_value()&#123;</span><br><span class="line">    import_data sku_sale_attr_value &quot;select</span><br><span class="line">                                                            id,</span><br><span class="line">                                                            sku_id,</span><br><span class="line">                                                            spu_id,</span><br><span class="line">                                                            sale_attr_value_id,</span><br><span class="line">                                                            sale_attr_id,</span><br><span class="line">                                                            sale_attr_name,</span><br><span class="line">                                                            sale_attr_value_name</span><br><span class="line">                                                        from sku_sale_attr_value</span><br><span class="line">                                                        where 1=1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">  &quot;order_info&quot;)</span><br><span class="line">     import_order_info</span><br><span class="line">;;</span><br><span class="line">  &quot;base_category1&quot;)</span><br><span class="line">     import_base_category1</span><br><span class="line">;;</span><br><span class="line">  &quot;base_category2&quot;)</span><br><span class="line">     import_base_category2</span><br><span class="line">;;</span><br><span class="line">  &quot;base_category3&quot;)</span><br><span class="line">     import_base_category3</span><br><span class="line">;;</span><br><span class="line">  &quot;order_detail&quot;)</span><br><span class="line">     import_order_detail</span><br><span class="line">;;</span><br><span class="line">  &quot;sku_info&quot;)</span><br><span class="line">     import_sku_info</span><br><span class="line">;;</span><br><span class="line">  &quot;user_info&quot;)</span><br><span class="line">     import_user_info</span><br><span class="line">;;</span><br><span class="line">  &quot;payment_info&quot;)</span><br><span class="line">     import_payment_info</span><br><span class="line">;;</span><br><span class="line">  &quot;base_province&quot;)</span><br><span class="line">     import_base_province</span><br><span class="line">;;</span><br><span class="line">  &quot;activity_info&quot;)</span><br><span class="line">      import_activity_info</span><br><span class="line">;;</span><br><span class="line">  &quot;cart_info&quot;)</span><br><span class="line">      import_cart_info</span><br><span class="line">;;</span><br><span class="line">  &quot;comment_info&quot;)</span><br><span class="line">      import_comment_info</span><br><span class="line">;;</span><br><span class="line">  &quot;coupon_info&quot;)</span><br><span class="line">      import_coupon_info</span><br><span class="line">;;</span><br><span class="line">  &quot;coupon_use&quot;)</span><br><span class="line">      import_coupon_use</span><br><span class="line">;;</span><br><span class="line">  &quot;favor_info&quot;)</span><br><span class="line">      import_favor_info</span><br><span class="line">;;</span><br><span class="line">  &quot;order_refund_info&quot;)</span><br><span class="line">      import_order_refund_info</span><br><span class="line">;;</span><br><span class="line">  &quot;order_status_log&quot;)</span><br><span class="line">      import_order_status_log</span><br><span class="line">;;</span><br><span class="line">  &quot;spu_info&quot;)</span><br><span class="line">      import_spu_info</span><br><span class="line">;;</span><br><span class="line">  &quot;activity_rule&quot;)</span><br><span class="line">      import_activity_rule</span><br><span class="line">;;</span><br><span class="line">  &quot;base_dic&quot;)</span><br><span class="line">      import_base_dic</span><br><span class="line">;;</span><br><span class="line">  &quot;order_detail_activity&quot;)</span><br><span class="line">      import_order_detail_activity</span><br><span class="line">;;</span><br><span class="line">  &quot;order_detail_coupon&quot;)</span><br><span class="line">      import_order_detail_coupon</span><br><span class="line">;;</span><br><span class="line">  &quot;refund_payment&quot;)</span><br><span class="line">      import_refund_payment</span><br><span class="line">;;</span><br><span class="line">  &quot;sku_attr_value&quot;)</span><br><span class="line">      import_sku_attr_value</span><br><span class="line">;;</span><br><span class="line">  &quot;sku_sale_attr_value&quot;)</span><br><span class="line">      import_sku_sale_attr_value</span><br><span class="line">;;</span><br><span class="line">&quot;all&quot;)</span><br><span class="line">   import_base_category1</span><br><span class="line">   import_base_category2</span><br><span class="line">   import_base_category3</span><br><span class="line">   import_order_info</span><br><span class="line">   import_order_detail</span><br><span class="line">   import_sku_info</span><br><span class="line">   import_user_info</span><br><span class="line">   import_payment_info</span><br><span class="line">   import_base_trademark</span><br><span class="line">   import_activity_info</span><br><span class="line">   import_cart_info</span><br><span class="line">   import_comment_info</span><br><span class="line">   import_coupon_use</span><br><span class="line">   import_coupon_info</span><br><span class="line">   import_favor_info</span><br><span class="line">   import_order_refund_info</span><br><span class="line">   import_order_status_log</span><br><span class="line">   import_spu_info</span><br><span class="line">   import_activity_rule</span><br><span class="line">   import_base_dic</span><br><span class="line">   import_order_detail_activity</span><br><span class="line">   import_order_detail_coupon</span><br><span class="line">   import_refund_payment</span><br><span class="line">   import_sku_attr_value</span><br><span class="line">   import_sku_sale_attr_value</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h1 id="sqoop-import-mysql-to-hdfs-ods-first-day-sh"><a href="#sqoop-import-mysql-to-hdfs-ods-first-day-sh" class="headerlink" title="sqoop_import_mysql_to_hdfs_ods_first_day.sh"></a>sqoop_import_mysql_to_hdfs_ods_first_day.sh</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line">#mysql 业务数据库库名</span><br><span class="line">APP=gmall</span><br><span class="line">#sqoop命令的绝对路径</span><br><span class="line">sqoop=/opt/module/sqoop-1.4.6/bin/sqoop</span><br><span class="line"></span><br><span class="line">#$2: 脚本的第二个参数 做非空判断</span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">   do_date=$2</span><br><span class="line">else </span><br><span class="line">   echo &quot;请传入日期参数&quot;</span><br><span class="line">   exit</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line">#公用的函数 主要是sqoop的imoport命令</span><br><span class="line">import_data()&#123;</span><br><span class="line">$sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/$APP \</span><br><span class="line">--username root \</span><br><span class="line">--password shangbaishuyao \</span><br><span class="line">--target-dir /origin_data/$APP/db/$1/$do_date \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--query &quot;$2 where \$CONDITIONS&quot; \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; \</span><br><span class="line">--compress \</span><br><span class="line">--compression-codec lzop \</span><br><span class="line">--null-string &#x27;\\N&#x27; \</span><br><span class="line">--null-non-string &#x27;\\N&#x27;</span><br><span class="line"></span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 通过创建时间和变化的时间获取每天新增及变化的数据 也就是  where 创建时间 = 今天 or 操作时间 = 今天</span><br><span class="line">import_order_info()&#123;</span><br><span class="line">  import_data order_info &quot;select</span><br><span class="line">                            id, </span><br><span class="line">                            total_amount, </span><br><span class="line">                            order_status, </span><br><span class="line">                            user_id, </span><br><span class="line">                            payment_way,</span><br><span class="line">                            delivery_address,</span><br><span class="line">                            out_trade_no, </span><br><span class="line">                            create_time, </span><br><span class="line">                            operate_time,</span><br><span class="line">                            expire_time,</span><br><span class="line">                            tracking_no,</span><br><span class="line">                            province_id,</span><br><span class="line">                            activity_reduce_amount,</span><br><span class="line">                            coupon_reduce_amount,                            </span><br><span class="line">                            original_total_amount,</span><br><span class="line">                            feight_fee,</span><br><span class="line">                            feight_fee_reduce      </span><br><span class="line">                        from order_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_coupon_use()&#123;</span><br><span class="line">  import_data coupon_use &quot;select</span><br><span class="line">                          id,</span><br><span class="line">                          coupon_id,</span><br><span class="line">                          user_id,</span><br><span class="line">                          order_id,</span><br><span class="line">                          coupon_status,</span><br><span class="line">                          get_time,</span><br><span class="line">                          using_time,</span><br><span class="line">                          used_time,</span><br><span class="line">                          expire_time</span><br><span class="line">                        from coupon_use&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_order_status_log()&#123;</span><br><span class="line">  import_data order_status_log &quot;select</span><br><span class="line">                                  id,</span><br><span class="line">                                  order_id,</span><br><span class="line">                                  order_status,</span><br><span class="line">                                  operate_time</span><br><span class="line">                                from order_status_log&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_user_info()&#123;</span><br><span class="line">  import_data &quot;user_info&quot; &quot;select </span><br><span class="line">                            id,</span><br><span class="line">                            login_name,</span><br><span class="line">                            nick_name,</span><br><span class="line">                            name,</span><br><span class="line">                            phone_num,</span><br><span class="line">                            email,</span><br><span class="line">                            user_level, </span><br><span class="line">                            birthday,</span><br><span class="line">                            gender,</span><br><span class="line">                            create_time,</span><br><span class="line">                            operate_time</span><br><span class="line">                          from user_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_order_detail()&#123;</span><br><span class="line">  import_data order_detail &quot;select </span><br><span class="line">                              id,</span><br><span class="line">                              order_id, </span><br><span class="line">                              sku_id,</span><br><span class="line">                              sku_name,</span><br><span class="line">                              order_price,</span><br><span class="line">                              sku_num, </span><br><span class="line">                              create_time,</span><br><span class="line">                              source_type,</span><br><span class="line">                              source_id,</span><br><span class="line">                              split_total_amount,</span><br><span class="line">                              split_activity_amount,</span><br><span class="line">                              split_coupon_amount</span><br><span class="line">                            from order_detail&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_payment_info()&#123;</span><br><span class="line">  import_data &quot;payment_info&quot;  &quot;select </span><br><span class="line">                                id,  </span><br><span class="line">                                out_trade_no, </span><br><span class="line">                                order_id, </span><br><span class="line">                                user_id, </span><br><span class="line">                                payment_type, </span><br><span class="line">                                trade_no, </span><br><span class="line">                                total_amount,  </span><br><span class="line">                                subject, </span><br><span class="line">                                payment_status,</span><br><span class="line">                                create_time,</span><br><span class="line">                                callback_time </span><br><span class="line">                              from payment_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_comment_info()&#123;</span><br><span class="line">  import_data comment_info &quot;select</span><br><span class="line">                              id,</span><br><span class="line">                              user_id,</span><br><span class="line">                              sku_id,</span><br><span class="line">                              spu_id,</span><br><span class="line">                              order_id,</span><br><span class="line">                              appraise,</span><br><span class="line">                              create_time</span><br><span class="line">                            from comment_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_order_refund_info()&#123;</span><br><span class="line">  import_data order_refund_info &quot;select</span><br><span class="line">                                id,</span><br><span class="line">                                user_id,</span><br><span class="line">                                order_id,</span><br><span class="line">                                sku_id,</span><br><span class="line">                                refund_type,</span><br><span class="line">                                refund_num,</span><br><span class="line">                                refund_amount,</span><br><span class="line">                                refund_reason_type,</span><br><span class="line">                                refund_status,</span><br><span class="line">                                create_time</span><br><span class="line">                              from order_refund_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_sku_info()&#123;</span><br><span class="line">  import_data sku_info &quot;select </span><br><span class="line">                          id,</span><br><span class="line">                          spu_id,</span><br><span class="line">                          price,</span><br><span class="line">                          sku_name,</span><br><span class="line">                          sku_desc,</span><br><span class="line">                          weight,</span><br><span class="line">                          tm_id,</span><br><span class="line">                          category3_id,</span><br><span class="line">                          is_sale,</span><br><span class="line">                          create_time</span><br><span class="line">                        from sku_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_category1()&#123;</span><br><span class="line">  import_data &quot;base_category1&quot; &quot;select </span><br><span class="line">                                  id,</span><br><span class="line">                                  name </span><br><span class="line">                                from base_category1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_category2()&#123;</span><br><span class="line">  import_data &quot;base_category2&quot; &quot;select</span><br><span class="line">                                  id,</span><br><span class="line">                                  name,</span><br><span class="line">                                  category1_id </span><br><span class="line">                                from base_category2&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_category3()&#123;</span><br><span class="line">  import_data &quot;base_category3&quot; &quot;select</span><br><span class="line">                                  id,</span><br><span class="line">                                  name,</span><br><span class="line">                                  category2_id</span><br><span class="line">                                from base_category3&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_province()&#123;</span><br><span class="line">  import_data base_province &quot;select</span><br><span class="line">                              id,</span><br><span class="line">                              name,</span><br><span class="line">                              region_id,</span><br><span class="line">                              area_code,</span><br><span class="line">                              iso_code,</span><br><span class="line">                              iso_3166_2</span><br><span class="line">                            from base_province&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_region()&#123;</span><br><span class="line">  import_data base_region &quot;select</span><br><span class="line">                              id,</span><br><span class="line">                              region_name</span><br><span class="line">                            from base_region&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_trademark()&#123;</span><br><span class="line">  import_data base_trademark &quot;select</span><br><span class="line">                                id,</span><br><span class="line">                                tm_name</span><br><span class="line">                              from base_trademark&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_spu_info()&#123;</span><br><span class="line">  import_data spu_info &quot;select</span><br><span class="line">                            id,</span><br><span class="line">                            spu_name,</span><br><span class="line">                            category3_id,</span><br><span class="line">                            tm_id</span><br><span class="line">                          from spu_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_favor_info()&#123;</span><br><span class="line">  import_data favor_info &quot;select</span><br><span class="line">                          id,</span><br><span class="line">                          user_id,</span><br><span class="line">                          sku_id,</span><br><span class="line">                          spu_id,</span><br><span class="line">                          is_cancel,</span><br><span class="line">                          create_time,</span><br><span class="line">                          cancel_time</span><br><span class="line">                        from favor_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_cart_info()&#123;</span><br><span class="line">  import_data cart_info &quot;select</span><br><span class="line">                        id,</span><br><span class="line">                        user_id,</span><br><span class="line">                        sku_id,</span><br><span class="line">                        cart_price,</span><br><span class="line">                        sku_num,</span><br><span class="line">                        sku_name,</span><br><span class="line">                        create_time,</span><br><span class="line">                        operate_time,</span><br><span class="line">                        is_ordered,</span><br><span class="line">                        order_time,</span><br><span class="line">                        source_type,</span><br><span class="line">                        source_id</span><br><span class="line">                      from cart_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_coupon_info()&#123;</span><br><span class="line">  import_data coupon_info &quot;select</span><br><span class="line">                          id,</span><br><span class="line">                          coupon_name,</span><br><span class="line">                          coupon_type,</span><br><span class="line">                          condition_amount,</span><br><span class="line">                          condition_num,</span><br><span class="line">                          activity_id,</span><br><span class="line">                          benefit_amount,</span><br><span class="line">                          benefit_discount,</span><br><span class="line">                          create_time,</span><br><span class="line">                          range_type,</span><br><span class="line">                          limit_num,</span><br><span class="line">                          taken_count,</span><br><span class="line">                          start_time,</span><br><span class="line">                          end_time,</span><br><span class="line">                          operate_time,</span><br><span class="line">                          expire_time</span><br><span class="line">                        from coupon_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_activity_info()&#123;</span><br><span class="line">  import_data activity_info &quot;select</span><br><span class="line">                              id,</span><br><span class="line">                              activity_name,</span><br><span class="line">                              activity_type,</span><br><span class="line">                              start_time,</span><br><span class="line">                              end_time,</span><br><span class="line">                              create_time</span><br><span class="line">                            from activity_info&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_activity_rule()&#123;</span><br><span class="line">    import_data activity_rule &quot;select</span><br><span class="line">                                    id,</span><br><span class="line">                                    activity_id,</span><br><span class="line">                                    activity_type,</span><br><span class="line">                                    condition_amount,</span><br><span class="line">                                    condition_num,</span><br><span class="line">                                    benefit_amount,</span><br><span class="line">                                    benefit_discount,</span><br><span class="line">                                    benefit_level</span><br><span class="line">                                from activity_rule&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_base_dic()&#123;</span><br><span class="line">    import_data base_dic &quot;select</span><br><span class="line">                            dic_code,</span><br><span class="line">                            dic_name,</span><br><span class="line">                            parent_code,</span><br><span class="line">                            create_time,</span><br><span class="line">                            operate_time</span><br><span class="line">                          from base_dic&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import_order_detail_activity()&#123;</span><br><span class="line">    import_data order_detail_activity &quot;select</span><br><span class="line">                                                                id,</span><br><span class="line">                                                                order_id,</span><br><span class="line">                                                                order_detail_id,</span><br><span class="line">                                                                activity_id,</span><br><span class="line">                                                                activity_rule_id,</span><br><span class="line">                                                                sku_id,</span><br><span class="line">                                                                create_time</span><br><span class="line">                                                            from order_detail_activity&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import_order_detail_coupon()&#123;</span><br><span class="line">    import_data order_detail_coupon &quot;select</span><br><span class="line">                                                                id,</span><br><span class="line">                                                order_id,</span><br><span class="line">                                                                order_detail_id,</span><br><span class="line">                                                                coupon_id,</span><br><span class="line">                                                                coupon_use_id,</span><br><span class="line">                                                                sku_id,</span><br><span class="line">                                                                create_time</span><br><span class="line">                                                            from order_detail_coupon&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import_refund_payment()&#123;</span><br><span class="line">    import_data refund_payment &quot;select</span><br><span class="line">                                                        id,</span><br><span class="line">                                                        out_trade_no,</span><br><span class="line">                                                        order_id,</span><br><span class="line">                                                        sku_id,</span><br><span class="line">                                                        payment_type,</span><br><span class="line">                                                        trade_no,</span><br><span class="line">                                                        total_amount,</span><br><span class="line">                                                        subject,</span><br><span class="line">                                                        refund_status,</span><br><span class="line">                                                        create_time,</span><br><span class="line">                                                        callback_time</span><br><span class="line">                                                    from refund_payment&quot;                                                    </span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import_sku_attr_value()&#123;</span><br><span class="line">    import_data sku_attr_value &quot;select</span><br><span class="line">                                                    id,</span><br><span class="line">                                                    attr_id,</span><br><span class="line">                                                    value_id,</span><br><span class="line">                                                    sku_id,</span><br><span class="line">                                                    attr_name,</span><br><span class="line">                                                    value_name</span><br><span class="line">                                                from sku_attr_value&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import_sku_sale_attr_value()&#123;</span><br><span class="line">    import_data sku_sale_attr_value &quot;select</span><br><span class="line">                                                            id,</span><br><span class="line">                                                            sku_id,</span><br><span class="line">                                                            spu_id,</span><br><span class="line">                                                            sale_attr_value_id,</span><br><span class="line">                                                            sale_attr_id,</span><br><span class="line">                                                            sale_attr_name,</span><br><span class="line">                                                            sale_attr_value_name</span><br><span class="line">                                                        from sku_sale_attr_value&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">  &quot;order_info&quot;)</span><br><span class="line">     import_order_info</span><br><span class="line">;;</span><br><span class="line">  &quot;base_category1&quot;)</span><br><span class="line">     import_base_category1</span><br><span class="line">;;</span><br><span class="line">  &quot;base_category2&quot;)</span><br><span class="line">     import_base_category2</span><br><span class="line">;;</span><br><span class="line">  &quot;base_category3&quot;)</span><br><span class="line">     import_base_category3</span><br><span class="line">;;</span><br><span class="line">  &quot;order_detail&quot;)</span><br><span class="line">     import_order_detail</span><br><span class="line">;;</span><br><span class="line">  &quot;sku_info&quot;)</span><br><span class="line">     import_sku_info</span><br><span class="line">;;</span><br><span class="line">  &quot;user_info&quot;)</span><br><span class="line">     import_user_info</span><br><span class="line">;;</span><br><span class="line">  &quot;payment_info&quot;)</span><br><span class="line">     import_payment_info</span><br><span class="line">;;</span><br><span class="line">  &quot;base_province&quot;)</span><br><span class="line">     import_base_province</span><br><span class="line">;;</span><br><span class="line">  &quot;base_region&quot;)</span><br><span class="line">     import_base_region</span><br><span class="line">;;</span><br><span class="line">  &quot;base_trademark&quot;)</span><br><span class="line">     import_base_trademark</span><br><span class="line">;;</span><br><span class="line">  &quot;activity_info&quot;)</span><br><span class="line">      import_activity_info</span><br><span class="line">;;</span><br><span class="line">  &quot;cart_info&quot;)</span><br><span class="line">      import_cart_info</span><br><span class="line">;;</span><br><span class="line">  &quot;comment_info&quot;)</span><br><span class="line">      import_comment_info</span><br><span class="line">;;</span><br><span class="line">  &quot;coupon_info&quot;)</span><br><span class="line">      import_coupon_info</span><br><span class="line">;;</span><br><span class="line">  &quot;coupon_use&quot;)</span><br><span class="line">      import_coupon_use</span><br><span class="line">;;</span><br><span class="line">  &quot;favor_info&quot;)</span><br><span class="line">      import_favor_info</span><br><span class="line">;;</span><br><span class="line">  &quot;order_refund_info&quot;)</span><br><span class="line">      import_order_refund_info</span><br><span class="line">;;</span><br><span class="line">  &quot;order_status_log&quot;)</span><br><span class="line">      import_order_status_log</span><br><span class="line">;;</span><br><span class="line">  &quot;spu_info&quot;)</span><br><span class="line">      import_spu_info</span><br><span class="line">;;</span><br><span class="line">  &quot;activity_rule&quot;)</span><br><span class="line">      import_activity_rule</span><br><span class="line">;;</span><br><span class="line">  &quot;base_dic&quot;)</span><br><span class="line">      import_base_dic</span><br><span class="line">;;</span><br><span class="line">  &quot;order_detail_activity&quot;)</span><br><span class="line">      import_order_detail_activity</span><br><span class="line">;;</span><br><span class="line">  &quot;order_detail_coupon&quot;)</span><br><span class="line">      import_order_detail_coupon</span><br><span class="line">;;</span><br><span class="line">  &quot;refund_payment&quot;)</span><br><span class="line">      import_refund_payment</span><br><span class="line">;;</span><br><span class="line">  &quot;sku_attr_value&quot;)</span><br><span class="line">      import_sku_attr_value</span><br><span class="line">;;</span><br><span class="line">  &quot;sku_sale_attr_value&quot;)</span><br><span class="line">      import_sku_sale_attr_value</span><br><span class="line">;;</span><br><span class="line">  &quot;all&quot;)</span><br><span class="line">   import_base_category1</span><br><span class="line">   import_base_category2</span><br><span class="line">   import_base_category3</span><br><span class="line">   import_order_info</span><br><span class="line">   import_order_detail</span><br><span class="line">   import_sku_info</span><br><span class="line">   import_user_info</span><br><span class="line">   import_payment_info</span><br><span class="line">   import_base_region</span><br><span class="line">   import_base_province</span><br><span class="line">   import_base_trademark</span><br><span class="line">   import_activity_info</span><br><span class="line">   import_cart_info</span><br><span class="line">   import_comment_info</span><br><span class="line">   import_coupon_use</span><br><span class="line">   import_coupon_info</span><br><span class="line">   import_favor_info</span><br><span class="line">   import_order_refund_info</span><br><span class="line">   import_order_status_log</span><br><span class="line">   import_spu_info</span><br><span class="line">   import_activity_rule</span><br><span class="line">   import_base_dic</span><br><span class="line">   import_order_detail_activity</span><br><span class="line">   import_order_detail_coupon</span><br><span class="line">   import_refund_payment</span><br><span class="line">   import_sku_attr_value</span><br><span class="line">   import_sku_sale_attr_value</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;myhadoop-sh&quot;&gt;&lt;a href=&quot;#myhadoop-sh&quot; class=&quot;headerlink&quot; title=&quot;myhadoop.sh&quot;&gt;&lt;/a&gt;myhadoop.sh&lt;/h1&gt;&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;#!/bin/bash&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;if [ $# -lt 1 ]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;then&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    echo &amp;quot;No Args Input...&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    exit ;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;fi&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;case $1 in&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;quot;start&amp;quot;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        echo &amp;quot; =================== 启动 hadoop集群 ===================&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        echo &amp;quot; --------------- 启动 hdfs ---------------&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ssh hadoop102 &amp;quot;/opt/module/hadoop-3.1.3/sbin/start-dfs.sh&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        echo &amp;quot; --------------- 启动 yarn ---------------&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ssh hadoop103 &amp;quot;/opt/module/hadoop-3.1.3/sbin/start-yarn.sh&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        echo &amp;quot; --------------- 启动 historyserver ---------------&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ssh hadoop102 &amp;quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;quot;stop&amp;quot;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        echo &amp;quot; =================== 关闭 hadoop集群 ===================&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        echo &amp;quot; --------------- 关闭 historyserver ---------------&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ssh hadoop102 &amp;quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        echo &amp;quot; --------------- 关闭 yarn ---------------&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ssh hadoop103 &amp;quot;/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        echo &amp;quot; --------------- 关闭 hdfs ---------------&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ssh hadoop102 &amp;quot;/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;*)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    echo &amp;quot;Input Args Error...&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;esac&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="shell" scheme="http://xubatian.cn/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>数据仓库原理与实现: 维度建模理论之事实表</title>
    <link href="http://xubatian.cn/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1%E7%90%86%E8%AE%BA%E4%B9%8B%E4%BA%8B%E5%AE%9E%E8%A1%A8/"/>
    <id>http://xubatian.cn/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1%E7%90%86%E8%AE%BA%E4%B9%8B%E4%BA%8B%E5%AE%9E%E8%A1%A8/</id>
    <published>2022-02-16T05:30:32.000Z</published>
    <updated>2022-02-16T05:39:18.349Z</updated>
    
    <content type="html"><![CDATA[<h2 id="事实表概述"><a href="#事实表概述" class="headerlink" title="事实表概述"></a>事实表概述</h2><p>​        事实表作为数据仓库维度建模的核心，紧紧围绕着业务过程来设计。其包含与该业务过程有关的维度引用（维度表外键）以及该业务过程的度量（通常是可累加的数字类型字段）。</p><h3 id="事实表特点"><a href="#事实表特点" class="headerlink" title="事实表特点"></a><strong>事实表特点</strong></h3><p>​        事实表通常比较“细长”，即列较少，但行较多，且行的增速快。</p><h3 id="事实表分类"><a href="#事实表分类" class="headerlink" title="事实表分类"></a><strong>事实表分类</strong></h3><p>​        事实表有三种类型：分别是事务事实表、周期快照事实表和累积快照事实表，每种事实表都具有不同的特点和适用场景，下面逐个介绍。</p><span id="more"></span><h2 id="事务型事实表"><a href="#事务型事实表" class="headerlink" title="事务型事实表"></a>事务型事实表</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h3><p>​        事务事实表用来记录各业务过程，它保存的是各业务过程的原子操作事件，即最细粒度的操作事件。粒度是指事实表中一行数据所表达的业务细节程度。</p><p>​        事务型事实表可用于分析与各业务过程相关的各项统计指标，由于其保存了最细粒度的记录，可以提供最大限度的灵活性，可以支持无法预期的各种细节层次的统计需求。</p><h3 id="设计流程"><a href="#设计流程" class="headerlink" title="设计流程"></a><strong>设计流程</strong></h3><p>​        设计事务事实表时一般可遵循以下四个步骤：</p><p>​        <strong>选择业务过程→声明粒度→确认维度→确认事实</strong></p><p><strong>1）选择业务过程</strong></p><p>​        在业务系统中，挑选我们感兴趣的业务过程，业务过程可以概括为一个个不可拆分的行为事件，例如电商交易中的下单，取消订单，付款，退单等，都是业务过程。通常情况下，一个业务过程对应一张事务型事实表。</p><p><strong>2）声明粒度</strong></p><p>​        业务过程确定后，需要为每个业务过程声明粒度。即精确定义每张事务型事实表的每行数据表示什么，应该尽可能选择最细粒度，以此来应各种细节程度的需求。</p><p><strong>典型的粒度声明如下：</strong></p><p>​        订单事实表中一行数据表示的是一个订单中的一个商品项。</p><p><strong>3）确定维度</strong></p><p>​        确定维度具体是指，确定与每张事务型事实表相关的维度有哪些。</p><p>​        确定维度时应尽量多的选择与业务过程相关的环境信息。因为维度的丰富程度就决定了维度模型能够支持的指标丰富程度。</p><p><strong>4）确定事实</strong></p><p>​        此处的“事实”一词，指的是每个业务过程的度量值（通常是可累加的数字类型的值，例如：次数、个数、件数、金额等）。</p><p>经过上述四个步骤，事务型事实表就基本设计完成了。第一步选择业务过程可以确定有哪些事务型事实表，第二步可以确定每张事务型事实表的每行数据是什么，第三步可以确定每张事务型事实表的维度外键，第四步可以确定每张事务型事实表的度量值字段。</p><h3 id="不足"><a href="#不足" class="headerlink" title="不足"></a><strong>不足</strong></h3><p>​        事务型事实表可以保存所有业务过程的最细粒度的操作事件，故理论上其可以支撑与各业务过程相关的各种统计粒度的需求。但对于某些特定类型的需求，其逻辑可能会比较复杂，或者效率会比较低下。例如：</p><p><strong>1）存量型指标</strong></p><p>​        例如商品库存，账户余额等。此处以电商中的虚拟货币为例，虚拟货币业务包含的业务过程主要包括获取货币和使用货币，两个业务过程各自对应一张事务型事实表，一张存储所有的获取货币的原子操作事件，另一张存储所有使用货币的原子操作事件。</p><p>假定现有一个需求，要求统计截至当日的各用户虚拟货币余额。由于获取货币和使用货币均会影响到余额，故需要对两张事务型事实表进行聚合，且需要区分两者对余额的影响（加或减），另外需要对两张表的全表数据聚合才能得到统计结果。</p><p>可以看到，不论是从逻辑上还是效率上考虑，这都不是一个好的方案。</p><p><strong>2）多事务关联统计</strong></p><p>​        例如，现需要统计最近30天，用户下单到支付的时间间隔的平均值。统计思路应该是找到下单事务事实表和支付事务事实表，过滤出最近30天的记录，然后按照订单id对两张事实表进行关联，之后用支付时间减去下单时间，然后再求平均值。</p><p>逻辑上虽然并不复杂，但是其效率较低，应为下单事务事实表和支付事务事实表均为大表，大表join大表的操作应尽量避免。</p><p>可以看到，在上述两种场景下事务型事实表的表现并不理想。下面要介绍的另外两种类型的事实表就是为了弥补事务型事实表的不足的。</p><h2 id="周期型快照事实表"><a href="#周期型快照事实表" class="headerlink" title="周期型快照事实表"></a><strong>周期型快照事实表</strong></h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a><strong>概述</strong></h3><p>​        周期快照事实表以具有规律性的、可预见的时间间隔来记录事实，主要用于分析一些存量型（例如商品库存，账户余额）或者状态型（空气温度，行驶速度）指标。</p><p>​        对于商品库存、账户余额这些存量型指标，业务系统中通常就会计算并保存最新结果，所以定期同步一份全量数据到数据仓库，构建周期型快照事实表，就能轻松应对此类统计需求，而无需再对事务型事实表中大量的历史记录进行聚合了。</p><p>对于空气温度、行驶速度这些状态型指标，由于它们的值往往是连续的，我们无法捕获其变动的原子事务操作，所以无法使用事务型事实表统计此类需求。而只能定期对其进行采样，构建周期型快照事实表。</p><h3 id="设计流程-1"><a href="#设计流程-1" class="headerlink" title="设计流程"></a><strong>设计流程</strong></h3><p><strong>1)确定粒度</strong></p><p>​        周期型快照事实表的粒度可由采样周期和维度描述，故确定采样周期和维度后即可确定粒度。</p><p>​        采样周期通常选择每日。</p><p>​        维度可根据统计指标决定，例如指标为统计每个仓库中每种商品的库存，则可确定维度为仓库和商品。</p><p>​        确定完采样周期和维度后，即可确定该表粒度为每日-仓库-商品。</p><p><strong>2)确认事实</strong></p><p>​        事实也可根据统计指标决定，例如指标为统计每个仓库中每种商品的库存，则事实为商品库存。</p><h3 id="事实类型"><a href="#事实类型" class="headerlink" title="事实类型"></a>事实类型</h3><p>​        此处的事实类型是指度量值的类型，而非事实表的类型。事实（度量值）共分为三类，分别是可加事实，半可加事实和不可加事实。</p><p><strong>1）可加事实</strong></p><p>​        可加事实是指可以按照与事实表相关的所有维度进行累加，例如事务型事实表中的事实。</p><p><strong>2）半可加事实</strong></p><p>​        半可加事实是指只能按照与事实表相关的一部分维度进行累加，例如周期型快照事实表中的事实。以上述各仓库中各商品的库存每天快照事实表为例，这张表中的库存事实可以按照仓库或者商品维度进行累加，但是不能按照时间维度进行累加，因为将每天的库存累加起来是没有任何意义的。</p><p><strong>3）不可加事实</strong></p><p>​        不可加事实是指完全不具备可加性，例如比率型事实。不可加事实通常需要转化为可加事实，例如比率可转化为分子和分母。</p><h2 id="累计型快照事实表"><a href="#累计型快照事实表" class="headerlink" title="累计型快照事实表"></a><strong>累计型快照事实表</strong></h2><h3 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a><strong>概述</strong></h3><p>​        累计快照事实表是基于一个业务流程中的多个关键业务过程联合处理而构建的事实表，如交易流程中的下单、支付、发货、确认收货业务过程。</p><p>累积型快照事实表通常具有多个日期字段，每个日期对应业务流程中的一个关键业务过程（里程碑）。</p><table><thead><tr><th>订单id</th><th>用户id</th><th>下单日期</th><th>支付日期</th><th>发货日期</th><th>确认收货日期</th><th>订单金额</th><th>支付金额</th></tr></thead><tbody><tr><td>1001</td><td>1234</td><td>2020-06-14</td><td>2020-06-15</td><td>2020-06-16</td><td>2020-06-17</td><td>1000</td><td>1000</td></tr></tbody></table><p>​        累积型快照事实表主要用于分析业务过程（里程碑）之间的时间间隔等需求。例如前文提到的用户下单到支付的平均时间间隔，使用累积型快照事实表进行统计，就能避免两个事务事实表的关联操作，从而变得十分简单高效。</p><h3 id="设计流程-2"><a href="#设计流程-2" class="headerlink" title="设计流程"></a><strong>设计流程</strong></h3><p>​        累积型快照事实表的设计流程同事务型事实表类似，也可采用以下四个步骤，下面重点描述与事务型事实表的不同之处。</p><p>​        <strong>选择业务过程→声明粒度→确认维度→确认事实。</strong></p><p><strong>1）选择业务过程</strong></p><p>​        选择一个业务流程中需要关联分析的多个关键业务过程，多个业务过程对应一张累积型快照事实表。</p><p><strong>2）声明粒度</strong></p><p>​        精确定义每行数据表示的是什么，尽量选择最小粒度。</p><p><strong>3）确认维度</strong></p><p>​        选择与各业务过程相关的维度，需要注意的是，每各业务过程均需要一个日期维度。</p><p><strong>4）确认事实</strong></p><p>​     选择各业务过程的度量值。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;事实表概述&quot;&gt;&lt;a href=&quot;#事实表概述&quot; class=&quot;headerlink&quot; title=&quot;事实表概述&quot;&gt;&lt;/a&gt;事实表概述&lt;/h2&gt;&lt;p&gt;​        事实表作为数据仓库维度建模的核心，紧紧围绕着业务过程来设计。其包含与该业务过程有关的维度引用（维度表外键）以及该业务过程的度量（通常是可累加的数字类型字段）。&lt;/p&gt;
&lt;h3 id=&quot;事实表特点&quot;&gt;&lt;a href=&quot;#事实表特点&quot; class=&quot;headerlink&quot; title=&quot;事实表特点&quot;&gt;&lt;/a&gt;&lt;strong&gt;事实表特点&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;​        事实表通常比较“细长”，即列较少，但行较多，且行的增速快。&lt;/p&gt;
&lt;h3 id=&quot;事实表分类&quot;&gt;&lt;a href=&quot;#事实表分类&quot; class=&quot;headerlink&quot; title=&quot;事实表分类&quot;&gt;&lt;/a&gt;&lt;strong&gt;事实表分类&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;​        事实表有三种类型：分别是事务事实表、周期快照事实表和累积快照事实表，每种事实表都具有不同的特点和适用场景，下面逐个介绍。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="数据仓库" scheme="http://xubatian.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>数据仓库原理与实现: 数据仓库建模概述</title>
    <link href="http://xubatian.cn/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E6%A8%A1%E6%A6%82%E8%BF%B0/"/>
    <id>http://xubatian.cn/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E6%A8%A1%E6%A6%82%E8%BF%B0/</id>
    <published>2022-02-16T05:25:20.000Z</published>
    <updated>2022-02-16T05:29:59.177Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据仓库建模概述"><a href="#数据仓库建模概述" class="headerlink" title="数据仓库建模概述"></a>数据仓库建模概述</h1><h2 id="数据仓库建模的意义"><a href="#数据仓库建模的意义" class="headerlink" title="数据仓库建模的意义"></a>数据仓库建模的意义</h2><p>​        如果把数据看作图书馆里的书，我们希望看到它们在书架上分门别类地放置；如果把数据看作城市的建筑，我们希望城市规划布局合理；如果把数据看作电脑文件和文件夹，我们希望按照自己的习惯有很好的文件夹组织方式，而不是糟糕混乱的桌面，经常为找一个文件而不知所措。</p><p>数据模型就是数据组织和存储方法，它强调从业务、数据存取和使用角度合理存储数据。只有将数据有序的组织和存储起来之后，数据才能得到高性能、低成本、高效率、高质量的使用。</p><p>高性能：良好的数据模型能够帮助我们快速查询所需要的数据。</p><p>低成本：良好的数据模型能减少重复计算，实现计算结果的复用，降低计算成本。</p><p>高效率：良好的数据模型能极大的改善用户使用数据的体验，提高使用数据的效率。</p><p>高质量：良好的数据模型能改善数据统计口径的混乱，减少计算错误的可能性。</p><span id="more"></span><h2 id="数据仓库建模方法论"><a href="#数据仓库建模方法论" class="headerlink" title="数据仓库建模方法论"></a>数据仓库建模方法论</h2><h3 id="ER模型"><a href="#ER模型" class="headerlink" title="ER模型"></a>ER模型</h3><p>​        数据仓库之父Bill Inmon提出的建模方法是从全企业的高度，用实体关系（Entity Relationship，ER）模型来描述企业业务，并用规范化的方式表示出来，在范式理论上符合3NF。</p><p><strong>1）实体关系模型</strong><br>        实体关系模型将复杂的数据抽象为两个概念——实体和关系。实体表示一个对象，例如学生、班级，关系是指两个实体之间的关系，例如学生和班级之间的从属关系。</p><p><strong>2）数据库规范化</strong><br>        数据库规范化是使用一系列范式设计数据库（通常是关系型数据库）的过程，其目的是减少数据冗余，增强数据的一致性。<br>这一系列范式就是指在设计关系型数据库时，需要遵从的不同的规范。关系型数据库的范式一共有六种，分别是第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF）。遵循的范式级别越高，数据冗余性就越低。</p><p>下图为一个采用Bill Inmon倡导的建模方法构建的模型，从图中可以看出，较为松散、零碎，物理表数量多。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220216132807.png"></p><p>​        这种建模方法的出发点是整合数据，其目的是将整个企业的数据进行组合和合并，并进行规范处理，减少数据冗余性，保证数据的一致性。这种模型并不适合直接用于分析统计。</p><h3 id="维度模型"><a href="#维度模型" class="headerlink" title="维度模型"></a>维度模型</h3><p>​        数据仓库领域的令一位大师——Ralph Kimball倡导的建模方法为维度建模。维度模型将复杂的业务通过事实和维度两个概念进行呈现。事实通常对应业务过程，而维度通常对应业务过程发生时所处的环境。</p><p><strong>注</strong>：业务过程可以概括为一个个不可拆分的行为事件，例如电商交易中的下单，取消订单，付款，退单等，都是业务过程。</p><p>​        下图为一个典型的维度模型，其中位于中心的SalesOrder为事实表，其中保存的是下单这个业务过程的所有记录。位于周围每张表都是维度表，包括Date（日期），Customer（顾客），Product（产品），Location（地区）等，这些维度表就组成了每个订单发生时所处的环境，即何人、何时、在何地下单了何种产品。从图中可以看出，模型相对清晰、简洁。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220216132909.png"></p><p>​        维度建模以数据分析作为出发点，为数据分析服务，因此它关注的重点的用户如何更快的完成需求分析以及如何实现较好的大规模复杂查询的响应性能。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;数据仓库建模概述&quot;&gt;&lt;a href=&quot;#数据仓库建模概述&quot; class=&quot;headerlink&quot; title=&quot;数据仓库建模概述&quot;&gt;&lt;/a&gt;数据仓库建模概述&lt;/h1&gt;&lt;h2 id=&quot;数据仓库建模的意义&quot;&gt;&lt;a href=&quot;#数据仓库建模的意义&quot; class=&quot;headerlink&quot; title=&quot;数据仓库建模的意义&quot;&gt;&lt;/a&gt;数据仓库建模的意义&lt;/h2&gt;&lt;p&gt;​        如果把数据看作图书馆里的书，我们希望看到它们在书架上分门别类地放置；如果把数据看作城市的建筑，我们希望城市规划布局合理；如果把数据看作电脑文件和文件夹，我们希望按照自己的习惯有很好的文件夹组织方式，而不是糟糕混乱的桌面，经常为找一个文件而不知所措。&lt;/p&gt;
&lt;p&gt;数据模型就是数据组织和存储方法，它强调从业务、数据存取和使用角度合理存储数据。只有将数据有序的组织和存储起来之后，数据才能得到高性能、低成本、高效率、高质量的使用。&lt;/p&gt;
&lt;p&gt;高性能：良好的数据模型能够帮助我们快速查询所需要的数据。&lt;/p&gt;
&lt;p&gt;低成本：良好的数据模型能减少重复计算，实现计算结果的复用，降低计算成本。&lt;/p&gt;
&lt;p&gt;高效率：良好的数据模型能极大的改善用户使用数据的体验，提高使用数据的效率。&lt;/p&gt;
&lt;p&gt;高质量：良好的数据模型能改善数据统计口径的混乱，减少计算错误的可能性。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="数据仓库" scheme="http://xubatian.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink SQL CDC</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink-SQL-CDC/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink-SQL-CDC/</id>
    <published>2022-02-15T16:47:32.000Z</published>
    <updated>2022-02-15T16:58:42.361Z</updated>
    
    <content type="html"><![CDATA[<p>摘要：20年7月，Flink 1.11 新版发布，在生态及易用性上有大幅提升，其中 Table &amp; SQL 开始支持 Change Data Capture（CDC）。CDC 被广泛使用在复制数据、更新缓存、微服务间同步数据、审计日志等场景，本文由社区由曾庆东同学分享，主要介绍 Flink SQL CDC 在生产环境的落地实践以及总结的实战经验</p><p>Tips：点击下方链接可查看社区直播的 Flink SQL CDC 相关视频～<br><a href="https://flink-learning.org.cn/developers/flink-training-course3/">https://flink-learning.org.cn/developers/flink-training-course3/</a></p><span id="more"></span><h1 id="01-项目背景"><a href="#01-项目背景" class="headerlink" title="01 项目背景"></a><strong>01 项目背景</strong></h1><p>本人目前参与的项目属于公司里面数据密集、计算密集的一个重要项目，需要提供高效且准确的 OLAP 服务，提供灵活且实时的报表。业务数据存储在 MySQL  中，通过主从复制同步到报表库。作为集团级公司，数据增长多而且快，出现了多个千万级、亿级的大表。为了实现各个维度的各种复杂的报表业务，有些千万级大表仍然需要进行 Join，计算规模非常惊人，经常不能及时响应请求。</p><p>随着数据量的日益增长和实时分析的需求越来越大，急需对系统进行流式计算、实时化改造。正是在这个背景下，开始了我们与 Flink SQL CDC 的故事。</p><h1 id="02-解决方案"><a href="#02-解决方案" class="headerlink" title="02 解决方案"></a><strong>02 解决方案</strong></h1><p>针对平台现在存在的问题，我们提出了把报表的数据实时化的方案。该方案主要通过 Flink SQL CDC + Elasticsearch 实现。Flink SQL 支持 CDC 模式的数据同步，将 MySQL  中的全增量数据实时地采集、预计算、并同步到 Elasticsearch 中，Elasticsearch  作为我们的实时报表和即席分析引擎。项目整体架构图如下所示：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220216005047.png"></p><p>实时报表实现具体思路是，使用 Flink CDC 读取全量数据，全量数据同步完成后，Flink CDC 会无缝切换至 MySQL 的 binlog  位点继续消费增量的变更数据，且保证不会多消费一条也不会少消费一条。读取到的账单和订单的全增量数据会与产品表做关联补全信息，并做一些预聚合，然后将聚合结果输出到 Elasticsearch，前端页面只需要到 Elasticsearch 通过精准匹配（terms）查找数据，或者再使用 agg  做高维聚合统计得到多个服务中心的报表数据。</p><p>从整体架构中，可以看到，Flink SQL 及其 CDC 功能在我们的架构中扮演着核心角色。我们采用 Flink SQL CDC，而不是 Canal + Kafka  的传统架构，主要原因还是因为其依赖组件少，维护成本低，开箱即用，上手容易。具体来说 Flink SQL CDC  是一个集采集、计算、传输于一体的工具，其吸引我们的优点有：</p><p>① 减少维护的组件、简化实现链路； </p><p>② 减少端到端延迟； </p><p>③ 减轻维护成本和开发成本； </p><p>④ 支持 Exactly Once 的读取和计算（由于我们是账务系统，所以数据一致性非常重要）； </p><p>⑤ 数据不落地，减少存储成本； </p><p>⑥ 支持全量和增量流式读取；</p><p>有关 Flink SQL CDC 的介绍和教程，可以观看 Apache Flink 社区发布的相关视频：</p><p><a href="https://www.bilibili.com/video/BV1zt4y1D7kt/">https://www.bilibili.com/video/BV1zt4y1D7kt/</a></p><p>项目使用的是 flink-cdc-connectors 中提供的 mysql-cdc 组件。这是一个 Flink 数据源，支持对 MySQL  数据库的全量和增量读取。它在扫描全表前会先加一个全局读锁，然后获取此时的 binlog  position，紧接着释放全局读锁。随后开始扫描全表，当全表快照读取完后，会从之前获取的 binlog position  获取增量的变更记录。因此这个读锁是非常轻量的，持锁时间非常短，不会对线上业务造成太大影响。更多信息可以参考  flink-cdc-connectors 项目官网：<a href="https://github.com/ververica/flink-cdc-connectors%E3%80%82">https://github.com/ververica/flink-cdc-connectors。</a></p><h1 id="03-项目运行环境与现状"><a href="#03-项目运行环境与现状" class="headerlink" title="03 项目运行环境与现状"></a><strong>03 项目运行环境与现状</strong></h1><p>我们在生产环境搭建了 Hadoop + Flink + Elasticsearch 分布式环境，采用的 Flink on YARN 的 per-job  模式运行，使用 RocksDB 作为 state backend，HDFS 作为 checkpoint 持久化地址，并且做好了 HDFS  的容错，保证 checkpoint 数据不丢失。我们使用 SQL Client 提交作业，所有作业统一使用纯 SQL，没有写一行 Java  代码。</p><p>目前已上线了 3 个基于 Flink CDC 的作业，已稳定在线上运行了两个星期，并且业务产生的订单实收和账单实收数据能实时聚合输出到  Elasticsearch，输出的数据准确无误。现在也正在对其他报表采用 Flink SQL CDC  进行实时化改造，替换旧的业务系统，让系统数据更实时。</p><h1 id="04-具体实现"><a href="#04-具体实现" class="headerlink" title="04 具体实现"></a><strong>04 具体实现</strong></h1><p>① 进入 Flink/bin，使用 ./sql-client.sh embedded 启动 SQL CLI 客户端。 </p><p>② 使用 DDL 创建 Flink Source 和 Sink 表。这里创建的表字段个数不一定要与 MySQL 的字段个数和顺序一致，只需要挑选 MySQL 表中业务需要的字段即可，并且字段类型保持一致。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">-- 在Flink创建账单实收source表</span><br><span class="line">CREATE TABLE bill_info (</span><br><span class="line">  billCode STRING,</span><br><span class="line">  serviceCode STRING,</span><br><span class="line">  accountPeriod STRING,</span><br><span class="line">  subjectName STRING ,</span><br><span class="line">  subjectCode STRING,</span><br><span class="line">  occurDate TIMESTAMP,</span><br><span class="line">  amt  DECIMAL(11,2),</span><br><span class="line">  status STRING,</span><br><span class="line">  proc_time AS PROCTIME() -–使用维表时需要指定该字段</span><br><span class="line">) WITH (</span><br><span class="line">  &#x27;connector&#x27; = &#x27;mysql-cdc&#x27;, -- 连接器</span><br><span class="line">  &#x27;hostname&#x27; = &#x27;******&#x27;,   --mysql地址</span><br><span class="line">  &#x27;port&#x27; = &#x27;3307&#x27;,  -- mysql端口</span><br><span class="line">  &#x27;username&#x27; = &#x27;******&#x27;,  --mysql用户名</span><br><span class="line">  &#x27;password&#x27; = &#x27;******&#x27;,  -- mysql密码</span><br><span class="line">  &#x27;database-name&#x27; = &#x27;cdc&#x27;, --  数据库名称</span><br><span class="line">  &#x27;table-name&#x27; = &#x27;***&#x27;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">-- 在Flink创建订单实收source表</span><br><span class="line">CREATE TABLE order_info (</span><br><span class="line">  orderCode STRING,</span><br><span class="line">  serviceCode STRING,</span><br><span class="line">  accountPeriod STRING,</span><br><span class="line">  subjectName STRING ,</span><br><span class="line">  subjectCode STRING,</span><br><span class="line">  occurDate TIMESTAMP,</span><br><span class="line">  amt  DECIMAL(11, 2),</span><br><span class="line">  status STRING,</span><br><span class="line">  proc_time AS PROCTIME()  -–使用维表时需要指定该字段</span><br><span class="line">) WITH (</span><br><span class="line">  &#x27;connector&#x27; = &#x27;mysql-cdc&#x27;,</span><br><span class="line">  &#x27;hostname&#x27; = &#x27;******&#x27;,</span><br><span class="line">  &#x27;port&#x27; = &#x27;3307&#x27;,</span><br><span class="line">  &#x27;username&#x27; = &#x27;******&#x27;,</span><br><span class="line">  &#x27;password&#x27; = &#x27;******&#x27;,</span><br><span class="line">  &#x27;database-name&#x27; = &#x27;cdc&#x27;,</span><br><span class="line">  &#x27;table-name&#x27; = &#x27;***&#x27;,</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">-- 创建科目维表</span><br><span class="line">CREATE TABLE subject_info (</span><br><span class="line">  code VARCHAR(32) NOT NULL,</span><br><span class="line">  name VARCHAR(64) NOT NULL,</span><br><span class="line">  PRIMARY KEY (code) NOT ENFORCED  --指定主键</span><br><span class="line">) WITH (</span><br><span class="line">  &#x27;connector&#x27; = &#x27;jdbc&#x27;,</span><br><span class="line">  &#x27;url&#x27; = &#x27;jdbc:mysql://xxxx:xxxx/spd?useSSL=false&amp;autoReconnect=true&#x27;,</span><br><span class="line">  &#x27;driver&#x27; = &#x27;com.mysql.cj.jdbc.Driver&#x27;,</span><br><span class="line">  &#x27;table-name&#x27; = &#x27;***&#x27;,</span><br><span class="line">  &#x27;username&#x27; = &#x27;******&#x27;,</span><br><span class="line">  &#x27;password&#x27; = &#x27;******&#x27;,</span><br><span class="line">  &#x27;lookup.cache.max-rows&#x27; = &#x27;3000&#x27;,</span><br><span class="line">  &#x27;lookup.cache.ttl&#x27; = &#x27;10s&#x27;,</span><br><span class="line">  &#x27;lookup.max-retries&#x27; = &#x27;3&#x27;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">-- 创建实收分布结果表，把结果写到 Elasticsearch</span><br><span class="line">CREATE TABLE income_distribution (</span><br><span class="line">  serviceCode STRING,</span><br><span class="line">  accountPeriod STRING,</span><br><span class="line">  subjectCode STRING,</span><br><span class="line">  subjectName STRING,</span><br><span class="line">  amt  DECIMAL(13,2),</span><br><span class="line">  PRIMARY KEY (serviceCode, accountPeriod, subjectCode) NOT ENFORCED</span><br><span class="line">) WITH (</span><br><span class="line">  &#x27;connector&#x27; = &#x27;elasticsearch-7&#x27;,</span><br><span class="line">  &#x27;hosts&#x27; = &#x27;http://xxxx:9200&#x27;,</span><br><span class="line">  &#x27;index&#x27; = &#x27;income_distribution&#x27;,</span><br><span class="line">  &#x27;sink.bulk-flush.backoff.strategy&#x27; = &#x27;EXPONENTIAL&#x27;</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>以上的建表 DDL 分别创建了订单实收 source 表、账单实收 source 表、产品科目维表和 Elasticsearch  结果表。建表完成后，Flink 是不会马上去同步 MySQL 的数据，而是等到用户提交了一个 insert 作业后才会执行同步数据，并且  Flink 不会存储数据。我们的第一个作业是计算收入分布，数据来源于 bill_info 和 order_info 两张 MySQL  表，并且账单实收表和订单实收表都需要关联维表数据获取应收科目的最新中文名称，按照服务中心、账期、科目代码和科目名称进行分组计算实收金额的 sum 值，实收分布具体 DML 如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO income_distribution</span><br><span class="line">SELECT t1.serviceCode, t1.accountPeriod, t1.subjectCode, t1.subjectName, SUM(amt) AS amt </span><br><span class="line">FROM (</span><br><span class="line">  SELECT b.serviceCode, b.accountPeriod, b.subjectCode, s.name AS subjectName, SUM(amt) AS amt </span><br><span class="line">  FROM bill_info AS b</span><br><span class="line">  JOIN subject_info FOR SYSTEM_TIME AS OF b.proc_time s ON b.subjectCode = s.code </span><br><span class="line">  GROUP BY b.serviceCode, b.accountPeriod, b.subjectCode, s.name</span><br><span class="line">UNION ALL</span><br><span class="line">  SELECT b.serviceCode, b.accountPeriod, b.subjectCode, s.name AS subjectName, SUM(amt) AS amt</span><br><span class="line">  FROM order_info AS b</span><br><span class="line">  JOIN subject_info FOR SYSTEM_TIME AS OF b.proc_time s ON b.subjectCode = s.code </span><br><span class="line">  GROUP BY b.serviceCode, b.accountPeriod, b.subjectCode, s.name</span><br><span class="line">) AS t1</span><br><span class="line">GROUP BY t1.serviceCode, t1.accountPeriod, t1.subjectCode, t1.subjectName;</span><br></pre></td></tr></table></figure><p>Flink SQL 的维表 JOIN 和双流 JOIN 写法上不太一样，对于维表，还需要在 Flink source table 上添加一个  proctime 字段 proc_time AS PROCTIME()，关联的时候使用 FOR SYSTEM_TIME AS OF 的 SQL  语法查询时态表，意思是关联查询最新版本的维表数据。关于维表 JOIN 的使用可参阅：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/streaming/joins.html%E3%80%82">https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/streaming/joins.html。</a></p><p>③ 在 SQL Client 执行以上作业后，YARN 会创建一个 Flink 集群运行作业，并且用户可以在 Hadoop  上查看到执行作业的所有信息，并且能进入 Flink 的 Web UI 页面查看 Flink 作业详情，以下是 Hadoop 所有作业情况。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220216005207.png"></p><p>④ 作业提交后，Flink SQL CDC 会扫描指定的 MySQL 表，在这期间 Flink 也会进行  checkpoint，所以需要按照上文所述的配置 checkpoint 的重试策略和重试次数。当数据被读取进 Flink 后，Flink  会流式地进行作业逻辑的计算，实时统计出聚合结果输出到 Elasticsearch（sink 端）。相当于我们使用 Flink 在 MySQL 的表上维护了一个实时的物化视图，并将这个实时物化视图的结果存在了 Elasticsearch 中。在  Elasticsearch 中使用 GET /income_distribution/_search{ “query”:  {“match_all”: {}}} 命令查看输出的实收分布结果，如下图：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220216005222.png"></p><p>通过图中的结果可以看出聚合结果被实时的计算出来，并写到了 Elasticsearch 中了。</p><h1 id="05-踩过的坑和学到的经验"><a href="#05-踩过的坑和学到的经验" class="headerlink" title="05 踩过的坑和学到的经验"></a><strong>05 踩过的坑和学到的经验</strong></h1><h2 id="1-Flink-作业原来运行在-standalone-session-模式下，提交多个-Flink-作业会导致作业失败报错。"><a href="#1-Flink-作业原来运行在-standalone-session-模式下，提交多个-Flink-作业会导致作业失败报错。" class="headerlink" title="1. Flink 作业原来运行在 standalone session 模式下，提交多个 Flink 作业会导致作业失败报错。"></a><strong>1. Flink 作业原来运行在 standalone session 模式下，提交多个 Flink 作业会导致作业失败报错。</strong></h2><ul><li><strong>原因：</strong>因为 standalone session 模式下启动多个作业会导致多个作业的 Task 共享一个 JVM，可能会导致一些不稳定的问题。并且排查问题时，多个作业的日志混在一个 TaskManager 中，增加了排查的难度。</li></ul><ul><li><strong>解决方法：</strong>采用 YARN 的 per-job 模式启动多个作业，能有更好的隔离性。</li></ul><h2 id="2-SELECT-elasticsearch-table-报以下错误："><a href="#2-SELECT-elasticsearch-table-报以下错误：" class="headerlink" title="2. SELECT elasticsearch table 报以下错误："></a><strong>2. SELECT elasticsearch table 报以下错误：</strong></h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220216005300.png"></p><ul><li><strong>原因：</strong>Elasticsearch connector 目前只支持了 sink，不支持 source 。所以不能 SELECT elasticsearch table。</li></ul><h2 id="3-在-flink-conf-yaml-里修改默认并行度，但是在-Web-UI-看到作业的并行度还是-1，并行度修改不生效。"><a href="#3-在-flink-conf-yaml-里修改默认并行度，但是在-Web-UI-看到作业的并行度还是-1，并行度修改不生效。" class="headerlink" title="3. 在 flink-conf.yaml 里修改默认并行度，但是在 Web UI 看到作业的并行度还是 1，并行度修改不生效。"></a><strong>3. 在 flink-conf.yaml 里修改默认并行度，但是在 Web UI 看到作业的并行度还是 1，并行度修改不生效。</strong></h2><ul><li><strong>解决办法：</strong>在使用 SQL Client 时 sql-client-defaults.yaml 中的并行度配置的优先级更高。在  sql-client-defaults.yaml 中修改并行度，或者删除 sql-client-defaults.yaml  中的并行度配置。更建议采用后者。</li></ul><h2 id="4-Flink-作业在扫描-MySQL-全量数据时，checkpoint-超时，出现作业-failover，如下图："><a href="#4-Flink-作业在扫描-MySQL-全量数据时，checkpoint-超时，出现作业-failover，如下图：" class="headerlink" title="4. Flink 作业在扫描 MySQL 全量数据时，checkpoint 超时，出现作业 failover，如下图："></a><strong>4. Flink 作业在扫描 MySQL 全量数据时，checkpoint 超时，出现作业 failover，如下图：</strong></h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220216005324.png"></p><ul><li><strong>原因：</strong>Flink CDC 在 scan 全表数据（我们的实收表有千万级数据）需要小时级的时间（受下游聚合反压影响），而在 scan 全表过程中是没有  offset 可以记录的（意味着没法做 checkpoint），但是 Flink 框架任何时候都会按照固定间隔时间做  checkpoint，所以此处 mysql-cdc source 做了比较取巧的方式，即在 scan 全表的过程中，会让执行中的  checkpoint 一直等待甚至超时。超时的 checkpoint 会被仍未认为是 failed checkpoint，默认配置下，这会触发  Flink 的 failover 机制，而默认的 failover 机制是不重启。所以会造成上面的现象。</li></ul><ul><li><strong>解决办法：</strong>在 flink-conf.yaml 配置 failed checkpoint 容忍次数，以及失败重启策略，如下：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">execution.checkpointing.interval: 10min   # checkpoint间隔时间</span><br><span class="line">execution.checkpointing.tolerable-failed-checkpoints: 100  # checkpoint 失败容忍次数</span><br><span class="line">restart-strategy: fixed-delay  # 重试策略</span><br><span class="line">restart-strategy.fixed-delay.attempts: 2147483647   # 重试次数</span><br></pre></td></tr></table></figure><p>目前 Flink 社区也有一个 issue（FLINK-18578）来支持 source 主动拒绝 checkpoint 的机制，将来基于该机制，能比较优雅地解决这个问题。</p><h2 id="5-Flink-怎么样开启-YARN-的-per-job-模式？"><a href="#5-Flink-怎么样开启-YARN-的-per-job-模式？" class="headerlink" title="5. Flink 怎么样开启 YARN 的 per-job 模式？"></a><strong>5. Flink 怎么样开启 YARN 的 per-job 模式？</strong></h2><ul><li><strong>解决方法：</strong>在 flink-conf.yaml 中配置 execution.target: yarn-per-job。</li></ul><h2 id="6-进入-SQL-Client-创建-table-后，在另外一个节点进入-SQL-Client-查询不到-table。"><a href="#6-进入-SQL-Client-创建-table-后，在另外一个节点进入-SQL-Client-查询不到-table。" class="headerlink" title="6. 进入 SQL Client 创建 table 后，在另外一个节点进入 SQL Client 查询不到 table。"></a><strong>6. 进入 SQL Client 创建 table 后，在另外一个节点进入 SQL Client 查询不到 table。</strong></h2><ul><li><strong>原因：</strong>因为 SQL Client 默认的 Catalog 是在 in-memory 的，不是持久化 Catalog，所以这属于正常现象，每次启动 Catalog 里面都是空的。</li></ul><h2 id="7-作业在运行时-Elasticsearch-报如下错误："><a href="#7-作业在运行时-Elasticsearch-报如下错误：" class="headerlink" title="7. 作业在运行时 Elasticsearch 报如下错误："></a><strong>7. 作业在运行时 Elasticsearch 报如下错误：</strong></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: org.apache.Flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException: Elasticsearch exception [type=illegal_argument_exception, reason=mapper [amt] cannot be changed from type [long] to [float]]</span><br></pre></td></tr></table></figure><ul><li><strong>原因：</strong>数据库表的字段 amt 的类型是 decimal，DDL 创建输出到 es 的 amt 字段的类型也是 decimal，因为输出到 es  的第一条数据的amt如果是整数，比如是 10，输出到 es 的类型是 long 类型的，es client 会自动创建 es 的索引并且设置  amt 字段为 long 类型的格式，那么如果下一次输出到 es 的 amt 是非整数 10.1，那么输出到 es  的时候就会出现类型不匹配的错误。</li></ul><ul><li><strong>解决方法：</strong>手动生成 es 索引和 mapping 的信息，指定好 decimal 类型的数据格式是 saclefloat，但是在 DDL 处仍然可以保留该字段类型是 decimal。</li></ul><h2 id="8-作业在运行时-mysql-cdc-source-报如下错误："><a href="#8-作业在运行时-mysql-cdc-source-报如下错误：" class="headerlink" title="8. 作业在运行时 mysql cdc source 报如下错误："></a><strong>8. 作业在运行时 mysql cdc source 报如下错误：</strong></h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220216005414.png"></p><ul><li><strong>原因：</strong>因为数据库中别的表做了字段修改，CDC source 同步到了 ALTER DDL 语句，但是解析失败抛出的异常。</li></ul><ul><li><strong>解决方法：</strong>在 flink-cdc-connectors 最新版本中已经修复该问题（跳过了无法解析的 DDL）。升级 connector jar 包到最新版本 1.1.0：flink-sql-connector-mysql-cdc-1.1.0.jar，替换 flink/lib 下的旧包。</li></ul><h2 id="9-扫描全表阶段慢，在-Web-UI-出现如下现象："><a href="#9-扫描全表阶段慢，在-Web-UI-出现如下现象：" class="headerlink" title="9. 扫描全表阶段慢，在 Web UI 出现如下现象："></a><strong>9. 扫描全表阶段慢，在 Web UI 出现如下现象：</strong></h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220216005428.png"></p><ul><li><strong>原因：</strong>扫描全表阶段慢不一定是 cdc source 的问题，可能是下游节点处理太慢反压了。</li></ul><ul><li><strong>解决方法：</strong>通过 Web UI 的反压工具排查发现，瓶颈主要在聚合节点上。通过在 sql-client-defaults.yaml 文件配上 MiniBatch 相关参数和开启 distinct 优化（我们的聚合中有 count distinct），作业的 scan 效率得到了很大的提升，从原先的 10 小时，提升到了 1 小时。关于性能调优的参数可以参阅：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/tuning/streaming_aggregation_optimization.html%E3%80%82">https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/tuning/streaming_aggregation_optimization.html。</a></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">configuration:</span><br><span class="line">  table.exec.mini-batch.enabled: true</span><br><span class="line">  table.exec.mini-batch.allow-latency: 2s</span><br><span class="line">  table.exec.mini-batch.size: 5000</span><br><span class="line">  table.optimizer.distinct-agg.split.enabled: true</span><br></pre></td></tr></table></figure><h2 id="10-CDC-source-扫描-MySQL-表期间，发现无法往该表-insert-数据。"><a href="#10-CDC-source-扫描-MySQL-表期间，发现无法往该表-insert-数据。" class="headerlink" title="10. CDC source 扫描 MySQL 表期间，发现无法往该表 insert 数据。"></a><strong>10. CDC source 扫描 MySQL 表期间，发现无法往该表 insert 数据。</strong></h2><ul><li><strong>原因：</strong>由于使用的 MySQL 用户未授权 RELOAD 权限，导致无法获取全局读锁（FLUSH TABLES WITH READ LOCK）， CDC  source 就会退化成表级读锁，而使用表级读锁需要等到全表 scan 完，才能释放锁，所以会发现持锁时间过长的现象，影响其他业务写入数据。</li></ul><ul><li><strong>解决方法：</strong>给使用的 MySQL 用户授予 RELOAD  权限即可。所需的权限列表详见文档：<a href="https://github.com/ververica/flink-cdc-connectors/wiki/mysql-cdc-connector#setup-mysql-server%E3%80%82%E5%A6%82%E6%9E%9C%E5%87%BA%E4%BA%8E%E6%9F%90%E4%BA%9B%E5%8E%9F%E5%9B%A0%E6%97%A0%E6%B3%95%E6%8E%88%E4%BA%88">https://github.com/ververica/flink-cdc-connectors/wiki/mysql-cdc-connector#setup-mysql-server。如果出于某些原因无法授予</a> RELOAD 权限，也可以显式配上 ‘debezium.snapshot.locking.mode’ =  ‘none’来避免所有锁的获取，但要注意只有当快照期间表的 schema 不会变更才安全。</li></ul><h2 id="11-多个作业共用同一张-source-table-时，没有修改-server-id-导致读取出来的数据有丢失。"><a href="#11-多个作业共用同一张-source-table-时，没有修改-server-id-导致读取出来的数据有丢失。" class="headerlink" title="11. 多个作业共用同一张 source table 时，没有修改 server id 导致读取出来的数据有丢失。"></a><strong>11. 多个作业共用同一张 source table 时，没有修改 server id 导致读取出来的数据有丢失。</strong></h2><ul><li><strong>原因：</strong>MySQL binlog 数据同步的原理是，CDC source 会伪装成 MySQL 集群的一个 slave（使用指定的 server id 作为唯一  id），然后从 MySQL 拉取 binlog 数据。如果一个 MySQL 集群中有多个 slave 有同样的  id，就会导致拉取数据错乱的问题。</li></ul><ul><li><strong>解决方法：</strong>默认会随机生成一个 server id，容易有碰撞的风险。所以建议使用动态参数（table hint）在 query 中覆盖 server id。如下所示：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SELECT *</span><br><span class="line">FROM bill_info /*+ OPTIONS(&#x27;server-id&#x27;=&#x27;123456&#x27;) */ ;</span><br></pre></td></tr></table></figure><h2 id="12-在启动作业时，YARN-接收了任务，但作业一直未启动："><a href="#12-在启动作业时，YARN-接收了任务，但作业一直未启动：" class="headerlink" title="12. 在启动作业时，YARN 接收了任务，但作业一直未启动："></a><strong>12. 在启动作业时，YARN 接收了任务，但作业一直未启动：</strong></h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220216005503.png"></p><ul><li><strong>原因：</strong>Queue Resource Limit for AM 超过了限制资源限制。默认的最大内存是 30G (集群内存) * 0.1 = 3G，而每个 JM 申请 2G 内存，当提交第二个任务时，资源就不够了。</li></ul><ul><li><strong>解决方法：</strong>调大 AM 的 resource limit，在 capacity-scheduler.xml 配置  yarn.scheduler.capacity.maximum-am-resource-percent，代表AM的占总资源的百分比，默认为0.1，改成0.3（根据服务器的性能灵活配置）。</li></ul><h2 id="13-AM-进程起不来，一直被-kill-掉。"><a href="#13-AM-进程起不来，一直被-kill-掉。" class="headerlink" title="13. AM 进程起不来，一直被 kill 掉。"></a><strong>13. AM 进程起不来，一直被 kill 掉。</strong></h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220216005522.png"></p><ul><li><strong>原因：</strong>386.9 MB of 1 GB physical memory used; 2.1 GB of 2.1 GB virtual memory  use。默认物理内存是 1GB，动态申请到了 1GB，其中使用了386.9 MB。物理内存 x 2.1=虚拟内存，1GBx2.1≈2.1GB  ，2.1GB 虚拟内存已经耗尽，当虚拟内存不够时候，AM 的 container 就会自杀。</li></ul><ul><li><strong>解决方法：</strong>两个解决方案，或调整 yarn.nodemanager.vmem-pmem-ratio 值大点，或 yarn.nodemanager.vmem-check-enabled=false，关闭虚拟内存检查。参考：<a href="https://blog.csdn.net/lzxlfly/article/details/89175452%E3%80%82">https://blog.csdn.net/lzxlfly/article/details/89175452。</a></li></ul><h1 id="06-总结"><a href="#06-总结" class="headerlink" title="06 总结"></a><strong>06 总结</strong></h1><p>为了提升了实时报表服务的可用性和实时性，一开始我们采用了 Canal+Kafka+Flink 的方案，可是发现需要写比较多的 Java 代码，而且还需要处理好 DataStream 和 Table  的转换以及 binlong 位置的获取，开发难度相对较大。另外，需要维护 Kafka 和 Canal  这两个组件的稳定运行，对于我们小团队来说成本也不小。由于我们公司已经有基于 Flink 的任务在线上运行，因此采用 Flink SQL CDC  就成了顺理成章的事情。基于 Flink SQL CDC 的方案只需要编写 SQL ，不用写一行 Java  代码就能完成实时链路的打通和实时报表的计算，对于我们来说非常的简单易用，而且在线上运行的稳定性和性能表现也让我们满意。</p><p>知识源于积累,登峰造极源于自律!</p><p>好文章就得收藏慢慢品, 文章转载于: <a href="https://mp.weixin.qq.com/s/Mfn-fFegb5wzI8BIHhNGvQ">https://mp.weixin.qq.com/s/Mfn-fFegb5wzI8BIHhNGvQ</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;摘要：20年7月，Flink 1.11 新版发布，在生态及易用性上有大幅提升，其中 Table &amp;amp; SQL 开始支持 Change Data Capture（CDC）。CDC 被广泛使用在复制数据、更新缓存、微服务间同步数据、审计日志等场景，本文由社区由曾庆东同学分享，主要介绍 Flink SQL CDC 在生产环境的落地实践以及总结的实战经验&lt;/p&gt;
&lt;p&gt;Tips：点击下方链接可查看社区直播的 Flink SQL CDC 相关视频～&lt;br&gt;&lt;a href=&quot;https://flink-learning.org.cn/developers/flink-training-course3/&quot;&gt;https://flink-learning.org.cn/developers/flink-training-course3/&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>央视直播</title>
    <link href="http://xubatian.cn/%E5%A4%AE%E8%A7%86%E7%9B%B4%E6%92%AD/"/>
    <id>http://xubatian.cn/%E5%A4%AE%E8%A7%86%E7%9B%B4%E6%92%AD/</id>
    <published>2022-02-15T16:06:42.000Z</published>
    <updated>2022-02-15T16:32:40.565Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关注央视网-关注国家最新动态"><a href="#关注央视网-关注国家最新动态" class="headerlink" title="关注央视网,关注国家最新动态"></a><a href="https://2022.cctv.com/live/cctv1/index.shtml?spm=C73465.PnE5ZJN2e0Bi.EHVA0D7NetYz.39">关注央视网,关注国家最新动态</a></h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220216000755.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;关注央视网-关注国家最新动态&quot;&gt;&lt;a href=&quot;#关注央视网-关注国家最新动态&quot; class=&quot;headerlink&quot; title=&quot;关注央视网,关注国家最新动态&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://2022.cctv.com/live/cctv1/in</summary>
      
    
    
    
    <category term="轻松一刻" scheme="http://xubatian.cn/categories/%E8%BD%BB%E6%9D%BE%E4%B8%80%E5%88%BB/"/>
    
    
    <category term="轻松一刻" scheme="http://xubatian.cn/tags/%E8%BD%BB%E6%9D%BE%E4%B8%80%E5%88%BB/"/>
    
  </entry>
  
  <entry>
    <title>Spark原理与实现: Spark SQL编程</title>
    <link href="http://xubatian.cn/Spark%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Spark-SQL%E7%BC%96%E7%A8%8B/"/>
    <id>http://xubatian.cn/Spark%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Spark-SQL%E7%BC%96%E7%A8%8B/</id>
    <published>2022-02-15T15:27:25.000Z</published>
    <updated>2022-02-15T15:44:49.053Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spark-SQL编程"><a href="#Spark-SQL编程" class="headerlink" title="Spark SQL编程"></a>Spark SQL编程</h1><p>注意: 建表一定是数据集,对数据集进行建表即用df. 而执行SQL是spark.sql()</p><h1 id="SparkSession新的起始点"><a href="#SparkSession新的起始点" class="headerlink" title="SparkSession新的起始点"></a>SparkSession新的起始点</h1><p>​        在老的版本中，SparkSQL提供两种SQL查询起始点：一个叫SQLContext，用于Spark自己提供的SQL查询；一个叫HiveContext，用于连接Hive的查询。<br>​        现在是使用的是SparkSession了.实际上就是将两个结合了.所以你很方便的去查寻一个json文件,也可以查询一个hive数据.统一的数据入口<br>​        SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContex和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。<br>在Spark SQL中SparkSession是创建DataFrame和执行SQL的入口</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215232827.png"></p><p>DataFrame和dataset可以相互转换. DataFrame是Dataset里面的一种特殊形式</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215232850.png"></p><p>ResultSet():这个方法很恶心,就是编译期不做类型校验,但是你一运行就会报类型转换异常</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215232913.png"></p><h1 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h1><h2 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在Spark SQL中SparkSession是创建DataFrame和执行SQL的入口，所以DataFrame的创建是来自于sparkSession.从sparkSession中去找.而sparkSession是依赖于sparkContext来构建的.它里面放了一个sparkcontext对象,你可以拿出来. 你也可以直接通过SparkConf来创建一个SparkSession.但是这种方式它里面也是在构建sparkSession之前构建了SparkContext. 因为SparkContext是用于跟Spark集群连接的.</span><br><span class="line"></span><br><span class="line">创建DataFrame有三种方式：</span><br><span class="line">1.通过Spark的数据源进行创建；</span><br><span class="line">2.从一个存在的RDD进行转换；</span><br><span class="line">3.还可以从Hive Table进行查询返回。</span><br></pre></td></tr></table></figure><p>1）从Spark数据源进行创建<br>（1）查看Spark数据源进行创建的文件格式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.read.</span><br><span class="line">csv   format   jdbc   json   load   option   options   orc   parquet   schema   table   text   textFile</span><br></pre></td></tr></table></figure><p>（2）读取json文件创建DataFrame</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = spark.read.json(&quot;/opt/module/spark/examples/src/main/resources/people.json&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215233030.png"></p><p>（3）展示结果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.show   这里面用的比较多的行动算子,我们之前写sparkCore是用collect的这种方式展现的是有结构信息的</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215233059.png"></p><p>2）从RDD进行转换<br>      后面讨论<br>3）从Hive Table进行查询返回<br>     后面讨论</p><h2 id="SQL风格语法-主要"><a href="#SQL风格语法-主要" class="headerlink" title="SQL风格语法(主要)"></a>SQL风格语法(主要)</h2><p>1）创建一个DataFrame(注意在DataSet里面定义的一些函数,我DataFrame也是可以用的)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = spark.read.json(&quot;/opt/module/spark/examples/src/main/resources/people.json&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br></pre></td></tr></table></figure><p>2）对DataFrame创建一个临时表<br>View(视图),视图和table(表)有什么区别呢?视图使用来查的, 而表是用来增删改查的.因为当前的分布式数据集RDD具有不可变性.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.createOrReplaceTempView(&quot;people&quot;)  //参数是视图名</span><br></pre></td></tr></table></figure><p>3）通过SQL语句实现查询全表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val sqlDF = spark.sql(&quot;SELECT * FROM people&quot;)</span><br><span class="line">sqlDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br></pre></td></tr></table></figure><p>4）结果展示</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sqlDF.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><p>注意：普通临时表是Session范围内的，如果想应用范围内有效，可以使用全局临时表。使用全局临时表时需要全路径访问，如：global_temp.people</p><p>5）对于DataFrame创建一个全局表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(&quot;SELECT * FROM global_temp.people&quot;).show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line"></span><br><span class="line">scala&gt; spark.newSession().sql(&quot;SELECT * FROM global_temp.people&quot;).show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215233353.png"></p><h2 id="DSL风格语法-次要-DSL风格叫领域特定语言"><a href="#DSL风格语法-次要-DSL风格叫领域特定语言" class="headerlink" title="DSL风格语法(次要) (DSL风格叫领域特定语言)"></a>DSL风格语法(次要) (DSL风格叫领域特定语言)</h2><p>DSL风格叫领域特定语言,就是说,他只能在sparkSQL当中能用,换一个地方就不能用了.很恶心.<br>DSL语言风格就是使用select,filter,map等这些函数.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215233442.png"></p><p>1）创建一个DataFrame</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = spark.read.json(&quot;/opt/module/spark/examples/src/main/resources/people.json&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br></pre></td></tr></table></figure><p>2）查看DataFrame的Schema信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br></pre></td></tr></table></figure><p>3）只查看”name”列数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select(&quot;name&quot;).show()</span><br><span class="line">+-------+</span><br><span class="line">|   name|</span><br><span class="line">+-------+</span><br><span class="line">|Michael|</span><br><span class="line">|   Andy|</span><br><span class="line">| Justin|</span><br><span class="line">+-------+</span><br></pre></td></tr></table></figure><p>4）查看”name”列数据以及”age+1”数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select($&quot;name&quot;, $&quot;age&quot; + 1).show()</span><br><span class="line">+-------+---------+</span><br><span class="line">|   name|(age + 1)|</span><br><span class="line">+-------+---------+</span><br><span class="line">|Michael|     null|</span><br><span class="line">|   Andy|       31|</span><br><span class="line">| Justin|       20|</span><br><span class="line">+-------+---------+</span><br></pre></td></tr></table></figure><p>5）查看”age”大于”21”的数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.filter($&quot;age&quot; &gt; 21).show()</span><br><span class="line">+---+----+</span><br><span class="line">|age|name|</span><br><span class="line">+---+----+</span><br><span class="line">| 30|Andy|</span><br><span class="line">+---+----+</span><br></pre></td></tr></table></figure><p>6）按照”age”分组，查看数据条数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.groupBy(&quot;age&quot;).count().show()</span><br><span class="line">+----+-----+</span><br><span class="line">| age|count|</span><br><span class="line">+----+-----+</span><br><span class="line">|  19|     1|</span><br><span class="line">|null|     1|</span><br><span class="line">|  30|     1|</span><br><span class="line">+----+-----+</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>创建DataFrame有三种方式：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">4.通过Spark的数据源进行创建；</span><br><span class="line">5.从一个存在的RDD进行转换；</span><br><span class="line">6.还可以从Hive Table进行查询返回。</span><br></pre></td></tr></table></figure><h2 id="RDD转换为DataFrame"><a href="#RDD转换为DataFrame" class="headerlink" title="RDD转换为DataFrame"></a>RDD转换为DataFrame</h2><p>注意：如果需要RDD与DF(DataFrame)或者DS(DataSet)之间操作，那么都需要引入 import spark.implicits._  （spark不是包名，而是sparkSession对象的名称）</p><p>前置条件：导入隐式转换并创建一个RDD</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; import spark.implicits._</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">scala&gt; val peopleRDD = sc.textFile(&quot;examples/src/main/resources/people.txt&quot;)</span><br><span class="line">peopleRDD: org.apache.spark.rdd.RDD[String] = examples/src/main/resources/people.txt MapPartitionsRDD[3] at textFile at &lt;console&gt;:27</span><br></pre></td></tr></table></figure><p>1）通过手动确定转换</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; peopleRDD.map&#123;x=&gt;val para = x.split(&quot;,&quot;);(para(0),para(1).trim.toInt)&#125;.toDF(&quot;name&quot;,&quot;age&quot;)</span><br><span class="line">res1: org.apache.spark.sql.DataFrame = [name: string, age: int]</span><br></pre></td></tr></table></figure><p>2）通过反射确定（需要用到样例类）<br>（1）创建一个样例类,样例类其实就是和Java中的类一样</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; case class People(name:String, age:Int)</span><br></pre></td></tr></table></figure><p>（2）根据样例类将RDD转换为DataFrame</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; peopleRDD.map&#123; x =&gt; val para = x.split(&quot;,&quot;);People(para(0),para(1).trim.toInt)&#125;.toDF</span><br><span class="line">res2: org.apache.spark.sql.DataFrame = [name: string, age: int]</span><br></pre></td></tr></table></figure><p>3）通过编程的方式（了解）<br>（1）导入所需的类型</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; import org.apache.spark.sql.types._</span><br><span class="line">import org.apache.spark.sql.types._</span><br></pre></td></tr></table></figure><p>（2）创建Schema</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val structType: StructType = StructType(StructField(&quot;name&quot;, StringType) :: StructField(&quot;age&quot;, IntegerType) :: Nil)</span><br><span class="line">structType: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(age,IntegerType,true))</span><br></pre></td></tr></table></figure><p>（3）导入所需的类型</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; import org.apache.spark.sql.Row</span><br><span class="line">import org.apache.spark.sql.Row</span><br></pre></td></tr></table></figure><p>（4）根据给定的类型创建二元组RDD</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val data = peopleRDD.map&#123; x =&gt; val para = x.split(&quot;,&quot;);Row(para(0),para(1).trim.toInt)&#125;</span><br><span class="line">data: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[6] at map at &lt;console&gt;:33</span><br></pre></td></tr></table></figure><p>（5）根据数据及给定的schema创建DataFrame</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val dataFrame = spark.createDataFrame(data, structType)</span><br><span class="line">dataFrame: org.apache.spark.sql.DataFrame = [name: string, age: int]</span><br></pre></td></tr></table></figure><h2 id="DataFrame转换为RDD"><a href="#DataFrame转换为RDD" class="headerlink" title="DataFrame转换为RDD"></a>DataFrame转换为RDD</h2><p>直接调用rdd即可<br>1）创建一个DataFrame</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = spark.read.json(&quot;/opt/module/spark/examples/src/main/resources/people.json&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br></pre></td></tr></table></figure><p>2）将DataFrame转换为RDD</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val dfToRDD = df.rdd</span><br><span class="line">dfToRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[19] at rdd at &lt;console&gt;:29</span><br></pre></td></tr></table></figure><p>3）打印RDD</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; dfToRDD.collect</span><br><span class="line">res13: Array[org.apache.spark.sql.Row] = Array([Michael, 29], [Andy, 30], [Justin, 19])</span><br></pre></td></tr></table></figure><h1 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h1><p>DataSet是具有强类型的数据集合，需要提供对应的类型信息。</p><h2 id="创建-1"><a href="#创建-1" class="headerlink" title="创建"></a>创建</h2><p>1）创建一个样例类</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; case class Person(name: String, age: Long)</span><br><span class="line">defined class Person</span><br></pre></td></tr></table></figure><p>2）创建DataSet</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val caseClassDS = Seq(Person(&quot;Andy&quot;, 32)).toDS()</span><br><span class="line">caseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]</span><br></pre></td></tr></table></figure><h2 id="RDD转换为DataSet"><a href="#RDD转换为DataSet" class="headerlink" title="RDD转换为DataSet"></a>RDD转换为DataSet</h2><p>SparkSQL能够自动将包含有case类的RDD转换成DataFrame，case类定义了table的结构，case类属性通过反射变成了表的列名。Case类可以包含诸如Seqs或者Array等复杂的结构。<br>1）创建一个RDD</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val peopleRDD = sc.textFile(&quot;examples/src/main/resources/people.txt&quot;)</span><br><span class="line">peopleRDD: org.apache.spark.rdd.RDD[String] = examples/src/main/resources/people.txt MapPartitionsRDD[3] at textFile at &lt;console&gt;:27</span><br></pre></td></tr></table></figure><p>2）创建一个样例类</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; case class Person(name: String, age: Long)</span><br><span class="line">defined class Person</span><br></pre></td></tr></table></figure><p>3）将RDD转化为DataSet</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; peopleRDD.map(line =&gt; &#123;val para = line.split(&quot;,&quot;);Person(para(0),para(1).trim.toInt)&#125;).toDS</span><br><span class="line">res8: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]</span><br></pre></td></tr></table></figure><h2 id="DataSet转换为RDD"><a href="#DataSet转换为RDD" class="headerlink" title="DataSet转换为RDD"></a>DataSet转换为RDD</h2><p>调用rdd方法即可。<br>1）创建一个DataSet</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val DS = Seq(Person(&quot;Andy&quot;, 32)).toDS()</span><br><span class="line">DS: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]</span><br></pre></td></tr></table></figure><p>2）将DataSet转换为RDD</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; DS.rdd</span><br><span class="line">res11: org.apache.spark.rdd.RDD[Person] = MapPartitionsRDD[15] at rdd at &lt;console&gt;:28</span><br></pre></td></tr></table></figure><h2 id="DataFrame与DataSet的互操作"><a href="#DataFrame与DataSet的互操作" class="headerlink" title="DataFrame与DataSet的互操作"></a>DataFrame与DataSet的互操作</h2><h3 id="DataFrame转DataSet"><a href="#DataFrame转DataSet" class="headerlink" title="DataFrame转DataSet"></a>DataFrame转DataSet</h3><p>1）创建一个DateFrame</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = spark.read.json(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br></pre></td></tr></table></figure><p>2）创建一个样例类</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; case class Person(name: String, age: Long)</span><br><span class="line">defined class Person</span><br></pre></td></tr></table></figure><p>3）将DataFrame转化为DataSet</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.as[Person]</span><br><span class="line">res14: org.apache.spark.sql.Dataset[Person] = [age: bigint, name: string]</span><br></pre></td></tr></table></figure><h3 id="Dataset转DataFrame"><a href="#Dataset转DataFrame" class="headerlink" title="Dataset转DataFrame"></a>Dataset转DataFrame</h3><p>1）创建一个样例类</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; case class Person(name: String, age: Long)</span><br><span class="line">defined class Person</span><br></pre></td></tr></table></figure><p>2）创建DataSet</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val ds = Seq(Person(&quot;Andy&quot;, 32)).toDS()</span><br><span class="line">ds: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]</span><br></pre></td></tr></table></figure><p>3）将DataSet转化为DataFrame</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = ds.toDF</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [name: string, age: bigint]</span><br></pre></td></tr></table></figure><p>4）展示</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.show</span><br><span class="line">+----+---+</span><br><span class="line">|name|age|</span><br><span class="line">+----+---+</span><br><span class="line">|Andy| 32|</span><br><span class="line">+----+---+</span><br></pre></td></tr></table></figure><p>这种方法就是在给出每一列的类型后，使用as方法，转成Dataset，这在数据类型是DataFrame又需要针对各个字段处理时极为方便。在使用一些特殊的操作时，一定要加上 import spark.implicits._ 不然toDF、toDS无法使用。</p><h2 id="RDD、DataFrame和DataSet"><a href="#RDD、DataFrame和DataSet" class="headerlink" title="RDD、DataFrame和DataSet"></a>RDD、DataFrame和DataSet</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215234017.png"></p><p>在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。他们和RDD有什么区别呢？首先从版本的产生上来看:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt; Dataset(Spark1.6)</span><br><span class="line">如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。在后期的Spark版本中，DataSet有可能会逐步取代RDD和DataFrame成为唯一的API接口。</span><br></pre></td></tr></table></figure><h3 id="三者的共性"><a href="#三者的共性" class="headerlink" title="三者的共性"></a>三者的共性</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（1）RDD、DataFrame、DataSet全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利;</span><br><span class="line">（2）三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算;</span><br><span class="line">（3）三者有许多共同的函数，如filter，排序等;</span><br><span class="line">（4）在对DataFrame和Dataset进行操作许多操作都需要这个包:import spark.implicits._（在创建好SparkSession对象后尽量直接导入）,DataFrame和DataSet特别是和RDD进行操作的时候,则需要引入一个隐式转换.   因为这个spark session对象是由我们启动的spark-shell来给我们创建的,这个spark-shell窗口在启动的时候就已经将隐式转换导入进来了. 那就算把RDD在代码里面转成了样例类对象了,你调用.toDS调用不出来. 因为你没有加隐式转换.  而且这个隐式转换mport spark.implicits._  这个spark看起来像包,但是他不是, 这个spark是sparkSession的对象的对象 </span><br></pre></td></tr></table></figure><h3 id="三者的互相转化"><a href="#三者的互相转化" class="headerlink" title="三者的互相转化"></a>三者的互相转化</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215234137.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">RDD到DataFrame或者到dataSet,以及DataFrame到DataSet,这个过程我们认为是由简单到复杂,需要加东西的过程. 这一套过程都需要用到一个样例类. </span><br><span class="line">RDD到DataFrame用的是.toDF</span><br><span class="line">RDD到DataSet用的是.toDS</span><br><span class="line">DataFrame到DataSet用的是.as[样例类]</span><br><span class="line"></span><br><span class="line">DataFrame和dataSet到RDD或者Dataset到DataFrame 这是简的过程</span><br><span class="line">DataFrame和DataSet到RDD直接使用.rdd 即可</span><br><span class="line">DataSet到DataFrame使用.toDF</span><br></pre></td></tr></table></figure><h3 id="IDEA创建SparkSQL程序"><a href="#IDEA创建SparkSQL程序" class="headerlink" title="IDEA创建SparkSQL程序"></a>IDEA创建SparkSQL程序</h3><p>1）添加依赖</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;2.1.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>2）代码实现</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">object HelloWorld &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line"></span><br><span class="line">    //创建SparkConf()并设置App名称</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line"></span><br><span class="line">.master(&quot;local[*]&quot;)</span><br><span class="line">      .appName(&quot;HelloWorld&quot;)</span><br><span class="line">      //.config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">//导入隐式转换</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">//读取本地文件，创建DataFrame</span><br><span class="line">val df = spark.read.json(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line"></span><br><span class="line">//打印</span><br><span class="line">df.show()</span><br><span class="line"></span><br><span class="line">//DSL风格：查询年龄在21岁以上的</span><br><span class="line">df.filter($&quot;age&quot; &gt; 21).show()</span><br><span class="line"></span><br><span class="line">//创建临时表</span><br><span class="line">df.createOrReplaceTempView(&quot;persons&quot;)</span><br><span class="line"></span><br><span class="line">//SQL风格：查询年龄在21岁以上的</span><br><span class="line">spark.sql(&quot;SELECT * FROM persons where age &gt; 21&quot;).show()</span><br><span class="line"></span><br><span class="line">//关闭连接</span><br><span class="line">spark.stop()</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="用户自定义函数"><a href="#用户自定义函数" class="headerlink" title="用户自定义函数"></a>用户自定义函数</h2><p>在Shell窗口中可以通过spark.udf功能用户可以自定义函数。</p><h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><p>1）创建DataFrame</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val df = spark.read.json(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br></pre></td></tr></table></figure><p>2）打印数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><p>3）注册UDF，功能为在数据前添加字符串<br>因为自定义函数,最终是在SQL里面去用,所以你得有函数名addName,  后面要有函数如何添加? 相当于你在hive当中自定义的函数. </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.udf.register(&quot;addName&quot;, (x:String)=&gt; &quot;Name:&quot;+x)</span><br><span class="line">res5: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType)))</span><br></pre></td></tr></table></figure><p>4）创建临时表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.createOrReplaceTempView(&quot;people&quot;)</span><br></pre></td></tr></table></figure><p>5）应用UDF</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(&quot;Select addName(name), age from people&quot;).show()</span><br><span class="line">+-----------------+----+</span><br><span class="line">|UDF:addName(name)| age|</span><br><span class="line">+-----------------+----+</span><br><span class="line">|     Name:Michael|null|</span><br><span class="line">|        Name:Andy|  30|</span><br><span class="line">|      Name:Justin|  19|</span><br><span class="line">+-----------------+----+</span><br></pre></td></tr></table></figure><h3 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">自定义UDAF函数,多进一出,聚合函数.</span><br><span class="line">多进:关心进来的数据是什么样子的</span><br><span class="line">出:出去看你做什么事情</span><br><span class="line">对于Spark程序和MR程序,他是一行一行读取的.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">强类型的Dataset和弱类型的DataFrame都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。通过继承UserDefinedAggregateFunction来实现用户自定义聚合函数。</span><br></pre></td></tr></table></figure><p>1）需求：实现求平均工资的自定义聚合函数。<br>2）代码实现</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.expressions.MutableAggregationBuffer</span><br><span class="line">import org.apache.spark.sql.expressions.UserDefinedAggregateFunction</span><br><span class="line">import org.apache.spark.sql.types._</span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object MyAverage extends UserDefinedAggregateFunction &#123;</span><br><span class="line"></span><br><span class="line">// 聚合函数输入参数的数据类型</span><br><span class="line">def inputSchema: StructType = StructType(StructField(&quot;inputColumn&quot;, LongType) :: Nil)</span><br><span class="line"></span><br><span class="line">// 聚合缓冲区中值得数据类型</span><br><span class="line">def bufferSchema: StructType = &#123;</span><br><span class="line">StructType(StructField(&quot;sum&quot;, LongType) :: StructField(&quot;count&quot;, LongType) :: Nil)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 返回值的数据类型</span><br><span class="line">def dataType: DataType = DoubleType</span><br><span class="line"></span><br><span class="line">// 对于相同的输入是否一直返回相同的输出。</span><br><span class="line">def deterministic: Boolean = true</span><br><span class="line"></span><br><span class="line">// 初始化</span><br><span class="line">def initialize(buffer: MutableAggregationBuffer): Unit = &#123;</span><br><span class="line"></span><br><span class="line">// 存工资的总额</span><br><span class="line">buffer(0) = 0L</span><br><span class="line"></span><br><span class="line">// 存工资的个数</span><br><span class="line">buffer(1) = 0L</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 同一个分区数据合并</span><br><span class="line">def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123;</span><br><span class="line">if (!input.isNullAt(0)) &#123;</span><br><span class="line">buffer(0) = buffer.getLong(0) + input.getLong(0)</span><br><span class="line">buffer(1) = buffer.getLong(1) + 1</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 不同分区间数据合并</span><br><span class="line">def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123;</span><br><span class="line">buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)</span><br><span class="line">buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 计算最终结果</span><br><span class="line">def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>3）函数使用</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">// 注册函数</span><br><span class="line">spark.udf.register(&quot;myAverage&quot;, MyAverage)</span><br><span class="line"></span><br><span class="line">val df = spark.read.json(&quot;examples/src/main/resources/employees.json&quot;)</span><br><span class="line">df.createOrReplaceTempView(&quot;employees&quot;)</span><br><span class="line">df.show()</span><br><span class="line">// +-------+------+</span><br><span class="line">// |   name|salary|</span><br><span class="line">/ +-------+------+</span><br><span class="line">// |Michael|  3000|</span><br><span class="line">// |   Andy|  4500|</span><br><span class="line">// | Justin|  3500|</span><br><span class="line">// |  Berta|  4000|</span><br><span class="line">// +-------+------+</span><br><span class="line"></span><br><span class="line">val result = spark.sql(&quot;SELECT myAverage(salary) as average_salary FROM employees&quot;)</span><br><span class="line">result.show()</span><br><span class="line">// +--------------+</span><br><span class="line">// |average_salary|</span><br><span class="line">// +--------------+</span><br><span class="line">// |        3750.0 |</span><br><span class="line">// +--------------+</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Spark-SQL编程&quot;&gt;&lt;a href=&quot;#Spark-SQL编程&quot; class=&quot;headerlink&quot; title=&quot;Spark SQL编程&quot;&gt;&lt;/a&gt;Spark SQL编程&lt;/h1&gt;&lt;p&gt;注意: 建表一定是数据集,对数据集进行建表即用df. 而执行SQL是spark.sql()&lt;/p&gt;
&lt;h1 id=&quot;SparkSession新的起始点&quot;&gt;&lt;a href=&quot;#SparkSession新的起始点&quot; class=&quot;headerlink&quot; title=&quot;SparkSession新的起始点&quot;&gt;&lt;/a&gt;SparkSession新的起始点&lt;/h1&gt;&lt;p&gt;​        在老的版本中，SparkSQL提供两种SQL查询起始点：一个叫SQLContext，用于Spark自己提供的SQL查询；一个叫HiveContext，用于连接Hive的查询。&lt;br&gt;​        现在是使用的是SparkSession了.实际上就是将两个结合了.所以你很方便的去查寻一个json文件,也可以查询一个hive数据.统一的数据入口&lt;br&gt;​        SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContex和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。&lt;br&gt;在Spark SQL中SparkSession是创建DataFrame和执行SQL的入口&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="spark" scheme="http://xubatian.cn/tags/spark/"/>
    
    <category term="spark SQL" scheme="http://xubatian.cn/tags/spark-SQL/"/>
    
  </entry>
  
  <entry>
    <title>Spark原理与实现: SparkSQL的概述</title>
    <link href="http://xubatian.cn/Spark%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-SparkSQL%E7%9A%84%E6%A6%82%E8%BF%B0/"/>
    <id>http://xubatian.cn/Spark%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-SparkSQL%E7%9A%84%E6%A6%82%E8%BF%B0/</id>
    <published>2022-02-15T15:13:48.000Z</published>
    <updated>2022-02-15T15:22:27.156Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是Spark-SQL"><a href="#什么是Spark-SQL" class="headerlink" title="什么是Spark SQL"></a>什么是Spark SQL</h1><p>​        Spark SQL是Spark用来处理结构化数据的一个模块，它提供了2个编程抽象：DataFrame和DataSet，并且作为分布式SQL查询引擎的作用。</p><p>​        我们已经学习了Hive，它是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduc的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将Spark SQL转换成SparkCore来运行，然后提交到集群执行，执行效率非常快！</p><p>​        Spark SQL其实和hive替代MapReduce一样的. </p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215231509.png"></p><h1 id="Spark-SQL的特点"><a href="#Spark-SQL的特点" class="headerlink" title="Spark SQL的特点"></a>Spark SQL的特点</h1><p><strong>1）易整合</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215231741.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">集成</span><br><span class="line">SQL查询与Spark程序无缝结合。</span><br><span class="line">Spark SQL允许您使用SQL或familliar DataFrame API在Spark程序中查询结构化数据。适用于Java、Scala、Python和R</span><br></pre></td></tr></table></figure><p><strong>2）统一的数据访问方式</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">以前我们读hive,读JDBC,读Json都是要创建对象的,现在我们统一有一个对象 </span><br><span class="line">直接用Spark session这个对象来读这个数据就可以了</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215231819.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">统一的数据访问</span><br><span class="line">以同样的方式连接到任何数据源。</span><br><span class="line">DataFrames和SQL提供了一种通用的方法来访问各种数据源，包括Hive、Avro、Parquet、ORC、JSON和JDBC。您甚至可以跨这些源联接数据。</span><br><span class="line"></span><br><span class="line">查询和连接不同的数据源。</span><br></pre></td></tr></table></figure><p><strong>3）兼容Hive</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark内置hive的数据库是der, 所以我们换成外部的hive,用外部的hive也比较简单,我不需要告诉他计算引擎MR在哪,只需要告诉他元数据信息就可以了,你能让spark通过元数据找到实际数据所在地就行了,元数据在hive当中存在哪呢?在mysql当中,hive不是天生就存在mysql当中的,是有一个配置文件告诉他的,如果说你的spark sql想用之前hive里面的数据很简单,你把配置文件hive-site.xml配置给他移到spark.conf里面就够了,然后你一打开他就完成了之前和你hive数据的对接了</span><br><span class="line"></span><br><span class="line">Spark当中有一个内置的hive,你不用外部hive的元数据,spark内部hive自己管理是可以的但是默认的数据库是der. </span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215231853.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">具有和Hive的兼容性</span><br><span class="line"> 运行未修改的Hive查询现有数据</span><br><span class="line"> Spark SQL重用Hive前端和substore，使您与现有的Hive数据、查询和UDF完全兼容。 简单地安装在Hive旁边</span><br><span class="line"></span><br><span class="line">Spark SQL可以使用现有的Hive metastore，SerDes和UDFs</span><br></pre></td></tr></table></figure><p><strong>4）标准的数据连接</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215231918.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">标准的连接</span><br><span class="line">通过JDBC或ODBC连接。服务器模式为商业智能工具提供了行业标准的JDBC和ODBC连接</span><br><span class="line"></span><br><span class="line">使用现有的BI工具查询大数据</span><br></pre></td></tr></table></figure><h1 id="什么是DataFrame-数据框"><a href="#什么是DataFrame-数据框" class="headerlink" title="什么是DataFrame(数据框)"></a>什么是DataFrame(数据框)</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。</span><br><span class="line">DataFrame与RDD的主要区别在于:</span><br><span class="line">DataFrame带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215232009.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ataFrame也是懒执行的，但性能上比RDD要高，主要原因：高在哪呢?主要就是他有优化器</span><br><span class="line">优化的执行计划，即查询计划通过Spark catalyst optimiser进行优化。比如下面一个例子:</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215232035.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ResultSet():这个方法很恶心,就是编译期不做类型校验,但是你一运行就会报类型转换异常</span><br><span class="line"></span><br><span class="line">上图这个例子在做什么事情呢? </span><br><span class="line">RDD.join(...)filter(....).   对于RDD里面的操作,因为当前的数据集,我是单独的对Event进行过滤的,不是对join出来的,我累加一个和进行过滤的. 如果说你是两个表join. 一个表的某一列加另外一个表的某一列,在这个结果集进行过滤. 那你只能先join再过滤. 但是现在是对表当中的原始数据进行过滤. 其实他在优化的时候做到了一个东西叫谓词下推技术.</span><br><span class="line">谓词下推的基本思想即：</span><br><span class="line">将过滤表达式尽可能移动至靠近数据源的位置，以使真正执行时能直接跳过无关的数据。</span><br><span class="line">就是说在SQL的整个优化过程当中,他将能够提前过滤的数据先自己放在前面去执行. 他自己就能做这个事情.这就叫优化器. 那你SQL写出来之后呢,人家就把你数据先给你过滤出来.先去执行.</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215232054.png"></p><h1 id="什么是DataSet"><a href="#什么是DataSet" class="headerlink" title="什么是DataSet"></a>什么是DataSet</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DataSet是分布式数据集合。DataSet是Spark 1.6中添加的一个新抽象，是DataFrame的一个扩展。它提供了RDD的优势（强类型，使用强大的lambda函数的能力）以及Spark SQL优化执行引擎的优点。而对于SparkSQl的Setframe来说,我们可以说DataFrame是弱类型的.因为他在编译期间不做类型检查.这就给用户带来很不舒服. 而dataSet是可以放一个泛型为具体的样例类.痛过样例类来获取他的一个属性.那这个类型在编译期间是一定能够检查的.</span><br><span class="line">DataFrame虽然有结构信息,但是他在编译器的时候不用,而dataSet他也是有结构信息,但是他在编译期间的时候就给他用上了.这就是DataFrame和dataSet的主要区别. DataFrame只是dataSet的一个特殊形式.</span><br><span class="line">DataSet也可以使用功能性的转换（操作map，flatMap，filter等等）。</span><br><span class="line">1）是DataFrame API的一个扩展，是SparkSQL最新的数据抽象；</span><br><span class="line">2）用户友好的API风格，既具有类型安全检查也具有DataFrame的查询优化特性；</span><br><span class="line">3）用样例类来对DataSet中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称；</span><br><span class="line">4）DataSet是强类型的。比如可以有DataSet[Car]，DataSet[Person]里面必须放具体的类。</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215232124.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215232144.png"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;什么是Spark-SQL&quot;&gt;&lt;a href=&quot;#什么是Spark-SQL&quot; class=&quot;headerlink&quot; title=&quot;什么是Spark SQL&quot;&gt;&lt;/a&gt;什么是Spark SQL&lt;/h1&gt;&lt;p&gt;​        Spark SQL是Spark用来处理结构化数据的一个模块，它提供了2个编程抽象：DataFrame和DataSet，并且作为分布式SQL查询引擎的作用。&lt;/p&gt;
&lt;p&gt;​        我们已经学习了Hive，它是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduc的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将Spark SQL转换成SparkCore来运行，然后提交到集群执行，执行效率非常快！&lt;/p&gt;
&lt;p&gt;​        Spark SQL其实和hive替代MapReduce一样的. &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="spark" scheme="http://xubatian.cn/tags/spark/"/>
    
    <category term="spark SQL" scheme="http://xubatian.cn/tags/spark-SQL/"/>
    
  </entry>
  
  <entry>
    <title>Kafka API</title>
    <link href="http://xubatian.cn/Kafka-API/"/>
    <id>http://xubatian.cn/Kafka-API/</id>
    <published>2022-02-15T14:24:06.000Z</published>
    <updated>2022-02-15T14:41:21.164Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Producer-API"><a href="#Producer-API" class="headerlink" title="Producer API"></a>Producer API</h1><h2 id="消息发送流程"><a href="#消息发送流程" class="headerlink" title="消息发送流程"></a>消息发送流程</h2><p>​        Kafka的Producer发送消息采用的是<strong>异步发送</strong>的方式。在消息发送的过程中，涉及到了<strong>两个线程——main线程和Sender线程</strong>，以及<strong>一个线程共享变量——RecordAccumulator</strong>。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215222621.png"></p><p><strong>相关参数：</strong><br>batch.size：只有数据积累到batch.size之后，sender才会发送数据。<br>linger.ms：如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。</p><h2 id="异步发送API"><a href="#异步发送API" class="headerlink" title="异步发送API"></a>异步发送API</h2><p>1）导入依赖</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;0.11.0.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>2）编写代码<br>需要用到的类：<br>KafkaProducer：需要创建一个生产者对象，用来发送数据<br>ProducerConfig：获取所需的一系列配置参数<br>ProducerRecord：每条数据都要封装成一个ProducerRecord对象</p><h3 id="1-不带回调函数的API"><a href="#1-不带回调函数的API" class="headerlink" title="1.不带回调函数的API"></a><strong>1.不带回调函数的API</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.kafka.clients.producer.*;</span><br><span class="line"></span><br><span class="line">import java.util.Properties;</span><br><span class="line">import java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line">public class CustomProducer &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws ExecutionException, InterruptedException &#123;</span><br><span class="line">        Properties props = new Properties();</span><br><span class="line"></span><br><span class="line">        //kafka集群，broker-list</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);</span><br><span class="line"></span><br><span class="line">        props.put(&quot;acks&quot;, &quot;all&quot;);</span><br><span class="line"></span><br><span class="line">        //重试次数</span><br><span class="line">        props.put(&quot;retries&quot;, 1); </span><br><span class="line"></span><br><span class="line">        //批次大小</span><br><span class="line">        props.put(&quot;batch.size&quot;, 16384); </span><br><span class="line"></span><br><span class="line">        //等待时间</span><br><span class="line">        props.put(&quot;linger.ms&quot;, 1); </span><br><span class="line"></span><br><span class="line">        //RecordAccumulator缓冲区大小</span><br><span class="line">        props.put(&quot;buffer.memory&quot;, 33554432);</span><br><span class="line"></span><br><span class="line">        props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        for (int i = 0; i &lt; 100; i++) &#123;</span><br><span class="line">            producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;, Integer.toString(i), Integer.toString(i)));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-带回调函数的API"><a href="#2-带回调函数的API" class="headerlink" title="2.带回调函数的API"></a><strong>2.带回调函数的API</strong></h3><p>​    回调函数会在producer收到ack时调用，为异步调用，该方法有两个参数，分别是RecordMetadata和Exception，如果Exception为null，说明消息发送成功，如果Exception不为null，说明消息发送失败。</p><p>注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.kafka.clients.producer.*;</span><br><span class="line"></span><br><span class="line">import java.util.Properties;</span><br><span class="line">import java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line">public class CustomProducer &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) throws ExecutionException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        Properties props = new Properties();</span><br><span class="line"></span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);//kafka集群，broker-list</span><br><span class="line"></span><br><span class="line">        props.put(&quot;acks&quot;, &quot;all&quot;);</span><br><span class="line"></span><br><span class="line">        props.put(&quot;retries&quot;, 1);//重试次数</span><br><span class="line"></span><br><span class="line">        props.put(&quot;batch.size&quot;, 16384);//批次大小</span><br><span class="line"></span><br><span class="line">        props.put(&quot;linger.ms&quot;, 1);//等待时间</span><br><span class="line"></span><br><span class="line">        props.put(&quot;buffer.memory&quot;, 33554432);//RecordAccumulator缓冲区大小</span><br><span class="line"></span><br><span class="line">        props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        for (int i = 0; i &lt; 100; i++) &#123;</span><br><span class="line">            producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;, Integer.toString(i), Integer.toString(i)), new Callback() &#123;</span><br><span class="line"></span><br><span class="line">                //回调函数，该方法会在Producer收到ack时调用，为异步调用</span><br><span class="line">                @Override</span><br><span class="line">                public void onCompletion(RecordMetadata metadata, Exception exception) &#123;</span><br><span class="line">                    if (exception == null) &#123;</span><br><span class="line">                        System.out.println(&quot;success-&gt;&quot; + metadata.offset());</span><br><span class="line">                    &#125; else &#123;</span><br><span class="line">                        exception.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="同步发送API"><a href="#同步发送API" class="headerlink" title="同步发送API"></a>同步发送API</h2><p>​    同步发送的意思就是，一条消息发送之后，会阻塞当前线程，直至返回ack。<br>由于send方法返回的是一个Future对象，根据Futrue对象的特点，我们也可以实现同步发送的效果，只需在调用Future对象的get方发即可。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line">import org.apache.kafka.clients.producer.Producer;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line">import java.util.Properties;</span><br><span class="line">import java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line">public class CustomProducer &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) throws ExecutionException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        Properties props = new Properties();</span><br><span class="line"></span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);//kafka集群，broker-list</span><br><span class="line"></span><br><span class="line">        props.put(&quot;acks&quot;, &quot;all&quot;);</span><br><span class="line"></span><br><span class="line">        props.put(&quot;retries&quot;, 1);//重试次数</span><br><span class="line"></span><br><span class="line">        props.put(&quot;batch.size&quot;, 16384);//批次大小</span><br><span class="line"></span><br><span class="line">        props.put(&quot;linger.ms&quot;, 1);//等待时间</span><br><span class="line"></span><br><span class="line">        props.put(&quot;buffer.memory&quot;, 33554432);//RecordAccumulator缓冲区大小</span><br><span class="line"></span><br><span class="line">        props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);</span><br><span class="line">        for (int i = 0; i &lt; 100; i++) &#123;</span><br><span class="line">            producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;, Integer.toString(i), Integer.toString(i))).get();</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Consumer-API"><a href="#Consumer-API" class="headerlink" title="Consumer API"></a>Consumer API</h1><p>​        Consumer消费数据时的可靠性是很容易保证的，因为数据在Kafka中是持久化的，故不用担心数据丢失问题。<br>​        由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。<br>​        <strong>所以offset的维护是Consumer消费数据是必须考虑的问题。</strong></p><h3 id="自动提交offset"><a href="#自动提交offset" class="headerlink" title="自动提交offset"></a>自动提交offset</h3><p>1）导入依赖</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;0.11.0.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>2）编写代码<br>需要用到的类：<br>KafkaConsumer：需要创建一个消费者对象，用来消费数据<br>ConsumerConfig：获取所需的一系列配置参数<br>ConsuemrRecord：每条数据都要封装成一个ConsumerRecord对象<br>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。 </p><p><strong>自动提交offset的相关参数</strong></p><p>enable.auto.commit：是否开启自动提交offset功能<br>auto.commit.interval.ms：自动提交offset的时间间隔</p><p><strong>以下为自动提交offset的代码</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line">import org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line">import org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Properties;</span><br><span class="line"></span><br><span class="line">public class CustomConsumer &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">        Properties props = new Properties();</span><br><span class="line"></span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);</span><br><span class="line"></span><br><span class="line">        props.put(&quot;group.id&quot;, &quot;test&quot;);</span><br><span class="line"></span><br><span class="line">        props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);</span><br><span class="line"></span><br><span class="line">        props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);</span><br><span class="line"></span><br><span class="line">        props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">        props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        consumer.subscribe(Arrays.asList(&quot;first&quot;));</span><br><span class="line"></span><br><span class="line">        while (true) &#123;</span><br><span class="line"></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);</span><br><span class="line"></span><br><span class="line">            for (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line"></span><br><span class="line">                System.out.printf(&quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="手动提交offset"><a href="#手动提交offset" class="headerlink" title="手动提交offset"></a>手动提交offset</h3><p>​        虽然自动提交offset十分简介便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此Kafka还提供了手动提交offset的API。<br>手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。<br>两者的相同点是，都会将本次poll的一批数据最高的偏移量提交；不同点是，commitSync阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而commitAsync则没有失败重试机制，故有可能提交失败。<br><strong>1）同步提交offset</strong><br>由于同步提交offset有失败重试机制，故更加可靠，以下为同步提交offset的示例。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line">import org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line">import org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Properties;</span><br><span class="line"></span><br><span class="line">public class CustomComsumer &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">        Properties props = new Properties();</span><br><span class="line"></span><br><span class="line">//Kafka集群</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;); </span><br><span class="line"></span><br><span class="line">//消费者组，只要group.id相同，就属于同一个消费者组</span><br><span class="line">        props.put(&quot;group.id&quot;, &quot;test&quot;); </span><br><span class="line"></span><br><span class="line">        props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;);//关闭自动提交offset</span><br><span class="line"></span><br><span class="line">        props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">        props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        consumer.subscribe(Arrays.asList(&quot;first&quot;));//消费者订阅主题</span><br><span class="line"></span><br><span class="line">        while (true) &#123;</span><br><span class="line"></span><br><span class="line">//消费者拉取数据</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); </span><br><span class="line"></span><br><span class="line">            for (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line"></span><br><span class="line">                System.out.printf(&quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value());</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">//同步提交，当前线程会阻塞直到offset提交成功</span><br><span class="line">            consumer.commitSync();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>2).异步提交offset</strong></p><p>​        虽然同步提交offset更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会收到很大的影响。因此更多的情况下，会选用异步提交offset的方式。<br>以下为异步提交offset的示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.kafka.clients.consumer.*;</span><br><span class="line">import org.apache.kafka.common.TopicPartition;</span><br><span class="line"></span><br><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import java.util.Properties;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">public class CustomConsumer &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">        Properties props = new Properties();</span><br><span class="line"></span><br><span class="line">        //Kafka集群</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;); </span><br><span class="line"></span><br><span class="line">        //消费者组，只要group.id相同，就属于同一个消费者组</span><br><span class="line">        props.put(&quot;group.id&quot;, &quot;test&quot;); </span><br><span class="line"></span><br><span class="line">        //关闭自动提交offset</span><br><span class="line">        props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;);</span><br><span class="line"></span><br><span class="line">        props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">        props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(&quot;first&quot;));//消费者订阅主题</span><br><span class="line"></span><br><span class="line">        while (true) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);//消费者拉取数据</span><br><span class="line">            for (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.printf(&quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">//异步提交</span><br><span class="line">            consumer.commitAsync(new OffsetCommitCallback() &#123;</span><br><span class="line">                @Override</span><br><span class="line">                public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception) &#123;</span><br><span class="line">                    if (exception != null) &#123;</span><br><span class="line">                        System.err.println(&quot;Commit failed for&quot; + offsets);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;); </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>3）数据漏消费和重复消费分析</strong><br>     无论是同步提交还是异步提交offset，都有可能会造成数据的漏消费或者重复消费。先提交offset后消费，有可能造成数据的漏消费；而先消费后提交offset，有可能会造成数据的重复消费。</p><h3 id="自定义存储offset"><a href="#自定义存储offset" class="headerlink" title="自定义存储offset"></a>自定义存储offset</h3><p>​        Kafka 0.9版本之前，offset存储在zookeeper，0.9版本及之后，默认将offset存储在Kafka的一个内置的topic中。除此之外，Kafka还可以选择自定义存储offset。<br>​        offset的维护是相当繁琐的，因为需要考虑到消费者的Rebalace（再度平衡）。<br>​        <strong>当有新的消费者加入消费者组、已有的消费者退出消费者组或者所订阅的主题的分区发生变化，就会触发到分区的重新分配，重新分配的过程叫做Rebalance。</strong><br>​        消费者发生Rebalance之后，每个消费者消费的分区就会发生变化。因此消费者要首先获取到自己被重新分配到的分区，并且定位到每个分区最近提交的offset位置继续消费。<br>​        要实现自定义存储offset，需要借助ConsumerRebalanceListener，以下为示例代码，其中提交和获取offset的方法，需要根据所选的offset存储系统自行实现。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.kafka.clients.consumer.*;</span><br><span class="line">import org.apache.kafka.common.TopicPartition;</span><br><span class="line"></span><br><span class="line">import java.util.*;</span><br><span class="line"></span><br><span class="line">public class CustomConsumer &#123;</span><br><span class="line"></span><br><span class="line">    private static Map&lt;TopicPartition, Long&gt; currentOffset = new HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">//创建配置信息</span><br><span class="line">        Properties props = new Properties();</span><br><span class="line"></span><br><span class="line">//Kafka集群</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;); </span><br><span class="line"></span><br><span class="line">//消费者组，只要group.id相同，就属于同一个消费者组</span><br><span class="line">        props.put(&quot;group.id&quot;, &quot;test&quot;); </span><br><span class="line"></span><br><span class="line">//关闭自动提交offset</span><br><span class="line">        props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;);</span><br><span class="line"></span><br><span class="line">        //Key和Value的反序列化类</span><br><span class="line">        props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">        props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line"></span><br><span class="line">        //创建一个消费者</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        //消费者订阅主题</span><br><span class="line">        consumer.subscribe(Arrays.asList(&quot;first&quot;), new ConsumerRebalanceListener() &#123;</span><br><span class="line">            </span><br><span class="line">            //该方法会在Rebalance之前调用</span><br><span class="line">            @Override</span><br><span class="line">            public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123;</span><br><span class="line">                commitOffset(currentOffset);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            //该方法会在Rebalance之后调用</span><br><span class="line">            @Override</span><br><span class="line">            public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123;</span><br><span class="line">                currentOffset.clear();</span><br><span class="line">                for (TopicPartition partition : partitions) &#123;</span><br><span class="line">                    consumer.seek(partition, getOffset(partition));//定位到最近提交的offset位置继续消费</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        while (true) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);//消费者拉取数据</span><br><span class="line">            for (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.printf(&quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value());</span><br><span class="line">                currentOffset.put(new TopicPartition(record.topic(), record.partition()), record.offset());</span><br><span class="line">            &#125;</span><br><span class="line">            commitOffset(currentOffset);//异步提交</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //获取某分区的最新offset</span><br><span class="line">    private static long getOffset(TopicPartition partition) &#123;</span><br><span class="line">        return 0;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //提交该消费者所有分区的offset</span><br><span class="line">    private static void commitOffset(Map&lt;TopicPartition, Long&gt; currentOffset) &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">这两个方法会集中在spark的框架中，前面的代码很复杂</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="自定义Interceptor（拦截器）"><a href="#自定义Interceptor（拦截器）" class="headerlink" title="自定义Interceptor（拦截器）"></a>自定义Interceptor（拦截器）</h1><h2 id="拦截器原理"><a href="#拦截器原理" class="headerlink" title="拦截器原理"></a>拦截器原理</h2><p>​        Producer拦截器(interceptor)是在Kafka 0.10版本被引入的，主要用于实现clients端的定制化控制逻辑。<br>​        对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，其定义的方法包括：<br><strong>（1）configure(configs)</strong><br>获取配置信息和初始化数据时调用。<br><strong>（2）onSend(ProducerRecord)：</strong><br>​        该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算。<br><strong>（3）onAcknowledgement(RecordMetadata, Exception)：</strong><br>​        该方法会在消息从RecordAccumulator成功发送到Kafka Broker之后，或者在发送过程中失败时调用。并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率。<br><strong>（4）close：</strong><br>​        关闭interceptor，主要用于执行一些资源清理工作<br>​        如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外倘若指定了多个interceptor，则producer将按照指定顺序调用它们，并仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。</p><h2 id="拦截器案例"><a href="#拦截器案例" class="headerlink" title="拦截器案例"></a>拦截器案例</h2><p><strong>1）需求：</strong></p><p>​        实现一个简单的双interceptor组成的拦截链。第一个interceptor会在消息发送前将时间戳信息加到消息value的最前部；第二个interceptor会在消息发送后更新成功发送消息数或失败发送消息数。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215223823.png"></p><p><strong>2）案例实操</strong></p><p><strong>1）增加时间戳拦截器</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Map;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerInterceptor;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line">import org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"></span><br><span class="line">public class TimeInterceptor implements ProducerInterceptor&lt;String, String&gt; &#123;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void configure(Map&lt;String, ?&gt; configs) &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123;</span><br><span class="line"></span><br><span class="line">// 创建一个新的record，把时间戳写入消息体的最前部</span><br><span class="line">return new ProducerRecord(record.topic(), record.partition(), record.timestamp(), record.key(),</span><br><span class="line">System.currentTimeMillis() + &quot;,&quot; + record.value().toString());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void close() &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>（2）统计发送消息成功和发送失败消息数，并在producer关闭时打印这两个计数器</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Map;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerInterceptor;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line">import org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"></span><br><span class="line">public class CounterInterceptor implements ProducerInterceptor&lt;String, String&gt;&#123;</span><br><span class="line">    private int errorCounter = 0;</span><br><span class="line">    private int successCounter = 0;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void configure(Map&lt;String, ?&gt; configs) &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123;</span><br><span class="line"> return record;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123;</span><br><span class="line">// 统计成功和失败的次数</span><br><span class="line">        if (exception == null) &#123;</span><br><span class="line">            successCounter++;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            errorCounter++;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void close() &#123;</span><br><span class="line">        // 保存结果</span><br><span class="line">        System.out.println(&quot;Successful sent: &quot; + successCounter);</span><br><span class="line">        System.out.println(&quot;Failed sent: &quot; + errorCounter);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>（3）producer主程序</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line">import org.apache.kafka.clients.producer.Producer;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line">public class InterceptorProducer &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) throws Exception &#123;</span><br><span class="line">// 1 设置配置信息</span><br><span class="line">Properties props = new Properties();</span><br><span class="line">props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);</span><br><span class="line">props.put(&quot;acks&quot;, &quot;all&quot;);</span><br><span class="line">props.put(&quot;retries&quot;, 3);</span><br><span class="line">props.put(&quot;batch.size&quot;, 16384);</span><br><span class="line">props.put(&quot;linger.ms&quot;, 1);</span><br><span class="line">props.put(&quot;buffer.memory&quot;, 33554432);</span><br><span class="line">props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"></span><br><span class="line">// 2 构建拦截链</span><br><span class="line">List&lt;String&gt; interceptors = new ArrayList&lt;&gt;();</span><br><span class="line">interceptors.add(&quot;com.atguigu.kafka.interceptor.TimeInterceptor&quot;); interceptors.add(&quot;com.atguigu.kafka.interceptor.CounterInterceptor&quot;); </span><br><span class="line">props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);</span><br><span class="line"> </span><br><span class="line">String topic = &quot;first&quot;;</span><br><span class="line">Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">// 3 发送消息</span><br><span class="line">for (int i = 0; i &lt; 10; i++) &#123;</span><br><span class="line"></span><br><span class="line">    ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic, &quot;message&quot; + i);</span><br><span class="line">    producer.send(record);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">// 4 一定要关闭producer，这样才会调用interceptor的close方法</span><br><span class="line">producer.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Producer-API&quot;&gt;&lt;a href=&quot;#Producer-API&quot; class=&quot;headerlink&quot; title=&quot;Producer API&quot;&gt;&lt;/a&gt;Producer API&lt;/h1&gt;&lt;h2 id=&quot;消息发送流程&quot;&gt;&lt;a href=&quot;#消息发送流程&quot; class=&quot;headerlink&quot; title=&quot;消息发送流程&quot;&gt;&lt;/a&gt;消息发送流程&lt;/h2&gt;&lt;p&gt;​        Kafka的Producer发送消息采用的是&lt;strong&gt;异步发送&lt;/strong&gt;的方式。在消息发送的过程中，涉及到了&lt;strong&gt;两个线程——main线程和Sender线程&lt;/strong&gt;，以及&lt;strong&gt;一个线程共享变量——RecordAccumulator&lt;/strong&gt;。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Kafka" scheme="http://xubatian.cn/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka架构深入</title>
    <link href="http://xubatian.cn/Kafka%E6%9E%B6%E6%9E%84%E6%B7%B1%E5%85%A5/"/>
    <id>http://xubatian.cn/Kafka%E6%9E%B6%E6%9E%84%E6%B7%B1%E5%85%A5/</id>
    <published>2022-02-15T05:25:20.000Z</published>
    <updated>2022-02-15T13:08:20.098Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Kafka工作流程及文件存储机制"><a href="#Kafka工作流程及文件存储机制" class="headerlink" title="Kafka工作流程及文件存储机制"></a>Kafka工作流程及文件存储机制</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215193142.png"></p><span id="more"></span>    <p>​    生产者将消息存到kafka中就已经将消息标记了offset(偏移量)了,只是消费者消费的时候再去存一份,老版本存在zookeeper中,新版本是存在__consumer_offsets这个主题里面的<br>​        Kafka中消息是以topic(主题)进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。<br>​        topic是逻辑上的概念，而partition(分区)是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215193439.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215193530.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">00000000000000000000.index</span><br><span class="line">00000000000000000000.log</span><br><span class="line">00000000000000170410.index</span><br><span class="line">00000000000000170410.log</span><br><span class="line">00000000000000239430.index</span><br><span class="line">00000000000000239430.log</span><br></pre></td></tr></table></figure><p>index和log文件以当前segment的第一条消息的offset命名。下图为index文件和log文件的结构示意图。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215194058.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215194118.png"></p><h1 id="Kafka生产者"><a href="#Kafka生产者" class="headerlink" title="Kafka生产者"></a>Kafka生产者</h1><p><strong>如何决定我的消息进入到那个分区?</strong></p><h1 id="分区策略"><a href="#分区策略" class="headerlink" title="分区策略"></a>分区策略</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215194251.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215194301.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215194731.png"></p><h1 id="数据可靠性保证-重点"><a href="#数据可靠性保证-重点" class="headerlink" title="数据可靠性保证(重点)"></a>数据可靠性保证(重点)</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215194918.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215194847.png"></p><p><strong>1）副本数据同步策略</strong></p><table><thead><tr><th><strong>方案</strong></th><th><strong>优点</strong></th><th><strong>缺点</strong></th></tr></thead><tbody><tr><td><strong>半数以上完成同步，就发送ack</strong></td><td>延迟低</td><td>选举新的leader时，容忍n台节点的故障，需要2n+1个副本</td></tr><tr><td><strong>全部完成同步，才发送ack</strong></td><td>选举新的leader时，容忍n台节点的故障，需要n+1个副本</td><td><strong>延迟高</strong></td></tr></tbody></table><p><strong>Kafka选择了第二种方案，原因如下：</strong><br>1.同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。<br>2.虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小。</p><p><strong>2）ISR,就是为了选取leader用的</strong><br>    采用第二种方案之后，设想以下情景：leader收到数据，所有follower都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？<br>    Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给follower发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定。Leader发生故障之后，就会从ISR中选举新的leader。</p><p><strong>3）ack应答机制</strong><br>    对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR中的follower全部接收成功。<br>所以Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。<br>acks参数配置：<br><strong>acks：</strong><br>0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据；<br>1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据；</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215200941.png"></p><p>-1（all）：producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成<strong>数据重复</strong>。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215201151.png"></p><p><strong>4）故障处理细节</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215201320.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215201341.png"></p><h1 id="Exactly-Once语义"><a href="#Exactly-Once语义" class="headerlink" title="Exactly Once语义"></a>Exactly Once语义</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215201436.png"></p><h1 id="Kafka消费者"><a href="#Kafka消费者" class="headerlink" title="Kafka消费者"></a>Kafka消费者</h1><h2 id="消费方式"><a href="#消费方式" class="headerlink" title="消费方式"></a>消费方式</h2><p><strong>consumer采用pull（拉）模式从broker中读取数据。</strong><br>        push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。<br>        pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。如果某一次没有拉去到数据,我就让他多等一会儿针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。</p><h2 id="分区分配策略"><a href="#分区分配策略" class="headerlink" title="分区分配策略"></a>分区分配策略</h2><p>​        一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。<br>Kafka有两种分配策略，一是RoundRobin轮询，一是Range范围。<br><strong>1）RoundRobin轮询</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215201613.png"></p><p><strong>轮询的方式只适合所有消费者消费同样的主题,如果是不同的主题可能会出现这种情况,不同主题的消息,给了不-样的消费者了,出现消费混乱了</strong></p><p><strong>轮询是好,解决了不同负载的问题,但是会出问题</strong></p><p><strong>2）Range范围7/3,除不尽,则前面多一些</strong></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215201743.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215201838.png"></p><h2 id="offset的维护"><a href="#offset的维护" class="headerlink" title="offset的维护"></a>offset的维护</h2><p>​        由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215201911.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215202014.png"></p><h2 id="Kafka-高效读写数据"><a href="#Kafka-高效读写数据" class="headerlink" title="Kafka 高效读写数据"></a>Kafka 高效读写数据</h2><p>Kafka集群读写效率高是因为分布式和分区<br>但是单台kafka读写效率高就是下面的问题了</p><p>1）顺序写磁盘<br>        Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到600M/s，而随机写只有100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。<br>2）零复制技术</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215202127.png"></p><p>Page Cache : 页面缓存</p><p>Application Cache : 应用缓存</p><p>Socket Cache : 套接字缓存</p><h2 id="Zookeeper在Kafka中的作用"><a href="#Zookeeper在Kafka中的作用" class="headerlink" title="Zookeeper在Kafka中的作用"></a>Zookeeper在Kafka中的作用</h2><p>Kafka集群中有一个broker会被选举为Controller(控制器)，负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。<br>Controller的管理工作都是依赖于Zookeeper的。<br>    以下为partition的leader选举过程：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215202449.png"></p><h2 id="Kafka事务"><a href="#Kafka事务" class="headerlink" title="Kafka事务"></a>Kafka事务</h2><p>​        Kafka从0.11版本开始引入了事务支持。事务可以保证Kafka在Exactly Once语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。</p><h3 id="Producer事务"><a href="#Producer事务" class="headerlink" title="Producer事务"></a>Producer事务</h3><p>​        为了实现跨分区跨会话的事务，需要引入一个全局唯一的Transaction ID，并将Producer获得的PID和Transaction ID绑定。这样当Producer重启后就可以通过正在进行的Transaction ID获得原来的PID。<br>为了管理Transaction，Kafka引入了一个新的组件Transaction Coordinator。Producer就是通过和Transaction Coordinator交互获得Transaction ID对应的任务状态。Transaction Coordinator还负责将事务所有写入Kafka的一个内部Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。</p><h3 id="Consumer事务"><a href="#Consumer事务" class="headerlink" title="Consumer事务"></a>Consumer事务</h3><p>​        上述事务机制主要是从Producer方面考虑，对于Consumer而言，事务的保证就会相对较弱，尤其时无法保证Commit的信息被精确消费。这是由于Consumer可以通过offset访问任意信息，而且不同的Segment File生命周期不同，同一事务的消息可能会出现重启后被删除的情况。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Kafka工作流程及文件存储机制&quot;&gt;&lt;a href=&quot;#Kafka工作流程及文件存储机制&quot; class=&quot;headerlink&quot; title=&quot;Kafka工作流程及文件存储机制&quot;&gt;&lt;/a&gt;Kafka工作流程及文件存储机制&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215193142.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Kafka" scheme="http://xubatian.cn/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka概述</title>
    <link href="http://xubatian.cn/kafka%E6%A6%82%E8%BF%B0/"/>
    <id>http://xubatian.cn/kafka%E6%A6%82%E8%BF%B0/</id>
    <published>2022-02-15T04:27:45.000Z</published>
    <updated>2022-02-15T05:24:00.214Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Kafka概述"><a href="#Kafka概述" class="headerlink" title="Kafka概述"></a>Kafka概述</h1><h2 id="kafka定义"><a href="#kafka定义" class="headerlink" title="kafka定义"></a>kafka定义</h2><p>Kafka是一个分布式的<strong>基于发布/订阅模式</strong>的消息队列（Message Queue），主要应用于大数据实时处理领域。<br><strong>队列先进先出</strong></p><span id="more"></span><h2 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h2><h3 id="传统消息队列的应用场景"><a href="#传统消息队列的应用场景" class="headerlink" title="传统消息队列的应用场景"></a>传统消息队列的应用场景</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215122958.png"></p><p><strong>使用消息队列的好处</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1）解耦</span><br><span class="line">允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</span><br><span class="line">2）可恢复性</span><br><span class="line">系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。</span><br><span class="line">3）缓冲</span><br><span class="line">有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。</span><br><span class="line">4）灵活性 &amp; 峰值处理能力</span><br><span class="line">在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。</span><br><span class="line">5）异步通信</span><br><span class="line">很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</span><br></pre></td></tr></table></figure><h3 id="消息队列的两种模式-不是kafka的模式-kafka是发布订阅模式"><a href="#消息队列的两种模式-不是kafka的模式-kafka是发布订阅模式" class="headerlink" title="消息队列的两种模式   (不是kafka的模式, kafka是发布订阅模式)"></a>消息队列的两种模式   (不是kafka的模式, kafka是发布订阅模式)</h3><p><strong>1）点对点模式</strong>（一对一,只在两个系统之间,不会给到其他系统，消费者主动拉取数据，消息收到后消息清除）<br>        消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并且消费消息。<br>        消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215131828.png"></p><p><strong>（2）发布/订阅模式（一对多，消费者消费数据之后不会清除消息）</strong><br>Kafka是发布/订阅模式的基于消费者主动拉取得模式<br>消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215131903.png"></p><p>A系统和B系统之间加一个消息中间件 , 中间件只管暂存一下,不负责处理. 消息队列,先进先出的原则.  B系统10M/s的速度读取数据. 如果我A系统生产速度是10M/s, B系统消费跟不上, 那么我们就得升级 ,加机器.</p><h2 id="Kafka基础架构"><a href="#Kafka基础架构" class="headerlink" title="Kafka基础架构"></a>Kafka基础架构</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215132044.png"></p><p><strong>同一个消费者组里面的消费者不能同时消费同一个分区的数据</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1）Producer ：消息生产者，就是向kafka broker发消息的客户端;</span><br><span class="line">2）Consumer ：消息消费者，向kafka broker取消息的客户端;</span><br><span class="line">3）Consumer Group （CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</span><br><span class="line">4）Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。</span><br><span class="line">5）Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic；</span><br><span class="line">6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；</span><br><span class="line">7）Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。</span><br><span class="line">8）leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。</span><br><span class="line">9）follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的follower。</span><br></pre></td></tr></table></figure><h2 id="虚拟机上的命令"><a href="#虚拟机上的命令" class="headerlink" title="虚拟机上的命令"></a>虚拟机上的命令</h2><p>//查看kafka的消费记录offset需要使用命令：<br>kafka-consumer-offset-checker.sh –zookeeper hadoop101,hadoop102,hadoop103 –topic sensor –group flink-consumer_group</p><h2 id="offset是什么"><a href="#offset是什么" class="headerlink" title="offset是什么?"></a>offset是什么?</h2><p>对于每一个topic， Kafka集群都会维持一个分区日志<br>每个分区都是有序且顺序不可变的记录集，并且不断地追加到结构化的log文件。分区中的每一个记录都会分配一个id号来表示顺序，我们称之为offset，offset用来唯一的标识分区中每一条记录。</p><h2 id="offset有什么用"><a href="#offset有什么用" class="headerlink" title="offset有什么用?"></a>offset有什么用?</h2><p>消费者在消费数据时,发生宕机后,再次重新启动后,消费的数据需要从宕机位置开始读取</p><p>如果从头读取,有一部分消息一定出现了重复消费<br>如果从宕机时的消费位置读取,就不会出现重复消费<br>因此kafka设计了offset可以用于处理这种情况<br>如何维护offset的数值?<br>有两种方式,</p><p><strong>自动提交</strong>，设置enable.auto.commit=true，更新的频率根据参数【auto.commit.interval.ms】来定。这种方式也被称为【at most once】，fetch到消息后就可以更新offset，无论是否消费成功。默认就是true</p><p><strong>手动提交</strong>，设置enable.auto.commit=false，这种方式称为【at least once】。fetch到消息后，等消费完成再调用方法【consumer.commitSync()】，手动更新offset；如果消费失败，则offset也不会更新，此条消息会被重复消费一次</p><h2 id="offset实体在什么位置"><a href="#offset实体在什么位置" class="headerlink" title="offset实体在什么位置?"></a>offset实体在什么位置?</h2><p>0.9.0版本以前.这些数值维护在zookeeper中,但是zk不适合大量写入.后来做了改动<br>0.9.0 版本以后,数据维护在kafka的_consumer_offsets主题下.<br>内部结构包括groupid:topicName_partition offset</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215132209.png"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Kafka概述&quot;&gt;&lt;a href=&quot;#Kafka概述&quot; class=&quot;headerlink&quot; title=&quot;Kafka概述&quot;&gt;&lt;/a&gt;Kafka概述&lt;/h1&gt;&lt;h2 id=&quot;kafka定义&quot;&gt;&lt;a href=&quot;#kafka定义&quot; class=&quot;headerlink&quot; title=&quot;kafka定义&quot;&gt;&lt;/a&gt;kafka定义&lt;/h2&gt;&lt;p&gt;Kafka是一个分布式的&lt;strong&gt;基于发布/订阅模式&lt;/strong&gt;的消息队列（Message Queue），主要应用于大数据实时处理领域。&lt;br&gt;&lt;strong&gt;队列先进先出&lt;/strong&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Kafka" scheme="http://xubatian.cn/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink-CDC</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink-CDC/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink-CDC/</id>
    <published>2022-02-15T03:38:00.000Z</published>
    <updated>2022-02-15T03:44:33.995Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是CDC"><a href="#什么是CDC" class="headerlink" title="什么是CDC"></a>什么是CDC</h1><p>CDC是Change Data Capture(变更数据获取)的简称。核心思想是，监测并捕获数据库的变动（包括数据或数据表的插入、更新以及删除等），将这些变更按发生的顺序完整记录下来，写入到消息中间件中以供其他服务进行订阅及消费。</p><h1 id="CDC的种类"><a href="#CDC的种类" class="headerlink" title="CDC的种类"></a>CDC的种类</h1><p><strong>CDC主要分为基于查询的CDC和基于Binlog的CDC两种方式</strong>，我们主要了解一下这两种之间的区别：</p><span id="more"></span><table><thead><tr><th></th><th>基于查询的CDC</th><th>基于Binlog的CDC</th></tr></thead><tbody><tr><td>开源产品</td><td>Sqoop、Kafka JDBC Source</td><td>Canal（国内用的多）、Maxwell、Debezium(国外用的多)</td></tr><tr><td>执行模式</td><td>Batch(批处理)</td><td>Streaming(流式)</td></tr><tr><td>是否可以捕获所有数据变化</td><td>否</td><td>是</td></tr><tr><td>延迟性</td><td>高延迟</td><td>低延迟</td></tr><tr><td>是否增加数据库压力</td><td>是</td><td>否</td></tr></tbody></table><p>注意：</p><p>​         Flink里面的CDC其实内置了Debezium. 这个flinkcdc是阿里搞的,他并没有合并到flink的核心包里面.所以官网上是找不到的.他的资料在git上面. 我们要用只能去Github上去找资料.<br>​        <strong>Sqoop导数据是每天凌晨30分启动任务去导数据.一次性到数据.</strong><br>​        <strong>Maxwell是变化一条导一条.</strong></p><h1 id="Flink-CDC"><a href="#Flink-CDC" class="headerlink" title="Flink-CDC"></a>Flink-CDC</h1><p>​            Flink社区开发了 flink-cdc-connectors 组件，这是一个可以直接从 MySQL、PostgreSQL 等数据库直接读取全量数据和增量变更数据的 source 组件。目前也已开源，开源地址：<a href="https://github.com/ververica/flink-cdc-connectors">https://github.com/ververica/flink-cdc-connectors</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215114212.png"></p><h1 id="FlinkCDC案例实操"><a href="#FlinkCDC案例实操" class="headerlink" title="FlinkCDC案例实操"></a>FlinkCDC案例实操</h1><p>案例代码: <a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-cdc/src/main/java/com/shangbaishuyao/flinkcdc/">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-cdc/src/main/java/com/shangbaishuyao/flinkcdc/</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;什么是CDC&quot;&gt;&lt;a href=&quot;#什么是CDC&quot; class=&quot;headerlink&quot; title=&quot;什么是CDC&quot;&gt;&lt;/a&gt;什么是CDC&lt;/h1&gt;&lt;p&gt;CDC是Change Data Capture(变更数据获取)的简称。核心思想是，监测并捕获数据库的变动（包括数据或数据表的插入、更新以及删除等），将这些变更按发生的顺序完整记录下来，写入到消息中间件中以供其他服务进行订阅及消费。&lt;/p&gt;
&lt;h1 id=&quot;CDC的种类&quot;&gt;&lt;a href=&quot;#CDC的种类&quot; class=&quot;headerlink&quot; title=&quot;CDC的种类&quot;&gt;&lt;/a&gt;CDC的种类&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;CDC主要分为基于查询的CDC和基于Binlog的CDC两种方式&lt;/strong&gt;，我们主要了解一下这两种之间的区别：&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink流计算常用算子（Flink算子大全）</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E6%B5%81%E8%AE%A1%E7%AE%97%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90%EF%BC%88Flink%E7%AE%97%E5%AD%90%E5%A4%A7%E5%85%A8%EF%BC%89/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E6%B5%81%E8%AE%A1%E7%AE%97%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90%EF%BC%88Flink%E7%AE%97%E5%AD%90%E5%A4%A7%E5%85%A8%EF%BC%89/</id>
    <published>2022-02-15T01:20:27.000Z</published>
    <updated>2022-02-15T02:21:55.314Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215092137.png"></p><p>Flink和Spark类似，也是一种一站式处理的框架；既可以进行批处理（DataSet），也可以进行实时处理（DataStream）。</p><p>所以下面将Flink的算子分为两大类：一类是DataSet，一类是DataStream。</p><p><strong>Flink官网:</strong> <a href="https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/dev/datastream/operators/overview/">https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/dev/datastream/operators/overview/</a></p><p><strong>案例代码:</strong> <a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/tree/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/tree/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo</a></p><span id="more"></span><p>我们列举了一些Flink自带且常用的transformation算子，例如map、flatMap等。在Flink的编程体系中，我们获取到数据源之后，需要经过一系列的处理即transformation操作，再将最终结果输出到目的Sink（ES、mysql或者hdfs），使数据落地。因此，除了正确的继承重写RichSourceFunction&lt;&gt;和RichSinkFunction&lt;&gt;之外，最终要的就是实时处理这部分，下面的图介绍了Flink代码执行流程以及各模块的组成部分。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215092857.png"></p><p>在Flink中，Transformation算子就是将一个或多个DataStream转换为新的DataStream，可以将多个转换组合成复杂的数据流拓扑。如下图所示，DataStream会由不同的Transformation操作，转换、过滤、聚合成其他不同的流，从而完成我们的业务要求。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215092803.png"></p><h1 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h1><h3 id="一、Source算子"><a href="#一、Source算子" class="headerlink" title="一、Source算子"></a><strong>一、Source算子</strong></h3><h3 id="1-fromCollection"><a href="#1-fromCollection" class="headerlink" title="1. fromCollection"></a><strong>1. fromCollection</strong></h3><p>fromCollection：从本地集合读取数据</p><p>例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val env = ExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">val textDataSet: DataSet[String] = env.fromCollection(</span><br><span class="line">  List(&quot;1,张三&quot;, &quot;2,李四&quot;, &quot;3,王五&quot;, &quot;4,赵六&quot;)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="2-readTextFile"><a href="#2-readTextFile" class="headerlink" title="2. readTextFile"></a><strong>2. readTextFile</strong></h3><p>readTextFile：从文件中读取</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val textDataSet: DataSet[String]  = env.readTextFile(&quot;/data/a.txt&quot;)</span><br></pre></td></tr></table></figure><h3 id="3-readTextFile：遍历目录"><a href="#3-readTextFile：遍历目录" class="headerlink" title="3. readTextFile：遍历目录"></a><strong>3. readTextFile：遍历目录</strong></h3><p>readTextFile可以对一个文件目录内的所有文件，包括所有子目录中的所有文件的遍历访问方式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val parameters = new Configuration</span><br><span class="line">// recursive.file.enumeration 开启递归</span><br><span class="line">parameters.setBoolean(&quot;recursive.file.enumeration&quot;, true)</span><br><span class="line">val file = env.readTextFile(&quot;/data&quot;).withParameters(parameters)</span><br></pre></td></tr></table></figure><h3 id="4-readTextFile：读取压缩文件"><a href="#4-readTextFile：读取压缩文件" class="headerlink" title="4. readTextFile：读取压缩文件"></a><strong>4. readTextFile：读取压缩文件</strong></h3><p>对于以下压缩类型，不需要指定任何额外的inputformat方法，flink可以自动识别并且解压。但是，压缩文件可能不会并行读取，可能是顺序读取的，这样可能会影响作业的可伸缩性。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val file = env.readTextFile(&quot;/data/file.gz&quot;)</span><br></pre></td></tr></table></figure><h3 id="二、Transform转换算子"><a href="#二、Transform转换算子" class="headerlink" title="二、Transform转换算子"></a><strong>二、Transform转换算子</strong></h3><p>因为Transform算子基于Source算子操作，所以首先构建Flink执行环境及Source算子，后续Transform算子操作基于此：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val env = ExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">val textDataSet: DataSet[String] = env.fromCollection(</span><br><span class="line">  List(&quot;张三,1&quot;, &quot;李四,2&quot;, &quot;王五,3&quot;, &quot;张三,4&quot;)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="1-map"><a href="#1-map" class="headerlink" title="1. map"></a><strong>1. map</strong></h3><p>将DataSet中的每一个元素转换为另外一个元素</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// 使用map将List转换为一个Scala的样例类</span><br><span class="line"></span><br><span class="line">case class User(name: String, id: String)</span><br><span class="line"></span><br><span class="line">val userDataSet: DataSet[User] = textDataSet.map &#123;</span><br><span class="line">  text =&gt;</span><br><span class="line">    val fieldArr = text.split(&quot;,&quot;)</span><br><span class="line">    User(fieldArr(0), fieldArr(1))</span><br><span class="line">&#125;</span><br><span class="line">userDataSet.print()</span><br></pre></td></tr></table></figure><h3 id="2-flatMap"><a href="#2-flatMap" class="headerlink" title="2. flatMap"></a><strong>2. flatMap</strong></h3><p>将DataSet中的每一个元素转换为0…n个元素。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 使用flatMap操作，将集合中的数据：</span><br><span class="line">// 根据第一个元素，进行分组</span><br><span class="line">// 根据第二个元素，进行聚合求值 </span><br><span class="line"></span><br><span class="line">val result = textDataSet.flatMap(line =&gt; line)</span><br><span class="line">      .groupBy(0) // 根据第一个元素，进行分组</span><br><span class="line">      .sum(1) // 根据第二个元素，进行聚合求值</span><br><span class="line">      </span><br><span class="line">result.print()</span><br></pre></td></tr></table></figure><h3 id="3-mapPartition"><a href="#3-mapPartition" class="headerlink" title="3. mapPartition"></a><strong>3. mapPartition</strong></h3><p>将一个分区中的元素转换为另一个元素</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 使用mapPartition操作，将List转换为一个scala的样例类</span><br><span class="line"></span><br><span class="line">case class User(name: String, id: String)</span><br><span class="line"></span><br><span class="line">val result: DataSet[User] = textDataSet.mapPartition(line =&gt; &#123;</span><br><span class="line">      line.map(index =&gt; User(index._1, index._2))</span><br><span class="line">    &#125;)</span><br><span class="line">    </span><br><span class="line">result.print()</span><br></pre></td></tr></table></figure><h3 id="4-filter"><a href="#4-filter" class="headerlink" title="4. filter"></a><strong>4. filter</strong></h3><p>过滤出来一些符合条件的元素，返回<strong>boolean值为true</strong>的元素</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val source: DataSet[String] = env.fromElements(&quot;java&quot;, &quot;scala&quot;, &quot;java&quot;)</span><br><span class="line">val filter:DataSet[String] = source.filter(line =&gt; line.contains(&quot;java&quot;))//过滤出带java的数据</span><br><span class="line">filter.print()</span><br></pre></td></tr></table></figure><h3 id="5-reduce"><a href="#5-reduce" class="headerlink" title="5. reduce"></a><strong>5. reduce</strong></h3><p>可以对一个dataset或者一个group来进行聚合计算，最终<strong>聚合成一个元素</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// 使用 fromElements 构建数据源</span><br><span class="line">val source = env.fromElements((&quot;java&quot;, 1), (&quot;scala&quot;, 1), (&quot;java&quot;, 1))</span><br><span class="line">// 使用map转换成DataSet元组</span><br><span class="line">val mapData: DataSet[(String, Int)] = source.map(line =&gt; line)</span><br><span class="line">// 根据首个元素分组</span><br><span class="line">val groupData = mapData.groupBy(_._1)</span><br><span class="line">// 使用reduce聚合</span><br><span class="line">val reduceData = groupData.reduce((x, y) =&gt; (x._1, x._2 + y._2))</span><br><span class="line">// 打印测试</span><br><span class="line">reduceData.print()</span><br></pre></td></tr></table></figure><h3 id="6-reduceGroup"><a href="#6-reduceGroup" class="headerlink" title="6. reduceGroup"></a><strong>6. reduceGroup</strong></h3><p>将一个dataset或者一个group<strong>聚合成一个或多个元素</strong>。<br>reduceGroup是reduce的一种优化方案；<br>它会先分组reduce，然后在做整体的reduce；这样做的好处就是可以减少网络IO</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// 使用 fromElements 构建数据源</span><br><span class="line">val source: DataSet[(String, Int)] = env.fromElements((&quot;java&quot;, 1), (&quot;scala&quot;, 1), (&quot;java&quot;, 1))</span><br><span class="line">// 根据首个元素分组</span><br><span class="line">val groupData = source.groupBy(_._1)</span><br><span class="line">// 使用reduceGroup聚合</span><br><span class="line">val result: DataSet[(String, Int)] = groupData.reduceGroup &#123;</span><br><span class="line">      (in: Iterator[(String, Int)], out: Collector[(String, Int)]) =&gt;</span><br><span class="line">        val tuple = in.reduce((x, y) =&gt; (x._1, x._2 + y._2))</span><br><span class="line">        out.collect(tuple)</span><br><span class="line">    &#125;</span><br><span class="line">// 打印测试</span><br><span class="line">result.print()</span><br></pre></td></tr></table></figure><h3 id="7-minBy和maxBy"><a href="#7-minBy和maxBy" class="headerlink" title="7. minBy和maxBy"></a><strong>7. minBy和maxBy</strong></h3><p>选择具有最小值或最大值的<strong>元素</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// 使用minBy操作，求List中每个人的最小值</span><br><span class="line">// List(&quot;张三,1&quot;, &quot;李四,2&quot;, &quot;王五,3&quot;, &quot;张三,4&quot;)</span><br><span class="line"></span><br><span class="line">case class User(name: String, id: String)</span><br><span class="line">// 将List转换为一个scala的样例类</span><br><span class="line">val text: DataSet[User] = textDataSet.mapPartition(line =&gt; &#123;</span><br><span class="line">      line.map(index =&gt; User(index._1, index._2))</span><br><span class="line">    &#125;)</span><br><span class="line">    </span><br><span class="line">val result = text</span><br><span class="line">          .groupBy(0) // 按照姓名分组</span><br><span class="line">          .minBy(1)   // 每个人的最小值</span><br></pre></td></tr></table></figure><h3 id="8-Aggregate"><a href="#8-Aggregate" class="headerlink" title="8. Aggregate"></a><strong>8. Aggregate</strong></h3><p>在数据集上进行聚合求<strong>最值</strong>（最大值、最小值）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">val data = new mutable.MutableList[(Int, String, Double)]</span><br><span class="line">    data.+=((1, &quot;yuwen&quot;, 89.0))</span><br><span class="line">    data.+=((2, &quot;shuxue&quot;, 92.2))</span><br><span class="line">    data.+=((3, &quot;yuwen&quot;, 89.99))</span><br><span class="line">// 使用 fromElements 构建数据源</span><br><span class="line">val input: DataSet[(Int, String, Double)] = env.fromCollection(data)</span><br><span class="line">// 使用group执行分组操作</span><br><span class="line">val value = input.groupBy(1)</span><br><span class="line">            // 使用aggregate求最大值元素</span><br><span class="line">            .aggregate(Aggregations.MAX, 2) </span><br><span class="line">// 打印测试</span><br><span class="line">value.print()       </span><br></pre></td></tr></table></figure><p><strong>Aggregate只能作用于元组上</strong></p><blockquote><p> 注意：<br>要使用aggregate，只能使用字段索引名或索引名称来进行分组 <code>groupBy(0)</code> ，否则会报一下错误:<br>Exception in thread “main” java.lang.UnsupportedOperationException: Aggregate  does not support grouping with KeySelector functions, yet.</p></blockquote><h3 id="9-distinct"><a href="#9-distinct" class="headerlink" title="9. distinct"></a><strong>9. distinct</strong></h3><p>去除重复的数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 数据源使用上一题的</span><br><span class="line">// 使用distinct操作，根据科目去除集合中重复的元组数据</span><br><span class="line"></span><br><span class="line">val value: DataSet[(Int, String, Double)] = input.distinct(1)</span><br><span class="line">value.print()</span><br></pre></td></tr></table></figure><h3 id="10-first"><a href="#10-first" class="headerlink" title="10. first"></a><strong>10. first</strong></h3><p>取前N个数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input.first(2) // 取前两个数</span><br></pre></td></tr></table></figure><h3 id="11-join"><a href="#11-join" class="headerlink" title="11. join"></a><strong>11. join</strong></h3><p>将两个DataSet按照一定条件连接到一起，形成新的DataSet</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// s1 和 s2 数据集格式如下：</span><br><span class="line">// DataSet[(Int, String,String, Double)]</span><br><span class="line"></span><br><span class="line"> val joinData = s1.join(s2)  // s1数据集 join s2数据集</span><br><span class="line">             .where(0).equalTo(0) &#123;     // join的条件</span><br><span class="line">      (s1, s2) =&gt; (s1._1, s1._2, s2._2, s1._3)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="12-leftOuterJoin"><a href="#12-leftOuterJoin" class="headerlink" title="12. leftOuterJoin"></a><strong>12. leftOuterJoin</strong></h3><p>左外连接,左边的Dataset中的每一个元素，去连接右边的元素</p><p>此外还有：</p><p>rightOuterJoin：右外连接,左边的Dataset中的每一个元素，去连接左边的元素</p><p>fullOuterJoin：全外连接,左右两边的元素，全部连接</p><p>下面以 leftOuterJoin 进行示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> val data1 = ListBuffer[Tuple2[Int,String]]()</span><br><span class="line">    data1.append((1,&quot;zhangsan&quot;))</span><br><span class="line">    data1.append((2,&quot;lisi&quot;))</span><br><span class="line">    data1.append((3,&quot;wangwu&quot;))</span><br><span class="line">    data1.append((4,&quot;zhaoliu&quot;))</span><br><span class="line"></span><br><span class="line">val data2 = ListBuffer[Tuple2[Int,String]]()</span><br><span class="line">    data2.append((1,&quot;beijing&quot;))</span><br><span class="line">    data2.append((2,&quot;shanghai&quot;))</span><br><span class="line">    data2.append((4,&quot;guangzhou&quot;))</span><br><span class="line"></span><br><span class="line">val text1 = env.fromCollection(data1)</span><br><span class="line">val text2 = env.fromCollection(data2)</span><br><span class="line"></span><br><span class="line">text1.leftOuterJoin(text2).where(0).equalTo(0).apply((first,second)=&gt;&#123;</span><br><span class="line">      if(second==null)&#123;</span><br><span class="line">        (first._1,first._2,&quot;null&quot;)</span><br><span class="line">      &#125;else&#123;</span><br><span class="line">        (first._1,first._2,second._2)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;).print()</span><br></pre></td></tr></table></figure><h3 id="13-cross"><a href="#13-cross" class="headerlink" title="13. cross"></a><strong>13. cross</strong></h3><p>交叉操作，通过形成这个数据集和其他数据集的笛卡尔积，创建一个新的数据集</p><p>和join类似，但是这种交叉操作会产生笛卡尔积，在<strong>数据比较大的时候，是非常消耗内存的操作</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val cross = input1.cross(input2)&#123;</span><br><span class="line">      (input1 , input2) =&gt; (input1._1,input1._2,input1._3,input2._2)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">cross.print()</span><br></pre></td></tr></table></figure><h3 id="14-union"><a href="#14-union" class="headerlink" title="14. union"></a><strong>14. union</strong></h3><p>联合操作，创建包含来自该数据集和其他数据集的元素的新数据集,<strong>不会去重</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val unionData: DataSet[String] = elements1.union(elements2).union(elements3)</span><br><span class="line">// 去除重复数据</span><br><span class="line">val value = unionData.distinct(line =&gt; line)</span><br></pre></td></tr></table></figure><h3 id="15-rebalance"><a href="#15-rebalance" class="headerlink" title="15. rebalance"></a><strong>15. rebalance</strong></h3><p>Flink也有数据倾斜的时候，比如当前有数据量大概10亿条数据需要处理，在处理过程中可能会发生如图所示的状况：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215092303.png"></p><p>这个时候本来总体数据量只需要10分钟解决的问题，出现了数据倾斜，机器1上的任务需要4个小时才能完成，那么其他3台机器执行完毕也要等待机器1执行完毕后才算整体将任务完成； 所以在实际的工作中，出现这种情况比较好的解决方案就是接下来要介绍的—<strong>rebalance</strong>（内部使用round robin方法将数据均匀打散。这对于数据倾斜时是很好的选择。）</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215092318.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 使用rebalance操作，避免数据倾斜</span><br><span class="line">val rebalance = filterData.rebalance()</span><br></pre></td></tr></table></figure><h3 id="16-partitionByHash"><a href="#16-partitionByHash" class="headerlink" title="16. partitionByHash"></a><strong>16. partitionByHash</strong></h3><p>按照指定的key进行hash分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">val data = new mutable.MutableList[(Int, Long, String)]</span><br><span class="line">data.+=((1, 1L, &quot;Hi&quot;))</span><br><span class="line">data.+=((2, 2L, &quot;Hello&quot;))</span><br><span class="line">data.+=((3, 2L, &quot;Hello world&quot;))</span><br><span class="line"></span><br><span class="line">val collection = env.fromCollection(data)</span><br><span class="line">val unique = collection.partitionByHash(1).mapPartition&#123;</span><br><span class="line">  line =&gt;</span><br><span class="line">    line.map(x =&gt; (x._1 , x._2 , x._3))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">unique.writeAsText(&quot;hashPartition&quot;, WriteMode.NO_OVERWRITE)</span><br><span class="line">env.execute()</span><br></pre></td></tr></table></figure><h3 id="17-partitionByRange"><a href="#17-partitionByRange" class="headerlink" title="17. partitionByRange"></a><strong>17. partitionByRange</strong></h3><p>根据指定的key对数据集进行范围分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">val data = new mutable.MutableList[(Int, Long, String)]</span><br><span class="line">data.+=((1, 1L, &quot;Hi&quot;))</span><br><span class="line">data.+=((2, 2L, &quot;Hello&quot;))</span><br><span class="line">data.+=((3, 2L, &quot;Hello world&quot;))</span><br><span class="line">data.+=((4, 3L, &quot;Hello world, how are you?&quot;))</span><br><span class="line"></span><br><span class="line">val collection = env.fromCollection(data)</span><br><span class="line">val unique = collection.partitionByRange(x =&gt; x._1).mapPartition(line =&gt; line.map&#123;</span><br><span class="line">  x=&gt;</span><br><span class="line">    (x._1 , x._2 , x._3)</span><br><span class="line">&#125;)</span><br><span class="line">unique.writeAsText(&quot;rangePartition&quot;, WriteMode.OVERWRITE)</span><br><span class="line">env.execute()</span><br></pre></td></tr></table></figure><h3 id="18-sortPartition"><a href="#18-sortPartition" class="headerlink" title="18. sortPartition"></a><strong>18. sortPartition</strong></h3><p>根据指定的字段值进行分区的排序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">val data = new mutable.MutableList[(Int, Long, String)]</span><br><span class="line">    data.+=((1, 1L, &quot;Hi&quot;))</span><br><span class="line">    data.+=((2, 2L, &quot;Hello&quot;))</span><br><span class="line">    data.+=((3, 2L, &quot;Hello world&quot;))</span><br><span class="line">    data.+=((4, 3L, &quot;Hello world, how are you?&quot;))</span><br><span class="line"></span><br><span class="line">val ds = env.fromCollection(data)</span><br><span class="line">    val result = ds</span><br><span class="line">      .map &#123; x =&gt; x &#125;.setParallelism(2)</span><br><span class="line">      .sortPartition(1, Order.DESCENDING)//第一个参数代表按照哪个字段进行分区</span><br><span class="line">      .mapPartition(line =&gt; line)</span><br><span class="line">      .collect()</span><br><span class="line"></span><br><span class="line">println(result)</span><br></pre></td></tr></table></figure><h3 id="三、Sink算子"><a href="#三、Sink算子" class="headerlink" title="三、Sink算子"></a><strong>三、Sink算子</strong></h3><h3 id="1-collect"><a href="#1-collect" class="headerlink" title="1. collect"></a><strong>1. collect</strong></h3><p>将数据输出到本地集合</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result.collect()</span><br></pre></td></tr></table></figure><h3 id="2-writeAsText"><a href="#2-writeAsText" class="headerlink" title="2. writeAsText"></a><strong>2. writeAsText</strong></h3><p>将数据输出到文件</p><p>Flink支持多种存储设备上的文件，包括本地文件，hdfs文件等</p><p>Flink支持多种文件的存储格式，包括text文件，CSV文件等</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 将数据写入本地文件</span><br><span class="line">result.writeAsText(&quot;/data/a&quot;, WriteMode.OVERWRITE)</span><br><span class="line"></span><br><span class="line">// 将数据写入HDFS</span><br><span class="line">result.writeAsText(&quot;hdfs://node01:9000/data/a&quot;, WriteMode.OVERWRITE)</span><br></pre></td></tr></table></figure><h1 id="DataStream"><a href="#DataStream" class="headerlink" title="DataStream"></a><strong>DataStream</strong></h1><p>和DataSet一样，DataStream也包括一系列的Transformation操作</p><h3 id="一、Source算子-1"><a href="#一、Source算子-1" class="headerlink" title="一、Source算子"></a><strong>一、Source算子</strong></h3><p>Flink可以使用 StreamExecutionEnvironment.addSource(source) 来为我们的程序添加数据来源。<br>Flink 已经提供了若干实现好了的 source functions，当然我们也可以通过实现 SourceFunction  来自定义非并行的source或者实现 ParallelSourceFunction 接口或者扩展  RichParallelSourceFunction 来自定义并行的 source。</p><p>Flink在流处理上的source和在批处理上的source基本一致。大致有4大类：</p><ul><li>基于<strong>本地集合</strong>的source（Collection-based-source）</li><li>基于<strong>文件</strong>的source（File-based-source）- 读取文本文件，即符合 TextInputFormat 规范的文件，并将其作为字符串返回</li><li>基于<strong>网络套接字</strong>的source（Socket-based-source）- 从 socket 读取。元素可以用分隔符切分。</li><li><strong>自定义</strong>的source（Custom-source）</li></ul><p>下面使用addSource将Kafka数据写入Flink为例：</p><p>如果需要外部数据源对接，可使用addSource，如将Kafka数据写入Flink， 先引入依赖：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>将Kafka数据写入Flink：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val properties = new Properties()</span><br><span class="line">properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class="line">properties.setProperty(&quot;group.id&quot;, &quot;consumer-group&quot;)</span><br><span class="line">properties.setProperty(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;)</span><br><span class="line">properties.setProperty(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;)</span><br><span class="line">properties.setProperty(&quot;auto.offset.reset&quot;, &quot;latest&quot;)</span><br><span class="line"></span><br><span class="line">val source = env.addSource(new FlinkKafkaConsumer011[String](&quot;sensor&quot;, new SimpleStringSchema(), properties))</span><br></pre></td></tr></table></figure><p>基于网络套接字的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val source = env.socketTextStream(&quot;IP&quot;, PORT)</span><br></pre></td></tr></table></figure><h3 id="二、Transform转换算子-1"><a href="#二、Transform转换算子-1" class="headerlink" title="二、Transform转换算子"></a><strong>二、Transform转换算子</strong></h3><h3 id="1-map-1"><a href="#1-map-1" class="headerlink" title="1. map"></a><strong>1. map</strong></h3><p>将DataSet中的每一个元素转换为另外一个元素</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.map &#123; x =&gt; x * 2 &#125;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215093723.png"></p><h3 id="2-FlatMap"><a href="#2-FlatMap" class="headerlink" title="2. FlatMap"></a><strong>2. FlatMap</strong></h3><p>采用一个数据元并生成零个，一个或多个数据元。将句子分割为单词的flatmap函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.flatMap &#123; str =&gt; str.split(&quot; &quot;) &#125;</span><br></pre></td></tr></table></figure><h3 id="3-Filter"><a href="#3-Filter" class="headerlink" title="3. Filter"></a><strong>3. Filter</strong></h3><p>计算每个数据元的布尔函数，并保存函数返回true的数据元。过滤掉零值的过滤器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.filter &#123; _ != 0 &#125;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215093751.png"></p><h3 id="4-KeyBy"><a href="#4-KeyBy" class="headerlink" title="4. KeyBy"></a><strong>4. KeyBy</strong></h3><p>逻辑上将流分区为不相交的分区。具有相同Keys的所有记录都分配给同一分区。在内部，keyBy（）是使用散列分区实现的。指定键有不同的方法。</p><p>此转换返回KeyedStream，其中包括使用被Keys化状态所需的KeyedStream。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(0) </span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215093914.png"></p><p><strong>注意:</strong></p><p>​        这个keyBy,你给我一个dataStream,他调用keyBy之后得到一个KeyedStream,这个KeyedStream就相当于是Spark里面的,包含键值对的RDD.说白了就是元素是二元组的那个RDD.不过在我们Flink里面,没有所谓二元组的说法.不一定说二元组就是键值对.在Flink中没有这个说法.只有在Spark RDD中才有这个说法.意思就是说:你看到一个Dstream里面的元素是二元组类型,千万别认为这就是一个键值对的Dstream.这是一个非常普通的Dstream.除非他不是Dstream,而是一个KeyedStream.所以在<strong>Flink中到底判断他是不是一个键值对的Stream,就看他类型是不是KeyedStream,跟里面的元素没有关系.</strong> </p><h3 id="5-滚动聚合算子（Rolling-Aggregation）"><a href="#5-滚动聚合算子（Rolling-Aggregation）" class="headerlink" title="5. 滚动聚合算子（Rolling Aggregation）"></a>5. 滚动聚合算子（Rolling Aggregation）</h3><p>Aggregations 是 KeyedDataStream 接口提供的聚合算子，根据指定的字段进行聚合操作，滚动地产生一系列数据聚合结果。其实是将 Reduce 算子中的函数进行了封装，封装的聚合操作有sum,min,max 等，这样就不需要用户自己定义 Reduce 函数。<br>如下代码所示，指定数据集中第一个字段作为 key，用第二个字段作为累加字段，然后滚动地对第二个字段的数值进行累加并输出。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215100316.png"></p><p><strong>这些算子可以针对KeyedStream的每一个支流做聚合。</strong></p><ul><li><p> sum()</p></li><li><p> min()</p></li><li><p> max()</p></li><li><p> minBy()</p></li><li><p> maxBy()</p></li></ul><p>  <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215094055.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215100711.png"></p><h3 id="6-Reduce"><a href="#6-Reduce" class="headerlink" title="6. Reduce"></a><strong>6. Reduce</strong></h3><p>被Keys化数据流上的“滚动”Reduce。将当前数据元与最后一个Reduce的值组合并发出新值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keyedStream.reduce &#123; _ + _ &#125;  </span><br></pre></td></tr></table></figure><p><strong>KeyedStream → DataStream</strong>：一个分组数据流的聚合操作，合并当前的元素和上次聚合的结果，产生一个新的值，返回的流中包含每一次聚合的结果，而不是只返回最后一次聚合的最终结果。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val stream2 = env.readTextFile(&quot;YOUR_PATH\\sensor.txt&quot;)</span><br><span class="line">  .map( data =&gt; &#123;</span><br><span class="line">    val dataArray = data.split(&quot;,&quot;)</span><br><span class="line">    SensorReading(dataArray(0).trim, dataArray(1).trim.toLong, dataArray(2).trim.toDouble)</span><br><span class="line">  &#125;)</span><br><span class="line">  .keyBy(&quot;id&quot;)</span><br><span class="line">  .reduce( (x, y) =&gt; SensorReading(x.id, x.timestamp + 1, y.temperature) )</span><br></pre></td></tr></table></figure><h3 id="7-Fold"><a href="#7-Fold" class="headerlink" title="7. Fold"></a><strong>7. Fold</strong></h3><p>具有初始值的被Keys化数据流上的“滚动”折叠。将当前数据元与最后折叠的值组合并发出新值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val result: DataStream[String] =  keyedStream.fold(&quot;start&quot;)((str, i) =&gt; &#123; str + &quot;-&quot; + i &#125;) </span><br><span class="line"></span><br><span class="line">// 解释：当上述代码应用于序列（1,2,3,4,5）时，输出结果“start-1”，“start-1-2”，“start-1-2-3”，...</span><br></pre></td></tr></table></figure><h3 id="8-Aggregations"><a href="#8-Aggregations" class="headerlink" title="8. Aggregations"></a><strong>8. Aggregations</strong></h3><p>在被Keys化数据流上滚动聚合。min和minBy之间的差异是min返回最小值，而minBy返回该字段中具有最小值的数据元（max和maxBy相同）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">keyedStream.sum(0);</span><br><span class="line"></span><br><span class="line">keyedStream.min(0);</span><br><span class="line"></span><br><span class="line">keyedStream.max(0);</span><br><span class="line"></span><br><span class="line">keyedStream.minBy(0);</span><br><span class="line"></span><br><span class="line">keyedStream.maxBy(0);</span><br></pre></td></tr></table></figure><h3 id="9-Window"><a href="#9-Window" class="headerlink" title="9. Window"></a><strong>9. Window</strong></h3><p>可以在已经分区的KeyedStream上定义Windows。Windows根据某些特征（例如，在最后5秒内到达的数据）对每个Keys中的数据进行分组。这里不再对窗口进行详解，有关窗口的完整说明，请查看这篇文章： <strong><a href="https://mp.weixin.qq.com/s/S-RmP5OWiGqwn-C_TZNO5A">Flink 中极其重要的 Time 与 Window 详细解析</a></strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(5))); </span><br></pre></td></tr></table></figure><h3 id="10-WindowAll"><a href="#10-WindowAll" class="headerlink" title="10. WindowAll"></a><strong>10. WindowAll</strong></h3><p>Windows可以在常规DataStream上定义。Windows根据某些特征（例如，在最后5秒内到达的数据）对所有流事件进行分组。</p><p>注意：在许多情况下，这是非并行转换。所有记录将收集在windowAll 算子的一个任务中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(5)))</span><br></pre></td></tr></table></figure><h3 id="11-Window-Apply"><a href="#11-Window-Apply" class="headerlink" title="11. Window Apply"></a><strong>11. Window Apply</strong></h3><p>将一般函数应用于整个窗口。</p><p>注意：如果您正在使用windowAll转换，则需要使用AllWindowFunction。</p><p>下面是一个手动求和窗口数据元的函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">windowedStream.apply &#123; WindowFunction &#125;</span><br><span class="line"></span><br><span class="line">allWindowedStream.apply &#123; AllWindowFunction &#125;</span><br></pre></td></tr></table></figure><h3 id="12-Window-Reduce"><a href="#12-Window-Reduce" class="headerlink" title="12. Window Reduce"></a><strong>12. Window Reduce</strong></h3><p>将函数缩减函数应用于窗口并返回缩小的值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">windowedStream.reduce &#123; _ + _ &#125;</span><br></pre></td></tr></table></figure><h3 id="13-Window-Fold"><a href="#13-Window-Fold" class="headerlink" title="13. Window Fold"></a><strong>13. Window Fold</strong></h3><p>将函数折叠函数应用于窗口并返回折叠值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val result: DataStream[String] = windowedStream.fold(&quot;start&quot;, (str, i) =&gt; &#123; str + &quot;-&quot; + i &#125;) </span><br><span class="line"></span><br><span class="line">// 上述代码应用于序列（1,2,3,4,5）时，将序列折叠为字符串“start-1-2-3-4-5”</span><br></pre></td></tr></table></figure><h3 id="14-Union-真正意义上的汇合"><a href="#14-Union-真正意义上的汇合" class="headerlink" title="14. Union(真正意义上的汇合)"></a><strong>14. Union</strong>(真正意义上的汇合)</h3><p>两个或多个数据流的联合，创建包含来自所有流的所有数据元的新流。注意：如果将数据流与自身联合，则会在结果流中获取两次数据元</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.union(otherStream1, otherStream2, ...)</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215094828.png"></p><p><strong>DataStream → DataStream</strong>：对两个或者两个以上的DataStream进行union操作，产生一个包含所有DataStream元素的新DataStream。</p><h4 id="Connect与-Union-区别："><a href="#Connect与-Union-区别：" class="headerlink" title="Connect与 Union 区别："></a>Connect与 Union 区别：</h4><p>  1． Union之前两个流的类型必须是一样，Connect可以不一样，在之后的coMap中再去调整成为一样的。</p><ol start="2"><li>  Connect只能操作两个流，Union可以操作多个。</li></ol><h3 id="15-Window-Join"><a href="#15-Window-Join" class="headerlink" title="15. Window Join"></a><strong>15. Window Join</strong></h3><p>在给定Keys和公共窗口上连接两个数据流</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dataStream.join(otherStream)</span><br><span class="line">    .where(&lt;key selector&gt;).equalTo(&lt;key selector&gt;)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.seconds(3)))</span><br><span class="line">    .apply (new JoinFunction () &#123;...&#125;)</span><br></pre></td></tr></table></figure><h3 id="16-Interval-Join"><a href="#16-Interval-Join" class="headerlink" title="16. Interval Join"></a><strong>16. Interval Join</strong></h3><p>在给定的时间间隔内使用公共Keys关联两个被Key化的数据流的两个数据元e1和e2，以便e1.timestamp + lowerBound &lt;= e2.timestamp &lt;= e1.timestamp + upperBound</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">am.intervalJoin(otherKeyedStream)</span><br><span class="line">    .between(Time.milliseconds(-2), Time.milliseconds(2)) </span><br><span class="line">    .upperBoundExclusive(true) </span><br><span class="line">    .lowerBoundExclusive(true) </span><br><span class="line">    .process(new IntervalJoinFunction() &#123;...&#125;)</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214232600.png"></p><p><strong>案例代码:</strong></p><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/Flink-shangbaishuyao-RTDW-gmall-realtime/src/main/java/com/shangbaishuyao/gmall/realtime/app/DWM/OrderWideApp.java">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/Flink-shangbaishuyao-RTDW-gmall-realtime/src/main/java/com/shangbaishuyao/gmall/realtime/app/DWM/OrderWideApp.java</a></p><h3 id="17-Window-CoGroup"><a href="#17-Window-CoGroup" class="headerlink" title="17. Window CoGroup"></a><strong>17. Window CoGroup</strong></h3><p>在给定Keys和公共窗口上对两个数据流进行Cogroup</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dataStream.coGroup(otherStream)</span><br><span class="line">    .where(0).equalTo(1)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.seconds(3)))</span><br><span class="line">    .apply (new CoGroupFunction () &#123;...&#125;)</span><br></pre></td></tr></table></figure><h3 id="18-Connect"><a href="#18-Connect" class="headerlink" title="18. Connect"></a><strong>18. Connect</strong></h3><p>“连接”两个保存其类型的数据流。连接允许两个流之间的共享状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Integer&gt; someStream = ... DataStream&lt;String&gt; otherStream = ... ConnectedStreams&lt;Integer, String&gt; connectedStreams = someStream.connect(otherStream)</span><br><span class="line"></span><br><span class="line">// ... 代表省略中间操作</span><br></pre></td></tr></table></figure><h3 id="19-CoMap，CoFlatMap"><a href="#19-CoMap，CoFlatMap" class="headerlink" title="19. CoMap，CoFlatMap"></a><strong>19. CoMap，CoFlatMap</strong></h3><p>Connect做连接的,把两个流连在一起.也可以理解为做回合,把两个流汇合在一起.</p><p>Connect把两个流汇合在一起有条件吗?stream1是整形,stream2是字符串的这样的两个流可以汇合在一起吗?他是可以的.他不管你两个流是否类型一致.他都可以汇合在一起.connect会和其实和上面的splitStream一样,都没有被真正的汇合或切分.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215095927.png"></p><p>类似于连接数据流上的map和flatMap</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">connectedStreams.map(</span><br><span class="line">    (_ : Int) =&gt; true,</span><br><span class="line">    (_ : String) =&gt; false)connectedStreams.flatMap(</span><br><span class="line">    (_ : Int) =&gt; true,</span><br><span class="line">    (_ : String) =&gt; false)</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215094623.png"></p><p><strong>DataStream,DataStream → ConnectedStreams</strong>：连接两个保持他们类型的数据流，两个数据流被Connect之后，只是被放在了一个同一个流中，内部依然保持各自的数据和形式不发生任何变化，两个流相互独立。</p><p> <strong>CoMap,CoFlatMap</strong><br><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215094718.png"></p><p>从这张图可以看出,他只有调用map或者FlatMap才真正将两个流汇合起来的.<br>为什么要执行map或者FlatMap呢?因为执行map算子或者flatmap算子之后他就可以在map算子或者FlatMap算子中把这两个流分别做处理.分别处理成同样类型的这时返回得到一个新的对象,他就不会出现两个流了,他就变成一个流了.</p><p><strong>ConnectedStreams → DataStream</strong>：作用于ConnectedStreams上，功能与map和flatMap一样，对ConnectedStreams中的每一个Stream分别进行map和flatMap处理。</p><h3 id="20-Split-不是真正意义上切分流-只是打了标记"><a href="#20-Split-不是真正意义上切分流-只是打了标记" class="headerlink" title="20. Split(不是真正意义上切分流,只是打了标记)"></a><strong>20. Split(不是真正意义上切分流,只是打了标记)</strong></h3><p>根据某些标准将流拆分为两个或更多个流</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val split = someDataStream.split(</span><br><span class="line">  (num: Int) =&gt;</span><br><span class="line">    (num % 2) match &#123;</span><br><span class="line">      case 0 =&gt; List(&quot;even&quot;)</span><br><span class="line">      case 1 =&gt; List(&quot;odd&quot;)</span><br><span class="line">    &#125;)      </span><br></pre></td></tr></table></figure><h3 id="21-Select-真正意义上的切分流"><a href="#21-Select-真正意义上的切分流" class="headerlink" title="21. Select(真正意义上的切分流)"></a><strong>21. Select(真正意义上的切分流)</strong></h3><p>从拆分流中选择一个或多个流</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SplitStream&lt;Integer&gt; split;DataStream&lt;Integer&gt; even = split.select(&quot;even&quot;);DataStream&lt;Integer&gt; odd = split.select(&quot;odd&quot;);DataStream&lt;Integer&gt; all = split.select(&quot;even&quot;,&quot;odd&quot;)</span><br></pre></td></tr></table></figure><h4 id="Split-和-Select"><a href="#Split-和-Select" class="headerlink" title="Split 和 Select:"></a>Split 和 Select:</h4><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215095211.png"></p><p><strong>DataStream → SplitStream</strong>：根据某些特征把一个DataStream拆分成两个或者多个DataStream。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215095246.png"></p><p><strong>SplitStream→DataStream</strong>：从一个SplitStream中获取一个或者多个DataStream。</p><p>需求：传感器数据按照温度高低（以30度为界），拆分成两个流。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val splitStream = stream2</span><br><span class="line">  .split( sensorData =&gt; &#123;</span><br><span class="line">    if (sensorData.temperature &gt; 30) Seq(&quot;high&quot;) else Seq(&quot;low&quot;)</span><br><span class="line">  &#125; )</span><br><span class="line"></span><br><span class="line">val high = splitStream.select(&quot;high&quot;)</span><br><span class="line">val low = splitStream.select(&quot;low&quot;)</span><br><span class="line">val all = splitStream.select(&quot;high&quot;, &quot;low&quot;)</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215095354.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215095336.png"></p><h3 id="三、Sink算子-1"><a href="#三、Sink算子-1" class="headerlink" title="三、Sink算子"></a><strong>三、Sink算子</strong></h3><p>支持将数据输出到：</p><ul><li>本地文件(参考批处理)</li><li>本地集合(参考批处理)</li><li>HDFS(参考批处理)</li></ul><p>除此之外，还支持：</p><ul><li>sink到kafka</li><li>sink到mysql</li><li>sink到redis</li></ul><p>下面以sink到kafka为例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">val sinkTopic = &quot;test&quot;</span><br><span class="line"></span><br><span class="line">//样例类</span><br><span class="line">case class Student(id: Int, name: String, addr: String, sex: String)</span><br><span class="line">val mapper: ObjectMapper = new ObjectMapper()</span><br><span class="line"></span><br><span class="line">//将对象转换成字符串</span><br><span class="line">def toJsonString(T: Object): String = &#123;</span><br><span class="line">    mapper.registerModule(DefaultScalaModule)</span><br><span class="line">    mapper.writeValueAsString(T)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    //1.创建流执行环境</span><br><span class="line">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    //2.准备数据</span><br><span class="line">    val dataStream: DataStream[Student] = env.fromElements(</span><br><span class="line">      Student(8, &quot;xiaoming&quot;, &quot;beijing biejing&quot;, &quot;female&quot;)</span><br><span class="line">    )</span><br><span class="line">    //将student转换成字符串</span><br><span class="line">    val studentStream: DataStream[String] = dataStream.map(student =&gt;</span><br><span class="line">      toJsonString(student) // 这里需要显示SerializerFeature中的某一个，否则会报同时匹配两个方法的错误</span><br><span class="line">    )</span><br><span class="line">    //studentStream.print()</span><br><span class="line">    val prop = new Properties()</span><br><span class="line">    prop.setProperty(&quot;bootstrap.servers&quot;, &quot;node01:9092&quot;)</span><br><span class="line"></span><br><span class="line">    val myProducer = new FlinkKafkaProducer011[String](sinkTopic, new KeyedSerializationSchemaWrapper[String](new SimpleStringSchema()), prop)</span><br><span class="line">    studentStream.addSink(myProducer)</span><br><span class="line">    studentStream.print()</span><br><span class="line">    env.execute(&quot;Flink add sink&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>知识源于积累,登峰造极源于自律!</p><p>好文章就得收藏慢慢品, 文章转载于:<a href="https://zhuanlan.zhihu.com/p/356616078">https://zhuanlan.zhihu.com/p/356616078</a> 在此基础上做的CRUD.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215092137.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Flink和Spark类似，也是一种一站式处理的框架；既可以进行批处理（DataSet），也可以进行实时处理（DataStream）。&lt;/p&gt;
&lt;p&gt;所以下面将Flink的算子分为两大类：一类是DataSet，一类是DataStream。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flink官网:&lt;/strong&gt; &lt;a href=&quot;https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/dev/datastream/operators/overview/&quot;&gt;https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/dev/datastream/operators/overview/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;案例代码:&lt;/strong&gt; &lt;a href=&quot;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/tree/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo&quot;&gt;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/tree/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink实现UDF函数——更细粒度的控制流</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E5%AE%9E%E7%8E%B0UDF%E5%87%BD%E6%95%B0%E2%80%94%E2%80%94%E6%9B%B4%E7%BB%86%E7%B2%92%E5%BA%A6%E7%9A%84%E6%8E%A7%E5%88%B6%E6%B5%81/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink%E5%AE%9E%E7%8E%B0UDF%E5%87%BD%E6%95%B0%E2%80%94%E2%80%94%E6%9B%B4%E7%BB%86%E7%B2%92%E5%BA%A6%E7%9A%84%E6%8E%A7%E5%88%B6%E6%B5%81/</id>
    <published>2022-02-15T00:58:42.000Z</published>
    <updated>2022-02-15T01:16:29.101Z</updated>
    
    <content type="html"><![CDATA[<p>Flink的一个优势,是其他计算引擎所做不到的.或者说能做到,但是代码特别麻烦,我们的Flink中每一个算子,他都给你提供了一个函数对象作为参数</p><p><strong>好记心烂笔头</strong>:</p><p>为什么我们不用匿名函数(lambda function)去写Flink代码呢?</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">因为Flink中花括号里面的代码都是运行在slot中的,但是那么多slot,每个slot里面都可能执行,那数据库就被搞死了.所以我们就不用匿名函数的方式写代码了.而是使用函数类的方式.</span><br></pre></td></tr></table></figure><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215090044.png"></p><h1 id="函数类（Function-Classes）"><a href="#函数类（Function-Classes）" class="headerlink" title="函数类（Function Classes）"></a>函数类（Function Classes）</h1><p>Flink暴露了所有<strong>udf函数的接口</strong>(实现方式为接口或者抽象类)。例如MapFunction做转换的, FilterFunction做过滤的, ProcessFunction不知道做转换,做过滤还是做其他的,<strong>但是我肯定要处理数据,则使用processFunction,他是没有限制的,你想做什么你就在这里面写什么就好了</strong>等等。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220215090143.png"></p><p>下面例子实现了FilterFunction接口：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class FilterFilter extends FilterFunction[String] &#123;</span><br><span class="line">      override def filter(value: String): Boolean = &#123;</span><br><span class="line">        value.contains(&quot;flink&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br><span class="line">val flinkTweets = tweets.filter(new FlinkFilter)</span><br></pre></td></tr></table></figure><p>还可以将函数实现成匿名类</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val flinkTweets = tweets.filter(</span><br><span class="line">new RichFilterFunction[String] &#123;</span><br><span class="line">override def filter(value: String): Boolean = &#123;</span><br><span class="line">value.contains(&quot;flink&quot;)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>我们filter的字符串”flink”还可以当作参数传进去。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val tweets: DataStream[String] = ...</span><br><span class="line">val flinkTweets = tweets.filter(new KeywordFilter(&quot;flink&quot;))</span><br><span class="line"></span><br><span class="line">class KeywordFilter(keyWord: String) extends FilterFunction[String] &#123;</span><br><span class="line">override def filter(value: String): Boolean = &#123;</span><br><span class="line">value.contains(keyWord)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="匿名函数（Lambda-Functions）"><a href="#匿名函数（Lambda-Functions）" class="headerlink" title="匿名函数（Lambda Functions）"></a>匿名函数（Lambda Functions）</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val tweets: DataStream[String] = ...</span><br><span class="line">val flinkTweets = tweets.filter(_.contains(&quot;flink&quot;))</span><br></pre></td></tr></table></figure><h1 id="富函数（Rich-Functions"><a href="#富函数（Rich-Functions" class="headerlink" title="富函数（Rich Functions)"></a>富函数（Rich Functions)</h1><p><strong>富函数和函数类是一样的,不过他有一个特点.他增加了生命周期的管理.什么叫生命周期?就是    XXXFunction()什么时候初始化.什么时候销毁.什么时候执行你们的代码等就是所谓的生命周期的管理.XXXFunction()本身是没有生命周期的管理的.如果你需要增加生命周期的管理,你需要继承RichMapFunction()</strong></p><p>“富函数”是DataStream API提供的一个函数类的接口，所有Flink函数类都有其Rich版本。它与常规函数的不同在于，可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。</p><ul><li> RichMapFunction</li><li> RichFlatMapFunction</li><li> RichFilterFunction</li><li> …</li></ul><p>Rich Function有一个生命周期的概念。典型的生命周期方法有：</p><ul><li> open()方法是rich function的初始化方法，当一个算子例如map或者filter被调用之前open()会被调用。</li><li> close()方法是生命周期中的最后一个调用的方法，做一些清理工作。</li><li> getRuntimeContext()方法提供了函数的RuntimeContext的一些信息，例如函数执行的并行度，任务的名字，以及state状态</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class MyFlatMap extends RichFlatMapFunction[Int, (Int, Int)] &#123;</span><br><span class="line">var subTaskIndex = 0</span><br><span class="line"></span><br><span class="line">override def open(configuration: Configuration): Unit = &#123;</span><br><span class="line">subTaskIndex = getRuntimeContext.getIndexOfThisSubtask</span><br><span class="line">// 以下可以做一些初始化工作，例如建立一个和HDFS的连接</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">override def flatMap(in: Int, out: Collector[(Int, Int)]): Unit = &#123;</span><br><span class="line">if (in % 2 == subTaskIndex) &#123;</span><br><span class="line">out.collect((subTaskIndex, in))</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">override def close(): Unit = &#123;</span><br><span class="line">// 以下做一些清理工作，例如断开和HDFS的连接。</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="案例代码"><a href="#案例代码" class="headerlink" title="案例代码"></a>案例代码</h1><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-from-shangbaishuyao-wordCount/src/main/scala/com/shangbaishuyao/processFunction/">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-from-shangbaishuyao-wordCount/src/main/scala/com/shangbaishuyao/processFunction/</a></p><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo03/Flink04_Transform_Reduce.java">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo03/Flink04_Transform_Reduce.java</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Flink的一个优势,是其他计算引擎所做不到的.或者说能做到,但是代码特别麻烦,我们的Flink中每一个算子,他都给你提供了一个函数对象作为参数&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;好记心烂笔头&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;为什么我们不用匿名函数(lambda function)去写Flink代码呢?&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;因为Flink中花括号里面的代码都是运行在slot中的,但是那么多slot,每个slot里面都可能执行,那数据库就被搞死了.所以我们就不用匿名函数的方式写代码了.而是使用函数类的方式.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: Flink CEP复杂事件处理</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink-CEP%E5%A4%8D%E6%9D%82%E4%BA%8B%E4%BB%B6%E5%A4%84%E7%90%86/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Flink-CEP%E5%A4%8D%E6%9D%82%E4%BA%8B%E4%BB%B6%E5%A4%84%E7%90%86/</id>
    <published>2022-02-14T14:52:35.000Z</published>
    <updated>2022-02-14T15:31:08.053Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flink-CEP-代码案例"><a href="#Flink-CEP-代码案例" class="headerlink" title="Flink CEP 代码案例"></a>Flink CEP 代码案例</h1><p>登录告警系统:  一堆的登录日志从，匹配一个恶意登录的模式（如果一个用户连续失败三次，则是恶意登录），从而找到哪些用户名是用于恶意 登录</p><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-from-shangbaishuyao-wordCount/src/main/scala/com/shangbaishuyao/cep/TestCepDemo.scala">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-from-shangbaishuyao-wordCount/src/main/scala/com/shangbaishuyao/cep/TestCepDemo.scala</a></p><p>登录失败CEP模型:</p><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo09/Flink03_Practice_LoginFailWithCEP.java">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo09/Flink03_Practice_LoginFailWithCEP.java</a></p><p>支付失败CEP模型:</p><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo09/Flink04_Practice_OrderPayWithCEP.java">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo09/Flink04_Practice_OrderPayWithCEP.java</a></p><span id="more"></span><h1 id="什么是复杂事件处理CEP"><a href="#什么是复杂事件处理CEP" class="headerlink" title="什么是复杂事件处理CEP"></a>什么是复杂事件处理CEP</h1><p>​        CEP是做复杂事件处理的.如果你碰到一个业务需求非常之复杂,而且他的条件也是非常复杂的.比如说处理某一个,符合某一个条件,做什么事情.这个条件是非常复杂的.那么处理复杂的事件,Flink专门有一套API.这套API的名字叫CEP. 而spark是没有CEP的.</p><p>​    一个或多个由简单事件构成的事件流通过一定的规则匹配，然后输出用户想得到的数据，满足规则的复杂事件。</p><p>​    特征：</p><p>​        Ø 目标：从有序的简单事件流中发现一些高阶特征(无序的就得加延时操作)</p><p>​        Ø 输入：一个或多个由简单事件构成的事件流</p><p>​        Ø 处理：识别简单事件之间的内在联系，多个符合一定规则的简单事件构成复杂事件</p><p>​        Ø 输出：满足规则的复杂事件</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214225403.png"></p><p>​        如上图所示: 如果一个用户短时间内频繁登录失败，就有可能是出现了程序的恶意攻击，比如密码暴力破解。因此我们考虑，应该对用户的登录失败动作进行统计，具体来说，如果同一用户（可以是不同IP）在2秒之内连续两次登录失败，就认为存在恶意登录的风险，输出相关的信息进行报警提示。</p><p>这个恶意登录监控有两种解决办法. 一.状态编程. 二.CEP编程.  </p><p>​        比如同一个用户(可以是不同ip)在2秒内连续2次登录失败,我们就触发报警.打一个告警信息. 我们可以引入一个listState. 将上一次用户登录的数据放到这个列表中. 后面还出现相同数据,继续放到列表中. 列表的长度等于2了或者大于2了. 就开始发出一个告警. 不过用状态编程会有问题. 会造成内存的容量的增加.还可能造成一些精准的业务无法做到. 比如说我不要连续两秒内两次登录了. 改成两秒内连续5次登录失败.才将这样的用户找出来. 这种需求改起来特别复杂.因为中间很有可能穿插一些其他东西. 如上图: 比如我要删选长方形和紧跟着圆形的数据. 后来需求改成长方形后面有圆形的数据了. 这两个之间很可能穿插了一些其他的三角形的一些东西. 那这些穿插的需不需要考虑放到listState中去. 而且还要考虑,这个时间是否到达5秒. 所以用状态编程不适合做恶意登录风险.所以当前业务我们应该使用Flink的CEP的库来做.</p><p>​        CEP用于分析低延迟、频繁产生的不同来源的事件流。CEP可以帮助在复杂的、不相关的事件流中找出有意义的模式和复杂的关系，以接近实时或准实时的获得通知并阻止一些行为。</p><p>​        CEP支持在流上进行模式匹配，根据模式的条件不同，分为连续的条件或不连续的条件；模式的条件允许有时间的限制，当在条件范围内没有达到满足的条件时，会导致模式匹配超时。</p><p>看起来很简单，但是它有很多不同的功能：</p><p>​        Ø 输入的流数据，尽快产生结果</p><p>​        Ø 在2个event流上，基于时间进行聚合类的计算</p><p>​        Ø 提供实时/准实时的警告和通知</p><p>​        Ø 在多样的数据源中产生关联并分析模式</p><p>​        Ø 高吞吐、低延迟的处理</p><p>市场上有多种CEP的解决方案，例如Spark、Samza、Beam等，但他们都没有提供专门的library支持。但是Flink提供了专门的CEP library。</p><h1 id="Flink-CEP"><a href="#Flink-CEP" class="headerlink" title="Flink CEP"></a>Flink CEP</h1><p>Flink为CEP提供了专门的Flink CEP library，它包含如下组件：</p><p>FlinkCEP的步骤就四个. 一是准备好数据.该分组分组.改设置waterMark的设置Watermark.该过滤过滤. 因为CEP本质上就是一个窗口函数.它里面封装了开窗.因为一般情况下,我们会设置在某一个时间内满足这个条件的.如果说没有时间限制的话.那这个符合这个规则的数据找到一堆. 或者找不到. 比如找到一本矩形开头的,但是后面数据源源不断的来.因为他是一个无线的流.所以会造成他匹配不成功.所以一般来说我们一定要限制一个时间范围.当你限制一个时间范围的话,实际上就是开窗了.所以前面你要定义时间语义,定义是否有水位线之类的.</p><p>第二个就是定义我们的模式. 即定义我们的pattern. 定义好了之后,我们的pattern会帮我们检测.模式就是我们的规则. 检测实际上会出现两种情况.一种情况符合这个规则. 还有种情况是不符合这个规则.其实就是一种匹配成功了.一种没有匹配成功. 但是一般情况下我们只是处理匹配成功的. 匹配成功的数据怎么拿出来呢? 就是生成Alert.  这里检测的时候为什么要说能够帮我检测根据规则去匹配成功的,也可以匹配哪些不成功的呢?因为我们以后可能遇到一些用排除法做模式匹配. 如果不用排除法发现一些业务太复杂了,我们不好加一些条件去设置规则. 到时候发现这个业务可以用排除法的话.而且反证的条件是容易定义的.那我就把反证的模式定义好.定义好之后我只需要找那些没有匹配上的.没有匹配上的就是我所需要的.</p><p>​        Ø Event Stream</p><p>​        Ø pattern定义</p><p>​        Ø pattern检测</p><p>​        Ø 生成Alert</p><p><strong>Flink CEP 四步骤:</strong></p><p>定义模式,只要匹配订单在创建之后15分钟内有支付的事件.单一模式</p><p>模式检测</p><p>生产Alter. 找到所有的在15分钟内支付的订单事件, 并且还要找到没有支付的订单事件</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214225700.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214231635.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214231644.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214231739.png"></p><p>notFollowedBy的意思就是不想让某一个是事件在两个事件之间发生.</p><p>因为notFollowedBy()绝对不可能放在最后面. 他只会放在两个个体模式之间. 既然是在两个个体模式的意思.就意味着表示在两个个体不同的事件之间不出现一个什么样的事件. 所以称之为叫不想要某个事件在两个事件之间发生.这个模式也是可以指定时间的. 用within里面指定时间.</p><p>超时事件的提取: 就是提取匹配的反的模式</p><p>超时事件的提取. 假设我有一个窗口里面有许多数据, 我要匹配 :</p><p>矩形 followBy 圆形 的.  我只要是矩形开头圆形结尾就可以匹配到.</p><p>但是有这么一种情况就是: 我在窗口最后位置有一个矩形,后面就没数据了.  就是在这个5分钟窗口内找不到矩形开头后面紧跟着圆圈的了.像这样的一个数据事件我们也可以在后面选取的时候将他选着出来.所以他有两种事件的选取. 一种是匹配事件的选取. 一种是超时事件的提取. 超时事件提取的话,会用到另外一个函数类叫patternTimeOutFunction</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214232124.png"></p><p>首先，开发人员要在DataStream流上定义出模式条件，之后Flink CEP引擎进行模式检测，必要时生成告警。</p><p>为了使用Flink CEP，我们需要导入依赖：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-cep_$&#123;scala.binary.version&#125;&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h2 id="Event-Streams"><a href="#Event-Streams" class="headerlink" title="Event Streams"></a><strong>Event Streams</strong></h2><p>以登陆事件流为例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">case class LoginEvent(userId: String, ip: String, eventType: String, eventTime: String)</span><br><span class="line"></span><br><span class="line">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line">env.setParallelism(1)</span><br><span class="line"></span><br><span class="line">val loginEventStream = env.fromCollection(List(</span><br><span class="line">  LoginEvent(&quot;1&quot;, &quot;192.168.0.1&quot;, &quot;fail&quot;, &quot;1558430842&quot;),</span><br><span class="line">  LoginEvent(&quot;1&quot;, &quot;192.168.0.2&quot;, &quot;fail&quot;, &quot;1558430843&quot;),</span><br><span class="line">  LoginEvent(&quot;1&quot;, &quot;192.168.0.3&quot;, &quot;fail&quot;, &quot;1558430844&quot;),</span><br><span class="line">  LoginEvent(&quot;2&quot;, &quot;192.168.10.10&quot;, &quot;success&quot;, &quot;1558430845&quot;)</span><br><span class="line">)).assignAscendingTimestamps(_.eventTime.toLong)</span><br></pre></td></tr></table></figure><h2 id="Pattern-API"><a href="#Pattern-API" class="headerlink" title="Pattern API"></a><strong>Pattern API</strong></h2><p>每个Pattern都应该包含几个步骤，或者叫做state。从一个state到另一个state，通常我们需要定义一些条件，例如下列的代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val loginFailPattern = Pattern.begin[LoginEvent](&quot;begin&quot;)</span><br><span class="line">  .where(_.eventType.equals(&quot;fail&quot;))</span><br><span class="line">  .next(&quot;next&quot;)</span><br><span class="line">  .where(_.eventType.equals(&quot;fail&quot;))</span><br><span class="line">  .within(Time.seconds(10)</span><br></pre></td></tr></table></figure><p>每个state都应该有一个标示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例如 .begin[LoginEvent](&quot;begin&quot;)中的&quot;begin&quot;</span><br></pre></td></tr></table></figure><p>每个state都需要有一个唯一的名字，而且需要一个filter来过滤条件，这个过滤条件定义事件需要符合的条件，例如: </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.where(_.eventType.equals(&quot;fail&quot;))</span><br></pre></td></tr></table></figure><p>我们也可以通过subtype来限制event的子类型:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start.subtype(SubEvent.class).where(...);</span><br></pre></td></tr></table></figure><p>事实上，你可以多次调用subtype和where方法；而且如果where条件是不相关的，你可以通过or来指定一个单独的filter函数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pattern.where(...).or(...);</span><br></pre></td></tr></table></figure><p>之后，我们可以在此条件基础上，通过next或者followedBy方法切换到下一个state，next的意思是说上一步符合条件的元素之后紧挨着的元素；而followedBy并不要求一定是挨着的元素。这两者分别称为严格近邻和非严格近邻。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val strictNext = start.next(&quot;middle&quot;)</span><br><span class="line">val nonStrictNext = start.followedBy(&quot;middle&quot;)</span><br></pre></td></tr></table></figure><p>最后，我们可以将所有的Pattern的条件限定在一定的时间范围内：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">next.within(Time.seconds(10))</span><br></pre></td></tr></table></figure><p>这个时间可以是Processing Time，也可以是Event Time。</p><h2 id="Pattern-检测"><a href="#Pattern-检测" class="headerlink" title="Pattern 检测"></a><strong>Pattern 检测</strong></h2><p>通过一个input DataStream以及刚刚我们定义的Pattern，我们可以创建一个PatternStream：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val input = ...</span><br><span class="line">val pattern = ...</span><br><span class="line"></span><br><span class="line">val patternStream = CEP.pattern(input, pattern)</span><br><span class="line">val patternStream = CEP.pattern(loginEventStream.keyBy(_.userId), loginFailPattern)</span><br></pre></td></tr></table></figure><p>一旦获得PatternStream，我们就可以通过select或flatSelect，从一个Map序列找到我们需要的警告信息。</p><h2 id="select"><a href="#select" class="headerlink" title="select"></a><strong>select</strong></h2><p>select方法需要实现一个PatternSelectFunction，通过select方法来输出需要的警告。它接受一个Map对，包含string/event，其中key为state的名字，event则为真实的Event。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val loginFailDataStream = patternStream</span><br><span class="line">  .select((pattern: Map[String, Iterable[LoginEvent]]) =&gt; &#123;</span><br><span class="line">    val first = pattern.getOrElse(&quot;begin&quot;, null).iterator.next()</span><br><span class="line">    val second = pattern.getOrElse(&quot;next&quot;, null).iterator.next()</span><br><span class="line"></span><br><span class="line">    Warning(first.userId, first.eventTime, second.eventTime, &quot;warning&quot;)</span><br><span class="line">  &#125;)</span><br></pre></td></tr></table></figure><p>其返回值仅为1条记录。</p><h2 id="flatSelect"><a href="#flatSelect" class="headerlink" title="flatSelect"></a><strong>flatSelect</strong></h2><p>通过实现PatternFlatSelectFunction，实现与select相似的功能。唯一的区别就是flatSelect方法可以返回多条记录，它通过一个Collector[OUT]类型的参数来将要输出的数据传递到下游。</p><h2 id="超时事件的处理"><a href="#超时事件的处理" class="headerlink" title="超时事件的处理"></a><strong>超时事件的处理</strong></h2><p>通过within方法，我们的parttern规则将匹配的事件限定在一定的窗口范围内。当有超过窗口时间之后到达的event，我们可以通过在select或flatSelect中，实现PatternTimeoutFunction和PatternFlatTimeoutFunction来处理这种情况。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">val patternStream: PatternStream[Event] = CEP.pattern(input, pattern)</span><br><span class="line"></span><br><span class="line">val outputTag = OutputTag[String](&quot;side-output&quot;)</span><br><span class="line"></span><br><span class="line">val result: SingleOutputStreamOperator[ComplexEvent] = patternStream.select(outputTag)&#123;</span><br><span class="line">    (pattern: Map[String, Iterable[Event]], timestamp: Long) =&gt; TimeoutEvent()</span><br><span class="line">&#125; &#123;</span><br><span class="line">    pattern: Map[String, Iterable[Event]] =&gt; ComplexEvent()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val timeoutResult: DataStream&lt;TimeoutEvent&gt; = result.getSideOutput(outputTag)</span><br></pre></td></tr></table></figure><h1 id="附录-Flink的双流join"><a href="#附录-Flink的双流join" class="headerlink" title="附录:Flink的双流join"></a>附录:Flink的双流join</h1><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214232600.png"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Flink-CEP-代码案例&quot;&gt;&lt;a href=&quot;#Flink-CEP-代码案例&quot; class=&quot;headerlink&quot; title=&quot;Flink CEP 代码案例&quot;&gt;&lt;/a&gt;Flink CEP 代码案例&lt;/h1&gt;&lt;p&gt;登录告警系统:  一堆的登录日志从，匹配一个恶意登录的模式（如果一个用户连续失败三次，则是恶意登录），从而找到哪些用户名是用于恶意 登录&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-from-shangbaishuyao-wordCount/src/main/scala/com/shangbaishuyao/cep/TestCepDemo.scala&quot;&gt;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-from-shangbaishuyao-wordCount/src/main/scala/com/shangbaishuyao/cep/TestCepDemo.scala&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;登录失败CEP模型:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo09/Flink03_Practice_LoginFailWithCEP.java&quot;&gt;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo09/Flink03_Practice_LoginFailWithCEP.java&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;支付失败CEP模型:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo09/Flink04_Practice_OrderPayWithCEP.java&quot;&gt;https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo09/Flink04_Practice_OrderPayWithCEP.java&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: FlinkSQL的Table API 与SQL之函数（Functions）</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-FlinkSQL%E7%9A%84Table-API-%E4%B8%8ESQL%E4%B9%8B%E5%87%BD%E6%95%B0%EF%BC%88Functions%EF%BC%89/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-FlinkSQL%E7%9A%84Table-API-%E4%B8%8ESQL%E4%B9%8B%E5%87%BD%E6%95%B0%EF%BC%88Functions%EF%BC%89/</id>
    <published>2022-02-14T14:00:35.000Z</published>
    <updated>2022-02-14T14:14:24.887Z</updated>
    
    <content type="html"><![CDATA[<p>Flink Table 和 SQL内置了很多SQL中支持的函数；如果有无法满足的需要，则可以实现用户自定义的函数（UDF）来解决。</p><span id="more"></span><h2 id="系统内置函数"><a href="#系统内置函数" class="headerlink" title="系统内置函数"></a><strong>系统内置函数</strong></h2><p>Flink Table API 和 SQL为用户提供了一组用于数据转换的内置函数。SQL中支持的很多函数，Table API和SQL都已经做了实现，其它还在快速开发扩展中。</p><p>以下是一些典型函数的举例，全部的内置函数，可以参考官网介绍。</p><h3 id="1-比较函数"><a href="#1-比较函数" class="headerlink" title="1.比较函数"></a>1.比较函数</h3><p>SQL：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">value1 = value2</span><br><span class="line"></span><br><span class="line">value1 &gt; value2</span><br></pre></td></tr></table></figure><p>Table API：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ANY1 === ANY2</span><br><span class="line"></span><br><span class="line">ANY1 &gt; ANY2</span><br></pre></td></tr></table></figure><h3 id="2-逻辑函数"><a href="#2-逻辑函数" class="headerlink" title="2.逻辑函数"></a>2.逻辑函数</h3><p>SQL：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">boolean1 OR boolean2</span><br><span class="line"></span><br><span class="line">boolean IS FALSE</span><br><span class="line"></span><br><span class="line">NOT boolean</span><br></pre></td></tr></table></figure><p>Table API：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">BOOLEAN1 || BOOLEAN2</span><br><span class="line"></span><br><span class="line">BOOLEAN.isFalse</span><br><span class="line"></span><br><span class="line">!BOOLEAN</span><br></pre></td></tr></table></figure><h3 id="3-算术函数"><a href="#3-算术函数" class="headerlink" title="3.算术函数"></a>3.算术函数</h3><p>SQL：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">numeric1 + numeric2</span><br><span class="line"></span><br><span class="line">POWER(numeric1, numeric2)</span><br></pre></td></tr></table></figure><p>Table API：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">NUMERIC1 + NUMERIC2</span><br><span class="line"></span><br><span class="line">NUMERIC1.power(NUMERIC2)</span><br></pre></td></tr></table></figure><h3 id="4-字符串函数"><a href="#4-字符串函数" class="headerlink" title="4.字符串函数"></a>4.字符串函数</h3><p>SQL：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">string1 || string2</span><br><span class="line"></span><br><span class="line">UPPER(string)</span><br><span class="line"></span><br><span class="line">CHAR_LENGTH(string)</span><br></pre></td></tr></table></figure><p>Table API：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">STRING1 + STRING2</span><br><span class="line"></span><br><span class="line">STRING.upperCase()</span><br><span class="line"></span><br><span class="line">STRING.charLength()</span><br></pre></td></tr></table></figure><h3 id="5-时间函数"><a href="#5-时间函数" class="headerlink" title="5.时间函数"></a>5.时间函数</h3><p>SQL：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DATE string</span><br><span class="line"></span><br><span class="line">TIMESTAMP string</span><br><span class="line"></span><br><span class="line">CURRENT_TIME</span><br><span class="line"></span><br><span class="line">INTERVAL string range</span><br></pre></td></tr></table></figure><p>Table API：    </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">STRING.toDate</span><br><span class="line"></span><br><span class="line">STRING.toTimestamp</span><br><span class="line"></span><br><span class="line">currentTime()</span><br><span class="line"></span><br><span class="line">NUMERIC.days</span><br><span class="line"></span><br><span class="line">NUMERIC.minutes</span><br></pre></td></tr></table></figure><h3 id="6-聚合函数"><a href="#6-聚合函数" class="headerlink" title="6.聚合函数"></a>6.聚合函数</h3><p>SQL：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">COUNT(*)</span><br><span class="line"></span><br><span class="line">SUM([ ALL | DISTINCT ] expression)</span><br><span class="line"></span><br><span class="line">RANK()</span><br><span class="line"></span><br><span class="line">ROW_NUMBER()</span><br></pre></td></tr></table></figure><p>Table API：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FIELD.count</span><br><span class="line"></span><br><span class="line">FIELD.sum0  </span><br></pre></td></tr></table></figure><h2 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a><strong>UDF</strong></h2><p>​        用户定义函数（User-defined Functions，UDF）是一个重要的特性，因为它们显著地扩展了查询（Query）的表达能力。一些系统内置函数无法解决的需求，我们可以用UDF来自定义实现。</p><h3 id="注册用户自定义函数UDF"><a href="#注册用户自定义函数UDF" class="headerlink" title="注册用户自定义函数UDF"></a><strong>注册用户自定义函数UDF</strong></h3><p>​        在大多数情况下，用户定义的函数必须先注册，然后才能在查询中使用。不需要专门为Scala 的Table API注册函数。</p><p>函数通过调用registerFunction（）方法在TableEnvironment中注册。当用户定义的函数被注册时，它被插入到TableEnvironment的函数目录中，这样Table API或SQL解析器就可以识别并正确地解释它。</p><h3 id="标量函数（Scalar-Functions）"><a href="#标量函数（Scalar-Functions）" class="headerlink" title="标量函数（Scalar Functions）"></a><strong>标量函数（Scalar Functions）</strong></h3><p>​        用户定义的标量函数，可以将0、1或多个标量值，映射到新的标量值。</p><p>​        为了定义标量函数，必须在org.apache.flink.table.functions中扩展基类Scalar Function，并实现（一个或多个）求值（evaluation，eval）方法。标量函数的行为由求值方法决定，求值方法必须公开声明并命名为eval（直接def声明，没有override）。求值方法的参数类型和返回类型，确定了标量函数的参数和返回类型。</p><p>在下面的代码中，我们定义自己的HashCode函数，在TableEnvironment中注册它，并在查询中调用它。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// 自定义一个标量函数</span><br><span class="line">public static class HashCode extends ScalarFunction &#123;</span><br><span class="line">    private int factor = 13;</span><br><span class="line"></span><br><span class="line">    public HashCode(int factor) &#123;</span><br><span class="line">        this.factor = factor;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public int eval(String s) &#123;</span><br><span class="line">        return s.hashCode() * factor;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>主函数中调用，计算sensor id的哈希值（前面部分照抄，流环境、表环境、读取source、建表）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String[] args) throws Exception &#123;</span><br><span class="line">    // 1. 创建环境</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(1);</span><br><span class="line"></span><br><span class="line">    StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">    // 2. 读取文件，得到 DataStream</span><br><span class="line">    String filePath = &quot;..\\sensor.txt&quot;;</span><br><span class="line"></span><br><span class="line">    DataStream&lt;String&gt; inputStream = env.readTextFile(filePath);</span><br><span class="line"></span><br><span class="line">    // 3. 转换成 Java Bean，并指定timestamp和watermark</span><br><span class="line">    DataStream&lt;SensorReading&gt; dataStream = inputStream</span><br><span class="line">            .map( line -&gt; &#123;</span><br><span class="line">                String[] fields = line.split(&quot;,&quot;);</span><br><span class="line">                return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2]));</span><br><span class="line">            &#125; );</span><br><span class="line"></span><br><span class="line">    // 4. 将 DataStream 转换为 Table</span><br><span class="line">    Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;id, timestamp as ts, temperature&quot;);</span><br><span class="line"></span><br><span class="line">    // 5. 调用自定义hash函数，对id进行hash运算</span><br><span class="line">    HashCode hashCode = new HashCode(23);</span><br><span class="line">    tableEnv.registerFunction(&quot;hashCode&quot;, hashCode);</span><br><span class="line">    Table resultTable = sensorTable</span><br><span class="line">            .select(&quot;id, ts, hashCode(id)&quot;);</span><br><span class="line"></span><br><span class="line">    //  sql</span><br><span class="line">    tableEnv.createTemporaryView(&quot;sensor&quot;, sensorTable);</span><br><span class="line">    Table resultSqlTable = tableEnv.sqlQuery(&quot;select id, ts, hashCode(id) from sensor&quot;);</span><br><span class="line"></span><br><span class="line">    tableEnv.toAppendStream(resultTable, Row.class).print(&quot;result&quot;);</span><br><span class="line">    tableEnv.toRetractStream(resultSqlTable, Row.class).print(&quot;sql&quot;);</span><br><span class="line"></span><br><span class="line">    env.execute(&quot;scalar function test&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="表函数（Table-Functions）"><a href="#表函数（Table-Functions）" class="headerlink" title="表函数（Table Functions）"></a><strong>表函数（Table Functions）</strong></h3><p>与用户定义的标量函数类似，用户定义的表函数，可以将0、1或多个标量值作为输入参数；与标量函数不同的是，它可以返回任意数量的行作为输出，而不是单个值。</p><p>为了定义一个表函数，必须扩展org.apache.flink.table.functions中的基类TableFunction并实现（一个或多个）求值方法。表函数的行为由其求值方法决定，求值方法必须是public的，并命名为eval。求值方法的参数类型，决定表函数的所有有效参数。</p><p>返回表的类型由TableFunction的泛型类型确定。求值方法使用protected collect（T）方法发出输出行。</p><p>在Table API中，Table函数需要与.joinLateral或.leftOuterJoinLateral一起使用。</p><p>joinLateral算子，会将外部表中的每一行，与表函数（TableFunction，算子的参数是它的表达式）计算得到的所有行连接起来。</p><p>而leftOuterJoinLateral算子，则是左外连接，它同样会将外部表中的每一行与表函数计算生成的所有行连接起来；并且，对于表函数返回的是空表的外部行，也要保留下来。</p><p>在SQL中，则需要使用Lateral Table（<TableFunction>），或者带有ON TRUE条件的左连接。</p><p>下面的代码中，我们将定义一个表函数，在表环境中注册它，并在查询中调用它。</p><p>自定义TableFunction：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// 自定义TableFunction</span><br><span class="line">public static class Split extends TableFunction&lt;Tuple2&lt;String, Integer&gt;&gt; &#123;</span><br><span class="line">    private String separator = &quot;,&quot;;</span><br><span class="line"></span><br><span class="line">    public Split(String separator) &#123;</span><br><span class="line">        this.separator = separator;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 类似flatmap，没有返回值</span><br><span class="line">    public void eval(String str) &#123;</span><br><span class="line">        for (String s : str.split(separator)) &#123;</span><br><span class="line">            collect(new Tuple2&lt;String, Integer&gt;(s, s.length()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来，就是在代码中调用。首先是Table API的方式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Split split = new Split(&quot;_&quot;);</span><br><span class="line">tableEnv.registerFunction(&quot;split&quot;, split);</span><br><span class="line">Table resultTable = sensorTable</span><br><span class="line">        .joinLateral( &quot;split(id) as (word, length)&quot;)</span><br><span class="line">        .select(&quot;id, ts, word, length&quot;);</span><br></pre></td></tr></table></figure><p>然后是SQL的方式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.createTemporaryView(&quot;sensor&quot;, sensorTable);</span><br><span class="line">Table resultSqlTable = tableEnv.sqlQuery(&quot;select id, ts, word, length &quot; +</span><br><span class="line">        &quot;from sensor, lateral table( split(id) ) as splitId(word, length)&quot;);</span><br></pre></td></tr></table></figure><h3 id="聚合函数（Aggregate-Functions）"><a href="#聚合函数（Aggregate-Functions）" class="headerlink" title="聚合函数（Aggregate Functions）"></a><strong>聚合函数（Aggregate Functions）</strong></h3><p>​        用户自定义聚合函数（User-Defined Aggregate Functions，UDAGGs）可以把一个表中的数据，聚合成一个标量值。用户定义的聚合函数，是通过继承AggregateFunction抽象类实现的。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214220859.png"></p><p>上图中显示了一个聚合的例子。</p><p>假设现在有一张表，包含了各种饮料的数据。该表由三列（id、name和price）、五行组成数据。现在我们需要找到表中所有饮料的最高价格，即执行max（）聚合，结果将是一个数值。</p><p>AggregateFunction的工作原理如下。</p><ul><li> 首先，它需要一个累加器，用来保存聚合中间结果的数据结构（状态）。可以通过调用AggregateFunction的createAccumulator（）方法创建空累加器。</li><li> 随后，对每个输入行调用函数的accumulate（）方法来更新累加器。</li><li> 处理完所有行后，将调用函数的getValue（）方法来计算并返回最终结果。</li></ul><p>AggregationFunction要求必须实现的方法：</p><ul><li> createAccumulator()</li><li> accumulate()</li><li> getValue()</li></ul><p>​    除了上述方法之外，还有一些可选择实现的方法。其中一些方法，可以让系统执行查询更有效率，而另一些方法，对于某些场景是必需的。例如，如果聚合函数应用在会话窗口（session group window）的上下文中，则merge（）方法是必需的。</p><ul><li> retract() </li><li> merge() </li><li> resetAccumulator()</li></ul><p>接下来我们写一个自定义AggregateFunction，计算一下每个sensor的平均温度值.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// 定义AggregateFunction的Accumulator</span><br><span class="line">public static class AvgTempAcc &#123;</span><br><span class="line">    double sum = 0.0;</span><br><span class="line">    int count = 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 自定义一个聚合函数，求每个传感器的平均温度值，保存状态(tempSum, tempCount)</span><br><span class="line">public static class AvgTemp extends AggregateFunction&lt;Double, AvgTempAcc&gt;&#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Double getValue(AvgTempAcc accumulator) &#123;</span><br><span class="line">        return accumulator.sum / accumulator.count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public AvgTempAcc createAccumulator() &#123;</span><br><span class="line">        return new AvgTempAcc();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 实现一个具体的处理计算函数，accumulate</span><br><span class="line">    public void accumulate( AvgTempAcc accumulator, Double temp) &#123;</span><br><span class="line">        accumulator.sum += temp;</span><br><span class="line">        accumulator.count += 1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来就可以在代码中调用了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// 创建一个聚合函数实例</span><br><span class="line">AvgTemp avgTemp = new AvgTemp();</span><br><span class="line"></span><br><span class="line">// Table API的调用 </span><br><span class="line">tableEnv.registerFunction(&quot;avgTemp&quot;, avgTemp);</span><br><span class="line">Table resultTable = sensorTable</span><br><span class="line">        .groupBy(&quot;id&quot;)</span><br><span class="line">        .aggregate(&quot;avgTemp(temperature) as avgTemp&quot;)</span><br><span class="line">        .select(&quot;id, avgTemp&quot;);</span><br><span class="line">// sql</span><br><span class="line">tableEnv.createTemporaryView(&quot;sensor&quot;, sensorTable);</span><br><span class="line">Table resultSqlTable = tableEnv.sqlQuery(&quot;select id, avgTemp(temperature) &quot; +</span><br><span class="line">        &quot;from sensor group by id&quot;);</span><br><span class="line"></span><br><span class="line">tableEnv.toRetractStream(resultTable, Row.class).print(&quot;result&quot;);</span><br><span class="line">tableEnv.toRetractStream(resultSqlTable, Row.class).print(&quot;sql&quot;);</span><br></pre></td></tr></table></figure><h3 id="表聚合函数（Table-Aggregate-Functions）"><a href="#表聚合函数（Table-Aggregate-Functions）" class="headerlink" title="表聚合函数（Table Aggregate Functions）"></a><strong>表聚合函数（Table Aggregate Functions）</strong></h3><p>​        用户定义的表聚合函数（User-Defined Table Aggregate Functions，UDTAGGs），可以把一个表中数据，聚合为具有多行和多列的结果表。这跟AggregateFunction非常类似，只是之前聚合结果是一个标量值，现在变成了一张表。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214221100.png"></p><p>​        比如现在我们需要找到表中所有饮料的前2个最高价格，即执行top2（）表聚合。我们需要检查5行中的每一行，得到的结果将是一个具有排序后前2个值的表。</p><p>用户定义的表聚合函数，是通过继承TableAggregateFunction抽象类来实现的。</p><p>TableAggregateFunction的工作原理如下。</p><ul><li><p> 首先，它同样需要一个累加器（Accumulator），它是保存聚合中间结果的数据结构。通过调用TableAggregateFunction的createAccumulator（）方法可以创建空累加器。</p></li><li><p> 随后，对每个输入行调用函数的accumulate（）方法来更新累加器。</p></li><li><p>处理完所有行后，将调用函数的emitValue（）方法来计算并返回最终结果。</p></li></ul><p>AggregationFunction要求必须实现的方法：</p><ul><li> createAccumulator()</li><li> accumulate()</li></ul><p>除了上述方法之外，还有一些可选择实现的方法。</p><ul><li><p> retract() </p></li><li><p> merge() </p></li><li><p> resetAccumulator() </p></li><li><p> emitValue() </p></li><li><p>emitUpdateWithRetract()</p></li></ul><p>接下来我们写一个自定义TableAggregateFunction，用来提取每个sensor最高的两个温度值。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">// 先定义一个 Accumulator </span><br><span class="line">public static class Top2TempAcc &#123;</span><br><span class="line">    double highestTemp = Double.MIN_VALUE;</span><br><span class="line">    double secondHighestTemp = Double.MIN_VALUE;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 自定义表聚合函数</span><br><span class="line">public static class Top2Temp extends TableAggregateFunction&lt;Tuple2&lt;Double, Integer&gt;, Top2TempAcc&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Top2TempAcc createAccumulator() &#123;</span><br><span class="line">        return new Top2TempAcc();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 实现计算聚合结果的函数accumulate</span><br><span class="line">    public void accumulate(Top2TempAcc acc, Double temp) &#123;</span><br><span class="line">        if (temp &gt; acc.highestTemp) &#123;</span><br><span class="line">            acc.secondHighestTemp = acc.highestTemp;</span><br><span class="line">            acc.highestTemp = temp;</span><br><span class="line">        &#125; else if (temp &gt; acc.secondHighestTemp) &#123;</span><br><span class="line">            acc.secondHighestTemp = temp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    // 实现一个输出结果的方法，最终处理完表中所有数据时调用</span><br><span class="line">    public void emitValue(Top2TempAcc acc, Collector&lt;Tuple2&lt;Double, Integer&gt;&gt; out) &#123;</span><br><span class="line">        out.collect(new Tuple2&lt;&gt;(acc.highestTemp, 1));</span><br><span class="line">        out.collect(new Tuple2&lt;&gt;(acc.secondHighestTemp, 2));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来就可以在代码中调用了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 创建一个表聚合函数实例</span><br><span class="line">Top2Temp top2Temp = new Top2Temp();</span><br><span class="line">tableEnv.registerFunction(&quot;top2Temp&quot;, top2Temp);</span><br><span class="line">Table resultTable = sensorTable</span><br><span class="line">        .groupBy(&quot;id&quot;)</span><br><span class="line">        .flatAggregate(&quot;top2Temp(temperature) as (temp, rank)&quot;)</span><br><span class="line">        .select(&quot;id, temp, rank&quot;);</span><br><span class="line"></span><br><span class="line">tableEnv.toRetractStream(resultTable, Row.class).print(&quot;result&quot;);</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;Flink Table 和 SQL内置了很多SQL中支持的函数；如果有无法满足的需要，则可以实现用户自定义的函数（UDF）来解决。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: FlinkSQL的Table API 与SQL之窗口（Windows）</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-FlinkSQL%E7%9A%84Table-API-%E4%B8%8ESQL%E4%B9%8B%E7%AA%97%E5%8F%A3%EF%BC%88Windows%EF%BC%89/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-FlinkSQL%E7%9A%84Table-API-%E4%B8%8ESQL%E4%B9%8B%E7%AA%97%E5%8F%A3%EF%BC%88Windows%EF%BC%89/</id>
    <published>2022-02-14T12:21:59.000Z</published>
    <updated>2022-02-14T13:59:27.017Z</updated>
    
    <content type="html"><![CDATA[<p>​        时间语义，要配合窗口操作才能发挥作用。最主要的用途，当然就是开窗口、根据时间段做计算了。下面我们就来看看Table API和SQL中，怎么利用时间字段做窗口操作。</p><p>在Table API和SQL中，主要有两种窗口：Group Windows和Over Windows</p><span id="more"></span><h2 id="分组窗口（Group-Windows）"><a href="#分组窗口（Group-Windows）" class="headerlink" title="分组窗口（Group Windows）"></a><strong>分组窗口（Group Windows）</strong></h2><p>分组窗口（Group Windows）会根据时间或行计数间隔，将行聚合到有限的组（Group）中，并对每个组的数据执行一次聚合函数。</p><p>Table API中的Group Windows都是使用.window（w:GroupWindow）子句定义的，并且必须由as子句指定一个别名。为了按窗口对表进行分组，窗口的别名必须在group by子句中，像常规的分组字段一样引用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Table table = input</span><br><span class="line">  .window([w: GroupWindow] as &quot;w&quot;) // 定义窗口，别名 w</span><br><span class="line">  .groupBy(&quot;w, a&quot;)  // 以属性a和窗口w作为分组的key </span><br><span class="line">  .select(&quot;a, b.sum&quot;)  // 聚合字段b的值，求和</span><br></pre></td></tr></table></figure><p>或者，还可以把窗口的相关信息，作为字段添加到结果表中：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Table table = input</span><br><span class="line">  .window([w: GroupWindow] as &quot;w&quot;) </span><br><span class="line">  .groupBy(&quot;w, a&quot;) </span><br><span class="line">  .select(&quot;a, w.start, w.end, w.rowtime, b.count&quot;)</span><br></pre></td></tr></table></figure><p>Table API提供了一组具有特定语义的预定义Window类，这些类会被转换为底层DataStream或DataSet的窗口操作。</p><p>Table API支持的窗口定义，和我们熟悉的一样，主要也是三种：滚动（Tumbling）、滑动（Sliding）和会话（Session）。</p><h3 id="滚动窗口"><a href="#滚动窗口" class="headerlink" title="滚动窗口"></a><strong>滚动窗口</strong></h3><p>滚动窗口（Tumbling windows）要用Tumble类来定义，另外还有三个方法：</p><ul><li> over：定义窗口长度</li><li> on：用来分组（按时间间隔）或者排序（按行数）的时间字段</li><li> as：别名，必须出现在后面的groupBy中</li></ul><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// Tumbling Event-time Window</span><br><span class="line">.window(Tumble.over(&quot;10.minutes&quot;).on(&quot;rowtime&quot;).as(&quot;w&quot;))</span><br><span class="line"></span><br><span class="line">// Tumbling Processing-time Window</span><br><span class="line">.window(Tumble.over(&quot;10.minutes&quot;).on(&quot;proctime&quot;).as(&quot;w&quot;))</span><br><span class="line"></span><br><span class="line">// Tumbling Row-count Window</span><br><span class="line">.window(Tumble.over(&quot;10.rows&quot;).on(&quot;proctime&quot;).as(&quot;w&quot;))</span><br></pre></td></tr></table></figure><h3 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h3><p>滑动窗口（Sliding windows）要用Slide类来定义，另外还有四个方法：</p><ul><li> over：定义窗口长度</li><li> every：定义滑动步长</li><li> on：用来分组（按时间间隔）或者排序（按行数）的时间字段</li><li> as：别名，必须出现在后面的groupBy中</li></ul><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// Sliding Event-time Window</span><br><span class="line">.window(Slide.over(&quot;10.minutes&quot;).every(&quot;5.minutes&quot;).on(&quot;rowtime&quot;).as(&quot;w&quot;))</span><br><span class="line"></span><br><span class="line">// Sliding Processing-time window </span><br><span class="line">.window(Slide.over(&quot;10.minutes&quot;).every(&quot;5.minutes&quot;).on(&quot;proctime&quot;).as(&quot;w&quot;))</span><br><span class="line"></span><br><span class="line">// Sliding Row-count window</span><br><span class="line">.window(Slide.over(&quot;10.rows&quot;).every(&quot;5.rows&quot;).on(&quot;proctime&quot;).as(&quot;w&quot;))</span><br></pre></td></tr></table></figure><h3 id="会话窗口"><a href="#会话窗口" class="headerlink" title="会话窗口"></a><strong>会话窗口</strong></h3><p>会话窗口（Session windows）要用Session类来定义，另外还有三个方法：</p><ul><li> withGap：会话时间间隔</li><li> on：用来分组（按时间间隔）或者排序（按行数）的时间字段</li><li> as：别名，必须出现在后面的groupBy中</li></ul><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Session Event-time Window</span><br><span class="line">.window(Session.withGap.(&quot;10.minutes&quot;).on(&quot;rowtime&quot;).as(&quot;w&quot;))</span><br><span class="line"></span><br><span class="line">// Session Processing-time Window</span><br><span class="line">.window(Session.withGap.(&quot;10.minutes&quot;).on(“proctime&quot;).as(&quot;w&quot;))</span><br></pre></td></tr></table></figure><h2 id="Over-Windows"><a href="#Over-Windows" class="headerlink" title="Over Windows"></a><strong>Over Windows</strong></h2><p>Over window聚合是标准SQL中已有的（Over子句），可以在查询的SELECT子句中定义。Over window 聚合，会针对每个输入行，计算相邻行范围内的聚合。Over windows</p><p>使用.window（w:overwindows*）子句定义，并在select（）方法中通过别名来引用。</p><p>比如这样：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Table table = input</span><br><span class="line">  .window([w: OverWindow] as &quot;w&quot;)</span><br><span class="line">  .select(&quot;a, b.sum over w, c.min over w&quot;)</span><br></pre></td></tr></table></figure><p>Table API提供了Over类，来配置Over窗口的属性。可以在事件时间或处理时间，以及指定为时间间隔、或行计数的范围内，定义Over windows。</p><p>无界的over window是使用常量指定的。也就是说，时间间隔要指定UNBOUNDED_RANGE，或者行计数间隔要指定UNBOUNDED_ROW。而有界的over window是用间隔的大小指定的。</p><p>实际代码应用如下：</p><p>1） 无界的 over window</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 无界的事件时间 over window</span><br><span class="line">.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;rowtime&quot;).preceding.(UNBOUNDED_RANGE).as(&quot;w&quot;))</span><br><span class="line"></span><br><span class="line">// 无界的处理时间 over window</span><br><span class="line">.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;proctime&quot;).preceding.(UNBOUNDED_RANGE).as(&quot;w&quot;))</span><br><span class="line"></span><br><span class="line">// 无界的事件时间 Row-count over window</span><br><span class="line">.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;rowtime&quot;).preceding.(UNBOUNDED_ROW).as(&quot;w&quot;))</span><br><span class="line"></span><br><span class="line">//无界的处理时间 Row-count over window</span><br><span class="line">.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;proctime&quot;).preceding.(UNBOUNDED_ROW).as(&quot;w&quot;))</span><br></pre></td></tr></table></figure><p>2） 有界的over window</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 有界的事件时间 over window</span><br><span class="line">.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;rowtime&quot;).preceding(&quot;1.minutes&quot;).as(&quot;w&quot;))</span><br><span class="line">        </span><br><span class="line">// 有界的处理时间 over window</span><br><span class="line">.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;proctime&quot;).preceding(&quot;1.minutes&quot;).as(&quot;w&quot;))</span><br><span class="line">        </span><br><span class="line">// 有界的事件时间 Row-count over window</span><br><span class="line">.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;rowtime&quot;).preceding(&quot;10.rows&quot;).as(&quot;w&quot;))</span><br><span class="line">        </span><br><span class="line">// 有界的处理时间 Row-count over window</span><br><span class="line">.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;procime&quot;).preceding(&quot;10.rows&quot;).as(&quot;w&quot;))</span><br></pre></td></tr></table></figure><h2 id="SQL中窗口的定义"><a href="#SQL中窗口的定义" class="headerlink" title="SQL中窗口的定义"></a>SQL中窗口的定义</h2><p>我们已经了解了在Table API里window的调用方式，同样，我们也可以在SQL中直接加入窗口的定义和使用。</p><h3 id="Group-Windows"><a href="#Group-Windows" class="headerlink" title="Group Windows"></a><strong>Group Windows</strong></h3><p>Group Windows在SQL查询的Group BY子句中定义。与使用常规GROUP BY子句的查询一样，使用GROUP BY子句的查询会计算每个组的单个结果行。</p><p>SQL支持以下Group窗口函数:</p><ol><li>TUMBLE(time_attr, interval)</li></ol><p>​        定义一个滚动窗口，第一个参数是时间字段，第二个参数是窗口长度。</p><ol start="2"><li>HOP(time_attr, interval, interval)</li></ol><p>​        定义一个滑动窗口，第一个参数是时间字段，第二个参数是窗口滑动步长，第三个是窗口长度。</p><ol start="3"><li>SESSION(time_attr, interval)</li></ol><p>​        定义一个会话窗口，第一个参数是时间字段，第二个参数是窗口间隔（Gap）。</p><p>另外还有一些辅助函数，可以用来选择Group Window的开始和结束时间戳，以及时间属性。</p><p>这里只写TUMBLE_*，滑动和会话窗口是类似的（HOP_*，SESSION_*）。</p><ul><li> TUMBLE_START(time_attr, interval)</li><li> TUMBLE_END(time_attr, interval)</li><li> TUMBLE_ROWTIME(time_attr, interval)</li><li> TUMBLE_PROCTIME(time_attr, interval)</li></ul><h3 id="Over-Windows-1"><a href="#Over-Windows-1" class="headerlink" title="Over Windows"></a><strong>Over Windows</strong></h3><p>​        由于Over本来就是SQL内置支持的语法，所以这在SQL中属于基本的聚合操作。所有聚合必须在同一窗口上定义，也就是说，必须是相同的分区、排序和范围。目前仅支持在当前行范围之前的窗口（无边界和有边界）。</p><p>注意，ORDER BY必须在单一的时间属性上指定。</p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">SELECT COUNT(amount) OVER (</span><br><span class="line">  PARTITION BY user</span><br><span class="line">  ORDER BY proctime</span><br><span class="line">  ROWS BETWEEN 2 PRECEDING AND CURRENT ROW)</span><br><span class="line">FROM Orders</span><br><span class="line"></span><br><span class="line">// 也可以做多个聚合</span><br><span class="line">SELECT COUNT(amount) OVER w, SUM(amount) OVER w</span><br><span class="line">FROM Orders</span><br><span class="line">WINDOW w AS (</span><br><span class="line">  PARTITION BY user</span><br><span class="line">  ORDER BY proctime</span><br><span class="line">  ROWS BETWEEN 2 PRECEDING AND CURRENT ROW)</span><br></pre></td></tr></table></figure><h3 id="代码练习（以分组滚动窗口为例）"><a href="#代码练习（以分组滚动窗口为例）" class="headerlink" title="代码练习（以分组滚动窗口为例）"></a><strong>代码练习（以分组滚动窗口为例）</strong></h3><p>我们可以综合学习过的内容，用一段完整的代码实现一个具体的需求。例如，可以开一个滚动窗口，统计10秒内出现的每个sensor的个数。</p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        // 1. 创建环境</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(1);</span><br><span class="line">        // 设置事件时间</span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">        // 2. 读取文件，得到 DataStream</span><br><span class="line">        String filePath = &quot;..\\sensor.txt&quot;;</span><br><span class="line">        DataStream&lt;String&gt; inputStream = env.readTextFile(filePath);</span><br><span class="line"></span><br><span class="line">        // 3. 转换成 Java Bean，并指定timestamp和watermark</span><br><span class="line">        DataStream&lt;SensorReading&gt; dataStream = inputStream</span><br><span class="line">                .map( line -&gt; &#123;</span><br><span class="line">                    String[] fields = line.split(&quot;,&quot;);</span><br><span class="line">                    return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2]));</span><br><span class="line">                &#125; )</span><br><span class="line">                .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;SensorReading&gt;(Time.seconds(1)) &#123;</span><br><span class="line">                    @Override</span><br><span class="line">                    public long extractTimestamp(SensorReading element) &#123;</span><br><span class="line">                        return element.getTimestamp() * 1000L;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        // 4. 将 DataStream 转换为 Table</span><br><span class="line">        Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;id, timestamp.rowtime as ts, temperature&quot;);</span><br><span class="line"></span><br><span class="line">        // 5. 开窗聚合</span><br><span class="line">        Table resultTable = sensorTable</span><br><span class="line">                .window(Tumble.over(&quot;10.seconds&quot;).on(&quot;ts&quot;).as(&quot;tw&quot;))</span><br><span class="line">                // 每10秒统计一次，滚动时间窗口</span><br><span class="line">                .groupBy(&quot;id, tw&quot;)</span><br><span class="line">                .select(&quot;id, id.count, temperature.avg, tw.end&quot;);</span><br><span class="line"></span><br><span class="line">        //  sql</span><br><span class="line">        tableEnv.createTemporaryView(&quot;sensor&quot;, sensorTable);</span><br><span class="line">        Table resultSqlTable = tableEnv.sqlQuery(</span><br><span class="line">                &quot;select id, count(id), avg(temperature), tumble_end(ts, interval &#x27;10&#x27; second) &quot; +</span><br><span class="line">                        &quot;from sensor group by id, tumble(ts, interval &#x27;10&#x27; second)&quot;);</span><br><span class="line"></span><br><span class="line">        // 转换成流打印输出</span><br><span class="line">        tableEnv.toAppendStream(resultTable, Row.class).print(&quot;result&quot;);</span><br><span class="line">        tableEnv.toAppendStream(resultSqlTable, Row.class).print(&quot;sql&quot;);</span><br><span class="line"></span><br><span class="line">        env.execute(&quot;time and window test&quot;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;​        时间语义，要配合窗口操作才能发挥作用。最主要的用途，当然就是开窗口、根据时间段做计算了。下面我们就来看看Table API和SQL中，怎么利用时间字段做窗口操作。&lt;/p&gt;
&lt;p&gt;在Table API和SQL中，主要有两种窗口：Group Windows和Over Windows&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 原理与实现: FlinkSQL的Table API 与SQL之流处理中的特殊概念</title>
    <link href="http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-FlinkSQL%E7%9A%84Table-API-%E4%B8%8ESQL%E4%B9%8B%E6%B5%81%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E7%89%B9%E6%AE%8A%E6%A6%82%E5%BF%B5/"/>
    <id>http://xubatian.cn/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-FlinkSQL%E7%9A%84Table-API-%E4%B8%8ESQL%E4%B9%8B%E6%B5%81%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E7%89%B9%E6%AE%8A%E6%A6%82%E5%BF%B5/</id>
    <published>2022-02-14T12:07:03.000Z</published>
    <updated>2022-02-15T02:27:43.722Z</updated>
    
    <content type="html"><![CDATA[<p>​            Table API和SQL，本质上还是基于关系型表的操作方式；而关系型表、关系代数，以及SQL本身，一般是有界的，更适合批处理的场景。这就导致在进行流处理的过程中，理解会稍微复杂一些，需要引入一些特殊概念。</p><span id="more"></span><h2 id="流处理和关系代数（表，及SQL）的区别"><a href="#流处理和关系代数（表，及SQL）的区别" class="headerlink" title="流处理和关系代数（表，及SQL）的区别"></a><strong>流处理和关系代数（表，及SQL）的区别</strong></h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214200752.png"></p><p>可以看到，其实关系代数（主要就是指关系型数据库中的表）和SQL，主要就是针对批处理的，这和流处理有天生的隔阂。</p><h2 id="动态表（Dynamic-Tables）"><a href="#动态表（Dynamic-Tables）" class="headerlink" title="动态表（Dynamic Tables）"></a><strong>动态表（Dynamic Tables）</strong></h2><p>因为流处理面对的数据，是连续不断的，这和我们熟悉的关系型数据库中保存的“表”完全不同。所以，如果我们把流数据转换成Table，然后执行类似于table的select操作，结果就不是一成不变的，而是随着新数据的到来，会不停更新。</p><p>我们可以随着新数据的到来，不停地在之前的基础上更新结果。这样得到的表，在Flink Table API概念里，就叫做“<strong>动态表</strong>”（Dynamic Tables）。</p><p>动态表是Flink对流数据的Table API和SQL支持的核心概念。与表示批处理数据的静态表不同，动态表是随时间变化的。动态表可以像静态的批处理表一样进行查询，查询一个动态表会产生持续查询（Continuous Query）。连续查询永远不会终止，并会生成另一个动态表。查询（Query）会不断更新其动态结果表，以反映其动态输入表上的更改。</p><h2 id="流式持续查询的过程"><a href="#流式持续查询的过程" class="headerlink" title="流式持续查询的过程"></a><strong>流式持续查询的过程</strong></h2><p>下图显示了流、动态表和连续查询的关系：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214200844.png"></p><p>流式持续查询的过程为：</p><ol><li><p>流被转换为动态表。</p></li><li><p>对动态表计算连续查询，生成新的动态表。</p></li><li><p>生成的动态表被转换回流。</p></li></ol><h3 id="将流转换成表（Table）"><a href="#将流转换成表（Table）" class="headerlink" title="将流转换成表（Table）"></a><strong>将流转换成表（Table）</strong></h3><p>为了处理带有关系查询的流，必须先将其转换为表。</p><p>从概念上讲，流的每个数据记录，都被解释为对结果表的插入（Insert）修改。因为流式持续不断的，而且之前的输出结果无法改变。本质上，我们其实是从一个、只有插入操作的changelog（更新日志）流，来构建一个表。</p><p>为了更好地说明动态表和持续查询的概念，我们来举一个具体的例子。</p><p>比如，我们现在的输入数据，就是用户在网站上的访问行为，数据类型（Schema）如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  user:  VARCHAR,   // 用户名</span><br><span class="line">  cTime: TIMESTAMP, // 访问某个URL的时间戳</span><br><span class="line">  url:   VARCHAR    // 用户访问的URL</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>下图显示了如何将访问URL事件流，或者叫点击事件流（左侧）转换为表（右侧）。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214200937.png"></p><p>随着插入更多的访问事件流记录，生成的表将不断增长。</p><h3 id="持续查询（Continuous-Query）"><a href="#持续查询（Continuous-Query）" class="headerlink" title="持续查询（Continuous Query）"></a><strong>持续查询（Continuous Query）</strong></h3><p>持续查询，会在动态表上做计算处理，并作为结果生成新的动态表。与批处理查询不同，连续查询从不终止，并根据输入表上的更新更新其结果表。</p><p>在任何时间点，连续查询的结果在语义上，等同于在输入表的快照上，以批处理模式执行的同一查询的结果。</p><p>在下面的示例中，我们展示了对点击事件流中的一个持续查询。</p><p>这个Query很简单，是一个分组聚合做count统计的查询。它将用户字段上的clicks表分组，并统计访问的url数。图中显示了随着时间的推移，当clicks表被其他行更新时如何计算查询。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214201010.png"></p><h3 id="将动态表转换成流"><a href="#将动态表转换成流" class="headerlink" title="将动态表转换成流"></a><strong>将动态表转换成流</strong></h3><p>与常规的数据库表一样，动态表可以通过插入（Insert）、更新（Update）和删除（Delete）更改，进行持续的修改。将动态表转换为流或将其写入外部系统时，需要对这些更改进行编码。Flink的Table API和SQL支持三种方式对动态表的更改进行编码：</p><h4 id="1）仅追加（Append-only）流"><a href="#1）仅追加（Append-only）流" class="headerlink" title="1）仅追加（Append-only）流"></a><strong>1）仅追加（Append-only）流</strong></h4><p>仅通过插入（Insert）更改，来修改的动态表，可以直接转换为“仅追加”流。这个流中发出的数据，就是动态表中新增的每一行。</p><h4 id="2）撤回（Retract）流"><a href="#2）撤回（Retract）流" class="headerlink" title="2）撤回（Retract）流"></a><strong>2）撤回（Retract）流</strong></h4><p>Retract流是包含两类消息的流，添加（Add）消息和撤回（Retract）消息。</p><p>动态表通过将INSERT 编码为add消息、DELETE 编码为retract消息、UPDATE编码为被更改行（前一行）的retract消息和更新后行（新行）的add消息，转换为retract流。</p><p>下图显示了将动态表转换为Retract流的过程。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214201708.png"> </p><h4 id="3）Upsert（更新插入）流"><a href="#3）Upsert（更新插入）流" class="headerlink" title="3）Upsert（更新插入）流"></a><strong>3）Upsert（更新插入）流</strong></h4><p>Upsert流包含两种类型的消息：Upsert消息和delete消息。转换为upsert流的动态表，需要有唯一的键（key）。</p><p>通过将INSERT和UPDATE更改编码为upsert消息，将DELETE更改编码为DELETE消息，就可以将具有唯一键（Unique Key）的动态表转换为流。</p><p>下图显示了将动态表转换为upsert流的过程。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214201105.png"></p><p>这些概念我们之前都已提到过。需要注意的是，在代码里将动态表转换为DataStream时，仅支持Append和Retract流。而向外部系统输出动态表的TableSink接口，则可以有不同的实现，比如之前我们讲到的ES，就可以有Upsert模式。</p><h2 id="时间特性"><a href="#时间特性" class="headerlink" title="时间特性"></a><strong>时间特性</strong></h2><p>基于时间的操作（比如Table API和SQL中窗口操作），需要定义相关的时间语义和时间数据来源的信息。所以，Table可以提供一个逻辑上的时间字段，用于在表处理程序中，指示时间和访问相应的时间戳。</p><p>时间属性，可以是每个表schema的一部分。一旦定义了时间属性，它就可以作为一个字段引用，并且可以在基于时间的操作中使用。</p><p>时间属性的行为类似于常规时间戳，可以访问，并且进行计算。</p><h3 id="处理时间（Processing-Time）"><a href="#处理时间（Processing-Time）" class="headerlink" title="处理时间（Processing Time）"></a><strong>处理时间（Processing Time）</strong></h3><p>处理时间语义下，允许表处理程序根据机器的本地时间生成结果。它是时间的最简单概念。它既不需要提取时间戳，也不需要生成watermark。</p><p>定义处理时间属性有三种方法：在DataStream转化时直接指定；在定义Table Schema时指定；在创建表的DDL中指定。</p><h4 id="1-DataStream转化成Table时指定"><a href="#1-DataStream转化成Table时指定" class="headerlink" title="1) DataStream转化成Table时指定"></a><strong>1)</strong> <em>DataStream转化成Table时指定</em></h4><p>由DataStream转换成表时，可以在后面指定字段名来定义Schema。在定义Schema期间，可以使用.proctime，定义处理时间字段。</p><p>注意，这个proctime属性只能通过附加逻辑字段，来扩展物理schema。因此，只能在schema定义的末尾定义它。</p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// 定义好 DataStream</span><br><span class="line">DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;\\sensor.txt&quot;)</span><br><span class="line">DataStream&lt;SensorReading&gt; dataStream = inputStream</span><br><span class="line">        .map( line -&gt; &#123;</span><br><span class="line">            String[] fields = line.split(&quot;,&quot;);</span><br><span class="line">            return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2]));</span><br><span class="line">        &#125; );</span><br><span class="line"></span><br><span class="line">// 将 DataStream转换为 Table，并指定时间字段</span><br><span class="line">Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;id, temperature, timestamp, pt.proctime&quot;);</span><br></pre></td></tr></table></figure><h4 id="2-定义Table-Schema时指定"><a href="#2-定义Table-Schema时指定" class="headerlink" title="2) 定义Table Schema时指定"></a><strong>2)</strong> <strong>定义Table Schema时指定</strong></h4><p>这种方法其实也很简单，只要在定义Schema的时候，加上一个新的字段，并指定成proctime就可以了。</p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.connect(</span><br><span class="line">  new FileSystem().path(&quot;..\\sensor.txt&quot;))</span><br><span class="line">  .withFormat(new Csv())</span><br><span class="line">  .withSchema(new Schema()</span><br><span class="line">    .field(&quot;id&quot;, DataTypes.STRING())</span><br><span class="line">    .field(&quot;timestamp&quot;, DataTypes.BIGINT())</span><br><span class="line">    .field(&quot;temperature&quot;, DataTypes.DOUBLE())</span><br><span class="line">    .field(&quot;pt&quot;, DataTypes.TIMESTAMP(3))</span><br><span class="line">      .proctime()    // 指定 pt字段为处理时间</span><br><span class="line">  ) // 定义表结构</span><br><span class="line">  .createTemporaryTable(&quot;inputTable&quot;); // 创建临时表</span><br></pre></td></tr></table></figure><h4 id="3-创建表的DDL中指定"><a href="#3-创建表的DDL中指定" class="headerlink" title="3) 创建表的DDL中指定"></a><strong>3)</strong> <strong>创建表的DDL中指定</strong></h4><p>在创建表的DDL中，增加一个字段并指定成proctime，也可以指定当前的时间字段。</p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">String sinkDDL = &quot;create table dataTable (&quot; +</span><br><span class="line">        &quot; id varchar(20) not null, &quot; +</span><br><span class="line">        &quot; ts bigint, &quot; +</span><br><span class="line">        &quot; temperature double, &quot; +</span><br><span class="line">        &quot; pt AS PROCTIME() &quot; +</span><br><span class="line">        &quot;) with (&quot; +</span><br><span class="line">        &quot; &#x27;connector.type&#x27; = &#x27;filesystem&#x27;, &quot; +</span><br><span class="line">        &quot; &#x27;connector.path&#x27; = &#x27;/sensor.txt&#x27;, &quot; +</span><br><span class="line">        &quot; &#x27;format.type&#x27; = &#x27;csv&#x27;)&quot;;</span><br><span class="line"></span><br><span class="line">tableEnv.sqlUpdate(sinkDDL);</span><br></pre></td></tr></table></figure><p>注意：运行这段DDL，必须使用Blink Planner。</p><h3 id="事件时间（Event-Time）"><a href="#事件时间（Event-Time）" class="headerlink" title="事件时间（Event Time）"></a><strong>事件时间（Event Time）</strong></h3><p>事件时间语义，允许表处理程序根据每个记录中包含的时间生成结果。这样即使在有乱序事件或者延迟事件时，也可以获得正确的结果。</p><p>为了处理无序事件，并区分流中的准时和迟到事件；Flink需要从事件数据中，提取时间戳，并用来推进事件时间的进展（watermark）。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214201354.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;\\sensor.txt&quot;)</span><br><span class="line">DataStream&lt;SensorReading&gt; dataStream = inputStream</span><br><span class="line">        .map( line -&gt; &#123;</span><br><span class="line">            String[] fields = line.split(&quot;,&quot;);</span><br><span class="line">            return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2]));</span><br><span class="line">        &#125; )</span><br><span class="line">        .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;SensorReading&gt;(Time.seconds(1)) &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public long extractTimestamp(SensorReading element) &#123;</span><br><span class="line">                return element.getTimestamp() * 1000L;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;id, timestamp.rowtime as ts, temperature&quot;);</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214201454.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.connect(</span><br><span class="line">  new FileSystem().path(&quot;sensor.txt&quot;))</span><br><span class="line">  .withFormat(new Csv())</span><br><span class="line">  .withSchema(new Schema()</span><br><span class="line">    .field(&quot;id&quot;, DataTypes.STRING())</span><br><span class="line">    .field(&quot;timestamp&quot;, DataTypes.BIGINT())</span><br><span class="line">      .rowtime(</span><br><span class="line">        new Rowtime()</span><br><span class="line">          .timestampsFromField(&quot;timestamp&quot;)    // 从字段中提取时间戳</span><br><span class="line">          .watermarksPeriodicBounded(1000)    // watermark延迟1秒</span><br><span class="line">      )</span><br><span class="line">    .field(&quot;temperature&quot;, DataTypes.DOUBLE())</span><br><span class="line">  ) // 定义表结构</span><br><span class="line">  .createTemporaryTable(&quot;inputTable&quot;); // 创建临时表</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220214201528.png"></p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">String sinkDDL = &quot;create table dataTable (&quot; +</span><br><span class="line">        &quot; id varchar(20) not null, &quot; +</span><br><span class="line">        &quot; ts bigint, &quot; +</span><br><span class="line">        &quot; temperature double, &quot; +</span><br><span class="line">        &quot; rt AS TO_TIMESTAMP( FROM_UNIXTIME(ts) ), &quot; +</span><br><span class="line">        &quot; watermark for rt as rt - interval &#x27;1&#x27; second&quot; +</span><br><span class="line">        &quot;) with (&quot; +</span><br><span class="line">        &quot; &#x27;connector.type&#x27; = &#x27;filesystem&#x27;, &quot; +</span><br><span class="line">        &quot; &#x27;connector.path&#x27; = &#x27;/sensor.txt&#x27;, &quot; +</span><br><span class="line">        &quot; &#x27;format.type&#x27; = &#x27;csv&#x27;)&quot;;</span><br><span class="line"></span><br><span class="line">tableEnv.sqlUpdate(sinkDDL);</span><br></pre></td></tr></table></figure><p>这里<strong>FROM_UNIXTIME</strong>是系统内置的时间函数，用来将一个整数（秒数）转换成“YYYY-MM-DD hh:mm:ss”格式（默认，也可以作为第二个String参数传入）的日期时间字符串（date time string）；然后再用<strong>TO_TIMESTAMP</strong>将其转换成Timestamp。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;​            Table API和SQL，本质上还是基于关系型表的操作方式；而关系型表、关系代数，以及SQL本身，一般是有界的，更适合批处理的场景。这就导致在进行流处理的过程中，理解会稍微复杂一些，需要引入一些特殊概念。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
</feed>
