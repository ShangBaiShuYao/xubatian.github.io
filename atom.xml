<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>我的梦想是星辰大海</title>
  
  <subtitle>知识源于积累,登峰造极源于自律</subtitle>
  <link href="http://xubatian.cn/atom.xml" rel="self"/>
  
  <link href="http://xubatian.cn/"/>
  <updated>2022-01-18T06:05:54.085Z</updated>
  <id>http://xubatian.cn/</id>
  
  <author>
    <name>xubatian</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>来自你的首个flag</title>
    <link href="http://xubatian.cn/%E5%8A%A8%E6%80%81/"/>
    <id>http://xubatian.cn/%E5%8A%A8%E6%80%81/</id>
    <published>2022-01-18T05:08:58.000Z</published>
    <updated>2022-01-18T06:05:54.085Z</updated>
    
    <content type="html"><![CDATA[<p>2022年01月18日 博客框架基本搭建完成,后续完善则是在写文章的基础上进行定向更改. 首先立个flag. 争取在01月21日凌晨 将大数据笔记整理好. 不能耽误后续的目标达成, 时间紧任务重.生死看淡,不服就干.  加油!   —— 来自技术不好,却在努力挣扎的菜鸟</p><span id="more"></span>                                                                                                                <p>热爱生活,爱阳光</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_212.png" width = "" height = "" alt="xubatian的博客" align="center" />]]></content>
    
    
    <summary type="html">&lt;p&gt;2022年01月18日 博客框架基本搭建完成,后续完善则是在写文章的基础上进行定向更改. 首先立个flag. 争取在01月21日凌晨 将大数据笔记整理好. 不能耽误后续的目标达成, 时间紧任务重.生死看淡,不服就干.  加油!   —— 来自技术不好,却在努力挣扎的菜鸟&lt;/p&gt;</summary>
    
    
    
    <category term="动态" scheme="http://xubatian.cn/categories/%E5%8A%A8%E6%80%81/"/>
    
    
    <category term="动态" scheme="http://xubatian.cn/tags/%E5%8A%A8%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop生产调优手册</title>
    <link href="http://xubatian.cn/Hadoop%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C/"/>
    <id>http://xubatian.cn/Hadoop%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C/</id>
    <published>2022-01-17T13:57:58.000Z</published>
    <updated>2022-01-17T17:20:31.427Z</updated>
    
    <content type="html"><![CDATA[<p>我以为的天长地久，却终沦为曾经拥有                            </p><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_146.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="HDFS—核心参数"><a href="#HDFS—核心参数" class="headerlink" title="HDFS—核心参数"></a>HDFS—核心参数</h1><h2 id="NameNode内存生产配置"><a href="#NameNode内存生产配置" class="headerlink" title="NameNode内存生产配置"></a>NameNode内存生产配置</h2><p>1）NameNode内存计算<br>    每个文件块大概占用150byte，一台服务器128G内存为例，能存储多少文件块呢？<br>    128 * 1024 * 1024 * 1024  / 150Byte ≈  9.1亿<br>    G     MB    KB     Byte<br>2）Hadoop2.x系列，配置NameNode内存<br>    NameNode内存默认2000m，如果服务器内存4G，NameNode内存可以配置3g。在hadoop-env.sh文件中配置如下。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_NAMENODE_OPTS=-Xmx3072m</span><br></pre></td></tr></table></figure><p>3）Hadoop3.x系列，配置NameNode内存<br>    （1）hadoop-env.sh中描述Hadoop的内存是动态分配的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The maximum amount of heap to use (Java -Xmx).  If no unit</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> is provided, it will be converted to MB.  Daemons will</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> prefer any Xmx setting <span class="keyword">in</span> their respective _OPT variable.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> There is no default; the JVM will autoscale based upon machine</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> memory size.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">export</span> HADOOP_HEAPSIZE_MAX=</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The minimum amount of heap to use (Java -Xms).  If no unit</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> is provided, it will be converted to MB.  Daemons will</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> prefer any Xms setting <span class="keyword">in</span> their respective _OPT variable.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> There is no default; the JVM will autoscale based upon machine</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> memory size.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">export</span> HADOOP_HEAPSIZE_MIN=</span></span><br><span class="line">HADOOP_NAMENODE_OPTS=-Xmx102400m</span><br></pre></td></tr></table></figure><p>   （2）查看NameNode占用内存</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 ~]$ jps</span><br><span class="line">3088 NodeManager</span><br><span class="line">2611 NameNode</span><br><span class="line">3271 JobHistoryServer</span><br><span class="line">2744 DataNode</span><br><span class="line">3579 Jps</span><br><span class="line">[atguigu@hadoop102 ~]$ jmap -heap 2611</span><br><span class="line">Heap Configuration:</span><br><span class="line">   MaxHeapSize              = 1031798784 (984.0MB)</span><br></pre></td></tr></table></figure><p>  （3）查看DataNode占用内存</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 ~]$ jmap -heap 2744</span><br><span class="line">Heap Configuration:</span><br><span class="line">   MaxHeapSize              = 1031798784 (984.0MB)</span><br></pre></td></tr></table></figure><p>查看发现hadoop102上的NameNode和DataNode占用内存都是自动分配的，且相等。不是很合理。<br>经验参考：</p><p><a href="https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_hardware_requirements.html#concept_fzz_dq4_gbb">https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_hardware_requirements.html#concept_fzz_dq4_gbb</a></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_147.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>具体修改：hadoop-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export HDFS_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=INFO,RFAS -Xmx1024m&quot;</span><br><span class="line"></span><br><span class="line">export HDFS_DATANODE_OPTS=&quot;-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m&quot;</span><br></pre></td></tr></table></figure><h2 id="NameNode心跳并发配置"><a href="#NameNode心跳并发配置" class="headerlink" title="NameNode心跳并发配置"></a>NameNode心跳并发配置</h2><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_148.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>1）hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.</span><br><span class="line">NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。</span><br><span class="line">对于大集群或者有大量客户端的集群来说，通常需要增大该参数。默认值是10。</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;21&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>企业经验：dfs.namenode.handler.count=，比如集群规模（DataNode台数）为3台时，此参数设置为21。可通过简单的python代码计算该值，代码如下。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 ~]$ sudo yum install -y python</span><br><span class="line">[shangbaishuyao@hadoop102 ~]$ python</span><br><span class="line">Python 2.7.5 (default, Apr 11 2018, 07:36:10) </span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux2</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import math</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="built_in">print</span> int(20*math.log(3))</span></span><br><span class="line">21</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; quit()</span></span><br></pre></td></tr></table></figure><h2 id="开启回收站配置"><a href="#开启回收站配置" class="headerlink" title="开启回收站配置"></a>开启回收站配置</h2><p>开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用。</p><p>1）回收站工作机制</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_149.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>2）开启回收站功能参数说明<br>（1）默认值fs.trash.interval = 0，0表示禁用回收站；其他值表示设置文件的存活时间。<br>（2）默认值fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等。<br>（3）要求fs.trash.checkpoint.interval &lt;= fs.trash.interval。<br>3）启用回收站<br>修改core-site.xml，配置垃圾回收时间为1分钟。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>4）查看回收站<br>          回收站目录在HDFS集群中的路径：/user/shangbaishuyao/.Trash/….<br>5）注意：通过网页上直接删除的文件也不会走回收站。<br>6）通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Trash trash = New Trash(conf);</span><br><span class="line">trash.moveToTrash(path);</span><br></pre></td></tr></table></figure><p>7）只有在命令行利用hadoop fs -rm命令删除的文件才会走回收站。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop fs -rm -r /user/shangbaishuyao/input</span><br><span class="line">2021-07-14 16:13:42,643 INFO fs.TrashPolicyDefault: Moved: &#x27;hdfs://hadoop102:9820/user/shangbaishuyao/input&#x27; to trash at: hdfs://hadoop102:9820/user/shangbaishuyao/.Trash/Current/user/shangbaishuyao/input</span><br></pre></td></tr></table></figure><p>8）恢复回收站数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop fs -mv</span><br><span class="line">/user/shangbaishuyao/.Trash/Current/user/shangbaishuyao/input    /user/shangbaishuyao/input</span><br></pre></td></tr></table></figure><h1 id="HDFS—集群压测"><a href="#HDFS—集群压测" class="headerlink" title="HDFS—集群压测"></a>HDFS—集群压测</h1><p>在企业中非常关心每天从Java后台拉取过来的数据，需要多久能上传到集群？消费者关心多久能从HDFS上拉取需要的数据？<br>为了搞清楚HDFS的读写性能，生产环境上非常需要对集群进行压测。</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_150.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>HDFS的读写性能主要受网络和磁盘影响比较大。为了方便测试，将hadoop102、hadoop103、hadoop104虚拟机网络都设置为100mbps。</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_151.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>100Mbps单位是bit；10M/s单位是byte ; 1byte=8bit，100Mbps/8=12.5M/s。<br>测试网速：来到hadoop102的/opt/module目录，创建一个</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 software]$ python -m SimpleHTTPServer</span><br></pre></td></tr></table></figure><h2 id="测试HDFS写性能"><a href="#测试HDFS写性能" class="headerlink" title="测试HDFS写性能"></a>测试HDFS写性能</h2><p>0）写测试底层原理</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_152.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>1）测试内容：向HDFS集群写10个128M的文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB</span><br><span class="line"></span><br><span class="line">2021-02-09 10:43:16,853 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:             Date &amp; time: Tue Feb 09 10:43:16 CST 2021</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:         Number of files: 10</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:  Total MBytes processed: 1280</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:       Throughput mb/sec: 1.61</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:  Average IO rate mb/sec: 1.9</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:   IO rate std deviation: 0.76</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:      Test exec time sec: 133.05</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:</span><br></pre></td></tr></table></figure><p>注意：nrFiles n为生成mapTask的数量，生产环境一般可通过hadoop103:8088查看CPU核数，设置为（CPU核数 - 1）</p><p>Ø Number of files：生成mapTask数量，一般是集群中（CPU核数-1），我们测试虚拟机就按照实际的物理内存-1分配即可</p><p>Ø Total MBytes processed：单个map处理的文件大小</p><p>Ø Throughput mb/sec:单个mapTak的吞吐量 </p><p>​            计算方式：处理的总文件大小/每一个mapTask写数据的时间累加</p><p>​            集群整体吞吐量：生成mapTask数量*单个mapTak的吞吐量</p><p>Ø Average IO rate mb/sec::平均mapTak的吞吐量</p><p>​            计算方式：每个mapTask处理文件大小/每一个mapTask写数据的时间全部相加除以task数量</p><p>Ø IO rate std deviation:方差、反映各个mapTask处理的差值，越小越均衡</p><p>2）注意：如果测试过程中，出现异常</p><p>（1）可以在yarn-site.xml中设置虚拟内存检测为false</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（2）分发配置并重启Yarn集群</p><p>3）测试结果分析</p><p>​    （1）由于副本1就在本地，所以该副本不参与测试</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_153.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>一共参与测试的文件：10个文件 * 2个副本 = 20个<br>压测后的速度：1.61<br>实测速度：1.61M/s * 20个文件 ≈ 32M/s<br>三台服务器的带宽：12.5 + 12.5 + 12.5 ≈ 30m/s<br>所有网络资源都已经用满。<br><strong>如果实测速度远远小于网络，并且实测速度不能满足工作需求，可以考虑采用固态硬盘或者增加磁盘个数。</strong></p><p>（2）如果客户端不在集群节点，那就三个副本都参与计算</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_154.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><h2 id="测试HDFS读性能"><a href="#测试HDFS读性能" class="headerlink" title="测试HDFS读性能"></a>测试HDFS读性能</h2><p>1）测试内容：读取HDFS集群10个128M的文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB</span><br><span class="line"></span><br><span class="line">2021-02-09 11:34:15,847 INFO fs.TestDFSIO: ----- TestDFSIO ----- : read</span><br><span class="line">2021-02-09 11:34:15,847 INFO fs.TestDFSIO:             Date &amp; time: Tue Feb 09 11:34:15 CST 2021</span><br><span class="line">2021-02-09 11:34:15,847 INFO fs.TestDFSIO:         Number of files: 10</span><br><span class="line">2021-02-09 11:34:15,847 INFO fs.TestDFSIO:  Total MBytes processed: 1280</span><br><span class="line">2021-02-09 11:34:15,848 INFO fs.TestDFSIO:       Throughput mb/sec: 200.28</span><br><span class="line">2021-02-09 11:34:15,848 INFO fs.TestDFSIO:  Average IO rate mb/sec: 266.74</span><br><span class="line">2021-02-09 11:34:15,848 INFO fs.TestDFSIO:   IO rate std deviation: 143.12</span><br><span class="line">2021-02-09 11:34:15,848 INFO fs.TestDFSIO:      Test exec time sec: 20.83</span><br></pre></td></tr></table></figure><p>2）删除测试生成数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -clean</span><br></pre></td></tr></table></figure><p>3）测试结果分析：为什么读取文件速度大于网络带宽？由于目前只有三台服务器，且有三个副本，数据读取就近原则，相当于都是读取的本地磁盘数据，没有走网络。</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_155.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><h1 id="HDFS—多目录"><a href="#HDFS—多目录" class="headerlink" title="HDFS—多目录"></a>HDFS—多目录</h1><h2 id="NameNode多目录配置"><a href="#NameNode多目录配置" class="headerlink" title="NameNode多目录配置"></a>NameNode多目录配置</h2><p>1）NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_156.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>2）具体配置如下</p><p>（1）在hdfs-site.xml文件中添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;file://$&#123;hadoop.tmp.dir&#125;/dfs/name1,file://$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>注意：因为每台服务器节点的磁盘情况不同，所以这个配置配完之后，可以选择不分发</p><p>（2）停止集群，删除三台节点的data和logs中所有数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ rm -rf data/ logs/</span><br><span class="line">[shangbaishuyao@hadoop103 hadoop-3.1.3]$ rm -rf data/ logs/</span><br><span class="line">[shangbaishuyao@hadoop104 hadoop-3.1.3]$ rm -rf data/ logs/</span><br></pre></td></tr></table></figure><p>（3）格式化集群并启动。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ bin/hdfs namenode -format</span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p>3）查看结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 dfs]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">drwx------. 3 shangbaishuyao shangbaishuyao 4096 12月 11 08:03 data</span><br><span class="line">drwxrwxr-x. 3 shangbaishuyao shangbaishuyao 4096 12月 11 08:03 name1</span><br><span class="line">drwxrwxr-x. 3 shangbaishuyao shangbaishuyao 4096 12月 11 08:03 name2</span><br></pre></td></tr></table></figure><p><strong>检查name1和name2里面的内容，发现一模一样。</strong></p><h2 id="DataNode多目录配置"><a href="#DataNode多目录配置" class="headerlink" title="DataNode多目录配置"></a>DataNode多目录配置</h2><p>1）DataNode可以配置成多个目录，每个目录存储的数据不一样（数据不是副本）</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_157.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>2）具体配置如下</p><p>在hdfs-site.xml文件中添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;file://$&#123;hadoop.tmp.dir&#125;/dfs/data1,file://$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>3）查看结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 dfs]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">drwx------. 3 shangbaishuyao shangbaishuyao 4096 4月   4 14:22 data1</span><br><span class="line">drwx------. 3 shangbaishuyao shangbaishuyao 4096 4月   4 14:22 data2</span><br><span class="line">drwxrwxr-x. 3 shangbaishuyao shangbaishuyao 4096 12月 11 08:03 name1</span><br><span class="line">drwxrwxr-x. 3 shangbaishuyao shangbaishuyao 4096 12月 11 08:03 name2</span><br></pre></td></tr></table></figure><p>4）向集群上传一个文件，再次观察两个文件夹里面的内容发现不一致（一个有数一个没有）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop fs -put wcinput/word.txt /</span><br></pre></td></tr></table></figure><h2 id="集群数据均衡之磁盘间数据均衡"><a href="#集群数据均衡之磁盘间数据均衡" class="headerlink" title="集群数据均衡之磁盘间数据均衡"></a>集群数据均衡之磁盘间数据均衡</h2><p>生产环境，由于硬盘空间不足，往往需要增加一块硬盘。刚加载的硬盘没有数据时，可以执行磁盘数据均衡命令。（Hadoop3.x新特性）</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_158.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>（1）生成均衡计划（我们只有一块磁盘，不会生成计划）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs diskbalancer -plan hadoop103</span><br></pre></td></tr></table></figure><p>（2）执行均衡计划</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs diskbalancer -execute hadoop103.plan.json</span><br></pre></td></tr></table></figure><p>（3）查看当前均衡任务的执行情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs diskbalancer -query hadoop103</span><br></pre></td></tr></table></figure><p>（4）取消均衡任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs diskbalancer -cancel hadoop103.plan.json</span><br></pre></td></tr></table></figure><h1 id="HDFS—集群扩容及缩容"><a href="#HDFS—集群扩容及缩容" class="headerlink" title="HDFS—集群扩容及缩容"></a>HDFS—集群扩容及缩容</h1><h2 id="添加白名单"><a href="#添加白名单" class="headerlink" title="添加白名单"></a>添加白名单</h2><p>白名单：表示在白名单的主机IP地址可以，用来存储数据。<br>企业中：配置白名单，可以尽量防止黑客恶意访问攻击。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_159.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>配置白名单步骤如下：</p><p>1）在NameNode节点的/opt/module/hadoop-3.1.3/etc/hadoop目录下分别创建whitelist 和blacklist文件<br>（1）创建白名单</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ vim whitelist</span><br></pre></td></tr></table></figure><p>在whitelist中添加如下主机名称，假如集群正常工作的节点为102 103 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br></pre></td></tr></table></figure><p>（2）创建黑名单</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ touch blacklist</span><br></pre></td></tr></table></figure><p>​    保持空的就可以<br>2）在hdfs-site.xml配置文件中增加dfs.hosts配置参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 白名单 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.hosts&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/whitelist&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 黑名单 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/blacklist&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>3）分发配置文件whitelist，hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop104 hadoop]$ xsync hdfs-site.xml whitelist</span><br></pre></td></tr></table></figure><p>4）第一次添加白名单必须重启集群，不是第一次，只需要刷新NameNode节点即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ myhadoop.sh stop</span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ myhadoop.sh start</span><br></pre></td></tr></table></figure><p>5）在web浏览器上查看DN，<a href="http://hadoop102:9870/dfshealth.html#tab-datanode">http://hadoop102:9870/dfshealth.html#tab-datanode</a>   注意: hadoop102位服务器IP地址</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_160.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>6）在hadoop104上执行上传数据数据失败</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop104 hadoop-3.1.3]$ hadoop fs -put NOTICE.txt /</span><br></pre></td></tr></table></figure><p>7）二次修改白名单，增加hadoop104</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ vim whitelist</span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p>8）刷新NameNode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs dfsadmin -refreshNodes</span><br><span class="line">Refresh nodes successful</span><br></pre></td></tr></table></figure><p>9）在web浏览器上查看DN，<a href="http://hadoop102:9870/dfshealth.html#tab-datanode">http://hadoop102:9870/dfshealth.html#tab-datanode</a></p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_161.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><h2 id="服役新服务器"><a href="#服役新服务器" class="headerlink" title="服役新服务器"></a>服役新服务器</h2><p>1）需求<br>随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点。<br>2）环境准备<br>（1）在hadoop100主机上再克隆一台hadoop105主机<br>（2）修改IP地址和主机名称</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop105 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br><span class="line">[root@hadoop105 ~]# vim /etc/hostname</span><br></pre></td></tr></table></figure><p>（3）拷贝hadoop102的/opt/module目录和/etc/profile.d/my_env.sh到hadoop105</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 opt]$ scp -r module/* shangbaishuyao@hadoop105:/opt/module/</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@hadoop102 opt]$ sudo scp /etc/profile.d/my_env.sh root@hadoop105:/etc/profile.d/my_env.sh</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ source /etc/profile</span><br></pre></td></tr></table></figure><p>（4）删除hadoop105上Hadoop的历史数据，data和log数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ rm -rf data/ logs/</span><br></pre></td></tr></table></figure><p>（5）配置hadoop102和hadoop103到hadoop105的ssh无密登录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 .ssh]$ ssh-copy-id hadoop105</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@hadoop103 .ssh]$ ssh-copy-id hadoop105</span><br></pre></td></tr></table></figure><p>3）服役新节点具体步骤<br>（1）直接启动DataNode，即可关联到集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ hdfs --daemon start datanode</span><br><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ yarn --daemon start nodemanager</span><br></pre></td></tr></table></figure><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_162.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>4）在白名单中增加新服役的服务器<br>（1）在白名单whitelist中增加hadoop104、hadoop105，并重启集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ vim whitelist</span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br><span class="line">hadoop105</span><br></pre></td></tr></table></figure><p>（2）分发</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ xsync whitelist</span><br></pre></td></tr></table></figure><p>（3）刷新NameNode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs dfsadmin -refreshNodes</span><br><span class="line">Refresh nodes successful</span><br></pre></td></tr></table></figure><p>5）在hadoop105上上传文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ hadoop fs -put /opt/module/hadoop-3.1.3/LICENSE.txt /</span><br></pre></td></tr></table></figure><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_163.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p><strong>思考：如果数据不均衡（hadoop105数据少，其他节点数据多），怎么处理？</strong></p><h2 id="服务器间数据均衡"><a href="#服务器间数据均衡" class="headerlink" title="服务器间数据均衡"></a>服务器间数据均衡</h2><p>1）企业经验：<br>在企业开发中，如果经常在hadoop102和hadoop104上提交任务，且副本数为2，由于数据本地性原则，就会导致hadoop102和hadoop104数据过多，hadoop103存储的数据量小。<br>另一种情况，就是新服役的服务器数据量比较少，需要执行集群均衡命令。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_164.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>2）开启数据均衡命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ sbin/start-balancer.sh -threshold 10</span><br></pre></td></tr></table></figure><p>对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%，可根据实际情况进行调整。<br>3）停止数据均衡命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ sbin/stop-balancer.sh</span><br></pre></td></tr></table></figure><p>注意：由于HDFS需要启动单独的Rebalance Server来执行Rebalance操作，所以尽量不要在NameNode上执行start-balancer.sh，而是找一台比较空闲的机器。</p><h2 id="黑名单退役服务器"><a href="#黑名单退役服务器" class="headerlink" title="黑名单退役服务器"></a>黑名单退役服务器</h2><p>黑名单：表示在黑名单的主机IP地址不可以，用来存储数据。<br>企业中：配置黑名单，用来退役服务器。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_165.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>黑名单配置步骤如下：<br>1）编辑/opt/module/hadoop-3.1.3/etc/hadoop目录下的blacklist文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop] vim blacklist</span><br></pre></td></tr></table></figure><p>添加如下主机名称（要退役的节点）<br>hadoop105<br>注意：如果白名单中没有配置，需要在hdfs-site.xml配置文件中增加dfs.hosts配置参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 黑名单 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/blacklist&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>2）分发配置文件blacklist，hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop104 hadoop]$ xsync hdfs-site.xml blacklist</span><br></pre></td></tr></table></figure><p>3）第一次添加黑名单必须重启集群，不是第一次，只需要刷新NameNode节点即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs dfsadmin -refreshNodes</span><br><span class="line">Refresh nodes successful</span><br></pre></td></tr></table></figure><p>4）检查Web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_166.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>5）等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_167.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ hdfs --daemon stop datanode</span><br></pre></td></tr></table></figure><p>stopping datanode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop105 hadoop-3.1.3]$ yarn --daemon stop nodemanager</span><br></pre></td></tr></table></figure><p>stopping nodemanager</p><p>6）如果数据不均衡，可以用命令实现集群的再平衡</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ sbin/start-balancer.sh -threshold 10</span><br></pre></td></tr></table></figure><h1 id="HDFS—存储优化"><a href="#HDFS—存储优化" class="headerlink" title="HDFS—存储优化"></a>HDFS—存储优化</h1><p><strong>注：演示纠删码和异构存储需要一共5台虚拟机。尽量拿另外一套集群。提前准备5台服务器的集群。</strong></p><h2 id="纠删码"><a href="#纠删码" class="headerlink" title="纠删码"></a>纠删码</h2><h3 id="纠删码原理"><a href="#纠删码原理" class="headerlink" title="纠删码原理"></a>纠删码原理</h3><p>HDFS默认情况下，一个文件有3个副本，这样提高了数据的可靠性，但也带来了2倍的冗余开销。Hadoop3.x引入了纠删码，采用计算的方式，可以节省约50％左右的存储空间。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_168.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>1）纠删码操作相关的命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs ec</span><br><span class="line">Usage: bin/hdfs ec [COMMAND]</span><br><span class="line">          [-listPolicies]</span><br><span class="line">          [-addPolicies -policyFile &lt;file&gt;]</span><br><span class="line">          [-getPolicy -path &lt;path&gt;]</span><br><span class="line">          [-removePolicy -policy &lt;policy&gt;]</span><br><span class="line">          [-setPolicy -path &lt;path&gt; [-policy &lt;policy&gt;] [-replicate]]</span><br><span class="line">          [-unsetPolicy -path &lt;path&gt;]</span><br><span class="line">          [-listCodecs]</span><br><span class="line">          [-enablePolicy -policy &lt;policy&gt;]</span><br><span class="line">          [-disablePolicy -policy &lt;policy&gt;]</span><br><span class="line">          [-help &lt;command-name&gt;].</span><br></pre></td></tr></table></figure><p>2）查看当前支持的纠删码策略</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3] hdfs ec -listPolicies</span><br><span class="line"></span><br><span class="line">Erasure Coding Policies:</span><br><span class="line">ErasureCodingPolicy=[Name=RS-10-4-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=10, numParityUnits=4]], CellSize=1048576, Id=5], State=DISABLED</span><br><span class="line"></span><br><span class="line">ErasureCodingPolicy=[Name=RS-3-2-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=3, numParityUnits=2]], CellSize=1048576, Id=2], State=DISABLED</span><br><span class="line"></span><br><span class="line">ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1], State=ENABLED</span><br><span class="line"></span><br><span class="line">ErasureCodingPolicy=[Name=RS-LEGACY-6-3-1024k, Schema=[ECSchema=[Codec=rs-legacy, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=3], State=DISABLED</span><br><span class="line"></span><br><span class="line">ErasureCodingPolicy=[Name=XOR-2-1-1024k, Schema=[ECSchema=[Codec=xor, numDataUnits=2, numParityUnits=1]], CellSize=1048576, Id=4], State=DISABLED</span><br></pre></td></tr></table></figure><p>3）纠删码策略解释:<br>RS-3-2-1024k：使用RS编码，每3个数据单元，生成2个校验单元，共5个单元，也就是说：这5个单元中，只要有任意的3个单元存在（不管是数据单元还是校验单元，只要总数=3），就可以得到原始数据。每个单元的大小是1024k=1024*1024=1048576。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_169.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p><strong>RS-10-4-1024k</strong>：使用RS编码，每10个数据单元（cell），生成4个校验单元，共14个单元，也就是说：这14个单元中，只要有任意的10个单元存在（不管是数据单元还是校验单元，只要总数=10），就可以得到原始数据。每个单元的大小是1024k=1024<em>1024=1048576。</em></p><p><strong>RS-6-3-1024k</strong>：使用RS编码，每6个数据单元，生成3个校验单元，共9个单元，也就是说：这9个单元中，只要有任意的6个单元存在（不管是数据单元还是校验单元，只要总数=6），就可以得到原始数据。每个单元的大小是1024k=1024*1024=1048576。</p><p><strong>RS-LEGACY-6-3-1024k</strong>：策略和上面的RS-6-3-1024k一样，只是编码的算法用的是rs-legacy。 </p><p><strong>XOR-2-1-1024k</strong>：使用XOR编码（速度比RS编码快），每2个数据单元，生成1个校验单元，共3个单元，也就是说：这3个单元中，只要有任意的2个单元存在（不管是数据单元还是校验单元，只要总数= 2），就可以得到原始数据。每个单元的大小是1024k=1024*1024=1048576。</p><h3 id="纠删码案例实操"><a href="#纠删码案例实操" class="headerlink" title="纠删码案例实操"></a>纠删码案例实操</h3><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_170.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>纠删码策略是给具体一个路径设置。所有往此路径下存储的文件，都会执行此策略。<br>默认只开启对RS-6-3-1024k策略的支持，如要使用别的策略需要提前启用。</p><p>1）需求：将/input目录设置为RS-3-2-1024k策略<br>2）具体步骤<br>（1）开启对RS-3-2-1024k策略的支持</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$  hdfs ec -enablePolicy  -policy RS-3-2-1024k</span><br><span class="line">Erasure coding policy RS-3-2-1024k is enabled</span><br></pre></td></tr></table></figure><p>（2）在HDFS创建目录，并设置RS-3-2-1024k策略</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102  hadoop-3.1.3]$  hdfs dfs -mkdir /input</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@hadoop202 hadoop-3.1.3]$ hdfs ec -setPolicy -path /input -policy RS-3-2-1024k</span><br></pre></td></tr></table></figure><p>（3）上传文件，并查看文件编码后的存储情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs dfs -put web.log /input</span><br></pre></td></tr></table></figure><p>注：你所上传的文件需要大于2M才能看出效果。（低于2M，只有一个数据单元和两个校验单元）<br>（4）查看存储路径的数据单元和校验单元，并作破坏实验</p><h2 id="异构存储（冷热数据分离）"><a href="#异构存储（冷热数据分离）" class="headerlink" title="异构存储（冷热数据分离）"></a>异构存储（冷热数据分离）</h2><p>异构存储主要解决，不同的数据，存储在不同类型的硬盘中，达到最佳性能的问题。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_171.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_172.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><h3 id="异构存储Shell操作"><a href="#异构存储Shell操作" class="headerlink" title="异构存储Shell操作"></a>异构存储Shell操作</h3><p>（1）查看当前有哪些存储策略可以用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -listPolicies</span><br></pre></td></tr></table></figure><p>（2）为指定路径（数据存储目录）设置指定的存储策略</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs storagepolicies -setStoragePolicy -path xxx -policy xxx</span><br></pre></td></tr></table></figure><p>（3）获取指定路径（数据存储目录或文件）的存储策略</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs storagepolicies -getStoragePolicy -path xxx</span><br></pre></td></tr></table></figure><p>（4）取消存储策略；执行改命令之后该目录或者文件，以其上级的目录为准，如果是根目录，那么就是HOT</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs storagepolicies -unsetStoragePolicy -path xxx</span><br></pre></td></tr></table></figure><p>（5）查看文件块的分布</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs fsck xxx -files -blocks -locations</span><br></pre></td></tr></table></figure><p>（6）查看集群节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop dfsadmin -report</span><br></pre></td></tr></table></figure><h3 id="测试环境准备"><a href="#测试环境准备" class="headerlink" title="测试环境准备"></a>测试环境准备</h3><p>1）测试环境描述<br>服务器规模：5台<br>集群配置：副本数为2，创建好带有存储类型的目录（提前创建）<br>集群规划：</p><table><thead><tr><th>节点</th><th>存储类型分配</th></tr></thead><tbody><tr><td>hadoop102</td><td>RAM_DISK，SSD</td></tr><tr><td>hadoop103</td><td>SSD，DISK</td></tr><tr><td>hadoop104</td><td>DISK，RAM_DISK</td></tr><tr><td>hadoop105</td><td>ARCHIVE</td></tr><tr><td>hadoop106</td><td>ARCHIVE</td></tr></tbody></table><p>2）配置文件信息<br>（1）为hadoop102节点的hdfs-site.xml添加如下信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.storage.policy.enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; </span><br><span class="line">&lt;value&gt;[SSD]file:///opt/module/hadoop-3.1.3/hdfsdata/ssd,[RAM_DISK]file:///opt/module/hadoop-3.1.3/hdfsdata/ram_disk&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（2）为hadoop103节点的hdfs-site.xml添加如下信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.storage.policy.enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;[SSD]file:///opt/module/hadoop-3.1.3/hdfsdata/ssd,[DISK]file:///opt/module/hadoop-3.1.3/hdfsdata/disk&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（3）为hadoop104节点的hdfs-site.xml添加如下信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.storage.policy.enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;[RAM_DISK]file:///opt/module/hdfsdata/ram_disk,[DISK]file:///opt/module/hadoop-3.1.3/hdfsdata/disk&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（4）为hadoop105节点的hdfs-site.xml添加如下信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.storage.policy.enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;[ARCHIVE]file:///opt/module/hadoop-3.1.3/hdfsdata/archive&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（5）为hadoop106节点的hdfs-site.xml添加如下信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.storage.policy.enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;[ARCHIVE]file:///opt/module/hadoop-3.1.3/hdfsdata/archive&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>3）数据准备<br>（1）启动集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs namenode -format</span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ myhadoop.sh start</span><br></pre></td></tr></table></figure><p>（1）并在HDFS上创建文件目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop fs -mkdir /hdfsdata</span><br></pre></td></tr></table></figure><p>（2）并将文件资料上传</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop fs -put /opt/module/hadoop-3.1.3/NOTICE.txt /hdfsdata</span><br></pre></td></tr></table></figure><h3 id="HOT存储策略案例"><a href="#HOT存储策略案例" class="headerlink" title="HOT存储策略案例"></a>HOT存储策略案例</h3><p>（1）最开始我们未设置存储策略的情况下，我们获取该目录的存储策略</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -getStoragePolicy -path /hdfsdata</span><br></pre></td></tr></table></figure><p>（2）我们查看上传的文件块分布</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs fsck /hdfsdata -files -blocks -locations</span><br><span class="line"></span><br><span class="line">[DatanodeInfoWithStorage[192.168.10.104:9866,DS-0b133854-7f9e-48df-939b-5ca6482c5afb,DISK], DatanodeInfoWithStorage[192.168.10.103:9866,DS-ca1bd3b9-d9a5-4101-9f92-3da5f1baa28b,DISK]]</span><br></pre></td></tr></table></figure><p>未设置存储策略，所有文件块都存储在DISK下。所以，默认存储策略为HOT。</p><h3 id="WARM存储策略测试"><a href="#WARM存储策略测试" class="headerlink" title="WARM存储策略测试"></a>WARM存储策略测试</h3><p>（1）接下来我们为数据降温</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy WARM</span><br></pre></td></tr></table></figure><p>（2）再次查看文件块分布，我们可以看到文件块依然放在原处。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs fsck /hdfsdata -files -blocks -locations</span><br></pre></td></tr></table></figure><p>（3）我们需要让他HDFS按照存储策略自行移动文件块</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs mover /hdfsdata</span><br></pre></td></tr></table></figure><p>（4）再次查看文件块分布，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs fsck /hdfsdata -files -blocks -locations</span><br><span class="line"></span><br><span class="line">[DatanodeInfoWithStorage[192.168.10.105:9866,DS-d46d08e1-80c6-4fca-b0a2-4a3dd7ec7459,ARCHIVE], DatanodeInfoWithStorage[192.168.10.103:9866,DS-ca1bd3b9-d9a5-4101-9f92-3da5f1baa28b,DISK]]</span><br></pre></td></tr></table></figure><p>文件块一半在DISK，一半在ARCHIVE，符合我们设置的WARM策略</p><h3 id="COLD策略测试"><a href="#COLD策略测试" class="headerlink" title="COLD策略测试"></a>COLD策略测试</h3><p>（1）我们继续将数据降温为cold</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy COLD</span><br></pre></td></tr></table></figure><p>注意：当我们将目录设置为COLD并且我们未配置ARCHIVE存储目录的情况下，不可以向该目录直接上传文件，会报出异常。<br>（2）手动转移</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs mover /hdfsdata</span><br></pre></td></tr></table></figure><p>（3）检查文件块的分布</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ bin/hdfs fsck /hdfsdata -files -blocks -locations</span><br><span class="line"></span><br><span class="line">[DatanodeInfoWithStorage[192.168.10.105:9866,DS-d46d08e1-80c6-4fca-b0a2-4a3dd7ec7459,ARCHIVE], DatanodeInfoWithStorage[192.168.10.106:9866,DS-827b3f8b-84d7-47c6-8a14-0166096f919d,ARCHIVE]]</span><br></pre></td></tr></table></figure><p>所有文件块都在ARCHIVE，符合COLD存储策略。</p><h3 id="ONE-SSD策略测试"><a href="#ONE-SSD策略测试" class="headerlink" title="ONE_SSD策略测试"></a>ONE_SSD策略测试</h3><p>（1）接下来我们将存储策略从默认的HOT更改为One_SSD</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy One_SSD</span><br></pre></td></tr></table></figure><p>（2）手动转移文件块</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs mover /hdfsdata</span><br></pre></td></tr></table></figure><p>（3）转移完成后，我们查看文件块分布，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ bin/hdfs fsck /hdfsdata -files -blocks -locations</span><br><span class="line"></span><br><span class="line">[DatanodeInfoWithStorage[192.168.10.104:9866,DS-0b133854-7f9e-48df-939b-5ca6482c5afb,DISK], DatanodeInfoWithStorage[192.168.10.103:9866,DS-2481a204-59dd-46c0-9f87-ec4647ad429a,SSD]]</span><br></pre></td></tr></table></figure><p>文件块分布为一半在SSD，一半在DISK，符合One_SSD存储策略。</p><h3 id="ALL-SSD策略测试"><a href="#ALL-SSD策略测试" class="headerlink" title="ALL_SSD策略测试"></a>ALL_SSD策略测试</h3><p>（1）接下来，我们再将存储策略更改为All_SSD</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy All_SSD</span><br></pre></td></tr></table></figure><p>（2）手动转移文件块</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs mover /hdfsdata</span><br></pre></td></tr></table></figure><p>（3）查看文件块分布，我们可以看到，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ bin/hdfs fsck /hdfsdata -files -blocks -locations</span><br><span class="line"></span><br><span class="line">[DatanodeInfoWithStorage[192.168.10.102:9866,DS-c997cfb4-16dc-4e69-a0c4-9411a1b0c1eb,SSD], DatanodeInfoWithStorage[192.168.10.103:9866,DS-2481a204-59dd-46c0-9f87-ec4647ad429a,SSD]]</span><br></pre></td></tr></table></figure><p>所有的文件块都存储在SSD，符合All_SSD存储策略。</p><h3 id="LAZY-PERSIST策略测试"><a href="#LAZY-PERSIST策略测试" class="headerlink" title="LAZY_PERSIST策略测试"></a>LAZY_PERSIST策略测试</h3><p>（1）继续改变策略，将存储策略改为lazy_persist</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy lazy_persist</span><br></pre></td></tr></table></figure><p>（2）手动转移文件块</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs mover /hdfsdata</span><br></pre></td></tr></table></figure><p>（3）查看文件块分布</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs fsck /hdfsdata -files -blocks -locations</span><br><span class="line"></span><br><span class="line">[DatanodeInfoWithStorage[192.168.10.104:9866,DS-0b133854-7f9e-48df-939b-5ca6482c5afb,DISK], DatanodeInfoWithStorage[192.168.10.103:9866,DS-ca1bd3b9-d9a5-4101-9f92-3da5f1baa28b,DISK]]</span><br></pre></td></tr></table></figure><p>这里我们发现所有的文件块都是存储在DISK，按照理论一个副本存储在RAM_DISK，其他副本存储在DISK中，这是因为，我们还需要配置“dfs.datanode.max.locked.memory”，“dfs.block.size”参数。<br>那么出现存储策略为LAZY_PERSIST时，文件块副本都存储在DISK上的原因有如下两点：<br>（1）当客户端所在的DataNode节点没有RAM_DISK时，则会写入客户端所在的DataNode节点的DISK磁盘，其余副本会写入其他节点的DISK磁盘。<br>（2）当客户端所在的DataNode有RAM_DISK，但“dfs.datanode.max.locked.memory”参数值未设置或者设置过小（小于“dfs.block.size”参数值）时，则会写入客户端所在的DataNode节点的DISK磁盘，其余副本会写入其他节点的DISK磁盘。<br>但是由于虚拟机的“max locked memory”为64KB，所以，如果参数配置过大，还会报出错误：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain</span><br><span class="line">java.lang.RuntimeException: Cannot start datanode because the configured max locked memory size (dfs.datanode.max.locked.memory) of 209715200 bytes is more than the datanode&#x27;s available RLIMIT_MEMLOCK ulimit of 65536 bytes.</span><br></pre></td></tr></table></figure><p>我们可以通过该命令查询此参数的内存</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ ulimit -a</span><br><span class="line"></span><br><span class="line">max locked memory       (kbytes, -l) 64</span><br></pre></td></tr></table></figure><h1 id="HDFS—故障排除"><a href="#HDFS—故障排除" class="headerlink" title="HDFS—故障排除"></a>HDFS—故障排除</h1><p>注意：采用三台服务器即可，恢复到Yarn开始的服务器快照。</p><h2 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h2><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_173.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>1）需求：<br>NameNode进程挂了并且存储的数据也丢失了，如何恢复NameNode<br>2）故障模拟<br>（1）kill -9 NameNode进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 current]$ kill -9 19886</span><br></pre></td></tr></table></figure><p>（2）删除NameNode存储的数据（/opt/module/hadoop-3.1.3/data/tmp/dfs/name）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ rm -rf /opt/module/hadoop-3.1.3/data/dfs/name/*</span><br></pre></td></tr></table></figure><p>3）问题解决<br>（1）拷贝SecondaryNameNode中数据到原NameNode存储数据目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 dfs]$ scp -r shangbaishuyao@hadoop104:/opt/module/hadoop-3.1.3/data/dfs/namesecondary/* ./name/</span><br></pre></td></tr></table></figure><p>（2）重新启动NameNode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs --daemon start namenode</span><br></pre></td></tr></table></figure><p>（3）向集群上传一个文件</p><h2 id="集群安全模式-amp-磁盘修复"><a href="#集群安全模式-amp-磁盘修复" class="headerlink" title="集群安全模式&amp;磁盘修复"></a>集群安全模式&amp;磁盘修复</h2><p>1）安全模式：文件系统只接受读数据请求，而不接受删除、修改等变更请求<br>2）进入安全模式场景</p><p>Ø NameNode在加载镜像文件和编辑日志期间处于安全模式；</p><p>Ø NameNode再接收DataNode注册时，处于安全模式</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_174.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>3）退出安全模式条件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dfs.namenode.safemode.min.datanodes:最小可用datanode数量，默认0</span><br><span class="line">dfs.namenode.safemode.threshold-pct:副本数达到最小要求的block占系统总block数的百分比，默认0.999f。（只允许丢一个块）</span><br><span class="line">dfs.namenode.safemode.extension:稳定时间，默认值30000毫秒，即30秒</span><br></pre></td></tr></table></figure><p>4）基本语法<br>集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（1）bin/hdfs dfsadmin -safemode get（功能描述：查看安全模式状态）</span><br><span class="line">（2）bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态）</span><br><span class="line">（3）bin/hdfs dfsadmin -safemode leave（功能描述：离开安全模式状态）</span><br><span class="line">（4）bin/hdfs dfsadmin -safemode wait（功能描述：等待安全模式状态）</span><br></pre></td></tr></table></figure><p>5）案例1：启动集群进入安全模式<br>    （1）重新启动集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 subdir0]$ myhadoop.sh stop</span><br><span class="line">[shangbaishuyao@hadoop102 subdir0]$ myhadoop.sh start</span><br></pre></td></tr></table></figure><p>​    （2）集群启动后，立即来到集群上删除数据，提示集群处于安全模式</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_176.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>案例2：磁盘修复<br>    需求：数据块损坏，进入安全模式，如何处理<br>    （1）分别进入hadoop102、hadoop103、hadoop104的/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1015489500-192.168.10.102-1611909480872/current/finalized/subdir0/subdir0目录，统一删除某2个块信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 subdir0]$ pwd</span><br><span class="line">/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1015489500-192.168.10.102-1611909480872/current/finalized/subdir0/subdir0</span><br><span class="line">[shangbaishuyao@hadoop102 subdir0]$ rm -rf blk_1073741847 blk_1073741847_1023.meta</span><br><span class="line">[shangbaishuyao@hadoop102 subdir0]$ rm -rf blk_1073741865 blk_1073741865_1042.meta</span><br></pre></td></tr></table></figure><p>说明：hadoop103/hadoop104重复执行以上命令<br>    （2）重新启动集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 subdir0]$ myhadoop.sh stop</span><br><span class="line">[shangbaishuyao@hadoop102 subdir0]$ myhadoop.sh start</span><br></pre></td></tr></table></figure><p>​    （3）观察<a href="http://hadoop102:9870/dfshealth.html#tab-overview">http://hadoop102:9870/dfshealth.html#tab-overview</a></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_177.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>说明：安全模式已经打开，块的数量没有达到要求。<br>    （4）离开安全模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 subdir0]$ hdfs dfsadmin -safemode get</span><br><span class="line">Safe mode is ON</span><br><span class="line">[shangbaishuyao@hadoop102 subdir0]$ hdfs dfsadmin -safemode leave</span><br><span class="line">Safe mode is OFF</span><br></pre></td></tr></table></figure><p>​    （5）观察<a href="http://hadoop102:9870/dfshealth.html#tab-overview">http://hadoop102:9870/dfshealth.html#tab-overview</a></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_178.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>​    （6）将元数据删除</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_179.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>（7）观察<a href="http://hadoop102:9870/dfshealth.html#tab-overview%EF%BC%8C%E9%9B%86%E7%BE%A4%E5%B7%B2%E7%BB%8F%E6%AD%A3%E5%B8%B8">http://hadoop102:9870/dfshealth.html#tab-overview，集群已经正常</a></p><p>案例3：<br>    需求：模拟等待安全模式<br>（1）查看当前模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hdfs dfsadmin -safemode get</span><br><span class="line">Safe mode is OFF</span><br></pre></td></tr></table></figure><p>（2）先进入安全模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ bin/hdfs dfsadmin -safemode enter</span><br></pre></td></tr></table></figure><p>（3）创建并执行下面的脚本<br>在/opt/module/hadoop-3.1.3路径上，编辑一个脚本safemode.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ vim safemode.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">hdfs dfsadmin -safemode wait</span><br><span class="line">hdfs dfs -put /opt/module/hadoop-3.1.3/README.txt /</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ chmod 777 safemode.sh</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ ./safemode.sh </span><br></pre></td></tr></table></figure><p>（4）再打开一个窗口，执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ bin/hdfs dfsadmin -safemode leave</span><br></pre></td></tr></table></figure><p>（5）再观察上一个窗口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Safe mode is OFF</span><br></pre></td></tr></table></figure><p>（6）HDFS集群上已经有上传的数据了</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_180.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><h2 id="慢磁盘监控"><a href="#慢磁盘监控" class="headerlink" title="慢磁盘监控"></a>慢磁盘监控</h2><p>“慢磁盘”指的时写入数据非常慢的一类磁盘。其实慢性磁盘并不少见，当机器运行时间长了，上面跑的任务多了，磁盘的读写性能自然会退化，严重时就会出现写入数据延时的问题。<br>如何发现慢磁盘？<br>正常在HDFS上创建一个目录，只需要不到1s的时间。如果你发现创建目录超过1分钟及以上，而且这个现象并不是每次都有。只是偶尔慢了一下，就很有可能存在慢磁盘。<br>可以采用如下方法找出是哪块磁盘慢：<br><strong>1）通过心跳未联系时间。</strong><br>一般出现慢磁盘现象，会影响到DataNode与NameNode之间的心跳。正常情况心跳时间间隔是3s。超过3s说明有异常。</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_181.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p><strong>2）fio命令，测试磁盘的读写性能</strong><br>（1）顺序读测试</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 ~]# sudo yum install -y fio</span><br><span class="line">[shangbaishuyao@hadoop102 ~]# sudo fio -filename=/home/shangbaishuyao/test.log -direct=1 -iodepth 1 -thread -rw=read -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_r</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: bw=360MiB/s (378MB/s), 360MiB/s-360MiB/s (378MB/s-378MB/s), io=20.0GiB (21.5GB), run=56885-56885msec</span><br></pre></td></tr></table></figure><p>​          结果显示，磁盘的总体顺序读速度为360MiB/s。<br>（2）顺序写测试</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 ~]# sudo fio -filename=/home/shangbaishuyao/test.log -direct=1 -iodepth 1 -thread -rw=write -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: bw=341MiB/s (357MB/s), 341MiB/s-341MiB/s (357MB/s-357MB/s), io=19.0GiB (21.4GB), run=60001-60001msec</span><br></pre></td></tr></table></figure><p>​          结果显示，磁盘的总体顺序写速度为341MiB/s。<br>（3）随机写测试</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 ~]# sudo fio -filename=/home/shangbaishuyao/test.log -direct=1 -iodepth 1 -thread -rw=randwrite -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_randw</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: bw=309MiB/s (324MB/s), 309MiB/s-309MiB/s (324MB/s-324MB/s), io=18.1GiB (19.4GB), run=60001-60001msec</span><br></pre></td></tr></table></figure><p>​           结果显示，磁盘的总体随机写速度为309MiB/s。<br>（4）混合随机读写：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 ~]# sudo fio -filename=/home/shangbaishuyao/test.log -direct=1 -iodepth 1 -thread -rw=randrw -rwmixread=70 -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_r_w -ioscheduler=noop</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: bw=220MiB/s (231MB/s), 220MiB/s-220MiB/s (231MB/s-231MB/s), io=12.9GiB (13.9GB), run=60001-60001msec</span><br><span class="line">  WRITE: bw=94.6MiB/s (99.2MB/s), 94.6MiB/s-94.6MiB/s (99.2MB/s-99.2MB/s), io=5674MiB (5950MB), run=60001-60001msec</span><br></pre></td></tr></table></figure><p>结果显示，磁盘的总体混合随机读写，读速度为220MiB/s，写速度94.6MiB/s。</p><h2 id="小文件归档"><a href="#小文件归档" class="headerlink" title="小文件归档"></a>小文件归档</h2><p>1）HDFS存储小文件弊端</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_182.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB。</p><p>2）解决存储小文件办法之一<br>HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存。</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_183.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>3）案例实操<br>（1）需要启动YARN进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ start-yarn.sh</span><br></pre></td></tr></table></figure><p>（2）归档文件<br>    把/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/output路径下。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop archive -archiveName input.har -p  /input   /output</span><br></pre></td></tr></table></figure><p>（3）查看归档</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop fs -ls /output/input.har</span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop fs -ls har:///output/input.har</span><br></pre></td></tr></table></figure><p>（4）解归档文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop fs -cp har:///output/input.har/*    /</span><br></pre></td></tr></table></figure><h1 id="HDFS—集群迁移"><a href="#HDFS—集群迁移" class="headerlink" title="HDFS—集群迁移"></a>HDFS—集群迁移</h1><h2 id="Apache和Apache集群间数据拷贝"><a href="#Apache和Apache集群间数据拷贝" class="headerlink" title="Apache和Apache集群间数据拷贝"></a>Apache和Apache集群间数据拷贝</h2><p>1）scp实现两个远程主机之间的文件复制<br>    scp -r hello.txt root@hadoop103:/user/shangbaishuyao/hello.txt        // 推 push<br>    scp -r root@hadoop103:/user/shangbaishuyao/hello.txt  hello.txt        // 拉 pull<br>    scp -r root@hadoop103:/user/shangbaishuyao/hello.txt root@hadoop104:/user/shangbaishuyao   //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。<br>2）采用distcp命令实现两个Hadoop集群之间的递归数据复制</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$  bin/hadoop distcp hdfs://hadoop102:8020/user/shangbaishuyao/hello.txt hdfs://hadoop105:8020/user/shangbaishuyao/hello.txt</span><br></pre></td></tr></table></figure><h2 id="Apache和CDH集群间数据拷贝"><a href="#Apache和CDH集群间数据拷贝" class="headerlink" title="Apache和CDH集群间数据拷贝"></a>Apache和CDH集群间数据拷贝</h2><p>迁移数据</p><p>1）准备两套集群，我这使用apache集群和CDH集群。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_184.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>2）启动集群</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_185.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_186.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>3）启动完毕后，将apache集群中，hive库里dwd，dws，ads三个库的数据迁移到CDH集群</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_187.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_188.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>4）在apache集群里hosts加上CDH Namenode对应域名并分发给各机器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# vim /etc/hosts</span><br></pre></td></tr></table></figure><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_189.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# scp /etc/hosts hadoop102:/etc/                                                                                              </span><br><span class="line">[root@hadoop101 ~]# scp /etc/hosts hadoop103:/etc/</span><br></pre></td></tr></table></figure><p>5）因为集群都是HA模式，所以需要在apache集群上配置CDH集群,让distcp能识别出CDH的nameservice</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop]# vim /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml </span><br><span class="line">&lt;!--配置nameservice--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;mycluster,nameservice1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--指定本地服务--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.internal.nameservices&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;mycluster,nameservice1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!--配置多NamenNode--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;nn1,nn2,nn3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hadoop101:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hadoop102:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn3&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hadoop103:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!--配置nameservice1的namenode服务--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.namenodes.nameservice1&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;namenode30,namenode37&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.rpc-address.nameservice1.namenode30&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop104:8020&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.rpc-address.nameservice1.namenode37&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop106:8020&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address.nameservice1.namenode30&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop104:9870&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address.nameservice1.namenode37&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop106:9870&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.failover.proxy.provider.nameservice1&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;!--为NamneNode设置HTTP服务监听--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hadoop101:9870&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hadoop102:9870&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.http-address.mycluster.nn3&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hadoop103:9870&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!--配置HDFS客户端联系Active NameNode节点的Java类--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>6）修改CDH hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# vim /etc/hosts</span><br></pre></td></tr></table></figure><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_190.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>7）进行分发，这里的hadoop104，hadoop105，hadoop106分别对应apache的hadoop101，hadoop102，hadoop103</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# scp /etc/hosts hadoop102:/etc/</span><br><span class="line">[root@hadoop101 ~]# scp /etc/hosts hadoop103:/etc/</span><br></pre></td></tr></table></figure><p>8）同样修改CDH集群配置，在所有hdfs-site.xml文件里修改配置</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_191.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">&lt;value&gt;mycluster,nameservice1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.internal.nameservices&lt;/name&gt;</span><br><span class="line">&lt;value&gt;nameservice1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;</span><br><span class="line">&lt;value&gt;nn1,nn2,nn3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop104:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop105:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.rpc-address.mycluster.nn3&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop106:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop104:9870&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop105:9870&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.http-address.mycluster.nn3&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop106:9870&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;</span><br><span class="line">&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>9）最后注意：重点由于我的Apahce集群和CDH集群3台集群都是hadoop101，hadoop102，hadoop103所以要关闭域名访问，使用IP访问<br>CDH把钩去了</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_192.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>10）apache设置为false</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_193.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>11）再使用hadoop distcp命令进行迁移，-Dmapred.job.queue.name指定队列，默认是default队列。上面配置集群都配了的话，那么在CDH和apache集群下都可以执行这个命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop]# hadoop distcp -Dmapred.job.queue.name=hive  webhdfs://mycluster:9070/user/hive/warehouse/dwd.db/  hdfs://nameservice1/user/hive/warehouse</span><br><span class="line"></span><br></pre></td></tr></table></figure><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_194.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>12）会启动一个MR任务，正在迁移</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_195.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>13）查看cdh 9870 http地址</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_196.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_197.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_198.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>14）数据已经成功迁移。数据迁移成功之后，接下来迁移hive表结构，编写shell脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 module]# vim exportHive.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">hive -e &quot;use dwd;show tables&quot;&gt;tables.txt</span><br><span class="line">cat tables.txt |while read eachline</span><br><span class="line">do</span><br><span class="line">hive -e &quot;use dwd;show create table $eachline&quot;&gt;&gt;tablesDDL.txt</span><br><span class="line">echo &quot;;&quot; &gt;&gt; tablesDDL.txt</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>15）执行脚本后将tablesDDL.txt文件分发到CDH集群下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 module]# scp tablesDDL.txt  hadoop104:/opt/module/</span><br></pre></td></tr></table></figure><p>16）然后CDH下导入此表结构，先进到CDH的hive里创建dwd库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 module]# hive</span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create database dwd;</span></span><br></pre></td></tr></table></figure><p>17）创建数据库后，边界tablesDDL.txt在最上方加上use dwd;</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_199.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>18）并且将createtab_stmt都替换成空格</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 module]# sed -i s&quot;#createtab_stmt# #g&quot; tablesDDL.txt</span><br></pre></td></tr></table></figure><p>19）最后执行hive -f命令将表结构导入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 module]# hive -f tablesDDL.txt </span><br></pre></td></tr></table></figure><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_200.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>20）最后将表的分区重新刷新下，只有刷新分区才能把数据读出来，编写脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 module]# vim msckPartition.sh</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">hive -e &quot;use dwd;show tables&quot;&gt;tables.txt</span><br><span class="line">cat tables.txt |while read eachline</span><br><span class="line">do</span><br><span class="line">hive -e &quot;use dwd;MSCK REPAIR TABLE $eachline&quot;</span><br><span class="line">done</span><br><span class="line">[root@hadoop101 module]# chmod +777 msckPartition.sh </span><br><span class="line">[root@hadoop101 module]# ./msckPartition.sh </span><br></pre></td></tr></table></figure><p>21）刷完分区后，查询表数据</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_201.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><h1 id="MapReduce生产经验"><a href="#MapReduce生产经验" class="headerlink" title="MapReduce生产经验"></a>MapReduce生产经验</h1><h2 id="MapReduce跑的慢的原因"><a href="#MapReduce跑的慢的原因" class="headerlink" title="MapReduce跑的慢的原因"></a>MapReduce跑的慢的原因</h2><p>MapReduce程序效率的瓶颈在于两点：</p><p><strong>1）计算机性能</strong><br>CPU、内存、磁盘、网络</p><p><strong>2）I/O操作优化</strong><br>（1）数据倾斜<br>（2）Map运行时间太长，导致Reduce等待过久<br>（3）小文件过多</p><h2 id="MapReduce常用调优参数"><a href="#MapReduce常用调优参数" class="headerlink" title="MapReduce常用调优参数"></a>MapReduce常用调优参数</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_202.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_203.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><h2 id="MapReduce数据倾斜问题"><a href="#MapReduce数据倾斜问题" class="headerlink" title="MapReduce数据倾斜问题"></a>MapReduce数据倾斜问题</h2><p>1）数据倾斜现象<br>数据频率倾斜——某一个区域的数据量要远远大于其他区域。<br>数据大小倾斜——部分记录的大小远远大于平均值。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_204.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>2）减少数据倾斜的方法<br>（1）首先检查是否空值过多造成的数据倾斜<br>          生产环境，可以直接过滤掉空值；如果想保留空值，就自定义分区，将空值加随机数打散。最后再二次聚合。<br>（2）能在map阶段提前处理，最好先在Map阶段处理。如：Combiner、MapJoin<br>（3）设置多个reduce个数</p><h1 id="Hadoop-Yarn生产经验"><a href="#Hadoop-Yarn生产经验" class="headerlink" title="Hadoop-Yarn生产经验"></a>Hadoop-Yarn生产经验</h1><h2 id="常用的调优参数"><a href="#常用的调优参数" class="headerlink" title="常用的调优参数"></a>常用的调优参数</h2><p>1）调优参数列表<br>（1）Resourcemanager相关</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yarn.resourcemanager.scheduler.client.thread-countResourceManager处理调度器请求的线程数量</span><br><span class="line">yarn.resourcemanager.scheduler.class配置调度器</span><br></pre></td></tr></table></figure><p>（2）Nodemanager相关</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">yarn.nodemanager.resource.memory-mb              NodeManager使用内存数</span><br><span class="line">yarn.nodemanager.resource.system-reserved-memory-mb  NodeManager为系统保留多少内存，和上一个参数二者取一即可</span><br><span class="line"></span><br><span class="line">yarn.nodemanager.resource.cpu-vcoresNodeManager使用CPU核数</span><br><span class="line">yarn.nodemanager.resource.count-logical-processors-as-cores是否将虚拟核数当作CPU核数</span><br><span class="line">yarn.nodemanager.resource.pcores-vcores-multiplier虚拟核数和物理核数乘数，例如：4核8线程，该参数就应设为2</span><br><span class="line">yarn.nodemanager.resource.detect-hardware-capabilities是否让yarn自己检测硬件进行配置</span><br><span class="line"></span><br><span class="line">yarn.nodemanager.pmem-check-enabled是否开启物理内存检查限制container</span><br><span class="line">yarn.nodemanager.vmem-check-enabled是否开启虚拟内存检查限制container</span><br><span class="line">yarn.nodemanager.vmem-pmem-ratio        虚拟内存物理内存比例</span><br></pre></td></tr></table></figure><p>（3）Container容器相关<br>yarn.scheduler.minimum-allocation-mb         容器最小内存</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yarn.scheduler.maximum-allocation-mb     容器最大内存</span><br><span class="line">yarn.scheduler.minimum-allocation-vcores 容器最小核数</span><br><span class="line">yarn.scheduler.maximum-allocation-vcores 容器最大核数</span><br></pre></td></tr></table></figure><h1 id="Hadoop综合调优"><a href="#Hadoop综合调优" class="headerlink" title="Hadoop综合调优"></a>Hadoop综合调优</h1><h2 id="Hadoop小文件优化方法"><a href="#Hadoop小文件优化方法" class="headerlink" title="Hadoop小文件优化方法"></a>Hadoop小文件优化方法</h2><h2 id="Hadoop小文件弊端"><a href="#Hadoop小文件弊端" class="headerlink" title="Hadoop小文件弊端"></a>Hadoop小文件弊端</h2><p>HDFS上每个文件都要在NameNode上创建对应的元数据，这个元数据的大小约为150byte，这样当小文件比较多的时候，就会产生很多的元数据文件，一方面会大量占用NameNode的内存空间，另一方面就是元数据文件过多，使得寻址索引速度变慢。<br>小文件过多，在进行MR计算时，会生成过多切片，需要启动过多的MapTask。每个MapTask处理的数据量小，导致MapTask的处理时间比启动时间还小，白白消耗资源。</p><h2 id="Hadoop小文件解决方案"><a href="#Hadoop小文件解决方案" class="headerlink" title="Hadoop小文件解决方案"></a>Hadoop小文件解决方案</h2><p>1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS（数据源头）<br>2）Hadoop Archive（存储方向）<br>      是一个高效的将小文件放入HDFS块中的文件存档工具，能够将多个小文件打包成一个HAR文件，从而达到减少NameNode的内存使用<br>3）CombineTextInputFormat（计算方向）<br>CombineTextInputFormat用于将多个小文件在切片过程中生成一个单独的切片或者少量的切片。<br>4）开启uber模式，实现JVM重用（计算方向）<br>默认情况下，每个Task任务都需要启动一个JVM来运行，如果Task任务计算的数据量很小，我们可以让同一个Job的多个Task运行在一个JVM中，不必为每个Task都开启一个JVM。<br>    （1）未开启uber模式，在/input路径上上传多个小文件并执行wordcount程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output2</span><br></pre></td></tr></table></figure><p>​    （2）观察控制台<br>2021-02-14 16:13:50,607 INFO mapreduce.Job: Job job_1613281510851_0002 running in uber mode : false<br>（3）观察<a href="http://hadoop103:8088/cluster">http://hadoop103:8088/cluster</a></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_205.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><p>（4）开启uber模式，在mapred-site.xml中添加如下配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--  开启uber模式，默认关闭 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.ubertask.enable&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- uber模式中最大的mapTask数量，可向下修改  --&gt; </span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.ubertask.maxmaps&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;9&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- uber模式中最大的reduce数量，可向下修改 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.ubertask.maxreduces&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- uber模式中最大的输入数据量，默认使用dfs.blocksize 的值，可向下修改 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.ubertask.maxbytes&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（5）分发配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ xsync mapred-site.xml</span><br></pre></td></tr></table></figure><p>（6）再次执行wordcount程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output2</span><br></pre></td></tr></table></figure><p>​    （7）观察控制台</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2021-02-14 16:28:36,198 INFO mapreduce.Job: Job job_1613281510851_0003 running in uber mode : true</span><br></pre></td></tr></table></figure><p>（8）观察<a href="http://hadoop103:8088/cluster">http://hadoop103:8088/cluster</a></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_206.png" width = "" height = "" alt="xubatian的博客: www.xubatian.cn" align="center" /><h2 id="测试MapReduce计算性能"><a href="#测试MapReduce计算性能" class="headerlink" title="测试MapReduce计算性能"></a>测试MapReduce计算性能</h2><p>使用Sort程序评测MapReduce<br>注：一个虚拟机不超过150G磁盘尽量不要执行这段代码<br>（1）使用RandomWriter来产生随机数，每个节点运行10个Map任务，每个Map产生大约1G大小的二进制随机数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar randomwriter random-data</span><br></pre></td></tr></table></figure><p>（2）执行Sort程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar sort random-data sorted-data</span><br></pre></td></tr></table></figure><p>（3）验证数据是否真正排好序了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 mapreduce]$ </span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar testmapredsort -sortInput random-data -sortOutput sorted-data</span><br></pre></td></tr></table></figure><h1 id="企业开发场景案例"><a href="#企业开发场景案例" class="headerlink" title="企业开发场景案例"></a>企业开发场景案例</h1><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>（1）需求：从1G数据中，统计每个单词出现次数。服务器3台，每台配置4G内存，4核CPU，4线程。<br>（2）需求分析：<br>          1G / 128m = 8个MapTask；1个ReduceTask；1个mrAppMaster<br>  平均每个节点运行10个 / 3台 ≈ 3个任务（4    3    3）</p><h2 id="HDFS参数调优"><a href="#HDFS参数调优" class="headerlink" title="HDFS参数调优"></a>HDFS参数调优</h2><p>（1）修改：hadoop-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export HDFS_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=INFO,RFAS -Xmx1024m&quot;</span><br><span class="line"></span><br><span class="line">export HDFS_DATANODE_OPTS=&quot;-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m&quot;</span><br></pre></td></tr></table></figure><p>（2）修改hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- NameNode有一个工作线程池，默认值是10 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;21&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">（3）修改core-site.xml</span><br><span class="line">&lt;!-- 配置垃圾回收时间为60分钟 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;60&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（4）分发配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ xsync hadoop-env.sh hdfs-site.xml core-site.xml</span><br></pre></td></tr></table></figure><h2 id="MapReduce参数调优"><a href="#MapReduce参数调优" class="headerlink" title="MapReduce参数调优"></a>MapReduce参数调优</h2><p>（1）修改mapred-site.xml</p><!-- 环形缓冲区大小，默认100m --><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.task.io.sort.mb&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;100&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 环形缓冲区溢写阈值，默认0.8 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.sort.spill.percent&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;0.80&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- merge合并次数，默认10个 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.task.io.sort.factor&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;10&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- maptask内存，默认1g； maptask堆内存大小默认和该值大小一致mapreduce.map.java.opts --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;-1&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;The amount of memory to request from the scheduler for each    map task. If this is not specified or is non-positive, it is inferred from mapreduce.map.java.opts and mapreduce.job.heap.memory-mb.ratio. If java-opts are also not specified, we set it to 1024.</span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- matask的CPU核数，默认1个 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.cpu.vcores&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- matask异常重试次数，默认4次 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.maxattempts&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 每个Reduce去Map中拉取数据的并行数。默认值是5 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.shuffle.parallelcopies&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;5&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Buffer大小占Reduce可用内存的比例，默认值0.7 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.shuffle.input.buffer.percent&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;0.70&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Buffer中的数据达到多少比例开始写入磁盘，默认值0.66。 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.shuffle.merge.percent&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;0.66&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- reducetask内存，默认1g；reducetask堆内存大小默认和该值大小一致mapreduce.reduce.java.opts --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;-1&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;The amount of memory to request from the scheduler for each    reduce task. If this is not specified or is non-positive, it is inferred</span><br><span class="line">    from mapreduce.reduce.java.opts and mapreduce.job.heap.memory-mb.ratio.</span><br><span class="line">    If java-opts are also not specified, we set it to 1024.</span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- reducetask的CPU核数，默认1个 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.cpu.vcores&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- reducetask失败重试次数，默认4次 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.maxattempts&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 当MapTask完成的比例达到该值后才会为ReduceTask申请资源。默认是0.05 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.reduce.slowstart.completedmaps&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;0.05&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 如果程序在规定的默认10分钟内没有读到数据，将强制超时退出 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.task.timeout&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;600000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（2）分发配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ xsync mapred-site.xml</span><br></pre></td></tr></table></figure><h2 id="Yarn参数调优"><a href="#Yarn参数调优" class="headerlink" title="Yarn参数调优"></a>Yarn参数调优</h2><p>（1）修改yarn-site.xml配置参数如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 选择调度器，默认容量 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;</span><br><span class="line">&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- ResourceManager处理调度器请求的线程数量,默认50；如果提交的任务数大于50，可以增加该值，但是不能超过3台 * 4线程 = 12线程（去除其他应用程序实际不能超过8） --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;Number of threads to handle scheduler interface.&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.scheduler.client.thread-count&lt;/name&gt;</span><br><span class="line">&lt;value&gt;8&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 是否让yarn自动检测硬件进行配置，默认是false，如果该节点有很多其他应用程序，建议手动配置。如果该节点没有其他应用程序，可以采用自动 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;Enable auto-detection of node capabilities such as</span><br><span class="line">memory and CPU.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.resource.detect-hardware-capabilities&lt;/name&gt;</span><br><span class="line">&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 是否将虚拟核数当作CPU核数，默认是false，采用物理CPU核数 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;Flag to determine if logical processors(such as</span><br><span class="line">hyperthreads) should be counted as cores. Only applicable on Linux</span><br><span class="line">when yarn.nodemanager.resource.cpu-vcores is set to -1 and</span><br><span class="line">yarn.nodemanager.resource.detect-hardware-capabilities is true.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.resource.count-logical-processors-as-cores&lt;/name&gt;</span><br><span class="line">&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 虚拟核数和物理核数乘数，默认是1.0 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;Multiplier to determine how to convert phyiscal cores to</span><br><span class="line">vcores. This value is used if yarn.nodemanager.resource.cpu-vcores</span><br><span class="line">is set to -1(which implies auto-calculate vcores) and</span><br><span class="line">yarn.nodemanager.resource.detect-hardware-capabilities is set to true. Thenumber of vcores will be calculated asnumber of CPUs * multiplier.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.resource.pcores-vcores-multiplier&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- NodeManager使用内存数，默认8G，修改为4G内存 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;Amount of physical memory, in MB, that can be allocated </span><br><span class="line">for containers. If set to -1 and</span><br><span class="line">yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">automatically calculated(in case of Windows and Linux).</span><br><span class="line">In other cases, the default is 8192MB.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">&lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- nodemanager的CPU核数，不按照硬件环境自动设定时默认是8个，修改为4个 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;Number of vcores that can be allocated</span><br><span class="line">for containers. This is used by the RM scheduler when allocating</span><br><span class="line">resources for containers. This is not used to limit the number of</span><br><span class="line">CPUs used by YARN containers. If it is set to -1 and</span><br><span class="line">yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">automatically determined from the hardware in case of Windows and Linux.</span><br><span class="line">In other cases, number of vcores is 8 by default.&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;</span><br><span class="line">&lt;value&gt;4&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 容器最小内存，默认1G --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;The minimum allocation for every container request at the RMin MBs. Memory requests lower than this will be set to the value of thisproperty. Additionally, a node manager that is configured to have less memorythan this value will be shut down by the resource manager.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 容器最大内存，默认8G，修改为2G --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;The maximum allocation for every container request at the RMin MBs. Memory requests higher than this will throw anInvalidResourceRequestException.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2048&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 容器最小CPU核数，默认1个 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;The minimum allocation for every container request at the RMin terms of virtual CPU cores. Requests lower than this will be set to thevalue of this property. Additionally, a node manager that is configured tohave fewer virtual cores than this value will be shut down by the resourcemanager.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 容器最大CPU核数，默认4个，修改为2个 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;The maximum allocation for every container request at the RMin terms of virtual CPU cores. Requests higher than this will throw an</span><br><span class="line">InvalidResourceRequestException.&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 虚拟内存检查，默认打开，修改为关闭 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;Whether virtual memory limits will be enforced for</span><br><span class="line">containers.&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 虚拟内存和物理内存设置比例,默认2.1 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;description&gt;Ratio between virtual memory to physical memory whensetting memory limits for containers. Container allocations areexpressed in terms of physical memory, and virtual memory usageis allowed to exceed this allocation by this ratio.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2.1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>（2）分发配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop]$ xsync yarn-site.xml</span><br></pre></td></tr></table></figure><h2 id="执行程序"><a href="#执行程序" class="headerlink" title="执行程序"></a>执行程序</h2><p>（1）重启集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ sbin/stop-yarn.sh</span><br><span class="line">[shangbaishuyao@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><p>（2）执行WordCount程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</span><br></pre></td></tr></table></figure><p>（3）观察Yarn任务执行页面<br><a href="http://hadoop103:8088/cluster/apps">http://hadoop103:8088/cluster/apps</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;我以为的天长地久，却终沦为曾经拥有                            &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="Hadoop企业级优化" scheme="http://xubatian.cn/tags/Hadoop%E4%BC%81%E4%B8%9A%E7%BA%A7%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop企业优化</title>
    <link href="http://xubatian.cn/Hadoop%E4%BC%81%E4%B8%9A%E4%BC%98%E5%8C%96/"/>
    <id>http://xubatian.cn/Hadoop%E4%BC%81%E4%B8%9A%E4%BC%98%E5%8C%96/</id>
    <published>2022-01-16T16:08:45.000Z</published>
    <updated>2022-01-17T13:03:16.049Z</updated>
    
    <content type="html"><![CDATA[<p>“走过高山雄伟，踏遍华夏大地，看过落花流水，见过大海波澜壮阔。还没放下是因为真正的感情根本放不下，不该带着放下的目的来游历。游历是充实，是开阔。不是放松，更不是放下。否则就糟蹋了眼前的美。有些东西做不到就不要勉强，放心…                                             </p><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_137.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><p>前言</p><p>mapreduce 是一个hadoop的计算引擎,是hadoop的几个模块之一. 他是可插拔的.就是说mapreduce是可以换的. 因为mapreduce计算的太慢了. 所以后期我们会将mapreduce换成hive,spark,Flink. 因需求而定.此处了解.</p><h1 id="MapReduce-跑的慢的原因"><a href="#MapReduce-跑的慢的原因" class="headerlink" title="MapReduce 跑的慢的原因"></a>MapReduce 跑的慢的原因</h1><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">MapReduce 程序效率的瓶颈在于两点：</span><br><span class="line">1.计算机性能</span><br><span class="line">       CPU、内存、磁盘健康、网络</span><br><span class="line">2．I/O操作优化</span><br><span class="line">      （1）数据倾斜</span><br><span class="line">      （2）Map和Reduce数设置不合理</span><br><span class="line">      （3）Map运行时间太长，导致Reduce等待过久</span><br><span class="line">      （4）小文件过多</span><br><span class="line">      （5）大量的不可分块的超大文件</span><br><span class="line">      （6）Spill次数过多</span><br><span class="line">      （7）Merge次数过多等。</span><br></pre></td></tr></table></figure><h1 id="MapReduce优化方法"><a href="#MapReduce优化方法" class="headerlink" title="MapReduce优化方法"></a>MapReduce优化方法</h1><p>MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数。</p><h2 id="数据输入"><a href="#数据输入" class="headerlink" title="数据输入"></a>数据输入</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(1) 合并小文件：在执行MR任务前将小文件进行合并，大量的小文件会产生大量的Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢。</span><br><span class="line">(2) 采用CombineTextlnputFormat来作为输入，解决输入端大量小文件场景。</span><br></pre></td></tr></table></figure><h2 id="Map阶段"><a href="#Map阶段" class="headerlink" title="Map阶段"></a>Map阶段</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">（1）减少溢写（Spill）次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO。</span><br><span class="line">（2）减少合并（Merge）次数：通过调整io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间。</span><br><span class="line">（3）在Map之后，不影响业务逻辑前提下，先进行Combine处理，减少I/O </span><br></pre></td></tr></table></figure><h2 id="Reduce阶段"><a href="#Reduce阶段" class="headerlink" title="Reduce阶段"></a>Reduce阶段</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（1）合理设置Map和Reduce数：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map、Reduce任务间  竞争资源，造成处理超时等错误。</span><br><span class="line">（2）设置Map、Reduce共存：调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间。</span><br><span class="line">（3）规避使用Reduce：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。</span><br><span class="line">（4）合理设置Reduce端的Buffer：默认情况下，数据达到一个阈值的时候，Buffer中的数据就会写入磁盘，然后Reduce会从磁盘中获得所有的数据。也就是说，Buffer和Reduce是没有直接关联的，中间多次写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得Buffer中的一部分数据可以直接输送到Reduce，从而减少IO开销：mapreduce.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读Buffer中的数据直接拿给Reduce使用。这样一来，设置Buffer需要内存，读取数据需要内存，Reduce计算也要内存，所以要根据作业的运行情况进行调整。</span><br></pre></td></tr></table></figure><h2 id="I-O传输"><a href="#I-O传输" class="headerlink" title="I/O传输"></a>I/O传输</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(1)采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZO压缩编码器。</span><br><span class="line">(2)使用SequenceFile二进制文件。</span><br></pre></td></tr></table></figure><h2 id="数据倾斜问题"><a href="#数据倾斜问题" class="headerlink" title="数据倾斜问题"></a>数据倾斜问题</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1. 数据倾斜现象</span><br><span class="line">      数据频率倾斜------某一个区域的数据量要远远大于其他区域。</span><br><span class="line">      数据大小倾斜------部分记录的大小远远大于平均值。</span><br><span class="line">2. 减少数据倾斜的方法</span><br><span class="line">方法1: 抽样和范围分区可以通过对原始数据进行抽样得到的结果集来预设分区边界值。</span><br><span class="line">方法2：自定义分区基于输出键的背景知识进行自定义分区。例如，如果Map输出键的单词来源于一本书。且其中某几个专业词汇较多。那么就可以自定义分区将这这些专业词汇发送给固定的一部分Reduce实例。而将其他的都发送给剩余的Reduce实例。</span><br><span class="line">方法3：Combine使用Combine可以大量地减小数据倾斜。在可能的情况下，Combine的目的就是聚合并精简数据。</span><br><span class="line">方法4：采用Map Join，尽量避免Reduce Join 。</span><br></pre></td></tr></table></figure><h2 id="常用的调优参数"><a href="#常用的调优参数" class="headerlink" title="常用的调优参数"></a>常用的调优参数</h2><h3 id="1．资源相关参数"><a href="#1．资源相关参数" class="headerlink" title="1．资源相关参数"></a>1．资源相关参数</h3><p>（1）以下参数是在用户自己的MR应用程序中配置就可以生效（mapred-default.xml）</p><table><thead><tr><th>配置参数</th><th>参数说明</th></tr></thead><tbody><tr><td>mapreduce.map.memory.mb</td><td>一个MapTask可使用的资源上限（单位:MB），默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死。</td></tr><tr><td>mapreduce.reduce.memory.mb</td><td>一个ReduceTask可使用的资源上限（单位:MB），默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死。</td></tr><tr><td>mapreduce.map.cpu.vcores</td><td>每个MapTask可使用的最多cpu core数目，默认值: 1</td></tr><tr><td>mapreduce.reduce.cpu.vcores</td><td>每个ReduceTask可使用的最多cpu core数目，默认值: 1</td></tr><tr><td>mapreduce.reduce.shuffle.parallelcopies</td><td>每个Reduce去Map中取数据的并行数。默认值是5</td></tr><tr><td>mapreduce.reduce.shuffle.merge.percent</td><td>Buffer中的数据达到多少比例开始写入磁盘。默认值0.66</td></tr><tr><td>mapreduce.reduce.shuffle.input.buffer.percent</td><td>Buffer大小占Reduce可用内存的比例。默认值0.7</td></tr><tr><td>mapreduce.reduce.input.buffer.percent</td><td>指定多少比例的内存用来存放Buffer中的数据，默认值是0.0</td></tr></tbody></table><p>(2)  应该在YARN启动之前就配置在服务器的配置文件中才能生效（yarn-default.xml）</p><table><thead><tr><th>配置参数</th><th>参数说明</th></tr></thead><tbody><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>给应用程序Container分配的最小内存，默认值：1024</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>给应用程序Container分配的最大内存，默认值：8192</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>每个Container申请的最小CPU核数，默认值：1</td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>每个Container申请的最大CPU核数，默认值：32</td></tr><tr><td>yarn.nodemanager.resource.memory-mb</td><td>给Containers分配的最大物理内存，默认值：8192</td></tr></tbody></table><p>（3）Shuffle性能优化的关键参数，应在YARN启动之前就配置好（mapred-default.xml）</p><table><thead><tr><th>配置参数</th><th>参数说明</th></tr></thead><tbody><tr><td>mapreduce.task.io.sort.mb</td><td>Shuffle的环形缓冲区大小，默认100m</td></tr><tr><td>mapreduce.map.sort.spill.percent</td><td>环形缓冲区溢出的阈值，默认80%</td></tr></tbody></table><h3 id="2．容错相关参数-MapReduce性能优化"><a href="#2．容错相关参数-MapReduce性能优化" class="headerlink" title="2．容错相关参数(MapReduce性能优化)"></a>2．容错相关参数(MapReduce性能优化)</h3><table><thead><tr><th>配置参数</th><th>参数说明</th></tr></thead><tbody><tr><td>mapreduce.map.maxattempts</td><td>每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。</td></tr><tr><td>mapreduce.reduce.maxattempts</td><td>每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。</td></tr><tr><td>mapreduce.task.timeout</td><td>Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是600000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。</td></tr></tbody></table><h2 id="HDFS小文件优化方法"><a href="#HDFS小文件优化方法" class="headerlink" title="HDFS小文件优化方法"></a>HDFS小文件优化方法</h2><h3 id="HDFS小文件弊端"><a href="#HDFS小文件弊端" class="headerlink" title="HDFS小文件弊端"></a>HDFS小文件弊端</h3><p>HDFS上每个文件都要在NameNode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢。</p><h3 id="HDFS小文件解决方案"><a href="#HDFS小文件解决方案" class="headerlink" title="HDFS小文件解决方案"></a>HDFS小文件解决方案</h3><p>小文件的优化无非以下几种方式：</p><p>（1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS。</p><p>（2）在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并。</p><p>（3）在MapReduce处理时，可采用CombineTextInputFormat提高效率。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_140.png" width = "" height = "" alt="xubatian的博客" align="center" /><p><strong>重要:开启JVM重(chong)用效果是非常显著的.</strong></p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_141.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="MapReduce扩展案例"><a href="#MapReduce扩展案例" class="headerlink" title="MapReduce扩展案例"></a>MapReduce扩展案例</h1><h2 id="倒排索引案例（多job串联）"><a href="#倒排索引案例（多job串联）" class="headerlink" title="倒排索引案例（多job串联）"></a>倒排索引案例（多job串联）</h2><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><p>有大量的文本（文档、网页），需要建立搜索索引</p><h3 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h3><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_143.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="第一次处理案例代码"><a href="#第一次处理案例代码" class="headerlink" title="第一次处理案例代码"></a>第一次处理案例代码</h3><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/InvertedIndex/FirstTreatment/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/InvertedIndex/FirstTreatment/</a></p><h3 id="第二次处理案例代码"><a href="#第二次处理案例代码" class="headerlink" title="第二次处理案例代码"></a>第二次处理案例代码</h3><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/InvertedIndex/SecondTreatment/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/InvertedIndex/SecondTreatment/</a></p><h2 id="TopN案例"><a href="#TopN案例" class="headerlink" title="TopN案例"></a>TopN案例</h2><h3 id="需求-1"><a href="#需求-1" class="headerlink" title="需求"></a>需求</h3><p>对需求输出结果进行加工，输出流量使用量在前10的用户信息</p><h3 id="需求分析-1"><a href="#需求分析-1" class="headerlink" title="需求分析"></a>需求分析</h3><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_144.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="案例代码"><a href="#案例代码" class="headerlink" title="案例代码"></a>案例代码</h3><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/TopN/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/TopN/</a></p><h2 id="找博客共同好友案例"><a href="#找博客共同好友案例" class="headerlink" title="找博客共同好友案例"></a>找博客共同好友案例</h2><h3 id="需求-2"><a href="#需求-2" class="headerlink" title="需求"></a>需求</h3><p>以下是博客的好友列表数据，冒号前是一个用户，冒号后是该用户的所有好友（数据中的好友关系是单向的）<br>求出哪些人两两之间有共同好友，及他俩的共同好友都有谁？</p><p>数据输入:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">A:B,C,D,F,E,O</span><br><span class="line"></span><br><span class="line">B:A,C,E,K</span><br><span class="line"></span><br><span class="line">C:F,A,D,I</span><br><span class="line"></span><br><span class="line">D:A,E,F,L</span><br><span class="line"></span><br><span class="line">E:B,C,D,M,L</span><br><span class="line"></span><br><span class="line">F:A,B,C,D,E,O,M</span><br><span class="line"></span><br><span class="line">G:A,C,D,E,F</span><br><span class="line"></span><br><span class="line">H:A,C,D,E,O</span><br><span class="line"></span><br><span class="line">I:A,O</span><br><span class="line"></span><br><span class="line">J:B,O</span><br><span class="line"></span><br><span class="line">K:A,C,D</span><br><span class="line"></span><br><span class="line">L:D,E,F</span><br><span class="line"></span><br><span class="line">M:E,F,G</span><br><span class="line"></span><br><span class="line">O:A,H,I,J</span><br></pre></td></tr></table></figure><h3 id="需求分析-2"><a href="#需求分析-2" class="headerlink" title="需求分析"></a>需求分析</h3><p>先求出A、B、C、….等是谁的好友</p><p>第一次输出结果</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">AI,K,C,B,G,F,H,O,D,</span><br><span class="line"></span><br><span class="line">BA,F,J,E,</span><br><span class="line"></span><br><span class="line">CA,E,B,H,F,G,K,</span><br><span class="line"></span><br><span class="line">DG,C,K,A,L,F,E,H,</span><br><span class="line"></span><br><span class="line">EG,M,L,H,A,F,B,D,</span><br><span class="line"></span><br><span class="line">FL,M,D,C,G,A,</span><br><span class="line"></span><br><span class="line">GM,</span><br><span class="line"></span><br><span class="line">HO,</span><br><span class="line"></span><br><span class="line">IO,C,</span><br><span class="line"></span><br><span class="line">JO,</span><br><span class="line"></span><br><span class="line">KB,</span><br><span class="line"></span><br><span class="line">LD,E,</span><br><span class="line"></span><br><span class="line">ME,F,</span><br><span class="line"></span><br><span class="line">OA,H,I,J,F,</span><br></pre></td></tr></table></figure><p>第二次输出结果</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line">A-BE C </span><br><span class="line"></span><br><span class="line">A-CD F </span><br><span class="line"></span><br><span class="line">A-DE F </span><br><span class="line"></span><br><span class="line">A-ED B C </span><br><span class="line"></span><br><span class="line">A-FO B C D E </span><br><span class="line"></span><br><span class="line">A-GF E C D </span><br><span class="line"></span><br><span class="line">A-HE C D O </span><br><span class="line"></span><br><span class="line">A-IO </span><br><span class="line"></span><br><span class="line">A-JO B </span><br><span class="line"></span><br><span class="line">A-KD C </span><br><span class="line"></span><br><span class="line">A-LF E D </span><br><span class="line"></span><br><span class="line">A-ME F </span><br><span class="line"></span><br><span class="line">B-CA </span><br><span class="line"></span><br><span class="line">B-DA E </span><br><span class="line"></span><br><span class="line">B-EC </span><br><span class="line"></span><br><span class="line">B-FE A C </span><br><span class="line"></span><br><span class="line">B-GC E A </span><br><span class="line"></span><br><span class="line">B-HA E C </span><br><span class="line"></span><br><span class="line">B-IA </span><br><span class="line"></span><br><span class="line">B-KC A </span><br><span class="line"></span><br><span class="line">B-LE </span><br><span class="line"></span><br><span class="line">B-ME </span><br><span class="line"></span><br><span class="line">B-OA </span><br><span class="line"></span><br><span class="line">C-DA F </span><br><span class="line"></span><br><span class="line">C-ED </span><br><span class="line"></span><br><span class="line">C-FD A </span><br><span class="line"></span><br><span class="line">C-GD F A </span><br><span class="line"></span><br><span class="line">C-HD A </span><br><span class="line"></span><br><span class="line">C-IA </span><br><span class="line"></span><br><span class="line">C-KA D </span><br><span class="line"></span><br><span class="line">C-LD F </span><br><span class="line"></span><br><span class="line">C-MF </span><br><span class="line"></span><br><span class="line">C-OI A </span><br><span class="line"></span><br><span class="line">D-EL </span><br><span class="line"></span><br><span class="line">D-FA E </span><br><span class="line"></span><br><span class="line">D-GE A F </span><br><span class="line"></span><br><span class="line">D-HA E </span><br><span class="line"></span><br><span class="line">D-IA </span><br><span class="line"></span><br><span class="line">D-KA </span><br><span class="line"></span><br><span class="line">D-LE F </span><br><span class="line"></span><br><span class="line">D-MF E </span><br><span class="line"></span><br><span class="line">D-OA </span><br><span class="line"></span><br><span class="line">E-FD M C B </span><br><span class="line"></span><br><span class="line">E-GC D </span><br><span class="line"></span><br><span class="line">E-HC D </span><br><span class="line"></span><br><span class="line">E-JB </span><br><span class="line"></span><br><span class="line">E-KC D </span><br><span class="line"></span><br><span class="line">E-LD </span><br><span class="line"></span><br><span class="line">F-GD C A E </span><br><span class="line"></span><br><span class="line">F-HA D O E C </span><br><span class="line"></span><br><span class="line">F-IO A </span><br><span class="line"></span><br><span class="line">F-JB O </span><br><span class="line"></span><br><span class="line">F-KD C A </span><br><span class="line"></span><br><span class="line">F-LE D </span><br><span class="line"></span><br><span class="line">F-ME </span><br><span class="line"></span><br><span class="line">F-OA </span><br><span class="line"></span><br><span class="line">G-HD C E A </span><br><span class="line"></span><br><span class="line">G-IA </span><br><span class="line"></span><br><span class="line">G-KD A C </span><br><span class="line"></span><br><span class="line">G-LD F E </span><br><span class="line"></span><br><span class="line">G-ME F </span><br><span class="line"></span><br><span class="line">G-OA </span><br><span class="line"></span><br><span class="line">H-IO A </span><br><span class="line"></span><br><span class="line">H-JO </span><br><span class="line"></span><br><span class="line">H-KA C D </span><br><span class="line"></span><br><span class="line">H-LD E </span><br><span class="line"></span><br><span class="line">H-ME </span><br><span class="line"></span><br><span class="line">H-OA </span><br><span class="line"></span><br><span class="line">I-JO </span><br><span class="line"></span><br><span class="line">I-KA </span><br><span class="line"></span><br><span class="line">I-OA </span><br><span class="line"></span><br><span class="line">K-LD </span><br><span class="line"></span><br><span class="line">K-OA </span><br><span class="line"></span><br><span class="line">L-ME F</span><br></pre></td></tr></table></figure><h3 id="案例代码-1"><a href="#案例代码-1" class="headerlink" title="案例代码"></a>案例代码</h3><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/FindBlogFriends/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/FindBlogFriends/</a></p><h1 id="常见错误及解决方案"><a href="#常见错误及解决方案" class="headerlink" title="常见错误及解决方案"></a>常见错误及解决方案</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>）导包容易出错。尤其Text和CombineTextInputFormat。</span><br><span class="line"><span class="number">2</span>）Mapper中第一个输入的参数必须是LongWritable或者NullWritable，不可以是IntWritable.  报的错误是类型转换异常。</span><br><span class="line"><span class="number">3</span>）java.lang.Exception: java.io.IOException: Illegal partition <span class="keyword">for</span> <span class="number">13926435656</span> (<span class="number">4</span>)，说明Partition和ReduceTask个数没对上，调整ReduceTask个数。</span><br><span class="line"><span class="number">4</span>）如果分区数不是<span class="number">1</span>，但是reducetask为<span class="number">1</span>，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于<span class="number">1</span>。不大于<span class="number">1</span>肯定不执行。</span><br><span class="line"><span class="number">5</span>）在Windows环境编译的jar包导入到Linux环境中运行，</span><br><span class="line">hadoop jar wc.jar com.atguigu.mapreduce.wordcount.WordCountDriver /user/atguigu/ /user/atguigu/output</span><br><span class="line">报如下错误：</span><br><span class="line">Exception in thread <span class="string">&quot;main&quot;</span> java.lang.UnsupportedClassVersionError: com/atguigu/mapreduce/wordcount/WordCountDriver : Unsupported major.minor version <span class="number">52.0</span></span><br><span class="line">原因是Windows环境用的jdk1<span class="number">.7</span>，Linux环境用的jdk1<span class="number">.8</span>。</span><br><span class="line">解决方案：统一jdk版本。</span><br><span class="line"><span class="number">6</span>）缓存pd.txt小文件案例中，报找不到pd.txt文件</span><br><span class="line">原因：大部分为路径书写错误。还有就是要检查pd.txt.txt的问题。还有个别电脑写相对路径找不到pd.txt，可以修改为绝对路径。</span><br><span class="line"><span class="number">7</span>）报类型转换异常。</span><br><span class="line">通常都是在驱动函数中设置Map输出和最终输出时编写错误。</span><br><span class="line">Map输出的key如果没有排序，也会报类型转换异常。</span><br><span class="line"><span class="number">8</span>）集群中运行wc.jar时出现了无法获得输入文件。</span><br><span class="line">原因：WordCount案例的输入文件不能放用HDFS集群的根目录。</span><br><span class="line"><span class="number">9</span>）出现了如下相关异常</span><br><span class="line">Exception in thread <span class="string">&quot;main&quot;</span> java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z</span><br><span class="line">at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)</span><br><span class="line">at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:<span class="number">609</span>)</span><br><span class="line">at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:<span class="number">977</span>)</span><br><span class="line">java.io.IOException: Could not locate executable <span class="keyword">null</span>\bin\winutils.exe in the Hadoop binaries.</span><br><span class="line">at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:<span class="number">356</span>)</span><br><span class="line">at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:<span class="number">371</span>)</span><br><span class="line">at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:<span class="number">364</span>)</span><br><span class="line">解决方案：拷贝hadoop.dll文件到Windows目录C:\Windows\System32。个别同学电脑还需要修改Hadoop源码。</span><br><span class="line">方案二：创建如下包名，并将NativeIO.java拷贝到该包名下</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_145.png" width = "" height = "" alt="xubatian的博客" align="center" /><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span>）自定义Outputformat时，注意在RecordWirter中的close方法必须关闭流资源。否则输出的文件内容中数据为空。</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (atguigufos != <span class="keyword">null</span>) &#123;</span><br><span class="line">atguigufos.close();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (otherfos != <span class="keyword">null</span>) &#123;</span><br><span class="line">otherfos.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>NativelO.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br><span class="line">924</span><br><span class="line">925</span><br><span class="line">926</span><br><span class="line">927</span><br><span class="line">928</span><br><span class="line">929</span><br><span class="line">930</span><br><span class="line">931</span><br><span class="line">932</span><br><span class="line">933</span><br><span class="line">934</span><br><span class="line">935</span><br><span class="line">936</span><br><span class="line">937</span><br><span class="line">938</span><br><span class="line">939</span><br><span class="line">940</span><br><span class="line">941</span><br><span class="line">942</span><br><span class="line">943</span><br><span class="line">944</span><br><span class="line">945</span><br><span class="line">946</span><br><span class="line">947</span><br><span class="line">948</span><br><span class="line">949</span><br><span class="line">950</span><br><span class="line">951</span><br><span class="line">952</span><br><span class="line">953</span><br><span class="line">954</span><br><span class="line">955</span><br><span class="line">956</span><br><span class="line">957</span><br><span class="line">958</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment"> * or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment"> * distributed with this work for additional information</span></span><br><span class="line"><span class="comment"> * regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment"> * to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment"> * &quot;License&quot;); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment"> * with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">package</span> org.apache.hadoop.io.nativeio;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileDescriptor;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.RandomAccessFile;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Field;</span><br><span class="line"><span class="keyword">import</span> java.nio.ByteBuffer;</span><br><span class="line"><span class="keyword">import</span> java.nio.MappedByteBuffer;</span><br><span class="line"><span class="keyword">import</span> java.nio.channels.FileChannel;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ConcurrentHashMap;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.classification.InterfaceAudience;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.classification.InterfaceStability;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.CommonConfigurationKeys;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.HardLink;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.SecureIOUtils.AlreadyExistsException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.NativeCodeLoader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Shell;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.PerformanceAdvisory;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.logging.Log;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.logging.LogFactory;</span><br><span class="line"><span class="keyword">import</span> sun.misc.Unsafe;</span><br><span class="line"><span class="keyword">import</span> com.google.common.annotations.VisibleForTesting;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * JNI wrappers for various native IO-related calls not available in Java. These</span></span><br><span class="line"><span class="comment"> * functions should generally be used alongside a fallback to another more</span></span><br><span class="line"><span class="comment"> * portable mechanism.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@InterfaceAudience</span>.Private</span><br><span class="line"><span class="meta">@InterfaceStability</span>.Unstable</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NativeIO</span> </span>&#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">POSIX</span> </span>&#123;</span><br><span class="line"><span class="comment">// Flags for open() call from bits/fcntl.h</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_RDONLY = <span class="number">00</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_WRONLY = <span class="number">01</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_RDWR = <span class="number">02</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_CREAT = <span class="number">0100</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_EXCL = <span class="number">0200</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_NOCTTY = <span class="number">0400</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_TRUNC = <span class="number">01000</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_APPEND = <span class="number">02000</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_NONBLOCK = <span class="number">04000</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_SYNC = <span class="number">010000</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_ASYNC = <span class="number">020000</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_FSYNC = O_SYNC;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> O_NDELAY = O_NONBLOCK;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Flags for posix_fadvise() from bits/fcntl.h</span></span><br><span class="line"><span class="comment">/* No further special treatment. */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> POSIX_FADV_NORMAL = <span class="number">0</span>;</span><br><span class="line"><span class="comment">/* Expect random page references. */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> POSIX_FADV_RANDOM = <span class="number">1</span>;</span><br><span class="line"><span class="comment">/* Expect sequential page references. */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> POSIX_FADV_SEQUENTIAL = <span class="number">2</span>;</span><br><span class="line"><span class="comment">/* Will need these pages. */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> POSIX_FADV_WILLNEED = <span class="number">3</span>;</span><br><span class="line"><span class="comment">/* Don&#x27;t need these pages. */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> POSIX_FADV_DONTNEED = <span class="number">4</span>;</span><br><span class="line"><span class="comment">/* Data will be accessed once. */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> POSIX_FADV_NOREUSE = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Wait upon writeout of all pages in the range before performing the write.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SYNC_FILE_RANGE_WAIT_BEFORE = <span class="number">1</span>;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Initiate writeout of all those dirty pages in the range which are not</span></span><br><span class="line"><span class="comment"> * presently under writeback.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SYNC_FILE_RANGE_WRITE = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Wait upon writeout of all pages in the range after performing the write.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SYNC_FILE_RANGE_WAIT_AFTER = <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Log LOG = LogFactory.getLog(NativeIO.class);</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> nativeLoaded = <span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> fadvisePossible = <span class="keyword">true</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> syncFileRangePossible = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> String WORKAROUND_NON_THREADSAFE_CALLS_KEY = <span class="string">&quot;hadoop.workaround.non.threadsafe.getpwuid&quot;</span>;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">boolean</span> WORKAROUND_NON_THREADSAFE_CALLS_DEFAULT = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> cacheTimeout = -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> CacheManipulator cacheManipulator = <span class="keyword">new</span> CacheManipulator();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> CacheManipulator <span class="title">getCacheManipulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> cacheManipulator;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setCacheManipulator</span><span class="params">(CacheManipulator cacheManipulator)</span> </span>&#123;</span><br><span class="line">POSIX.cacheManipulator = cacheManipulator;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Used to manipulate the operating system cache.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@VisibleForTesting</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CacheManipulator</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mlock</span><span class="params">(String identifier, ByteBuffer buffer, <span class="keyword">long</span> len)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">POSIX.mlock(buffer, len);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getMemlockLimit</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> NativeIO.getMemlockLimit();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getOperatingSystemPageSize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> NativeIO.getOperatingSystemPageSize();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">posixFadviseIfPossible</span><span class="params">(String identifier, FileDescriptor fd, <span class="keyword">long</span> offset, <span class="keyword">long</span> len, <span class="keyword">int</span> flags)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> NativeIOException </span>&#123;</span><br><span class="line">NativeIO.POSIX.posixFadviseIfPossible(identifier, fd, offset, len, flags);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">verifyCanMlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> NativeIO.isAvailable();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A CacheManipulator used for testing which does not actually call mlock. This</span></span><br><span class="line"><span class="comment"> * allows many tests to be run even when the operating system does not allow</span></span><br><span class="line"><span class="comment"> * mlock, or only allows limited mlocking.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@VisibleForTesting</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">NoMlockCacheManipulator</span> <span class="keyword">extends</span> <span class="title">CacheManipulator</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mlock</span><span class="params">(String identifier, ByteBuffer buffer, <span class="keyword">long</span> len)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">LOG.info(<span class="string">&quot;mlocking &quot;</span> + identifier);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getMemlockLimit</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="number">1125899906842624L</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getOperatingSystemPageSize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="number">4096</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">verifyCanMlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (NativeCodeLoader.isNativeCodeLoaded()) &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">workaroundNonThreadSafePasswdCalls = conf.getBoolean(WORKAROUND_NON_THREADSAFE_CALLS_KEY,</span><br><span class="line">WORKAROUND_NON_THREADSAFE_CALLS_DEFAULT);</span><br><span class="line"></span><br><span class="line">initNative();</span><br><span class="line">nativeLoaded = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">cacheTimeout = conf.getLong(CommonConfigurationKeys.HADOOP_SECURITY_UID_NAME_CACHE_TIMEOUT_KEY,</span><br><span class="line">CommonConfigurationKeys.HADOOP_SECURITY_UID_NAME_CACHE_TIMEOUT_DEFAULT) * <span class="number">1000</span>;</span><br><span class="line">LOG.debug(<span class="string">&quot;Initialized cache for IDs to User/Group mapping with a &quot;</span> + <span class="string">&quot; cache timeout of &quot;</span></span><br><span class="line">+ cacheTimeout / <span class="number">1000</span> + <span class="string">&quot; seconds.&quot;</span>);</span><br><span class="line"></span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line"><span class="comment">// This can happen if the user has an older version of libhadoop.so</span></span><br><span class="line"><span class="comment">// installed - in this case we can continue without native IO</span></span><br><span class="line"><span class="comment">// after warning</span></span><br><span class="line">PerformanceAdvisory.LOG.debug(<span class="string">&quot;Unable to initialize NativeIO libraries&quot;</span>, t);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return true if the JNI-based native IO extensions are available.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isAvailable</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> NativeCodeLoader.isNativeCodeLoaded() &amp;&amp; nativeLoaded;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">assertCodeLoaded</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!isAvailable()) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;NativeIO was not loaded&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Wrapper around open(2) */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> FileDescriptor <span class="title">open</span><span class="params">(String path, <span class="keyword">int</span> flags, <span class="keyword">int</span> mode)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Wrapper around fstat(2) */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> Stat <span class="title">fstat</span><span class="params">(FileDescriptor fd)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Native chmod implementation. On UNIX, it is a wrapper around chmod(2) */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">chmodImpl</span><span class="params">(String path, <span class="keyword">int</span> mode)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">chmod</span><span class="params">(String path, <span class="keyword">int</span> mode)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!Shell.WINDOWS) &#123;</span><br><span class="line">chmodImpl(path, mode);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">chmodImpl(path, mode);</span><br><span class="line">&#125; <span class="keyword">catch</span> (NativeIOException nioe) &#123;</span><br><span class="line"><span class="keyword">if</span> (nioe.getErrorCode() == <span class="number">3</span>) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> NativeIOException(<span class="string">&quot;No such file or directory&quot;</span>, Errno.ENOENT);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">LOG.warn(</span><br><span class="line">String.format(<span class="string">&quot;NativeIO.chmod error (%d): %s&quot;</span>, nioe.getErrorCode(), nioe.getMessage()));</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> NativeIOException(<span class="string">&quot;Unknown error&quot;</span>, Errno.UNKNOWN);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Wrapper around posix_fadvise(2) */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">posix_fadvise</span><span class="params">(FileDescriptor fd, <span class="keyword">long</span> offset, <span class="keyword">long</span> len, <span class="keyword">int</span> flags)</span> <span class="keyword">throws</span> NativeIOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Wrapper around sync_file_range(2) */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">sync_file_range</span><span class="params">(FileDescriptor fd, <span class="keyword">long</span> offset, <span class="keyword">long</span> nbytes, <span class="keyword">int</span> flags)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> NativeIOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Call posix_fadvise on the given file descriptor. See the manpage for this</span></span><br><span class="line"><span class="comment"> * syscall for more information. On systems where this call is not available,</span></span><br><span class="line"><span class="comment"> * does nothing.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> NativeIOException</span></span><br><span class="line"><span class="comment"> *             if there is an error with the syscall</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">posixFadviseIfPossible</span><span class="params">(String identifier, FileDescriptor fd, <span class="keyword">long</span> offset, <span class="keyword">long</span> len, <span class="keyword">int</span> flags)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> NativeIOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (nativeLoaded &amp;&amp; fadvisePossible) &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">posix_fadvise(fd, offset, len, flags);</span><br><span class="line">&#125; <span class="keyword">catch</span> (UnsupportedOperationException uoe) &#123;</span><br><span class="line">fadvisePossible = <span class="keyword">false</span>;</span><br><span class="line">&#125; <span class="keyword">catch</span> (UnsatisfiedLinkError ule) &#123;</span><br><span class="line">fadvisePossible = <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Call sync_file_range on the given file descriptor. See the manpage for this</span></span><br><span class="line"><span class="comment"> * syscall for more information. On systems where this call is not available,</span></span><br><span class="line"><span class="comment"> * does nothing.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> NativeIOException</span></span><br><span class="line"><span class="comment"> *             if there is an error with the syscall</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">syncFileRangeIfPossible</span><span class="params">(FileDescriptor fd, <span class="keyword">long</span> offset, <span class="keyword">long</span> nbytes, <span class="keyword">int</span> flags)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> NativeIOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (nativeLoaded &amp;&amp; syncFileRangePossible) &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">sync_file_range(fd, offset, nbytes, flags);</span><br><span class="line">&#125; <span class="keyword">catch</span> (UnsupportedOperationException uoe) &#123;</span><br><span class="line">syncFileRangePossible = <span class="keyword">false</span>;</span><br><span class="line">&#125; <span class="keyword">catch</span> (UnsatisfiedLinkError ule) &#123;</span><br><span class="line">syncFileRangePossible = <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">mlock_native</span><span class="params">(ByteBuffer buffer, <span class="keyword">long</span> len)</span> <span class="keyword">throws</span> NativeIOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Locks the provided direct ByteBuffer into memory, preventing it from swapping</span></span><br><span class="line"><span class="comment"> * out. After a buffer is locked, future accesses will not incur a page fault.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * See the mlock(2) man page for more information.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> NativeIOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">mlock</span><span class="params">(ByteBuffer buffer, <span class="keyword">long</span> len)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">assertCodeLoaded();</span><br><span class="line"><span class="keyword">if</span> (!buffer.isDirect()) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;Cannot mlock a non-direct ByteBuffer&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">mlock_native(buffer, len);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Unmaps the block from memory. See munmap(2).</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * There isn&#x27;t any portable way to unmap a memory region in Java. So we use the</span></span><br><span class="line"><span class="comment"> * sun.nio method here. Note that unmapping a memory region could cause crashes</span></span><br><span class="line"><span class="comment"> * if code continues to reference the unmapped code. However, if we don&#x27;t</span></span><br><span class="line"><span class="comment"> * manually unmap the memory, we are dependent on the finalizer to do it, and we</span></span><br><span class="line"><span class="comment"> * have no idea when the finalizer will run.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> buffer</span></span><br><span class="line"><span class="comment"> *            The buffer to unmap.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">munmap</span><span class="params">(MappedByteBuffer buffer)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (buffer <span class="keyword">instanceof</span> sun.nio.ch.DirectBuffer) &#123;</span><br><span class="line">sun.misc.Cleaner cleaner = ((sun.nio.ch.DirectBuffer) buffer).cleaner();</span><br><span class="line">cleaner.clean();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Linux only methods used for getOwner() implementation */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">long</span> <span class="title">getUIDforFDOwnerforOwner</span><span class="params">(FileDescriptor fd)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> String <span class="title">getUserName</span><span class="params">(<span class="keyword">long</span> uid)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Result type of the fstat call</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Stat</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> ownerId, groupId;</span><br><span class="line"><span class="keyword">private</span> String owner, group;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> mode;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Mode constants</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFMT = <span class="number">0170000</span>; <span class="comment">/* type of file */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFIFO = <span class="number">0010000</span>; <span class="comment">/* named pipe (fifo) */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFCHR = <span class="number">0020000</span>; <span class="comment">/* character special */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFDIR = <span class="number">0040000</span>; <span class="comment">/* directory */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFBLK = <span class="number">0060000</span>; <span class="comment">/* block special */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFREG = <span class="number">0100000</span>; <span class="comment">/* regular */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFLNK = <span class="number">0120000</span>; <span class="comment">/* symbolic link */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFSOCK = <span class="number">0140000</span>; <span class="comment">/* socket */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IFWHT = <span class="number">0160000</span>; <span class="comment">/* whiteout */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_ISUID = <span class="number">0004000</span>; <span class="comment">/* set user id on execution */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_ISGID = <span class="number">0002000</span>; <span class="comment">/* set group id on execution */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_ISVTX = <span class="number">0001000</span>; <span class="comment">/* save swapped text even after use */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IRUSR = <span class="number">0000400</span>; <span class="comment">/* read permission, owner */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IWUSR = <span class="number">0000200</span>; <span class="comment">/* write permission, owner */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> S_IXUSR = <span class="number">0000100</span>; <span class="comment">/* execute/search permission, owner */</span></span><br><span class="line"></span><br><span class="line">Stat(<span class="keyword">int</span> ownerId, <span class="keyword">int</span> groupId, <span class="keyword">int</span> mode) &#123;</span><br><span class="line"><span class="keyword">this</span>.ownerId = ownerId;</span><br><span class="line"><span class="keyword">this</span>.groupId = groupId;</span><br><span class="line"><span class="keyword">this</span>.mode = mode;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Stat(String owner, String group, <span class="keyword">int</span> mode) &#123;</span><br><span class="line"><span class="keyword">if</span> (!Shell.WINDOWS) &#123;</span><br><span class="line"><span class="keyword">this</span>.owner = owner;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">this</span>.owner = stripDomain(owner);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (!Shell.WINDOWS) &#123;</span><br><span class="line"><span class="keyword">this</span>.group = group;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">this</span>.group = stripDomain(group);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">this</span>.mode = mode;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">&quot;Stat(owner=&#x27;&quot;</span> + owner + <span class="string">&quot;&#x27;, group=&#x27;&quot;</span> + group + <span class="string">&quot;&#x27;&quot;</span> + <span class="string">&quot;, mode=&quot;</span> + mode + <span class="string">&quot;)&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getOwner</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> owner;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getGroup</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> group;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getMode</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> mode;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns the file stat for a file descriptor.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> fd</span></span><br><span class="line"><span class="comment"> *            file descriptor.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the file descriptor file stat.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> *             thrown if there was an IO error while obtaining the file stat.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Stat <span class="title">getFstat</span><span class="params">(FileDescriptor fd)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">Stat stat = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">if</span> (!Shell.WINDOWS) &#123;</span><br><span class="line">stat = fstat(fd);</span><br><span class="line">stat.owner = getName(IdCache.USER, stat.ownerId);</span><br><span class="line">stat.group = getName(IdCache.GROUP, stat.groupId);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">stat = fstat(fd);</span><br><span class="line">&#125; <span class="keyword">catch</span> (NativeIOException nioe) &#123;</span><br><span class="line"><span class="keyword">if</span> (nioe.getErrorCode() == <span class="number">6</span>) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> NativeIOException(<span class="string">&quot;The handle is invalid.&quot;</span>, Errno.EBADF);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">LOG.warn(String.format(<span class="string">&quot;NativeIO.getFstat error (%d): %s&quot;</span>, nioe.getErrorCode(),</span><br><span class="line">nioe.getMessage()));</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> NativeIOException(<span class="string">&quot;Unknown error&quot;</span>, Errno.UNKNOWN);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> stat;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> String <span class="title">getName</span><span class="params">(IdCache domain, <span class="keyword">int</span> id)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">Map&lt;Integer, CachedName&gt; idNameCache = (domain == IdCache.USER) ? USER_ID_NAME_CACHE : GROUP_ID_NAME_CACHE;</span><br><span class="line">String name;</span><br><span class="line">CachedName cachedName = idNameCache.get(id);</span><br><span class="line"><span class="keyword">long</span> now = System.currentTimeMillis();</span><br><span class="line"><span class="keyword">if</span> (cachedName != <span class="keyword">null</span> &amp;&amp; (cachedName.timestamp + cacheTimeout) &gt; now) &#123;</span><br><span class="line">name = cachedName.name;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">name = (domain == IdCache.USER) ? getUserName(id) : getGroupName(id);</span><br><span class="line"><span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">String type = (domain == IdCache.USER) ? <span class="string">&quot;UserName&quot;</span> : <span class="string">&quot;GroupName&quot;</span>;</span><br><span class="line">LOG.debug(<span class="string">&quot;Got &quot;</span> + type + <span class="string">&quot; &quot;</span> + name + <span class="string">&quot; for ID &quot;</span> + id + <span class="string">&quot; from the native implementation&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">cachedName = <span class="keyword">new</span> CachedName(name, now);</span><br><span class="line">idNameCache.put(id, cachedName);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> name;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">native</span> String <span class="title">getUserName</span><span class="params">(<span class="keyword">int</span> uid)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">native</span> String <span class="title">getGroupName</span><span class="params">(<span class="keyword">int</span> uid)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CachedName</span> </span>&#123;</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">long</span> timestamp;</span><br><span class="line"><span class="keyword">final</span> String name;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">CachedName</span><span class="params">(String name, <span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.name = name;</span><br><span class="line"><span class="keyword">this</span>.timestamp = timestamp;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Map&lt;Integer, CachedName&gt; USER_ID_NAME_CACHE = <span class="keyword">new</span> ConcurrentHashMap&lt;Integer, CachedName&gt;();</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Map&lt;Integer, CachedName&gt; GROUP_ID_NAME_CACHE = <span class="keyword">new</span> ConcurrentHashMap&lt;Integer, CachedName&gt;();</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">enum</span> <span class="title">IdCache</span> </span>&#123;</span><br><span class="line">USER, GROUP</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> MMAP_PROT_READ = <span class="number">0x1</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> MMAP_PROT_WRITE = <span class="number">0x2</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> MMAP_PROT_EXEC = <span class="number">0x4</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">long</span> <span class="title">mmap</span><span class="params">(FileDescriptor fd, <span class="keyword">int</span> prot, <span class="keyword">boolean</span> shared, <span class="keyword">long</span> length)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">munmap</span><span class="params">(<span class="keyword">long</span> addr, <span class="keyword">long</span> length)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> workaroundNonThreadSafePasswdCalls = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Windows</span> </span>&#123;</span><br><span class="line"><span class="comment">// Flags for CreateFile() call on Windows</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> GENERIC_READ = <span class="number">0x80000000L</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> GENERIC_WRITE = <span class="number">0x40000000L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> FILE_SHARE_READ = <span class="number">0x00000001L</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> FILE_SHARE_WRITE = <span class="number">0x00000002L</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> FILE_SHARE_DELETE = <span class="number">0x00000004L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> CREATE_NEW = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> CREATE_ALWAYS = <span class="number">2</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> OPEN_EXISTING = <span class="number">3</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> OPEN_ALWAYS = <span class="number">4</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> TRUNCATE_EXISTING = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> FILE_BEGIN = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> FILE_CURRENT = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> FILE_END = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> FILE_ATTRIBUTE_NORMAL = <span class="number">0x00000080L</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a directory with permissions set to the specified mode. By setting</span></span><br><span class="line"><span class="comment"> * permissions at creation time, we avoid issues related to the user lacking</span></span><br><span class="line"><span class="comment"> * WRITE_DAC rights on subsequent chmod calls. One example where this can occur</span></span><br><span class="line"><span class="comment"> * is writing to an SMB share where the user does not have Full Control rights,</span></span><br><span class="line"><span class="comment"> * and therefore WRITE_DAC is denied.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> path</span></span><br><span class="line"><span class="comment"> *            directory to create</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> mode</span></span><br><span class="line"><span class="comment"> *            permissions of new directory</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> *             if there is an I/O error</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createDirectoryWithMode</span><span class="params">(File path, <span class="keyword">int</span> mode)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">createDirectoryWithMode0(path.getAbsolutePath(), mode);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Wrapper around CreateDirectory() on Windows */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">createDirectoryWithMode0</span><span class="params">(String path, <span class="keyword">int</span> mode)</span> <span class="keyword">throws</span> NativeIOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Wrapper around CreateFile() on Windows */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> FileDescriptor <span class="title">createFile</span><span class="params">(String path, <span class="keyword">long</span> desiredAccess, <span class="keyword">long</span> shareMode,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">long</span> creationDisposition)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a file for write with permissions set to the specified mode. By</span></span><br><span class="line"><span class="comment"> * setting permissions at creation time, we avoid issues related to the user</span></span><br><span class="line"><span class="comment"> * lacking WRITE_DAC rights on subsequent chmod calls. One example where this</span></span><br><span class="line"><span class="comment"> * can occur is writing to an SMB share where the user does not have Full</span></span><br><span class="line"><span class="comment"> * Control rights, and therefore WRITE_DAC is denied.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * This method mimics the semantics implemented by the JDK in</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@link</span> java.io.FileOutputStream&#125;. The file is opened for truncate or append,</span></span><br><span class="line"><span class="comment"> * the sharing mode allows other readers and writers, and paths longer than</span></span><br><span class="line"><span class="comment"> * MAX_PATH are supported. (See io_util_md.c in the JDK.)</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> path</span></span><br><span class="line"><span class="comment"> *            file to create</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> append</span></span><br><span class="line"><span class="comment"> *            if true, then open file for append</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> mode</span></span><br><span class="line"><span class="comment"> *            permissions of new directory</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> FileOutputStream of opened file</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> *             if there is an I/O error</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> FileOutputStream <span class="title">createFileOutputStreamWithMode</span><span class="params">(File path, <span class="keyword">boolean</span> append, <span class="keyword">int</span> mode)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">long</span> desiredAccess = GENERIC_WRITE;</span><br><span class="line"><span class="keyword">long</span> shareMode = FILE_SHARE_READ | FILE_SHARE_WRITE;</span><br><span class="line"><span class="keyword">long</span> creationDisposition = append ? OPEN_ALWAYS : CREATE_ALWAYS;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> FileOutputStream(</span><br><span class="line">createFileWithMode0(path.getAbsolutePath(), desiredAccess, shareMode, creationDisposition, mode));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Wrapper around CreateFile() with security descriptor on Windows */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> FileDescriptor <span class="title">createFileWithMode0</span><span class="params">(String path, <span class="keyword">long</span> desiredAccess, <span class="keyword">long</span> shareMode,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">long</span> creationDisposition, <span class="keyword">int</span> mode)</span> <span class="keyword">throws</span> NativeIOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Wrapper around SetFilePointer() on Windows */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">long</span> <span class="title">setFilePointer</span><span class="params">(FileDescriptor fd, <span class="keyword">long</span> distanceToMove, <span class="keyword">long</span> moveMethod)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Windows only methods used for getOwner() implementation */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> String <span class="title">getOwner</span><span class="params">(FileDescriptor fd)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Supported list of Windows access right flags */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">enum</span> <span class="title">AccessRight</span> </span>&#123;</span><br><span class="line">ACCESS_READ(<span class="number">0x0001</span>), <span class="comment">// FILE_READ_DATA</span></span><br><span class="line">ACCESS_WRITE(<span class="number">0x0002</span>), <span class="comment">// FILE_WRITE_DATA</span></span><br><span class="line">ACCESS_EXECUTE(<span class="number">0x0020</span>); <span class="comment">// FILE_EXECUTE</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> accessRight;</span><br><span class="line"></span><br><span class="line">AccessRight(<span class="keyword">int</span> access) &#123;</span><br><span class="line">accessRight = access;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">accessRight</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> accessRight;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Windows only method used to check if the current process has requested access</span></span><br><span class="line"><span class="comment"> * rights on the given path.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">boolean</span> <span class="title">access0</span><span class="params">(String path, <span class="keyword">int</span> requestedAccess)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Checks whether the current process has desired access rights on the given</span></span><br><span class="line"><span class="comment"> * path.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * Longer term this native function can be substituted with JDK7 function</span></span><br><span class="line"><span class="comment"> * Files#isReadable, isWritable, isExecutable.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> path</span></span><br><span class="line"><span class="comment"> *            input path</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> desiredAccess</span></span><br><span class="line"><span class="comment"> *            ACCESS_READ, ACCESS_WRITE or ACCESS_EXECUTE</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> true if access is allowed</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> *             I/O exception on error</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">access</span><span class="params">(String path, AccessRight desiredAccess)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"><span class="comment">// return access0(path, desiredAccess.accessRight());</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Extends both the minimum and maximum working set size of the current process.</span></span><br><span class="line"><span class="comment"> * This method gets the current minimum and maximum working set size, adds the</span></span><br><span class="line"><span class="comment"> * requested amount to each and then sets the minimum and maximum working set</span></span><br><span class="line"><span class="comment"> * size to the new values. Controlling the working set size of the process also</span></span><br><span class="line"><span class="comment"> * controls the amount of memory it can lock.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> delta</span></span><br><span class="line"><span class="comment"> *            amount to increment minimum and maximum working set size</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> *             for any error</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@see</span> POSIX#mlock(ByteBuffer, long)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">extendWorkingSetSize</span><span class="params">(<span class="keyword">long</span> delta)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (NativeCodeLoader.isNativeCodeLoaded()) &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">initNative();</span><br><span class="line">nativeLoaded = <span class="keyword">true</span>;</span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line"><span class="comment">// This can happen if the user has an older version of libhadoop.so</span></span><br><span class="line"><span class="comment">// installed - in this case we can continue without native IO</span></span><br><span class="line"><span class="comment">// after warning</span></span><br><span class="line">PerformanceAdvisory.LOG.debug(<span class="string">&quot;Unable to initialize NativeIO libraries&quot;</span>, t);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Log LOG = LogFactory.getLog(NativeIO.class);</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> nativeLoaded = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (NativeCodeLoader.isNativeCodeLoaded()) &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">initNative();</span><br><span class="line">nativeLoaded = <span class="keyword">true</span>;</span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line"><span class="comment">// This can happen if the user has an older version of libhadoop.so</span></span><br><span class="line"><span class="comment">// installed - in this case we can continue without native IO</span></span><br><span class="line"><span class="comment">// after warning</span></span><br><span class="line">PerformanceAdvisory.LOG.debug(<span class="string">&quot;Unable to initialize NativeIO libraries&quot;</span>, t);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return true if the JNI-based native IO extensions are available.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isAvailable</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> NativeCodeLoader.isNativeCodeLoaded() &amp;&amp; nativeLoaded;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Initialize the JNI method ID and class ID cache */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">initNative</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Get the maximum number of bytes that can be locked into memory at any given</span></span><br><span class="line"><span class="comment"> * point.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> 0 if no bytes can be locked into memory; Long.MAX_VALUE if there is</span></span><br><span class="line"><span class="comment"> *         no limit; The number of bytes that can be locked into memory</span></span><br><span class="line"><span class="comment"> *         otherwise.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">long</span> <span class="title">getMemlockLimit</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> isAvailable() ? getMemlockLimit0() : <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">long</span> <span class="title">getMemlockLimit0</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the operating system&#x27;s page size.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">long</span> <span class="title">getOperatingSystemPageSize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">Field f = Unsafe.class.getDeclaredField(<span class="string">&quot;theUnsafe&quot;</span>);</span><br><span class="line">f.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">Unsafe unsafe = (Unsafe) f.get(<span class="keyword">null</span>);</span><br><span class="line"><span class="keyword">return</span> unsafe.pageSize();</span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">LOG.warn(<span class="string">&quot;Unable to get operating system page size.  Guessing 4096.&quot;</span>, e);</span><br><span class="line"><span class="keyword">return</span> <span class="number">4096</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CachedUid</span> </span>&#123;</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">long</span> timestamp;</span><br><span class="line"><span class="keyword">final</span> String username;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">CachedUid</span><span class="params">(String username, <span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.timestamp = timestamp;</span><br><span class="line"><span class="keyword">this</span>.username = username;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Map&lt;Long, CachedUid&gt; uidCache = <span class="keyword">new</span> ConcurrentHashMap&lt;Long, CachedUid&gt;();</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> cacheTimeout;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> initialized = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The Windows logon name has two part, NetBIOS domain name and user account</span></span><br><span class="line"><span class="comment"> * name, of the format DOMAIN\UserName. This method will remove the domain part</span></span><br><span class="line"><span class="comment"> * of the full logon name.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> Fthe</span></span><br><span class="line"><span class="comment"> *            full principal name containing the domain</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> name with domain removed</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> String <span class="title">stripDomain</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> i = name.indexOf(<span class="string">&#x27;\\&#x27;</span>);</span><br><span class="line"><span class="keyword">if</span> (i != -<span class="number">1</span>)</span><br><span class="line">name = name.substring(i + <span class="number">1</span>);</span><br><span class="line"><span class="keyword">return</span> name;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getOwner</span><span class="params">(FileDescriptor fd)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">ensureInitialized();</span><br><span class="line"><span class="keyword">if</span> (Shell.WINDOWS) &#123;</span><br><span class="line">String owner = Windows.getOwner(fd);</span><br><span class="line">owner = stripDomain(owner);</span><br><span class="line"><span class="keyword">return</span> owner;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">long</span> uid = POSIX.getUIDforFDOwnerforOwner(fd);</span><br><span class="line">CachedUid cUid = uidCache.get(uid);</span><br><span class="line"><span class="keyword">long</span> now = System.currentTimeMillis();</span><br><span class="line"><span class="keyword">if</span> (cUid != <span class="keyword">null</span> &amp;&amp; (cUid.timestamp + cacheTimeout) &gt; now) &#123;</span><br><span class="line"><span class="keyword">return</span> cUid.username;</span><br><span class="line">&#125;</span><br><span class="line">String user = POSIX.getUserName(uid);</span><br><span class="line">LOG.info(<span class="string">&quot;Got UserName &quot;</span> + user + <span class="string">&quot; for UID &quot;</span> + uid + <span class="string">&quot; from the native implementation&quot;</span>);</span><br><span class="line">cUid = <span class="keyword">new</span> CachedUid(user, now);</span><br><span class="line">uidCache.put(uid, cUid);</span><br><span class="line"><span class="keyword">return</span> user;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a FileInputStream that shares delete permission on the file opened,</span></span><br><span class="line"><span class="comment"> * i.e. other process can delete the file the FileInputStream is reading. Only</span></span><br><span class="line"><span class="comment"> * Windows implementation uses the native interface.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> FileInputStream <span class="title">getShareDeleteFileInputStream</span><span class="params">(File f)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!Shell.WINDOWS) &#123;</span><br><span class="line"><span class="comment">// On Linux the default FileInputStream shares delete permission</span></span><br><span class="line"><span class="comment">// on the file opened.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> FileInputStream(f);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// Use Windows native interface to create a FileInputStream that</span></span><br><span class="line"><span class="comment">// shares delete permission on the file opened.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line">FileDescriptor fd = Windows.createFile(f.getAbsolutePath(), Windows.GENERIC_READ,</span><br><span class="line">Windows.FILE_SHARE_READ | Windows.FILE_SHARE_WRITE | Windows.FILE_SHARE_DELETE,</span><br><span class="line">Windows.OPEN_EXISTING);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> FileInputStream(fd);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a FileInputStream that shares delete permission on the file opened at</span></span><br><span class="line"><span class="comment"> * a given offset, i.e. other process can delete the file the FileInputStream is</span></span><br><span class="line"><span class="comment"> * reading. Only Windows implementation uses the native interface.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> FileInputStream <span class="title">getShareDeleteFileInputStream</span><span class="params">(File f, <span class="keyword">long</span> seekOffset)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!Shell.WINDOWS) &#123;</span><br><span class="line">RandomAccessFile rf = <span class="keyword">new</span> RandomAccessFile(f, <span class="string">&quot;r&quot;</span>);</span><br><span class="line"><span class="keyword">if</span> (seekOffset &gt; <span class="number">0</span>) &#123;</span><br><span class="line">rf.seek(seekOffset);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> FileInputStream(rf.getFD());</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// Use Windows native interface to create a FileInputStream that</span></span><br><span class="line"><span class="comment">// shares delete permission on the file opened, and set it to the</span></span><br><span class="line"><span class="comment">// given offset.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line">FileDescriptor fd = NativeIO.Windows.createFile(</span><br><span class="line">f.getAbsolutePath(), NativeIO.Windows.GENERIC_READ, NativeIO.Windows.FILE_SHARE_READ</span><br><span class="line">| NativeIO.Windows.FILE_SHARE_WRITE | NativeIO.Windows.FILE_SHARE_DELETE,</span><br><span class="line">NativeIO.Windows.OPEN_EXISTING);</span><br><span class="line"><span class="keyword">if</span> (seekOffset &gt; <span class="number">0</span>)</span><br><span class="line">NativeIO.Windows.setFilePointer(fd, seekOffset, NativeIO.Windows.FILE_BEGIN);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> FileInputStream(fd);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create the specified File for write access, ensuring that it does not exist.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> f</span></span><br><span class="line"><span class="comment"> *            the file that we want to create</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> permissions</span></span><br><span class="line"><span class="comment"> *            we want to have on the file (if security is enabled)</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> AlreadyExistsException</span></span><br><span class="line"><span class="comment"> *             if the file already exists</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> *             if any other error occurred</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> FileOutputStream <span class="title">getCreateForWriteFileOutputStream</span><span class="params">(File f, <span class="keyword">int</span> permissions)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!Shell.WINDOWS) &#123;</span><br><span class="line"><span class="comment">// Use the native wrapper around open(2)</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">FileDescriptor fd = NativeIO.POSIX.open(f.getAbsolutePath(),</span><br><span class="line">NativeIO.POSIX.O_WRONLY | NativeIO.POSIX.O_CREAT | NativeIO.POSIX.O_EXCL, permissions);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> FileOutputStream(fd);</span><br><span class="line">&#125; <span class="keyword">catch</span> (NativeIOException nioe) &#123;</span><br><span class="line"><span class="keyword">if</span> (nioe.getErrno() == Errno.EEXIST) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> AlreadyExistsException(nioe);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">throw</span> nioe;</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// Use the Windows native APIs to create equivalent FileOutputStream</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">FileDescriptor fd = NativeIO.Windows.createFile(</span><br><span class="line">f.getCanonicalPath(), NativeIO.Windows.GENERIC_WRITE, NativeIO.Windows.FILE_SHARE_DELETE</span><br><span class="line">| NativeIO.Windows.FILE_SHARE_READ | NativeIO.Windows.FILE_SHARE_WRITE,</span><br><span class="line">NativeIO.Windows.CREATE_NEW);</span><br><span class="line">NativeIO.POSIX.chmod(f.getCanonicalPath(), permissions);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> FileOutputStream(fd);</span><br><span class="line">&#125; <span class="keyword">catch</span> (NativeIOException nioe) &#123;</span><br><span class="line"><span class="keyword">if</span> (nioe.getErrorCode() == <span class="number">80</span>) &#123;</span><br><span class="line"><span class="comment">// ERROR_FILE_EXISTS</span></span><br><span class="line"><span class="comment">// 80 (0x50)</span></span><br><span class="line"><span class="comment">// The file exists</span></span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> AlreadyExistsException(nioe);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">throw</span> nioe;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">synchronized</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">ensureInitialized</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!initialized) &#123;</span><br><span class="line">cacheTimeout = <span class="keyword">new</span> Configuration().getLong(<span class="string">&quot;hadoop.security.uid.cache.secs&quot;</span>, <span class="number">4</span> * <span class="number">60</span> * <span class="number">60</span>) * <span class="number">1000</span>;</span><br><span class="line">LOG.info(<span class="string">&quot;Initialized cache for UID to User mapping with a cache&quot;</span> + <span class="string">&quot; timeout of &quot;</span> + cacheTimeout / <span class="number">1000</span></span><br><span class="line">+ <span class="string">&quot; seconds.&quot;</span>);</span><br><span class="line">initialized = <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A version of renameTo that throws a descriptive exception when it fails.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> src</span></span><br><span class="line"><span class="comment"> *            The source path</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> dst</span></span><br><span class="line"><span class="comment"> *            The destination path</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> NativeIOException</span></span><br><span class="line"><span class="comment"> *             On failure.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">renameTo</span><span class="params">(File src, File dst)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!nativeLoaded) &#123;</span><br><span class="line"><span class="keyword">if</span> (!src.renameTo(dst)) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;renameTo(src=&quot;</span> + src + <span class="string">&quot;, dst=&quot;</span> + dst + <span class="string">&quot;) failed.&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">renameTo0(src.getAbsolutePath(), dst.getAbsolutePath());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">link</span><span class="params">(File src, File dst)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!nativeLoaded) &#123;</span><br><span class="line">HardLink.createHardLink(src, dst);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">link0(src.getAbsolutePath(), dst.getAbsolutePath());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A version of renameTo that throws a descriptive exception when it fails.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> src</span></span><br><span class="line"><span class="comment"> *            The source path</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> dst</span></span><br><span class="line"><span class="comment"> *            The destination path</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> NativeIOException</span></span><br><span class="line"><span class="comment"> *             On failure.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">renameTo0</span><span class="params">(String src, String dst)</span> <span class="keyword">throws</span> NativeIOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">link0</span><span class="params">(String src, String dst)</span> <span class="keyword">throws</span> NativeIOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Unbuffered file copy from src to dst without tainting OS buffer cache</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * In POSIX platform: It uses FileChannel#transferTo() which internally attempts</span></span><br><span class="line"><span class="comment"> * unbuffered IO on OS with native sendfile64() support and falls back to</span></span><br><span class="line"><span class="comment"> * buffered IO otherwise.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * It minimizes the number of FileChannel#transferTo call by passing the the src</span></span><br><span class="line"><span class="comment"> * file size directly instead of a smaller size as the 3rd parameter. This saves</span></span><br><span class="line"><span class="comment"> * the number of sendfile64() system call when native sendfile64() is supported.</span></span><br><span class="line"><span class="comment"> * In the two fall back cases where sendfile is not supported,</span></span><br><span class="line"><span class="comment"> * FileChannle#transferTo already has its own batching of size 8 MB and 8 KB,</span></span><br><span class="line"><span class="comment"> * respectively.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * In Windows Platform: It uses its own native wrapper of CopyFileEx with</span></span><br><span class="line"><span class="comment"> * COPY_FILE_NO_BUFFERING flag, which is supported on Windows Server 2008 and</span></span><br><span class="line"><span class="comment"> * above.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Ideally, we should use FileChannel#transferTo() across both POSIX and Windows</span></span><br><span class="line"><span class="comment"> * platform. Unfortunately, the</span></span><br><span class="line"><span class="comment"> * wrapper(Java_sun_nio_ch_FileChannelImpl_transferTo0) used by</span></span><br><span class="line"><span class="comment"> * FileChannel#transferTo for unbuffered IO is not implemented on Windows. Based</span></span><br><span class="line"><span class="comment"> * on OpenJDK 6/7/8 source code, Java_sun_nio_ch_FileChannelImpl_transferTo0 on</span></span><br><span class="line"><span class="comment"> * Windows simply returns IOS_UNSUPPORTED.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Note: This simple native wrapper does minimal parameter checking before copy</span></span><br><span class="line"><span class="comment"> * and consistency check (e.g., size) after copy. It is recommended to use</span></span><br><span class="line"><span class="comment"> * wrapper function like the Storage#nativeCopyFileUnbuffered() function in</span></span><br><span class="line"><span class="comment"> * hadoop-hdfs with pre/post copy checks.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> src</span></span><br><span class="line"><span class="comment"> *            The source path</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> dst</span></span><br><span class="line"><span class="comment"> *            The destination path</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">copyFileUnbuffered</span><span class="params">(File src, File dst)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (nativeLoaded &amp;&amp; Shell.WINDOWS) &#123;</span><br><span class="line">copyFileUnbuffered0(src.getAbsolutePath(), dst.getAbsolutePath());</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">FileInputStream fis = <span class="keyword">null</span>;</span><br><span class="line">FileOutputStream fos = <span class="keyword">null</span>;</span><br><span class="line">FileChannel input = <span class="keyword">null</span>;</span><br><span class="line">FileChannel output = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">fis = <span class="keyword">new</span> FileInputStream(src);</span><br><span class="line">fos = <span class="keyword">new</span> FileOutputStream(dst);</span><br><span class="line">input = fis.getChannel();</span><br><span class="line">output = fos.getChannel();</span><br><span class="line"><span class="keyword">long</span> remaining = input.size();</span><br><span class="line"><span class="keyword">long</span> position = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">long</span> transferred = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (remaining &gt; <span class="number">0</span>) &#123;</span><br><span class="line">transferred = input.transferTo(position, remaining, output);</span><br><span class="line">remaining -= transferred;</span><br><span class="line">position += transferred;</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">IOUtils.cleanup(LOG, output);</span><br><span class="line">IOUtils.cleanup(LOG, fos);</span><br><span class="line">IOUtils.cleanup(LOG, input);</span><br><span class="line">IOUtils.cleanup(LOG, fis);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">copyFileUnbuffered0</span><span class="params">(String src, String dst)</span> <span class="keyword">throws</span> NativeIOException</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;“走过高山雄伟，踏遍华夏大地，看过落花流水，见过大海波澜壮阔。还没放下是因为真正的感情根本放不下，不该带着放下的目的来游历。游历是充实，是开阔。不是放松，更不是放下。否则就糟蹋了眼前的美。有些东西做不到就不要勉强，放心…                                             &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="Hadoop企业级优化" scheme="http://xubatian.cn/tags/Hadoop%E4%BC%81%E4%B8%9A%E7%BA%A7%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>hadoop组成模块之Yarn资源调度器</title>
    <link href="http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BYarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    <id>http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BYarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%99%A8/</id>
    <published>2022-01-16T13:22:00.000Z</published>
    <updated>2022-01-18T06:45:49.256Z</updated>
    
    <content type="html"><![CDATA[<p>我希望我这个人搁在你这儿，就是最最特殊的。 所有的先河都可以为我而开，所有的例都能为我破，是你的“以前从没有”和“以后也不会有”。 我希望我，是你的不冷静、不理智、不克制。 –来自网易有音乐《若把你》                                                                                 </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_119.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><p>前言</p><p> 你可以将yarn就理解为是一个操作系统. 他为你将来在yarn上面运行的n多个job去负责资源的管理.</p><p>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。</p><p>思考：<br>1）如何管理集群资源？<br>2）如何给任务合理分配资源？</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_213.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。</p><h1 id="Yarn基本架构"><a href="#Yarn基本架构" class="headerlink" title="Yarn基本架构"></a>Yarn基本架构</h1><p>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。</p><p>YARN主要由ResourceManager(负责整个集群资源管理)、NodeManager(负责单个节点的管理)、ApplicationMaster(负责资源的申请,容错,监控等)和Container(整个集群资源的封装,算是一个容器. 将来每个job都会跑在容器中.而container容器封装了job所要的一些资源)等组件构成,如图所示:</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_135.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="Yarn工作机制"><a href="#Yarn工作机制" class="headerlink" title="Yarn工作机制"></a>Yarn工作机制</h1><p>Yarn运行机制,如图:  mapTask就是一个线程</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_122.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>工作机制详解:</p><p>​    （1）MR程序提交到客户端所在的节点。</p><p>​    （2）YarnRunner向ResourceManager申请一个Application。</p><p>​    （3）RM将该应用程序的资源路径返回给YarnRunner。</p><p>​    （4）该程序将运行所需资源提交到HDFS上。</p><p>​    （5）程序资源提交完毕后，申请运行mrAppMaster。</p><p>​    （6）RM将用户的请求初始化成一个Task。</p><p>​    （7）其中一个NodeManager领取到Task任务。</p><p>​    （8）该NodeManager创建容器Container，并产生MRAppmaster。</p><p>​    （9）Container从HDFS上拷贝资源到本地。</p><p>​    （10）MRAppmaster向RM 申请运行MapTask资源。</p><p>​    （11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。</p><p>​    （12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。</p><p>​    （13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。</p><p>​    （14）ReduceTask向MapTask获取相应分区的数据。</p><p>​    （15）程序运行完毕后，MR会向RM申请注销自己。</p><h1 id="作业提交全过程"><a href="#作业提交全过程" class="headerlink" title="作业提交全过程"></a>作业提交全过程</h1><h2 id="作业提交过程之YARN"><a href="#作业提交过程之YARN" class="headerlink" title="作业提交过程之YARN"></a>作业提交过程之YARN</h2><p>如图:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_123.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>作业提交全过程详解<br>（1）作业提交<br>第1步：Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。<br>第2步：Client向RM申请一个作业id。<br>第3步：RM给Client返回该job资源的提交路径和作业id。<br>第4步：Client提交jar包、切片信息和配置文件到指定的资源提交路径。<br>第5步：Client提交完资源后，向RM申请运行MrAppMaster。<br>（2）作业初始化<br>第6步：当RM收到Client的请求后，将该job添加到容量调度器中。<br>第7步：某一个空闲的NM领取到该Job。<br>第8步：该NM创建Container，并产生MRAppmaster。<br>第9步：下载Client提交的资源到本地。<br>（3）任务分配<br>第10步：MrAppMaster向RM申请运行多个MapTask任务资源。<br>第11步：RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。<br>（4）任务运行<br>第12步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。<br>第13步：MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。<br>第14步：ReduceTask向MapTask获取相应分区的数据。<br>第15步：程序运行完毕后，MR会向RM申请注销自己。<br>（5）进度和状态更新<br>YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。<br>（6）作业完成<br>除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</p><h2 id="作业提交过程之MapReduce"><a href="#作业提交过程之MapReduce" class="headerlink" title="作业提交过程之MapReduce"></a>作业提交过程之MapReduce</h2><p>如图所示:</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_124.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="资源调度器-FIFO调度器-表示先进先出"><a href="#资源调度器-FIFO调度器-表示先进先出" class="headerlink" title="资源调度器(FIFO调度器 表示先进先出)"></a>资源调度器(FIFO调度器 表示先进先出)</h1><p>目前，Hadoop作业调度器主要有三种：FIFO Scheduler、Capacity Scheduler和Fair Scheduler。Hadoop2.7.2默认的资源调度器是Capacity Scheduler。<br>具体设置详见：yarn-default.xml文件</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;description&gt;The <span class="class"><span class="keyword">class</span> <span class="title">to</span> <span class="title">use</span> <span class="title">as</span> <span class="title">the</span> <span class="title">resource</span> <span class="title">scheduler</span>.&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">name</span>&gt;<span class="title">yarn</span>.<span class="title">resourcemanager</span>.<span class="title">scheduler</span>.<span class="title">class</span>&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">value</span>&gt;<span class="title">org</span>.<span class="title">apache</span>.<span class="title">hadoop</span>.<span class="title">yarn</span>.<span class="title">server</span>.<span class="title">resourcemanager</span>.<span class="title">scheduler</span>.<span class="title">capacity</span>.<span class="title">CapacityScheduler</span>&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="class">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br></pre></td></tr></table></figure><h2 id="1．先进先出调度器（FIFO）"><a href="#1．先进先出调度器（FIFO）" class="headerlink" title="1．先进先出调度器（FIFO）"></a>1．先进先出调度器（FIFO）</h2><p> FIFO调度器:</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_125.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>Hadoop最初设计目的是支持大数据批处理作业，如日志挖掘、Web索引等作业，</p><p>为此，Hadoop仅提供了一个非常简单的调度机制：FIFO，即先来先服务，在该调度机制下，所有作业被统一提交到一个队列中，Hadoop按照提交顺序依次运行这些作业。</p><p>但随着Hadoop的普及，单个Hadoop集群的用户量越来越大，不同用户提交的应用程序往往具有不同的服务质量要求，典型的应用有以下几种：</p><p>批处理作业：这种作业往往耗时较长，对时间完成一般没有严格要求，如数据挖掘、机器学习等方面的应用程序。</p><p>交互式作业：这种作业期望能及时返回结果，如SQL查询（Hive）等。</p><p>生产性作业：这种作业要求有一定量的资源保证，如统计值计算、垃圾数据分析等。</p><p>此外，这些应用程序对硬件资源需求量也是不同的，如过滤、统计类作业一般为CPU密集型作业，而数据挖掘、机器学习作业一般为I/O密集型作业。因此，简单的FIFO调度策略不仅不能满足多样化需求，也不能充分利用硬件资源。</p><h2 id="2．容量调度器（Capacity-Scheduler）"><a href="#2．容量调度器（Capacity-Scheduler）" class="headerlink" title="2．容量调度器（Capacity Scheduler）"></a>2．容量调度器（Capacity Scheduler）</h2><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_126.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>Capacity Scheduler Capacity Scheduler 是Yahoo开发的多用户调度器，它以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限，同时，每个用户也可设定一定的资源使用上限以防止资源滥用。而当一个队列的资源有剩余时，可暂时将剩余资源共享给其他队列。</p><p>​    总之，Capacity Scheduler 主要有以下几个特点：</p><p>①容量保证。管理员可为每个队列设置资源最低保证和资源使用上限，而所有提交到该队列的应用程序共享这些资源。</p><p>②灵活性，如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列释放的资源会归还给该队列。这种资源灵活分配的方式可明显提高资源利用率。</p><p>③多重租赁。支持多用户共享集群和多应用程序同时运行。为防止单个应用程序、用户或者队列独占集群中的资源，管理员可为之增加多重约束（比如单个应用程序同时运行的任务数等）。</p><p>④安全保证。每个队列有严格的ACL列表规定它的访问用户，每个用户可指定哪些用户允许查看自己应用程序的运行状态或者控制应用程序（比如杀死应用程序）。此外，管理员可指定队列管理员和集群系统管理员。</p><p>⑤动态更新配置文件。管理员可根据需要动态修改各种配置参数，以实现在线集群管理。</p><h2 id="3．公平调度器（Fair-Scheduler）"><a href="#3．公平调度器（Fair-Scheduler）" class="headerlink" title="3．公平调度器（Fair Scheduler）"></a>3．公平调度器（Fair Scheduler）</h2><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_127.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_128.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_129.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>Fair Scheduler Fair Schedulere是Facebook开发的多用户调度器。<br>    <strong>公平调度器的目的是让所有的作业随着时间的推移，都能平均地获取等同的共享资源！</strong>当一个作业在运行时，它会使用整个集群但是如果有其他作业提交上来，系统会将空闲的资源分配给新的作业！每个任务大致上会获取平等数量的资源！和传统的调度策略不同的是<br>它会让小的任务在合理的时间完成，同时不会让需要长时间运行的耗费大量资源的应用挨饿！<br>    同Capacity Scheduler类似，它以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限，同时，每个用户也可设定一定的资源使用上限以防止资源滥用；当一个队列的资源有剩余时，可暂时将剩余资源共享给其他队列。<br>    当然，Fair Scheduler也存在很多与Capacity Scheduler不同之处，这主要体现在以下几个方面：<br>①资源公平共享。在每个队列中，Fair Scheduler 可选择按照FIFO、Fair或DRF策略为应用程序分配资源。其中，Fair 策略(默认)是一种基于最大最小公平算法实现的资源多路复用方式，默认情况下，每个队列内部采用该方式分配资源。这意味着，如果一个队列中有两个应用程序同时运行，则每个应用程序可得到1/2的资源；如果三个应用程序同时运行，则每个应用程序可得到1/3的资源。<br>②支持资源抢占。当某个队列中有剩余资源时，调度器会将这些资源共享给其他队列，而当该队列中有新的应用程序提交时，调度器要为它回收资源。为了尽可能降低不必要的计算浪费，调度器采用了先等待再强制回收的策略，即如果等待一段时间后尚有未归还的资源，则会进行资源抢占：从那些超额使用资源的队列中杀死一部分任务，进而释放资源。<br>③负载均衡。Fair Scheduler提供了一个基于任务数目的负载均衡机制，该机制尽可能将系统中的任务均匀分配到各个节点上。此外，用户也可以根据自己的需要设计负载均衡机制。<br>④调度策略配置灵活。Fair Scheduler允许管理员为每个队列单独设置调度策略（当前支持FIFO、Fair或DRF三种）。<br>⑤提高小应用程序响应时间。由于采用了最大最小公平算法，小作业可以快速获取资源并运行完成</p><h1 id="容量调度器多队列提交案例"><a href="#容量调度器多队列提交案例" class="headerlink" title="容量调度器多队列提交案例"></a>容量调度器多队列提交案例</h1><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>​    Yarn默认的容量调度器是一条单队列的调度器，在实际使用中会出现单个任务阻塞整个队列的情况。同时，随着业务的增长，公司需要分业务限制集群使用率。这就需要我们按照业务种类配置多条任务队列。</p><h2 id="配置多队列的容量调度器"><a href="#配置多队列的容量调度器" class="headerlink" title="配置多队列的容量调度器"></a>配置多队列的容量调度器</h2><p>默认Yarn的配置下，容量调度器只有一条Default队列。在capacity-scheduler.xml中可以配置多条队列，并降低default队列资源占比：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="keyword">default</span>,hive&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;</span><br><span class="line">    <span class="function">The queues at the <span class="keyword">this</span> <span class="title">level</span> <span class="params">(root is the root queue)</span>.</span></span><br><span class="line"><span class="function">  &lt;/description&gt;</span></span><br><span class="line"><span class="function">&lt;/property&gt;</span></span><br><span class="line"><span class="function">&lt;property&gt;</span></span><br><span class="line"><span class="function">  &lt;name&gt;yarn.scheduler.capacity.root.<span class="keyword">default</span>.capacity&lt;/name&gt;</span></span><br><span class="line"><span class="function">  &lt;value&gt;40&lt;/value&gt;</span></span><br><span class="line"><span class="function">&lt;/property&gt;</span></span><br></pre></td></tr></table></figure><p>同时为新加队列添加必要属性：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--队列目标资源百分比，所有队列相加必须等于<span class="number">100</span>--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.capacity&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="number">60</span>&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;!--队列最大资源百分比--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.maximum-capacity&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="number">100</span>&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;!—单用户可用队列资源占比--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.user-limit-factor&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="number">1</span>&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;!--队列状态（RUNNING或STOPPING）--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.state&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;RUNNING&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;!—队列允许哪些用户提交--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.acl_submit_applications&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;!—队列允许哪些用户管理--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.acl_administer_queue&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br></pre></td></tr></table></figure><p>在配置完成后，重启Yarn，就可以看到两条队列：</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_130.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_131.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_132.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="向Hive队列提交任务"><a href="#向Hive队列提交任务" class="headerlink" title="向Hive队列提交任务"></a>向Hive队列提交任务</h2><p>默认的任务提交都是提交到default队列的。如果希望向其他队列提交任务，需要在Driver中声明：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WcDrvier</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">        configuration.set(<span class="string">&quot;mapred.job.queue.name&quot;</span>, <span class="string">&quot;hive&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1. 获取一个Job实例</span></span><br><span class="line">        Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 设置类路径</span></span><br><span class="line">        job.setJarByClass(WcDrvier.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 设置Mapper和Reducer</span></span><br><span class="line">        job.setMapperClass(WcMapper.class);</span><br><span class="line">        job.setReducerClass(WcReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 设置Mapper和Reducer的输出类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setCombinerClass(WcReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5. 设置输入输出文件</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//6. 提交Job</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>默认的任务提交都是提交到default队列的。如果希望向其他队列提交任务，需要在Driver中声明：</p><p>这样，这个任务在集群提交时，就会提交到hive队列：</p><p>图为公平调度器</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_133.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="任务的推测执行"><a href="#任务的推测执行" class="headerlink" title="任务的推测执行"></a>任务的推测执行</h2><h3 id="1．作业完成时间取决于最慢的任务完成时间"><a href="#1．作业完成时间取决于最慢的任务完成时间" class="headerlink" title="1．作业完成时间取决于最慢的任务完成时间"></a>1．作业完成时间取决于最慢的任务完成时间</h3><p>一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。</p><p>思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？</p><h3 id="2．推测执行机制"><a href="#2．推测执行机制" class="headerlink" title="2．推测执行机制"></a>2．推测执行机制</h3><p>发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。</p><h3 id="3．执行推测任务的前提条件"><a href="#3．执行推测任务的前提条件" class="headerlink" title="3．执行推测任务的前提条件"></a>3．执行推测任务的前提条件</h3><p>（1）每个Task只能有一个备份任务</p><p>（2）当前Job已完成的Task必须不小于0.05（5%）</p><p>（3）开启推测执行参数设置。mapred-site.xml文件中默认是打开的。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.speculative&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="keyword">true</span>&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;If <span class="keyword">true</span>, then multiple instances of some map tasks may be executed in parallel.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="keyword">true</span>&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;If <span class="keyword">true</span>, then multiple instances of some reduce tasks may be executed in parallel.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h3 id="4．不能启用推测执行机制情况"><a href="#4．不能启用推测执行机制情况" class="headerlink" title="4．不能启用推测执行机制情况"></a>4．不能启用推测执行机制情况</h3><p>  （1）任务间存在严重的负载倾斜；</p><p>  （2）特殊任务，比如任务向数据库中写数据。</p><h3 id="5．算法原理，如图所示"><a href="#5．算法原理，如图所示" class="headerlink" title="5．算法原理，如图所示"></a>5．算法原理，如图所示</h3><p>推测执行算法原理</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_134.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="Yarn常用命令"><a href="#Yarn常用命令" class="headerlink" title="Yarn常用命令"></a>Yarn常用命令</h1><p>Yarn状态的查询，除了可以在hadoop103:8088页面查看外，还可以通过命令操作。常见的命令操作如下所示：<br>需求：执行WordCount案例，并用Yarn命令查看任务运行情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ myhadoop.sh start</span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</span><br></pre></td></tr></table></figure><h2 id="yarn-application查看任务"><a href="#yarn-application查看任务" class="headerlink" title="yarn application查看任务"></a>yarn application查看任务</h2><p>（1）列出所有Application：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn application -list</span><br><span class="line">2021-02-06 10:21:19,238 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):0</span><br><span class="line">                Application-Id    Application-Name    Application-Type      User     Queue             State       Final-State       Progress                       Tracking-URL</span><br></pre></td></tr></table></figure><p>（2）根据Application状态过滤：yarn application -list -appStates （所有状态：ALL、NEW、NEW_SAVING、SUBMITTED、ACCEPTED、RUNNING、FINISHED、FAILED、KILLED）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn application -list -appStates FINISHED</span><br><span class="line">2021-02-06 10:22:20,029 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Total number of applications (application-types: [], states: [FINISHED] and tags: []):1</span><br><span class="line">                Application-Id    Application-Name    Application-Type      User     Queue             State       Final-State       Progress                       Tracking-URL</span><br><span class="line">application_1612577921195_0001          word count           MAPREDUCE   shangbaishuyao   default          FINISHED         SUCCEEDED           100%http://hadoop102:19888/jobhistory/job/job_1612577921195_0001</span><br></pre></td></tr></table></figure><p>（3）Kill掉Application：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn application -kill application_1612577921195_0001</span><br><span class="line">2021-02-06 10:23:48,530 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Application application_1612577921195_0001 has already finished</span><br></pre></td></tr></table></figure><h2 id="yarn-logs查看日志"><a href="#yarn-logs查看日志" class="headerlink" title="yarn logs查看日志"></a>yarn logs查看日志</h2><p>（1）查询Application日志：yarn logs -applicationId <ApplicationId></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn logs -applicationId application_1612577921195_0001</span><br></pre></td></tr></table></figure><p>（2）查询Container日志：yarn logs -applicationId <ApplicationId> -containerId <ContainerId> </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn logs -applicationId application_1612577921195_0001 -containerId container_1612577921195_0001_01_000001</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="yarn-applicationattempt查看尝试运行的任务"><a href="#yarn-applicationattempt查看尝试运行的任务" class="headerlink" title="yarn applicationattempt查看尝试运行的任务"></a>yarn applicationattempt查看尝试运行的任务</h2><p>（1）列出所有Application尝试的列表：yarn applicationattempt -list <ApplicationId></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn applicationattempt -list application_1612577921195_0001</span><br><span class="line">2021-02-06 10:26:54,195 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Total number of application attempts :1</span><br><span class="line">         ApplicationAttempt-Id               State                    AM-Container-Id                       Tracking-URL</span><br><span class="line">appattempt_1612577921195_0001_000001            FINISHEDcontainer_1612577921195_0001_01_000001http://hadoop103:8088/proxy/application_1612577921195_0001/</span><br></pre></td></tr></table></figure><p>（2）打印ApplicationAttemp状态：yarn applicationattempt -status <ApplicationAttemptId></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn applicationattempt -status appattempt_1612577921195_0001_000001</span><br><span class="line">2021-02-06 10:27:55,896 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Application Attempt Report : </span><br><span class="line">ApplicationAttempt-Id : appattempt_1612577921195_0001_000001</span><br><span class="line">State : FINISHED</span><br><span class="line">AMContainer : container_1612577921195_0001_01_000001</span><br><span class="line">Tracking-URL : http://hadoop103:8088/proxy/application_1612577921195_0001/</span><br><span class="line">RPC Port : 34756</span><br><span class="line">AM Host : hadoop104</span><br><span class="line">Diagnostics :</span><br></pre></td></tr></table></figure><h2 id="yarn-container查看容器"><a href="#yarn-container查看容器" class="headerlink" title="yarn container查看容器"></a>yarn container查看容器</h2><p>（1）列出所有Container：yarn container -list <ApplicationAttemptId></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn container -list appattempt_1612577921195_0001_000001</span><br><span class="line">2021-02-06 10:28:41,396 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Total number of containers :0</span><br><span class="line">                  Container-Id          Start Time         Finish Time               State                Host   Node Http Address</span><br></pre></td></tr></table></figure><p>​    （2）打印Container状态：    yarn container -status <ContainerId></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn container -status container_1612577921195_0001_01_000001</span><br><span class="line">2021-02-06 10:29:58,554 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Container with id &#x27;container_1612577921195_0001_01_000001&#x27; doesn&#x27;t exist in RM or Timeline Server.</span><br></pre></td></tr></table></figure><p>​    注：只有在任务跑的途中才能看到container的状态</p><h2 id="yarn-node查看节点状态"><a href="#yarn-node查看节点状态" class="headerlink" title="yarn node查看节点状态"></a>yarn node查看节点状态</h2><p>​    列出所有节点：yarn node -list -all</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn node -list -all</span><br><span class="line">2021-02-06 10:31:36,962 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Total Nodes:3</span><br><span class="line">         Node-Id     Node-StateNode-Http-AddressNumber-of-Running-Containers</span><br><span class="line"> hadoop103:38168        RUNNING   hadoop103:8042                           0</span><br><span class="line"> hadoop102:42012        RUNNING   hadoop102:8042                           0</span><br><span class="line"> hadoop104:39702        RUNNING   hadoop104:8042                           0</span><br></pre></td></tr></table></figure><h2 id="yarn-rmadmin更新配置"><a href="#yarn-rmadmin更新配置" class="headerlink" title="yarn rmadmin更新配置"></a>yarn rmadmin更新配置</h2><p>加载队列配置：yarn rmadmin -refreshQueues</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn rmadmin -refreshQueues</span><br><span class="line">2021-02-06 10:32:03,331 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8033</span><br></pre></td></tr></table></figure><h2 id="yarn-queue查看队列"><a href="#yarn-queue查看队列" class="headerlink" title="yarn queue查看队列"></a>yarn queue查看队列</h2><p>​    打印队列信息：yarn queue -status <QueueName></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ yarn queue -status default</span><br><span class="line">2021-02-06 10:32:33,403 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032</span><br><span class="line">Queue Information : </span><br><span class="line">Queue Name : default</span><br><span class="line">State : RUNNING</span><br><span class="line">Capacity : 100.0%</span><br><span class="line">Current Capacity : .0%</span><br><span class="line">Maximum Capacity : 100.0%</span><br><span class="line">Default Node Label expression : &lt;DEFAULT_PARTITION&gt;</span><br><span class="line">Accessible Node Labels : *</span><br><span class="line">Preemption : disabled</span><br><span class="line">Intra-queue Preemption : disabled</span><br></pre></td></tr></table></figure><h1 id="Yarn生产环境核心参数"><a href="#Yarn生产环境核心参数" class="headerlink" title="Yarn生产环境核心参数"></a>Yarn生产环境核心参数</h1><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_214.png" width = "" height = "" alt="xubatian的博客" align="center" />]]></content>
    
    
    <summary type="html">&lt;p&gt;我希望我这个人搁在你这儿，就是最最特殊的。 所有的先河都可以为我而开，所有的例都能为我破，是你的“以前从没有”和“以后也不会有”。 我希望我，是你的不冷静、不理智、不克制。 –来自网易有音乐《若把你》                                                                                 &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="Yarn" scheme="http://xubatian.cn/tags/Yarn/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop数据压缩</title>
    <link href="http://xubatian.cn/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/"/>
    <id>http://xubatian.cn/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/</id>
    <published>2022-01-16T12:38:22.000Z</published>
    <updated>2022-01-17T16:49:19.664Z</updated>
    
    <content type="html"><![CDATA[<p>雨过天晴架小船，酒在一边，鱼在一边 草舍茅屋有几间，行也安然，待也安然。 南山空谷书一卷，疯也痴癫，狂也痴癫。 一觉睡到日三竿，不是神仙，胜似神仙。 –来自墙友【云居-南宫公羽】投稿                                                </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_112.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><p>前言</p><p>虽然hadoop的HDFS理论上可以存储海量的数据.即使磁盘不够也是可以通过增加机器的方式扩展存储空间. 但是加机器是要花钱买的呀.所以,那群无聊的人就使用到了压缩的方式. 但是压缩的方式有很多种. 咱们mapreduce是需要切片的. 所以有哪些压缩是可以支持切片的呢?那些压缩格式的压缩速度更快呢? 那些压缩能够将数据压缩的更小呢? … </p><h1 id="压缩概述"><a href="#压缩概述" class="headerlink" title="压缩概述"></a>压缩概述</h1><p>压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时，I/O操作、网络数据传输、Shuffle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此，使用数据压缩显得非常重要。</p><p>鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源,最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启用压缩。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价。</p><h1 id="压缩策略和原则"><a href="#压缩策略和原则" class="headerlink" title="压缩策略和原则"></a>压缩策略和原则</h1><p>压缩是提高Hadoop运行效率的一种优化策略。<br>通过对Mapper、Reducer运行过程的数据进行压缩,以减少磁盘IO,提高MR程序运行速度。<br>注意：采用压缩技术减少了磁盘IO，但同时增加了CPU运算负担.所以，压缩特性运用得当能提高性能,但是运用不当也可能降低性能.</p><p>压缩基本原则：<br>  (1) 运算密集型的job, 少用压缩<br>  (2) IO密集型的job, 多用压缩</p><h1 id="MR支持的压缩编码"><a href="#MR支持的压缩编码" class="headerlink" title="MR支持的压缩编码"></a>MR支持的压缩编码</h1><p>Hadoop默认使用的是DEFLATE压缩格式 , bzip2 hadoop自带的</p><table><thead><tr><th>压缩格式</th><th>hadoop自带？</th><th>算法</th><th>文件扩展名</th><th>是否可切分</th><th>换成压缩格式后，原来的程序是否需要修改</th></tr></thead><tbody><tr><td>DEFLATE</td><td>是，直接使用</td><td>DEFLATE</td><td>.deflate</td><td>否</td><td>和文本处理一样，不需要修改</td></tr><tr><td>Gzip</td><td>是，直接使用</td><td>DEFLATE</td><td>.gz</td><td>否</td><td>和文本处理一样，不需要修改</td></tr><tr><td>bzip2</td><td>是，直接使用</td><td>bzip2</td><td>.bz2</td><td>是</td><td>和文本处理一样，不需要修改</td></tr><tr><td>LZO</td><td>否，需要安装</td><td>LZO</td><td>.lzo</td><td>是</td><td>需要建索引，还需要指定输入格式</td></tr><tr><td>Snappy</td><td>否，需要安装</td><td>Snappy</td><td>.snappy</td><td>否</td><td>和文本处理一样，不需要修改</td></tr></tbody></table><p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示。</p><table><thead><tr><th>压缩格式</th><th>对应的编码/解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><p>压缩性能的比较</p><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr></tbody></table><p><a href="http://google.github.io/snappy/">http://google.github.io/snappy/</a><br>On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.</p><h1 id="压缩方式选择"><a href="#压缩方式选择" class="headerlink" title="压缩方式选择"></a>压缩方式选择</h1><h2 id="Gzip压缩"><a href="#Gzip压缩" class="headerlink" title="Gzip压缩"></a>Gzip压缩</h2><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_113.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="Bzip2压缩"><a href="#Bzip2压缩" class="headerlink" title="Bzip2压缩"></a>Bzip2压缩</h2><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_114.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="Lzo压缩"><a href="#Lzo压缩" class="headerlink" title="Lzo压缩"></a>Lzo压缩</h2><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_115.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="Snappy压缩"><a href="#Snappy压缩" class="headerlink" title="Snappy压缩"></a>Snappy压缩</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_116.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="压缩位置选择"><a href="#压缩位置选择" class="headerlink" title="压缩位置选择"></a>压缩位置选择</h1><p>压缩可以在MapReduce作用的任意阶段启用,如图:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_117.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="压缩参数配置"><a href="#压缩参数配置" class="headerlink" title="压缩参数配置"></a>压缩参数配置</h1><p>要在Hadoop中启用压缩，可以配置如下参数：表4-10 配置参数</p><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs  （在core-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress（在mapred-site.xml中配置）</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec（在mapred-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）</td><td>org.apache.hadoop.io.compress. DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置）</td><td>RECORD</td><td>reducer输出</td><td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td></tr></tbody></table><h1 id="压缩实操案例"><a href="#压缩实操案例" class="headerlink" title="压缩实操案例"></a>压缩实操案例</h1><h2 id="数据流的压缩和解压缩"><a href="#数据流的压缩和解压缩" class="headerlink" title="数据流的压缩和解压缩"></a>数据流的压缩和解压缩</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_118.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>测试一下如下压缩方式: 表4-11</p><table><thead><tr><th>DEFLATE</th><th>org.apache.hadoop.io.compress.DefaultCodec</th></tr></thead><tbody><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr></tbody></table><h2 id="案例代码"><a href="#案例代码" class="headerlink" title="案例代码"></a>案例代码</h2><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/test/TestCompress.java">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/test/TestCompress.java</a></p><h1 id="Map输出端采用压缩"><a href="#Map输出端采用压缩" class="headerlink" title="Map输出端采用压缩"></a>Map输出端采用压缩</h1><p>即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可，我们来看下代码怎么设置。</p><h2 id="1．给大家提供的Hadoop源码支持的压缩格式有：BZip2Codec-、DefaultCodec"><a href="#1．给大家提供的Hadoop源码支持的压缩格式有：BZip2Codec-、DefaultCodec" class="headerlink" title="1．给大家提供的Hadoop源码支持的压缩格式有：BZip2Codec 、DefaultCodec"></a>1．给大家提供的Hadoop源码支持的压缩格式有：BZip2Codec 、DefaultCodec</h2><p>代码案例:</p><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/WordCountDriver.java">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/WordCountDriver.java</a></p><h2 id="2．Mapper保持不变"><a href="#2．Mapper保持不变" class="headerlink" title="2．Mapper保持不变"></a>2．Mapper保持不变</h2><p>代码案例:</p><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/WordCountMapper.java">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/WordCountMapper.java</a></p><h2 id="3．Reducer保持不变"><a href="#3．Reducer保持不变" class="headerlink" title="3．Reducer保持不变"></a>3．Reducer保持不变</h2><p>代码案例:</p><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/WordCountReducer.java">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/WordCountReducer.java</a></p><h1 id="Reduce输出端采用压缩-Mapper和Reducer保持不变"><a href="#Reduce输出端采用压缩-Mapper和Reducer保持不变" class="headerlink" title="Reduce输出端采用压缩 Mapper和Reducer保持不变"></a>Reduce输出端采用压缩 Mapper和Reducer保持不变</h1><p>基于Map输出端采用压缩案例处理。代码案例:</p><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/WordCountDriver.java">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/compress/WordCountDriver.java</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;雨过天晴架小船，酒在一边，鱼在一边 草舍茅屋有几间，行也安然，待也安然。 南山空谷书一卷，疯也痴癫，狂也痴癫。 一觉睡到日三竿，不是神仙，胜似神仙。 –来自墙友【云居-南宫公羽】投稿                                                &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="hadoop压缩" scheme="http://xubatian.cn/tags/hadoop%E5%8E%8B%E7%BC%A9/"/>
    
  </entry>
  
  <entry>
    <title>hadoop的计数器应用和数据清洗</title>
    <link href="http://xubatian.cn/hadoop%E7%9A%84%E8%AE%A1%E6%95%B0%E5%99%A8%E5%BA%94%E7%94%A8%E5%92%8C%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/"/>
    <id>http://xubatian.cn/hadoop%E7%9A%84%E8%AE%A1%E6%95%B0%E5%99%A8%E5%BA%94%E7%94%A8%E5%92%8C%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/</id>
    <published>2022-01-16T10:58:45.000Z</published>
    <updated>2022-01-17T16:47:42.655Z</updated>
    
    <content type="html"><![CDATA[<p>当我们凶狠地对待这个世界时，这个世界突然变得温文尔雅了。 ——余华                                                   </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.baidu.cn_109.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="计数器应用"><a href="#计数器应用" class="headerlink" title="计数器应用"></a>计数器应用</h1><p>Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。<br>1.计数器API<br>（1）采用枚举的方式统计计数<br>            enum MyCounter{MALFORORMED,NORMAL}<br>            //对枚举定义的自定义计数器加1<br>            context.getCounter(MyCounter.MALFORORMED).increment(1);<br>（2）采用计数器组、计数器名称的方式统计<br>            context.getCounter(“counterGroup”, “counter”).increment(1);<br>            组名和计数器名称随便起，但最好有意义。<br>（3）计数结果在程序运行后的控制台上查看。</p><ol start="2"><li>计数器案例实操<br>详见数据清洗案例。 案例代码: <a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/mapJoin/MapJoinMapper.java">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/mapJoin/MapJoinMapper.java</a></li></ol><p><strong>计数器, 看看这个方法被调用了多少次</strong></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_108.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_109.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_110.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="数据清洗（ETL）"><a href="#数据清洗（ETL）" class="headerlink" title="数据清洗（ETL）"></a>数据清洗（ETL）</h1><p>概念</p><p>在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序。</p><h2 id="数据清洗案例实操-简单解析版"><a href="#数据清洗案例实操-简单解析版" class="headerlink" title="数据清洗案例实操-简单解析版"></a>数据清洗案例实操-简单解析版</h2><p>需求<br>去除日志中字段个数小于等于11的日志</p><p>需求分析<br>    需要在Map阶段对输入的数据根据规则进行过滤清洗。</p><p>案例代码</p><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/ETL/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/ETL/</a></p><h2 id="数据清洗案例实操-复杂解析版"><a href="#数据清洗案例实操-复杂解析版" class="headerlink" title="数据清洗案例实操-复杂解析版"></a>数据清洗案例实操-复杂解析版</h2><p>需求<br>对Web访问日志中的各字段识别切分，去除日志中不合法的记录。根据清洗规则，输出过滤后的数据。</p><p>需求分析<br>    需要在Map阶段对输入的数据根据规则进行过滤清洗。</p><p>案例代码</p><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/ETL2/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/ETL2/</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;当我们凶狠地对待这个世界时，这个世界突然变得温文尔雅了。 ——余华                                                   &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="hadoop计数器应用" scheme="http://xubatian.cn/tags/hadoop%E8%AE%A1%E6%95%B0%E5%99%A8%E5%BA%94%E7%94%A8/"/>
    
    <category term="hadoop数据清洗" scheme="http://xubatian.cn/tags/hadoop%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/"/>
    
  </entry>
  
  <entry>
    <title>hadoop模块组成之Join多种应用</title>
    <link href="http://xubatian.cn/hadoop%E6%A8%A1%E5%9D%97%E7%BB%84%E6%88%90%E4%B9%8BJoin%E5%A4%9A%E7%A7%8D%E5%BA%94%E7%94%A8/"/>
    <id>http://xubatian.cn/hadoop%E6%A8%A1%E5%9D%97%E7%BB%84%E6%88%90%E4%B9%8BJoin%E5%A4%9A%E7%A7%8D%E5%BA%94%E7%94%A8/</id>
    <published>2022-01-15T16:50:47.000Z</published>
    <updated>2022-01-17T16:48:00.799Z</updated>
    
    <content type="html"><![CDATA[<p>“日子甜甜的，像清晨的柠檬水，像初冬的太阳，像梦里的大海，像第一次遇见你。” –短句                            </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.baidu.cn_108.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="Join多种应用"><a href="#Join多种应用" class="headerlink" title="Join多种应用"></a>Join多种应用</h3><h4 id="Reduce-Join"><a href="#Reduce-Join" class="headerlink" title="Reduce Join"></a>Reduce Join</h4><h5 id="Reduce-Join工作原理"><a href="#Reduce-Join工作原理" class="headerlink" title="Reduce Join工作原理"></a>Reduce Join工作原理</h5><p>Map端的主要工作：</p><p>​        为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key,其余部分和新加的标志作为value，最后进行输出。</p><p>Reduce端的主要工作：</p><p>​        在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录(在Map阶段已经打标<br>志)分开，最后进行合并就ok了。</p><h5 id="Reduce-Join案例实操及代码"><a href="#Reduce-Join案例实操及代码" class="headerlink" title="Reduce Join案例实操及代码"></a>Reduce Join案例实操及代码</h5><p>通过将关联条件作为Map输出的key，将两表满足Join条件的数据并携带数据所来源的文件信息，发往同一个ReduceTask，在Reduce中进行数据的串联，如图所示:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_106.png" width = "" height = "" alt="xubatian的博客" align="center" /><p><strong>案例代码</strong>: <a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/reduceJoin/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/reduceJoin/</a></p><h5 id="Reduce-Join的缺陷及解决方案"><a href="#Reduce-Join的缺陷及解决方案" class="headerlink" title="Reduce Join的缺陷及解决方案"></a>Reduce Join的缺陷及解决方案</h5><p>缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜。</p><p> 解决方案: Map端实现数据合并</p><h4 id="Map-Join"><a href="#Map-Join" class="headerlink" title="Map Join"></a>Map Join</h4><h5 id="mapJoin概念"><a href="#mapJoin概念" class="headerlink" title="mapJoin概念"></a>mapJoin概念</h5><p>1．使用场景<br>Map Join适用于一张表十分小、一张表很大的场景。<br>2．优点<br>思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？<br>在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。<br>3．具体办法：采用DistributedCache<br>    （1）在Mapper的setup阶段，将文件读取到缓存集合中。<br>    （2）在驱动函数中加载缓存。<br>// 缓存普通文件到Task运行节点。<br>job.addCacheFile(new URI(“file://e:/cache/pd.txt”));</p><h5 id="Map-Join案例实操及代码"><a href="#Map-Join案例实操及代码" class="headerlink" title="Map Join案例实操及代码"></a>Map Join案例实操及代码</h5><p><strong>案例代码</strong>:<a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/mapJoin/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/mapJoin/</a></p><p>需求分析</p><p>MapJoin适用于关联表中有小表的情形</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_107.png" width = "" height = "" alt="xubatian的博客" align="center" />]]></content>
    
    
    <summary type="html">&lt;p&gt;“日子甜甜的，像清晨的柠檬水，像初冬的太阳，像梦里的大海，像第一次遇见你。” –短句                            &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="mapJoin" scheme="http://xubatian.cn/tags/mapJoin/"/>
    
    <category term="ReduceJoin" scheme="http://xubatian.cn/tags/ReduceJoin/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce框架原理之InputFormat数据输入</title>
    <link href="http://xubatian.cn/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86%E4%B9%8BInputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5/"/>
    <id>http://xubatian.cn/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86%E4%B9%8BInputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5/</id>
    <published>2022-01-14T16:20:06.000Z</published>
    <updated>2022-01-17T17:45:12.081Z</updated>
    
    <content type="html"><![CDATA[<p>孤独这两个字拆开来看，有孩童，有瓜果，有小犬，有蚊蝇，足以撑起一个盛夏傍晚间的巷子口，人情味十足。 稚儿擎瓜柳棚下，细犬逐蝶窄巷中，人间繁华多笑语，惟我空余两鬓风。 孩童水果猫狗飞蝇当然热闹，可都和你无关，这就叫孤独。 …                            </p><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_94.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><p>前言</p><p>上文 对mapreduce的概述 简单了解什么是MapReduce? </p><p>MapReduce是hadoop解决大数据计算问题的而落地的一个计算引擎. 他是一个计算框架.</p><p>mapreduce如何做到海量数据的计算的呢?</p><p>mapreduce的流程是什么样子的呢?</p><p>mapreduce的有哪些部分组成的的?</p><p>图示:</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mm2.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>如上图所示, 这就是完整的mapreduce 的数据流图.</p><p>首先我们知道了hadoop的hdfs是将海量的数据进行了存储.  而hadoop的mapreduce是将存储的海量数据进行计算得出我们想要的结果.</p><p>那么他要计算这些数据.  首先他得将数据输入到我们的计算引擎mapreduce当中, 即使用Inputformat将数据导入到mapreduce里面来, 然后通过mapTask将数据打散,即map任务, 再通过reduceTask任务将数据聚合,即reduce任务. 最后通过outputFormat将计算的结果输出到某个文件或者某个数据库中.</p><p>那么由此可知. 整个mapreduce是由. Inputformart输入数据.  MapTask 打散数据. ReduceTask聚合数据. Outputformart输出结果.这几个部分组成的. 所以,要知道mapreduce的框架原理.需要学习组成mapreduce的四个环节.</p><p>那么这四个环节分别叫什么呢?</p><p><strong>InputFormat数据输入—–&gt;MapTask工作机制,Shuffle工作机制,ReduceTask工作机制—&gt;OutputFormat数据输出.</strong></p><p>着整个过程成为 mapreduce过程. 内容分这几个部分. 当然,这几部分也是很笼统的. 里面设计具体的很多概念.</p><h1 id="MapReduce框架原理之InputFormat数据输入"><a href="#MapReduce框架原理之InputFormat数据输入" class="headerlink" title="MapReduce框架原理之InputFormat数据输入"></a>MapReduce框架原理之InputFormat数据输入</h1><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mm.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>InputFormat, 这个是整个MapReduce里面非常关键的一个对象.</p><p>什么是InputFormat呢? 就是你输入到这个mapper里面的数据其实就是由InputFormat来负责的. 比如我怎么从文件里面去读这个数据. 这里还涉及到是否要切片这个概念.</p><p>MapReduce数据流是比较简单的.一个是mapper阶段. 一个是reduce阶段. 在mapper阶段前面就有一个InputFormat帮我们去读数据. 在Reducer后面有一个OutPutFormat帮我们去写数据.在mapper与reducer中间就是我们讲的shuffle过程.大部分工作都是在shuffle里面做的.</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mm2.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>那么 InputFormat他又是如何拿数据呢? 这涉及到了切片机制. 那么什么是切片呢?</p><p>我数据是按照块(block)的的方式去存到HDFS上的. 那么既然他是按照块存储的. 假设配置文件设置128M为一块,一个300M文件分成了三个块.可能存到不同的服务器上. 那么我mapreduce也是按照块为单位去拿数据吗? 不是的, 他是将这128M的块, 按照我们配置的配置文件的切片大小去获取数据. 比如 我们设置100M 为一个切片. 那么这个128M的块, 就会有两个切片.即100M 和 28M. mapreduce启动job任务去拿数据他是并行的, 所以一个mapTask只会获取一个切片,所以,他会启动两个mapTask同时去拿这两个切片.</p><p>注意: 切片不是物理上将块切开的,而是逻辑上切开的. </p><p>总结: 切片和InputFormat有啥联系呢?是在切片信息的时候涉及到. 要知道我们的切片是由我们InputFormat来负责的.</p><p>既然 切片是由Inputformat负责的,所以需要了解什么是切片.</p><h3 id="切片与MapTask并行度决定机制"><a href="#切片与MapTask并行度决定机制" class="headerlink" title="切片与MapTask并行度决定机制"></a>切片与MapTask并行度决定机制</h3><p>切片不从物理上切开,比如说,我有一个两百兆的文件.我上传到HDFS以后. 假如说的我一块的大小是128M. 那么这个200M的文件上传到HDFS以后被切成了两块了.<br>第一块128M,第二块72M. 到此为止只是块的概念.<br>接下来我就要通过map来分析你这个数据了.正常情况下,其实你的一个块的数据要交给mapTask去处理.但是严格意义上并不是这样的.而是一个切片要交给一个mapTask来处理.<br>那么这个切片是怎么来的呢?<br>切片和块的大小有点关系,但是我们可以自己去设置有多大. 比如假设我设置我的切片大小是64M. 那么对于128M块大小的数据在真正进入到map要处理的时候.会把这个128M的块的数据从逻辑上切成两片.<br>第一片交给一个mapTask去处理.<br>第二片也交给一个mapTask去处理.<br>所以说,严格来说不是一个块的数据交给mapTask.而是一个切片的数据交给一个mapTask.</p><p>72M的也是切成两片.<br>这里我们所说的切只是逻辑上的,这个块还是整个的一个块,128M.他是不会从物理上给你切开的. 只是逻辑上的.<br> 一个切片,一个mapTask.  注意: 默认情况下切片的大小和块的大小是一样的.<br>也就意味着不自己设置的情况下.他的一个切片的大小也是128M. 正好我的默认情况下块的大小是一样的.</p><p>那么问题来了, 我是不是一个切片越大,mapTask越多,我并行能力越强,是否我的性能就越高呢?</p><p><strong>问题引出</strong></p><p>MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度。<br>思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？</p><p><strong>MapTask并行度决定机制</strong></p><p>数据块：Block是HDFS物理上把数据分成一块一块。<br>数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。(逻辑上对数据块进行划分,让多个mapTask并行处理数据块的不同切片部分)</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mm4.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>假如我有300M的数据.假设切片大小我设置为100M. 块大小设置为128M. 那么这个300M的文件将来我存到dataNode上面的时候,如上图. 128M一块 ,128M一块, 44M一块.<br>块大小还是按照128M. 但是我的切片大小是100M. 那么将来我在处理我128M的数据的时候,那我就要从逻辑上将我的128M块 切片分成100M 和 28M.  其中这100M我交给mapTask去处理. 那么我剩下的28M怎么办呢? 他不是在往后面找72M数据合起来. 他有一个切片的概念. 切片他是不考虑数据的整体性. 正常情况下他是以块来作为一个单位来切的.  所以hadoop他默认就是按照128M来切的.这样就不用了跨机器读你的数据了.</p><p>记住: 你有多少个切片,将来你就有多少个mapTask.  每一个切片都分配一个mapTask来进行并行处理.   默认情况下切片大小就是你的块大小.</p><h3 id="通过源码查看切片机制"><a href="#通过源码查看切片机制" class="headerlink" title="通过源码查看切片机制"></a>通过源码查看切片机制</h3><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mm5.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="数据倾斜问题"><a href="#数据倾斜问题" class="headerlink" title="数据倾斜问题"></a>数据倾斜问题</h3><p>我切片,剩余的长度除以切片大小是大于1.1的,那我就接着去切. 如果是小于等于1.1就不切了,就是一块了. 假如我默认切片大小是128M,但是我的数据块是129M,这样我就不切了吗? 这就要看他是否大于默认切片大小(128M)的1.1倍了.  但是为什么不大于1.1我就不切了呢? 比如说我默认切片大小是32M,那么我32*1.1=35.2M   那就是只要我切片大小&lt;=35.3 我就不切了.  比如说我有一个35.2M的数据,如果说没有这个切片大小&lt;= 默认切片大小的1.1倍就不切的这个公式的话. 而是完全按照32M来切.那我就切出来一个32和一个3.2M的数据.这就是两个切片了. 两个切片将来就需要交给两个mapTask去处理. 那么负责处理32M切片的mapTask是负责3.2M切片的mapTask的数据的10倍. 这就导致一个mapTask负责数据量比较大,另一个负责数据量比较小.这就出现了数据倾斜的问题.</p><h3 id="Job提交流程源码和切片源码详解"><a href="#Job提交流程源码和切片源码详解" class="headerlink" title="Job提交流程源码和切片源码详解"></a>Job提交流程源码和切片源码详解</h3><p>所谓的Job提交流程就是当我们这个job提交之后,它后续做了哪些事情?</p><p>即,我们提交job之后,MapTask执行之前.我们程序都做了什么.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">Job提交流程(MapTask执行之前):</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> 在驱动类中job.waitForCompletion(<span class="keyword">true</span>) 提交Job</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> 执行submit()方法</span><br><span class="line">   [<span class="number">1</span>]. ensureState(JobState.DEFINE);  确认Job的状态</span><br><span class="line">   [<span class="number">2</span>]. setUseNewAPI(); 设置使用新API</span><br><span class="line">   [<span class="number">3</span>]. connect();</span><br><span class="line">   ① <span class="keyword">return</span> <span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line">   ② 调用构造器里面的重要方法initialize(jobTrackAddr, conf);</span><br><span class="line">      (<span class="number">1</span>). clientProtocol = provider.create(conf); 获取Job的运行方式(本地 LocalJobRunner / yarn YARNRunner)</span><br><span class="line"></span><br><span class="line">   [<span class="number">4</span>]. 获取到JobSubmitter submitter , 然后提交Job submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">   ① checkSpecs(job); 校验输出路径是否存在. 如果存在，直接抛出异常.</span><br><span class="line">   ②  Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line">       获取Job的临时的工作目录，例如: D:/tmp/hadoop-Administrator/mapred/staging/Administrator780971275/.staging </span><br><span class="line">   ③  JobID jobId = submitClient.getNewJobID(); 生成JobId</span><br><span class="line">   ④  Path submitJobDir = <span class="keyword">new</span> Path(jobStagingArea, jobId.toString()); </span><br><span class="line">       获取到提交Job的目录 ，实际就是   jobStagingArea + jobId 。</span><br><span class="line">   ⑤  copyAndConfigureFiles(job, submitJobDir); 复制并配置一些文件</span><br><span class="line">       (<span class="number">1</span>).  rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line">       (<span class="number">2</span>).  submitJobDir = jtFs.makeQualified(submitJobDir); 获取到提交job的全路径</span><br><span class="line">       (<span class="number">3</span>).  FileSystem.mkdirs(jtFs, submitJobDir, mapredSysPerms); 在文件系统创建Job提交的相关目录</span><br><span class="line">   ⑥  <span class="keyword">int</span> maps = writeSplits(job, submitJobDir); 生成切片信息，并将 job.split 切片信息的文件写到job提交目录中  (下面着重讲如何生成切片信息的)</span><br><span class="line">       (<span class="number">1</span>). maps = writeNewSplits(job, jobSubmitDir); </span><br><span class="line">       (<span class="number">2</span>). InputFormat&lt;?, ?&gt; input = ReflectionUtils.newInstance(job.getInputFormatClass(), conf);</span><br><span class="line">            切片信息是否InputFormat来生成的.  默认的的InputFormat是 TextInputFormat</span><br><span class="line">       (<span class="number">3</span>). List&lt;InputSplit&gt; splits = input.getSplits(job); 通过InputFormat来获取切片信息</span><br><span class="line">    a.  <span class="keyword">long</span> minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job)); ==&gt;<span class="number">1</span></span><br><span class="line">    b.  <span class="keyword">long</span> maxSize = getMaxSplitSize(job);  ==&gt; Long.MAX_VALUE</span><br><span class="line">    c.  <span class="keyword">long</span> blockSize = file.getBlockSize(); 获取块大小，本地默认的块大小32M.</span><br><span class="line">    d.  <span class="keyword">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize); 计算切片大小</span><br><span class="line">             ****** Math.max(minSize, Math.min(maxSize, blockSize));假如我想要切片大小变大我需要将minSize调大就可以了.如果我想要切片大小变的更小我就需要调整maxSize了.</span><br><span class="line">    e.   bytesRemaining)/splitSize &gt; SPLIT_SLOP(<span class="number">1.1</span>) 如果剩余文件的大小超过切片</span><br><span class="line">         大小的<span class="number">1.1</span>倍,继续切片，反之, 不切片.</span><br><span class="line"></span><br><span class="line">   ⑦  conf.setInt(MRJobConfig.NUM_MAPS, maps); 设置MapTask的个数</span><br><span class="line">   ⑧   writeConf(conf, submitJobFile); 将job相关的 job.xml 配置信息等写到job提交目录中</span><br><span class="line">   ⑨  status = submitClient.submitJob(</span><br><span class="line">             jobId, submitJobDir.toString(), job.getCredentials()); 真正提交Job，提交了job之后就需要执行我的mapTask了</span><br><span class="line">   ⑩  jtFs.delete(submitJobDir, <span class="keyword">true</span>); 等Job执行结束后，删除临时目录.</span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">----------------------------------------------------------------------------------------------------------------          </span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">MapTask:   如何生成切片信息的 (⑥  <span class="keyword">int</span> maps = writeSplits(job, submitJobDir); 生成切片信息，并将 job.split 切片信息的文件写到job提交目录中)</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span>Job job = <span class="keyword">new</span> Job(JobID.downgrade(jobid), jobSubmitDir); </span><br><span class="line">   创建一个真正执行的job. </span><br><span class="line">   Job: org.apache.hadoop.mapred.LocalJobRunner.Job</span><br><span class="line"><span class="number">2.</span> <span class="keyword">this</span>.start();   执行的是LocalJobRunner.Job 中的run方法. </span><br><span class="line"><span class="number">3.</span> 在LocalJobRunner.Job的run方法中， 执行runTasks(mapRunnables, mapService, <span class="string">&quot;map&quot;</span>);</span><br><span class="line"><span class="number">4.</span> 在runTasks中迭代出所有的MapTaskRunnable,并开始让线程执行。</span><br><span class="line"><span class="number">5.</span> 执行MapTaskRunnable的run方法. </span><br><span class="line"><span class="number">6.</span> 创建MapTask对象   MapTask map = <span class="keyword">new</span> MapTask(systemJobFile.toString(), mapId, taskId,info.getSplitIndex(), <span class="number">1</span>);</span><br><span class="line"><span class="number">7.</span> 执行MapTask的run方法</span><br><span class="line"><span class="number">8.</span> 执行runNewMapper(job, splitMetaInfo, umbilical, reporter);</span><br><span class="line"><span class="number">9.</span> 获取context， mapper ， inputFormat  ，recordReader 等</span><br><span class="line"><span class="number">10.</span> 获取Map端负责输出的对象 :</span><br><span class="line">output = <span class="keyword">new</span> NewOutputCollector(taskContext, job, umbilical, reporter);</span><br><span class="line"><span class="number">11.</span> mapper.run(mapperContext);</span><br><span class="line"><span class="number">12.</span> 执行Mapper中的map方法。</span><br></pre></td></tr></table></figure><p>&emsp;     </p><h3 id="Job提交流程源码详解"><a href="#Job提交流程源码详解" class="headerlink" title="Job提交流程源码详解"></a>Job提交流程源码详解</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">waitForCompletion()</span><br><span class="line"></span><br><span class="line">submit();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1建立连接</span></span><br><span class="line">connect();</span><br><span class="line"><span class="comment">// 1）创建提交Job的代理</span></span><br><span class="line"><span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line"><span class="comment">// （1）判断是本地yarn还是远程</span></span><br><span class="line">initialize(jobTrackAddr, conf); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 提交job</span></span><br><span class="line">submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster)</span><br><span class="line"><span class="comment">// 1）创建给集群提交数据的Stag路径</span></span><br><span class="line">Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2）获取jobid ，并创建Job路径</span></span><br><span class="line">JobID jobId = submitClient.getNewJobID();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3）拷贝jar包到集群</span></span><br><span class="line"></span><br><span class="line">copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4）计算切片，生成切片规划文件</span></span><br><span class="line">writeSplits(job, submitJobDir);</span><br><span class="line">maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">input.getSplits(job);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5）向Stag路径写XML配置文件</span></span><br><span class="line">writeConf(conf, submitJobFile);</span><br><span class="line">conf.writeXml(out);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6）提交Job,返回提交状态</span></span><br><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/job1.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="FileInputFormat切片源码解析-input-getSplits-job"><a href="#FileInputFormat切片源码解析-input-getSplits-job" class="headerlink" title="FileInputFormat切片源码解析(input.getSplits(job))"></a>FileInputFormat切片源码解析(input.getSplits(job))</h3><p>（1）程序先找到你数据存储的目录。<br>（2）开始遍历处理（规划切片）目录下的每一个文件<br>（3）遍历第一个文件ss.txt<br>        a）获取文件大小fs.sizeOf(ss.txt)<br>        b）计算切片大小<br>               computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M<br>        c）默认情况下，切片大小=blocksize<br>        d）开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M<br>                （每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片）<br>        e）将切片信息写到一个切片规划文件中<br>        f）整个切片的核心过程在getSplit()方法中完成<br>        g）InputSplit只记录了切片的元数据信息，比如起始位置、长度以及所在的节点列表等。<br>（4）提交切片规划文件到YARN上，YARN上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数。</p><h3 id="FileInputFormat切片机制"><a href="#FileInputFormat切片机制" class="headerlink" title="FileInputFormat切片机制"></a>FileInputFormat切片机制</h3><p>​     1、切片机制<br>​       （1）简单地按照文件的内容长度进行切片<br>​       （2）切片大小，默认等于Block大小<br>​       （3）切片时不考虑数据集整体，一个文件单独切片<br>​    2、案例分析<br>​      （1）输入数据有两个文件：<br>​                    filel.txt 320M<br>​                    file2.txt  10M<br>​          (2) 经过FileInputFormat的切片机制运算后，形成的切片信息如下：<br>​                    filel.txt.split2–     128<del>256<br>​                    file1.txt.split3–    256</del>320<br>​                    file2.txt.split1-      0~10M</p><h3 id="FileInputFormat切片大小的参数配置"><a href="#FileInputFormat切片大小的参数配置" class="headerlink" title="FileInputFormat切片大小的参数配置"></a>FileInputFormat切片大小的参数配置</h3><p>（1）源码中计算切片大小的公式<br>            Math.max(minSize,Math.min(maxSize,blockSize));<br>            mapreduce.input.fileinputformat.split.minsize=1 默认值为1<br>            mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue 默认值Long.MAXValue<br>            因此，默认情况下，切片大小=blocksize。<br>（2）切片大小设置<br>                maxsize（切片最大值）：参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值。<br>                minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blockSize还大。<br>(3)获取切片信息API<br>            // 获取切片的文件名称<br>            String name = inputSplit.getPath().getName();<br>            // 根据文件类型获取切片信息<br>            FileSplit inputSplit = (FileSplit) context.getInputSplit();</p><h3 id="有哪些形式的InputFormat"><a href="#有哪些形式的InputFormat" class="headerlink" title="有哪些形式的InputFormat"></a>有哪些形式的InputFormat</h3><p>思考：在运行MapReduce程序时，输入的文件格式包括：基于行的日志文件二进制格式文件、数据库表等。那么，针对不同的数据类型，MapReduce是如何读取这些数据的呢？</p><p>针对不容类型,不同场景,我们做一些实现类,自己写代码.</p><p>这里有最常见的,写好的实现类.</p><p>FileInputFormat 常 见 的 接 口 实 现 类 包 括:<br>TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat等。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_100.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_101.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_102.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="KeyValueTextInputFormat使用案例"><a href="#KeyValueTextInputFormat使用案例" class="headerlink" title="KeyValueTextInputFormat使用案例"></a>KeyValueTextInputFormat使用案例</h3><p><strong>案例代码</strong>: <a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/keyValueTextInputFormat/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/keyValueTextInputFormat/</a></p><p>keyValueTextInputFormat他读取文件的时候还是按照行来读取. 但是他读取进来时候如何去切分他, 他只会将文件切分成两份,切完后,左边是key, 右边就是value.</p><p>所以我们得带驱动类Driver中设置我们的分割符, 那些是key ,哪些是value.</p><h3 id="NLineInputFormat使用案例"><a href="#NLineInputFormat使用案例" class="headerlink" title="NLineInputFormat使用案例"></a>NLineInputFormat使用案例</h3><p><strong>案例代码</strong>:<a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/NLineInputFormat">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/NLineInputFormat</a></p><p>NLineInputFormat 表什么呢? N表示数字. Line表示行.  表达的意思是n行的InputFormat.  他是按照你指定的行号进行切片的. 比如说我有一个文件. 我希望你将来对我文件进行处理的时候是每三行, 或者说每四行进行处理. 这个N 是你具体指定的. 就是按照行数来进行切片.<br>所以NLineInputFormat改变的是你的切片的规则, 比如说我有三十一行数据,n为3表示每三行生成一个切片,最终我三十一行生成11个切片.</p><h3 id="CombineTextInputFormat切片机制"><a href="#CombineTextInputFormat切片机制" class="headerlink" title="CombineTextInputFormat切片机制"></a>CombineTextInputFormat切片机制</h3><p><strong>案例代码</strong>:<a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/CombineTextInputFormat/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/CombineTextInputFormat/</a></p><p>如果说我们使用框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。</p><p>比如说我有5个文件, 第一个是1M,第二个是2M,3M,4M,5M. 因为他们都小于我们的块大小128M.  所以说按照单个文件来去切的话, 每个文件都是一个切片, 最终生成5个切片. 且我的每一个切片都是对应一个mapTask, 但是这么小的切片对应我的mapTask不是杀鸡用牛刀吗?它都不值得我去启动一次mapTask.因为你数据量太小了.所以对于小文件来说我就不能使用TextInputFormat了, 我必须对你做一个处理.</p><p>比如接下来说的CombineTextInputFormat.</p><p>他是用的场景就是小文件过多的情况下.他的思想是什么呢?<br>首先我们使用CombineTextInputFormat的时候得先设置虚拟存储切片的最大值. 具体设置多大按照实际情况来定. 假如说我把虚拟存储最大值设置为4M    ,那么他会根据这个4M进行判断,我这个文件到底要不要去切. 然后最终我怎么去帮你生成切片.所以CombineTextInputFormat也是可以避免数据倾斜问题.</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/job6.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>如上图所示:</p><p>我们设置虚拟存储的文件大小(setMaxInputSplitSize)为4M. 比如说我们有几个文件(如上图) 这些小文件.  因为我们设置虚拟存储大小是4M.  在虚拟存储过程中有一个算法,就是如果你的文件的大小小于你虚拟存储的最大值(即4M). 那么对于你这个文件来说就将你划分成一块.这一块是虚拟的.不是真正划分的. 但是如果你实际文件大小是大于你设置的文件最大值的(4M) , 但是他又小于两倍的最大值. 即大于4M 小于8M.  在这种情况下我就将你这个文件一分为2. 如上面的两块的2.55M .最后生成最终存储的文件即虚拟文件. 接下来他就按照这个规划帮你进行切片. 生成切片的过程中他会判断你虚拟存储的文件大小是否大于我们设置的值(4M). 如果大于则单独形成一个切片.  如果你虚拟存储的大小不大于4M. 那么他就会和你下一个虚拟存储文件进行合并, 共同形成一个切片.  如上图的1.7M+2.55M 形成一个切片.  最终形成3个切片. 三个切片不会差太多.这就规避了数据倾斜问题.</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/job7.png" width = "" height = "" alt="xubatian的博客" align="center" /><p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/job8.png" width = "" height = "" alt="xubatian的博客" align="center" />“ </p><p>1、应用场景：</p><p>CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。</p><p>2、虚拟存储切片最大值设置</p><p>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m<br>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</p><p>3、切片机制</p><p>生成切片过程包括：虚拟存储过程和切片过程二部分。</p><h3 id="自定义InputFormat"><a href="#自定义InputFormat" class="headerlink" title="自定义InputFormat"></a>自定义InputFormat</h3><p>在企业开发中，Hadoop框架自带的InputFormat类型不能满足所有应用场景，需要自定义InputFormat来解决实际问题。<br>自定义InputFormat步骤如下：<br>（1）自定义一个类继承FileInputFormat。<br>（2）改写RecordReader，实现一次读取一个完整文件封装为KV。<br>（3）在输出时使用SequenceFileOutPutFormat输出合并文件。</p><p>无论HDFS还是MapReduce，在处理小文件时效率都非常低，但又难免面临处理大量小文件的场景，此时，就需要有相应解决方案。可以自定义InputFormat实现小文件的合并。</p><p>比如说我有n多个小文件,我想将他存到HDFS中,之前的一种方式是使用har文件.<br>但是现在我想把我的n多个小文件最终存到一个文件里面.就是死将这n多个小文件里面的内容拿出来存到一个文件里面,存成一个稍微大一点的文件. 但是我不能简单存成一个txt文件. 因为这样的话就会乱了.就揉到一起了.  我还是希望你存到这大文件里面以后他们之间还是有一个很明确的划分的. 所以这一次我会用到一个SequenceFlie.  SequenceFile就是一个序列文件. 这个文件的格式比较特殊. 虽然是一个文件, 但是它里面也是以k,v 的形式来存的. k 可以是原来文件的路径.v是文件的内容 (D:/one.txt  , 文件内容  ) . 虽然说我将多个文件存到一个文件当中, 但是这个文件里面他也是有明确的划分的.  将来我们去读的时候也是可以通过 k, v 的形式去读出来. 而且这种格式存起来特别的紧凑.  比如原先我n个小文件存起来是5M, 但是通过 SequenceFile这种方式存的话可能就 3M. 因为他是二进制的存储方式.  sequenceFile这种文件的格式hadoop是支持的 , 但是我们得自己操作, 将小文件里面的内容读出来,以 k ,v 的形式给他写进去.这个过程需要我们自己做.所以我在读我们每一个小文件的时候希望一次性将文件里面的内容都读出来然后向sequenceFile中去放. 比如说k 就是我们小文件的路径加名字. v就是我文件的内容. 我一次性将这个k,v 写到sequenceFile中.  但是我们上面的InputFormat没有一次性读整个文件的. 都是一行一行读的. 所以这就需要我们自定义了.</p><p>具体案例: 略.</p><h1 id="MapReduce工作流程"><a href="#MapReduce工作流程" class="headerlink" title="MapReduce工作流程"></a>MapReduce工作流程</h1><p>1.MapReduce工作流程:</p><p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/map1.png" width = "" height = "" alt="xubatian的博客" align="center" />“ </p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/map2.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>2.流程详解:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">上面的流程是整个MapReduce最全工作流程，但是Shuffle过程只是从第<span class="number">7</span>步开始到第<span class="number">16</span>步结束，具体Shuffle过程详解，如下：</span><br><span class="line"></span><br><span class="line"><span class="number">1</span>）MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>）多个溢出文件会被合并成大的溢出文件</span><br><span class="line"></span><br><span class="line"><span class="number">4</span>）在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序</span><br><span class="line"></span><br><span class="line"><span class="number">5</span>）ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据</span><br><span class="line"></span><br><span class="line"><span class="number">6</span>）ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）</span><br><span class="line"></span><br><span class="line"><span class="number">7</span>）合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>．注意</span><br><span class="line">Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。</span><br><span class="line">缓冲区的大小可以通过参数调整，参数：io.sort.mb默认100M。</span><br><span class="line"></span><br><span class="line"><span class="number">4</span>．源码解析流程</span><br><span class="line">context.write(k, NullWritable.get());</span><br><span class="line">output.write(key, value);</span><br><span class="line">collector.collect(key, value,partitioner.getPartition(key, value, partitions));</span><br><span class="line">HashPartitioner();</span><br><span class="line">collect()</span><br><span class="line">close()</span><br><span class="line">collect.flush()</span><br><span class="line">          sortAndSpill()</span><br><span class="line">         sort()  </span><br><span class="line">          <span class="function">QuickSort</span></span><br><span class="line"><span class="function"><span class="title">mergeParts</span><span class="params">()</span></span>;</span><br><span class="line">          file.out;</span><br><span class="line">file.out.index;</span><br><span class="line">collector.close();</span><br></pre></td></tr></table></figure><p>整个mapreduce分为:</p><p><strong>InputFormat数据输入—–&gt;MapTask工作机制,Shuffle工作机制,ReduceTask工作机制—&gt;OutputFormat数据输出.</strong></p><p>此篇文章讲解了InputFormat数据输入, 下一篇就是 MapTask工作机制,Shuffle工作机制,ReduceTask工作机制.</p><p>Shuffle工作机制比较重要,单独记录. 下一篇是: hadoop组成模块之mapreduce的MapTask机制和reduceTask机制</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;孤独这两个字拆开来看，有孩童，有瓜果，有小犬，有蚊蝇，足以撑起一个盛夏傍晚间的巷子口，人情味十足。 稚儿擎瓜柳棚下，细犬逐蝶窄巷中，人间繁华多笑语，惟我空余两鬓风。 孩童水果猫狗飞蝇当然热闹，可都和你无关，这就叫孤独。 …                            &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="mapreduce" scheme="http://xubatian.cn/tags/mapreduce/"/>
    
    <category term="InputFormat" scheme="http://xubatian.cn/tags/InputFormat/"/>
    
  </entry>
  
  <entry>
    <title>hadoop组成模块之mapreduce的MapTask机制和reduceTask机制</title>
    <link href="http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8Bmapreduce%E7%9A%84MapTask%E6%9C%BA%E5%88%B6%E5%92%8CreduceTask%E6%9C%BA%E5%88%B6/"/>
    <id>http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8Bmapreduce%E7%9A%84MapTask%E6%9C%BA%E5%88%B6%E5%92%8CreduceTask%E6%9C%BA%E5%88%B6/</id>
    <published>2022-01-14T14:22:08.000Z</published>
    <updated>2022-01-17T16:56:46.487Z</updated>
    
    <content type="html"><![CDATA[<p>MapReduce程序分为map阶段,和reduce阶段. map阶段有mapTask任务,reduce阶段有reduceTask任务.</p><p>那么什么是mapTask? 什么是reduceTask?</p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_210.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><p>前言</p><p>mapreduce过程概括为: InputFormat—-&gt; mapper,shuffle,reducer—&gt;OutputFormat</p><p>上文说了一下InputFormat概念,做的事情,他的一些实现类.</p><p>这里记录一下.Mapper和Reducer的工作机制. shuffle里面会有记录.此处简单记录.</p><h1 id="MapTask工作机制"><a href="#MapTask工作机制" class="headerlink" title="MapTask工作机制"></a>MapTask工作机制</h1><h2 id="MapTask流程图解读"><a href="#MapTask流程图解读" class="headerlink" title="MapTask流程图解读"></a>MapTask流程图解读</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_78.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>（1）Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。</p><p>（2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。</p><p>（3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部,</p><p>​          它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。</p><p>   (4)  Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入   </p><p>​          本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p><h2 id="溢写阶段详情解读"><a href="#溢写阶段详情解读" class="headerlink" title="溢写阶段详情解读"></a>溢写阶段详情解读</h2><p>步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，   经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。</p><p>步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p><p>步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。</p><p>Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p><p>在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p><p>让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p><h1 id="ReduceTask工作机制"><a href="#ReduceTask工作机制" class="headerlink" title="ReduceTask工作机制"></a>ReduceTask工作机制</h1><ol><li>runTasks(reduceRunnables, reduceService, “reduce”); 准备执行所有的ReduceTaskRunnable</li><li>执行ReduceTaskRunnable 的run方法</li><li>创建ReduceTask对象  ReduceTask reduce = new ReduceTask(systemJobFile.toString(),reduceId, taskId, mapIds.size(), 1);</li><li>执行ReduceTask的run方法  reduce.run(localConf, Job.this);</li><li>runNewReducer(job, umbilical, reporter, rIter, comparator, keyClass, valueClass);</li><li>reducer.run(reducerContext);</li><li>执行到Reducer中的reduce方法。</li></ol><h2 id="ReduceTask流程图解读"><a href="#ReduceTask流程图解读" class="headerlink" title="ReduceTask流程图解读"></a>ReduceTask流程图解读</h2><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_91.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>​    （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上， 否则直接放到内存中。<br>​    （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。<br>​    （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。<br>​    （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。</p><h2 id="设置ReduceTask并行度（个数）"><a href="#设置ReduceTask并行度（个数）" class="headerlink" title="设置ReduceTask并行度（个数）"></a>设置ReduceTask并行度（个数）</h2><p>ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 默认值是1，手动设置为4</span><br><span class="line"></span><br><span class="line">job.setNumReduceTasks(4);</span><br></pre></td></tr></table></figure><h2 id="ReduceTask注意事项"><a href="#ReduceTask注意事项" class="headerlink" title="ReduceTask注意事项"></a>ReduceTask注意事项</h2><p>（1）ReduceTask=0，表示没有Reduce阶段，输出文件个数和Map个数一致。<br>（2）ReduceTask默认值就是1，所以输出文件个数为一个。<br>（3）如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜<br>（4）ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask。<br>（5）具体多少个ReduceTask，需要根据集群性能而定。<br>（6）如果分区数不是1，但是ReduceTask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;MapReduce程序分为map阶段,和reduce阶段. map阶段有mapTask任务,reduce阶段有reduceTask任务.&lt;/p&gt;
&lt;p&gt;那么什么是mapTask? 什么是reduceTask?&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="MapReduce" scheme="http://xubatian.cn/tags/MapReduce/"/>
    
    <category term="MapTask" scheme="http://xubatian.cn/tags/MapTask/"/>
    
  </entry>
  
  <entry>
    <title>Java</title>
    <link href="http://xubatian.cn/Java/"/>
    <id>http://xubatian.cn/Java/</id>
    <published>2022-01-14T09:53:14.000Z</published>
    <updated>2022-01-17T17:00:47.445Z</updated>
    
    <content type="html"><![CDATA[<p>侧脸 偷看 数学 操场 阳光 篮球 这是暗恋 情书 摸头 换座位 讲题 壁咚 陪跑 这是明恋       –来自网易音乐《Abelvolks》                            </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.xn_02.gif" width = "" height = "" alt="xubatian的博客" align="center" />]]></content>
    
    
    <summary type="html">&lt;p&gt;侧脸 偷看 数学 操场 阳光 篮球 这是暗恋 情书 摸头 换座位 讲题 壁咚 陪跑 这是明恋       –来自网易音乐《Abelvolks》                            &lt;/p&gt;</summary>
    
    
    
    <category term="Java" scheme="http://xubatian.cn/categories/Java/"/>
    
    
  </entry>
  
  <entry>
    <title>纪念册</title>
    <link href="http://xubatian.cn/%E7%BA%AA%E5%BF%B5%E5%86%8C/"/>
    <id>http://xubatian.cn/%E7%BA%AA%E5%BF%B5%E5%86%8C/</id>
    <published>2022-01-14T09:40:55.000Z</published>
    <updated>2022-01-17T16:47:03.914Z</updated>
    
    <content type="html"><![CDATA[<p>现在我也用上了.那年,那月,那一天…. 回忆带着苦涩….</p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_01.gif" width = "" height = "" alt="xubatian的博客" align="center" />]]></content>
    
    
    <summary type="html">&lt;p&gt;现在我也用上了.那年,那月,那一天…. 回忆带着苦涩….&lt;/p&gt;</summary>
    
    
    
    <category term="纪念册" scheme="http://xubatian.cn/categories/%E7%BA%AA%E5%BF%B5%E5%86%8C/"/>
    
    
    <category term="纪念册" scheme="http://xubatian.cn/tags/%E7%BA%AA%E5%BF%B5%E5%86%8C/"/>
    
  </entry>
  
  <entry>
    <title>hadoop组成模块之Yarn-HA高可用</title>
    <link href="http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BYarn-HA%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    <id>http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BYarn-HA%E9%AB%98%E5%8F%AF%E7%94%A8/</id>
    <published>2022-01-14T07:54:34.000Z</published>
    <updated>2022-01-17T16:58:56.452Z</updated>
    
    <content type="html"><![CDATA[<p>“一起看日落的人比日落更浪漫。”       –来自网易有音乐《你站在日落的海平面》                                                        </p><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.2/aliyun/www.xubatian.cn_29.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><p>前言</p><p>和HDFS-HA一样.此处简单了解</p><h1 id="YARN-HA配置"><a href="#YARN-HA配置" class="headerlink" title="YARN-HA配置"></a>YARN-HA配置</h1><h2 id="YARN-HA工作机制"><a href="#YARN-HA工作机制" class="headerlink" title="YARN-HA工作机制"></a>YARN-HA工作机制</h2><h3 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a>官方文档</h3><p><a href="https://hadoop.apache.org/docs/r3.1.3/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">https://hadoop.apache.org/docs/r3.1.3/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html</a></p><h3 id="YARN-HA工作机制-1"><a href="#YARN-HA工作机制-1" class="headerlink" title="YARN-HA工作机制"></a>YARN-HA工作机制</h3><p>如图所示:</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_70.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="配置YARN-HA集群"><a href="#配置YARN-HA集群" class="headerlink" title="配置YARN-HA集群"></a>配置YARN-HA集群</h3><ol><li>环境准备<br> （1）修改IP<br> （2）修改主机名及主机名和IP地址的映射<br> （3）关闭防火墙<br> （4）ssh免密登录<br> （5）安装JDK，配置环境变量等<br> （6）配置Zookeeper集群</li><li>   集群规划</li></ol><pre><code>| hadoop102       | hadoop103       | hadoop104   || --------------- | --------------- | ----------- || JournalNode     | JournalNode     | JournalNode || NameNode        | NameNode        |             || DataNode        | DataNode        | DataNode    || ZK              | ZK              | ZK          || ResourceManager | ResourceManager |             || NodeManager     | NodeManager     | NodeManager |</code></pre><ol start="3"><li>   具体配置</li></ol><p>​        （1）yarn-site.xml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">  &lt;configuration&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!--启用resourcemanager ha--&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!--声明两台resourcemanager的地址--&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;cluster-yarn1&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;rm1,rm2&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hadoop102&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hadoop103&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!--指定zookeeper集群的地址--&gt; </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!--启用自动恢复--&gt; </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt; </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">             &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;</span><br><span class="line">             &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>​      (2) 同步更新其他节点的配置信息</p><ol start="4"><li><p>启动hdfs[第一次启动]<br>（1）在各个JournalNode节点上，输入以下命令启动journalnode服务</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure><p>（2）在[nn1]上，对其进行格式化，并启动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs namenode -format</span><br><span class="line">sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure><p>（3）在[nn2]上，同步nn1的元数据信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure><p>（4）启动[nn2]</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure><p>（5）启动所有DataNode</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemons.sh start datanode</span><br></pre></td></tr></table></figure><p>（6）将[nn1]切换为Active</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs haadmin -transitionToActive nn1</span><br></pre></td></tr></table></figure></li><li><p>启动YARN<br>（1）在hadoop102中执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><p>（2）在hadoop103中执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure><p>（3）查看服务状态，如图所示</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn rmadmin -getServiceState rm1</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_71.png" width = "" height = "" alt="xubatian的博客" align="center" /></li></ol><p>​        查看所有进程:</p><p>​                                     <img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_72.png" width = "" height = "" alt="xubatian的博客" align="center" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;“一起看日落的人比日落更浪漫。”       –来自网易有音乐《你站在日落的海平面》                                                        &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="Yarn" scheme="http://xubatian.cn/tags/Yarn/"/>
    
    <category term="Yarn HA" scheme="http://xubatian.cn/tags/Yarn-HA/"/>
    
  </entry>
  
  <entry>
    <title>hadoop组成模块之HDFS HA高可用</title>
    <link href="http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BHDFS-HA%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    <id>http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BHDFS-HA%E9%AB%98%E5%8F%AF%E7%94%A8/</id>
    <published>2022-01-14T06:55:00.000Z</published>
    <updated>2022-01-17T12:41:49.121Z</updated>
    
    <content type="html"><![CDATA[<p>或许是物质世界泛滥了 人们越来越孤独 或许是现实形式太泛泛了 人们越来越追求简单淡泊了 或许是功利的人与事太思空见惯了 所以很多人灵魂深处渴望一位知己。 只知我曲中暖，却不知我心寒…       –来自网易云音乐《弦外知音》                            </p><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.2/aliyun/www.xubatian.cn_28.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>前言</p><p>为什么会提出高可用这个概念呢?</p><p>比如说我有一个对外提供服务的集群. 我对外提供服务的集群是不能随随便便就停止的. 提出高可用的原因是,在我们的hadoop中,比如像我们hdfs,yarn中,这两个都会存在一个问题. 这个问题就是单点故障问题. 所谓的单点故障就是说, 在我们的hdfs或者yarn中, 只要其中一个节点出现故障,其实你整个hdfs或者yarn就不能正常工作了. hdfs说的是namenode, yarn说的是ResourceManager. 为什么会这样呢? 因为现在对于我的hdfs来说,我有一个namenode ,下面有许多个DataNode. 这就构成整个集群.  其实在整个集群中他都是围绕着namenode来进行工作的.  所以你发现, 当namenode出现问题的时候. 即使你DataNode还好好的,但是我们的DataNode是不能直接对外提供服务的. 因为我整个集群的核心是namenode.  所以说只要namenode节点故障了. 其实对我整个集群来讲,他就故障了.  就不能对外提供服务了.  yarn也是一样 . 我们yarn的NodeManager首先得先和我们的NodeManager进行交互. 如果我们NodeManager挂了. 整个集群就停止了对外服务了. </p><p>说明单点故障不是hdfs或者yarn上面有, 而是我整个分布式里面都有. 只要你整个分布式,整个集群,如果他都要围绕着某一个节点来干活,它都存在单点故障问题.</p><p>Redis也是有. 他有主从关系.  他是如何解决单点故障呢? 是主挂掉,从上位.</p><h1 id="HA概述"><a href="#HA概述" class="headerlink" title="HA概述"></a>HA概述</h1><h2 id="什么是HDFS-HA"><a href="#什么是HDFS-HA" class="headerlink" title="什么是HDFS HA"></a>什么是HDFS HA</h2><p>1）所谓HA（High Availablity），即高可用（7*24小时不中断服务）。<br>2）实现高可用最关键的策略是消除单点故障。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。<br>3）Hadoop2.0之前，在HDFS集群中NameNode存在单点故障SPOF(单点故障)（Single Points Of Failure）。<br>4）NameNode主要在以下两个方面影响HDFS集群<br>      NameNode机器发生意外，如宕机，集群将无法使用，直到管理员重启<br>      NameNode机器需要升级，包括软件、硬件升级，此时集群也将无法使用<br>HDFS HA功能通过配置Active/Standby两个NameNodes实现在集群中对NameNode的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，<br>这时可通过此种方式将NameNode很快的切换到另外一台机器。</p><p>说白了就是搞多台主机, 其中一台挂了能迅速切换到另一台. 不影响整个集群的使用.</p><h2 id="HDFS-HA工作机制"><a href="#HDFS-HA工作机制" class="headerlink" title="HDFS-HA工作机制"></a>HDFS-HA工作机制</h2><p>通过双NameNode消除单点故障.</p><h2 id="HDFS-HA手动故障转移-不好用"><a href="#HDFS-HA手动故障转移-不好用" class="headerlink" title="HDFS-HA手动故障转移(不好用)"></a>HDFS-HA手动故障转移(不好用)</h2><p>如果一台主机挂了,咱们手动让另一台主机代替挂掉的主机工作,从未不影响集群工作.    </p><p>手动进行故障转移，在该模式下，即使现役NameNode已经失效，系统也不会自动从现役NameNode转移到待机NameNode</p><h2 id="配置HDFS-HA自动故障转移-用到了zookeeper"><a href="#配置HDFS-HA自动故障转移-用到了zookeeper" class="headerlink" title="配置HDFS-HA自动故障转移(用到了zookeeper)"></a>配置HDFS-HA自动故障转移(用到了zookeeper)</h2><p>如果一台主机挂了,zookeeper让另一台主机代替挂掉的主机工作,从未不影响集群工作. 提前是在zookeeper上配置好.    </p><p>自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）这是hadoop的进程,不是zookeeper的进程。ZKFC可以简单理解为zookeeper的客户端. ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。HA的自动故障转移依赖于ZooKeeper</p><p>注意: </p><p>ZKFailoverController(ZKFC)是一个新的组件，它是一个ZooKeeper客户端，它还监视和管理NameNode的状态。运行NameNode的每台机器也运行ZKFC，他们之间是一对一的关系。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>）故障检测：集群中的每个NameNode在ZooKeeper中维护了一个持久会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。</span><br><span class="line"><span class="number">2</span>）现役NameNode选择：ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。</span><br></pre></td></tr></table></figure><h2 id="ZKFC负责那些工作"><a href="#ZKFC负责那些工作" class="headerlink" title="ZKFC负责那些工作"></a>ZKFC负责那些工作</h2><p>1）健康监测：ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认      为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。<br>2）ZooKeeper会话管理：当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状      态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。<br>3）基于ZooKeeper的选择：如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如        果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为Active。故障转移进程与前面描述的手动故障转         移相似，首先如果必要保护之前的现役NameNode，然后本地NameNode转换为Active状态</p><p>1.具体配置</p><p>(1) 在hdfs-site.xml中增加</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p> (2) 在core-site.xml文件中增加</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br></pre></td></tr></table></figure><ol start="2"><li> 启动</li></ol>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@Hadoop102 hadoop]$ cd ~</span><br><span class="line">[shangbaishuyao@Hadoop102 ~]$ cd bin/</span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ vim startZookeeper</span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ vim startZookeeper</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">for i in shangbaishuyao@Hadoop102 shangbaishuyao@Hadoop103 shangbaishuyao@Hadoop104</span><br><span class="line">do</span><br><span class="line">        echo&quot;==========================start $i Zookeeper===============================&quot;</span><br><span class="line">        ssh $i &#x27;/opt/module/zookeeper-3.4.10/bin/zkServer.sh start&#x27;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ chmod 777 startZookeeper</span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 170 3月   1 09:58 myjps</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 249 3月  10 13:21 startZookeeper</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 499 2月  29 20:16 xsync</span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ cp startZookeeper statusZookeeper</span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ ll</span><br><span class="line">总用量 16</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 170 3月   1 09:58 myjps</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 249 3月  10 13:21 startZookeeper</span><br><span class="line">-rwxrwxr-x. 1 shangbaishuyao shangbaishuyao 249 3月  10 13:25 statusZookeeper</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 499 2月  29 20:16 xsync</span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ vim statusZookeeper </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">for i in shangbaishuyao@Hadoop102 shangbaishuyao@Hadoop103 shangbaishuyao@Hadoop104</span><br><span class="line">do</span><br><span class="line">        echo&quot;==========================status $i Zookeeper===============================&quot;</span><br><span class="line">        ssh $i &#x27;/opt/module/zookeeper-3.4.10/bin/zkServer.sh status&#x27;</span><br><span class="line">done</span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ </span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ cp startZookeeper stopZookeeper</span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ vim stopZookeer</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">for i in shangbaishuyao@Hadoop102 shangbaishuyao@Hadoop103 shangbaishuyao@Hadoop104</span><br><span class="line">do</span><br><span class="line">        echo&quot;==========================stop $i Zookeeper===============================&quot;</span><br><span class="line">        ssh $i &#x27;/opt/module/zookeeper-3.4.10/bin/zkServer.sh stop&#x27;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@Hadoop102 bin]$ ll</span><br><span class="line">总用量 20</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 170 3月   1 09:58 myjps</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 249 3月  10 13:21 startZookeeper</span><br><span class="line">-rwxrwxr-x. 1 shangbaishuyao shangbaishuyao 251 3月  10 13:26 statusZookeeper</span><br><span class="line">-rwxrwxr-x. 1 shangbaishuyao shangbaishuyao 247 3月  10 13:29 stopZookeeper</span><br><span class="line">-rwxrwxrwx. 1 shangbaishuyao shangbaishuyao 499 2月  29 20:16 xsync</span><br></pre></td></tr></table></figure><p>   以上操作完成后还是起不起来,有问题,因为他ssh过去之后他不会去加载profile,就意味着他找不到java,因为我的java环境变量是在这里面配置的,所以我需要他ssh过去之后可以加载java文件:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@Hadoop102 ~]$ cd /opt/module/zookeeper-3.4.10/</span><br><span class="line">[shangbaishuyao@Hadoop102 zookeeper-3.4.10]$ start</span><br><span class="line">start                 start-all.sh          start-dfs.cmd         start-pulseaudio-x11  start_udev            start-yarn.cmd        startZookeeper        </span><br><span class="line">start-all.cmd         start-balancer.sh     start-dfs.sh          start-secure-dns.sh   startx                start-yarn.sh         </span><br><span class="line">[shangbaishuyao@Hadoop102 zookeeper-3.4.10]$ startZookeeper </span><br><span class="line">/home/shangbaishuyao/bin/startZookeeper: line 4: echo==========================start shangbaishuyao@Hadoop102 Zookeeper===============================: command not found</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">/home/shangbaishuyao/bin/startZookeeper: line 4: echo==========================start shangbaishuyao@Hadoop103 Zookeeper===============================: command not found</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">/home/shangbaishuyao/bin/startZookeeper: line 4: echo==========================start shangbaishuyao@Hadoop104 Zookeeper===============================: command not found</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">[shangbaishuyao@Hadoop102 zookeeper-3.4.10]$ statusZookeeper </span><br><span class="line">/home/shangbaishuyao/bin/statusZookeeper: line 4: echo==========================status shangbaishuyao@Hadoop102 Zookeeper===============================: command not found</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Error contacting service. It is probably not running.</span><br><span class="line">/home/shangbaishuyao/bin/statusZookeeper: line 4: echo==========================status shangbaishuyao@Hadoop103 Zookeeper===============================: command not found</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Error contacting service. It is probably not running.</span><br><span class="line">/home/shangbaishuyao/bin/statusZookeeper: line 4: echo==========================status shangbaishuyao@Hadoop104 Zookeeper===============================: command not found</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Error contacting service. It is probably not running.</span><br><span class="line">[shangbaishuyao@Hadoop102 zookeeper-3.4.10]$ myjps</span><br><span class="line">========================hadoop102========================</span><br><span class="line">2783 Jps</span><br><span class="line">========================hadoop103========================</span><br><span class="line">4111 Jps</span><br><span class="line">========================hadoop104========================</span><br></pre></td></tr></table></figure><p>解决办法是在隐藏文件中</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@Hadoop102 zookeeper-3.4.10]$ cd ~</span><br><span class="line">[shangbaishuyao@Hadoop102 ~]$ ll -a</span><br><span class="line">总用量 60</span><br><span class="line">drwx------. 7 shangbaishuyao shangbaishuyao 4096 3月  10 13:31 .</span><br><span class="line">drwxr-xr-x. 3 root           root           4096 2月  27 14:12 ..</span><br><span class="line">-rw-------. 1 shangbaishuyao shangbaishuyao 3554 3月   6 13:23 .bash_history</span><br><span class="line">-rw-r--r--. 1 shangbaishuyao shangbaishuyao   18 5月  11 2016 .bash_logout</span><br><span class="line">-rw-r--r--. 1 shangbaishuyao shangbaishuyao  176 5月  11 2016 .bash_profile</span><br><span class="line">-rw-r--r--. 1 shangbaishuyao shangbaishuyao  124 5月  11 2016 .bashrc</span><br><span class="line">drwxrwxr-x. 2 shangbaishuyao shangbaishuyao 4096 3月  10 13:29 bin</span><br><span class="line">drwxr-xr-x. 2 shangbaishuyao shangbaishuyao 4096 11月 12 2010 .gnome2</span><br><span class="line">drwxr-xr-x. 4 shangbaishuyao shangbaishuyao 4096 2月  27 19:53 .mozilla</span><br><span class="line">drwxrwxr-x. 2 shangbaishuyao shangbaishuyao 4096 2月  27 16:21 .oracle_jre_usage</span><br><span class="line">drwx------. 2 shangbaishuyao shangbaishuyao 4096 3月   1 00:41 .ssh</span><br><span class="line">-rw-------. 1 shangbaishuyao shangbaishuyao 8777 3月  10 13:29 .viminfo</span><br><span class="line">-rw-rw-r--. 1 shangbaishuyao shangbaishuyao   61 3月  10 13:31 zookeeper.out</span><br><span class="line">[shangbaishuyao@Hadoop102 ~]$ vim .bashrc </span><br><span class="line"></span><br><span class="line"># .bashrc</span><br><span class="line"></span><br><span class="line"># Source global definitions</span><br><span class="line"></span><br><span class="line">if [ -f /etc/bashrc ]; then</span><br><span class="line">        . /etc/bashrc</span><br><span class="line">fi</span><br><span class="line">JAVA_HOME=/opt/module/jdk1.8.0_144</span><br><span class="line">export JAVA_HOME</span><br><span class="line"></span><br><span class="line"># User specific aliases and functions</span><br><span class="line"></span><br><span class="line">~                                        </span><br><span class="line">[shangbaishuyao@Hadoop102 ~]$ xsync .bashrc</span><br><span class="line">[shangbaishuyao@Hadoop102 ~]$ echo $JAVA_HOME</span><br><span class="line">/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;或许是物质世界泛滥了 人们越来越孤独 或许是现实形式太泛泛了 人们越来越追求简单淡泊了 或许是功利的人与事太思空见惯了 所以很多人灵魂深处渴望一位知己。 只知我曲中暖，却不知我心寒…       –来自网易云音乐《弦外知音》                            &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="HDFS" scheme="http://xubatian.cn/tags/HDFS/"/>
    
    <category term="HDFS HA" scheme="http://xubatian.cn/tags/HDFS-HA/"/>
    
  </entry>
  
  <entry>
    <title>从Hadoop框架讨论大数据生态</title>
    <link href="http://xubatian.cn/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E8%AE%A8%E8%AE%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/"/>
    <id>http://xubatian.cn/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E8%AE%A8%E8%AE%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/</id>
    <published>2022-01-13T05:53:25.434Z</published>
    <updated>2022-01-17T16:45:09.847Z</updated>
    
    <content type="html"><![CDATA[<p>若是问我最想和谁在一起 我想到的只有你 我既想缠着你 又想放弃你 又想跟你联系 又不想跟你联系 既想慢慢退出你的世界 又怕失去你 终其一生满是遗憾      –来自网易音乐《要命》              </p><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_207.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="大数据和hadoop的关系"><a href="#大数据和hadoop的关系" class="headerlink" title="大数据和hadoop的关系"></a>大数据和hadoop的关系</h1><p>上文说道 <strong>大数据</strong> 其实说白了就是主要解决海量数据的存储和海量数据的分析计算问题.</p><p> 那么这个问题是如何解决的呢? 是有什么框架或者说什么工具来解决呢的? </p><p>答案就是 hadoop.  它是整个大数据体系中最主要的也是最核心的部分.</p><p> 因为它解决了大数据的痛点: 海量数据的存储问题,分析计算问题.</p><p>所以,要讨论大数据,那么 hadoop就是它的起点…因为解决了海量数据的存储和计算问题呀!</p><p>&emsp;     </p><p>在知道了什么是大数据, 为什么学习大数据需要从hadoop开始学之后,下面就要具体了解hadoop框架了.</p><p>在我看来学习一个框架最基本的步骤:</p><ol><li>是什么?</li><li>能做什么?</li><li>怎么学?</li></ol><p>那么 下面我们具体看看,hadoop到底是个什么东西? 它是怎么解决海量数据的存储和计算的问题的呢?</p><p>&emsp;     </p><h1 id="Hadoop是什么"><a href="#Hadoop是什么" class="headerlink" title="Hadoop是什么?"></a><a href="https://www.leixue.com/ask/what-is-hadoop?btwaf=52655100">Hadoop是什么?</a></h1><p>备注: hadoop官网地址: <a href="https://hadoop.apache.org/">https://hadoop.apache.org/</a></p><p><strong>首先从hadoop的官网入手,看看官网说的hadoop的是什么?</strong></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/hadoopguangwang1.png" width = "" height = "" alt="xubatian的博客" align="center" /><p><strong>由官网总结得出hadoop是什么?</strong></p><p>&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; Hadoop是一个开源软件框架，用于在商用硬件集群上存储数据和运行应用程序。它为任何类型的数据提供海量存储，巨大的处理能力以及处理几乎无限的并发任务或作业的能力。</p><p>① Hadoop是一个由Apache基金会所开发的分布式系统基础架构。<br>② 主要解决，海量数据的存储和海量数据的分析计算问题。<br>③ 广义上来说，Hadoop通常是指一个更广泛的概念-Hadoop生态</p><p><strong>那么什么是hadoop生态呢?</strong> </p><p>hadoop生态是指由hadoop中心衍生的一系列解决大数据问题的一些大数据组件或者框架. 目的依然是针对大数据的海量数据存储和海量数据计算问题开展或者研发的针对不同问题的解决方式.</p><p>如图下图: 展示了大数据是以hadoop为中心的生态体系,所以hadoop是大数据的核心. 他解决了大数据的存储和计算问题.</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/hadoopshengtaiquan2.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="Hadoop能做什么"><a href="#Hadoop能做什么" class="headerlink" title="Hadoop能做什么?"></a>Hadoop能做什么?</h1><p> hadoop就是为了解决大数据的痛点而孕育出的,所以hadoop一定是解决了海量数据的存储和计算的问题.但是海量数据的存储和海量数据的计算问题仅仅是大数据问题的解决方案. 比如: hadoop就利用了分布式文件存储来存储大量的数据. 但是因为用到分布式的解决方案,所以,他还得解决分布式方案出现的问题.eg: 容错. </p><p>伴随者一个问题的解决和新问题出现再次解决,最终解决完所有问题得到的hadoop有了这些能力:</p><ol><li><strong>能够快速存储和处理大量任何类型的数据</strong>。随着数据量和品种的不断增加，特别是来自社交媒体和物联网（IoT），这是一个关键考虑因素。</li><li><strong>计算能力</strong>。Hadoop 的分布式计算模型可以快速处理大数据。您使用的计算节点越多，您拥有的处理能力就越强。</li><li><strong>容错</strong>。数据和应用程序处理可防止硬件故障。如果节点发生故障，作业将自动重定向到其他节点，以确保分布式计算不会失败。自动存储所有数据的多个副本。</li><li><strong>灵活性</strong>。与传统的关系数据库不同，您不必在存储数据之前对其进行预处理。您可以根据需要存储尽可能多的数据，并决定以后如何使用它。这包括非结构化数据，如文本，图像和视频。</li><li><strong>低成本</strong>。开源框架是免费的，使用商用硬件来存储大量数据。</li><li><strong>可扩展性</strong>。只需添加节点，您就可以轻松扩展系统以处理更多数据。需要很少的管理。</li></ol><p>既然hadoop能做这些,他能解决大数据问题. 那么难道就没有其他框架可以解决这些问题了吗? 又是什么原因使得hadoop奠定了如今大数据的核心地位呢? 要知道这个我们就必须了解hadoop有哪些优势.</p><p>&emsp;     </p><h1 id="Hadoop的优势是什么"><a href="#Hadoop的优势是什么" class="headerlink" title="Hadoop的优势是什么?"></a>Hadoop的优势是什么?</h1><p>因为hadoop有这些优势,所以同时期,大多数公司更愿意使用hadoop,使得hadoop独领风骚.</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1）高可靠性：Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。</span><br><span class="line">2）高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。</span><br><span class="line">3）高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。</span><br><span class="line">4）高容错性：能够自动将失败的任务重新分配。</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/hadoop102.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/hadoop3.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>也正是因为hadoop具有这些优势,使得大数据初期公司都是使用的hadoop来解决大数据问题的. 所以,后期的框架也是由hadoop为基础进行的衍生, 这就造成了hadoop在大数据技术方面不可动摇的核心地位,一直独领风骚到现在.</p><p>&emsp;     </p><p>在了解到hadoop能做什么之后,又为啥学大数据必须用到hadoop之后,我们还需要知道,hadoop从研发到发行有哪些版本.最常用的是什么哪款的?</p><h1 id="hadoop从研发到发行有哪些版本呢"><a href="#hadoop从研发到发行有哪些版本呢" class="headerlink" title="hadoop从研发到发行有哪些版本呢?"></a>hadoop从研发到发行有哪些版本呢?</h1><p>Hadoop三大发行版本</p><p>Hadoop三大发行版本：Apache、Cloudera、Hortonworks。</p><p>Apache版本最原始（最基础）的版本，对于入门学习最好。06年,我们学习的版本</p><p>Cloudera在大型互联网企业中用的较多。09年出来的,收费的</p><p>Hortonworks文档较好。11-12年,收费的</p><p>&emsp; </p><ol><li> Apache Hadoop</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">官网地址：http:<span class="comment">//hadoop.apache.org/releases.html</span></span><br><span class="line"></span><br><span class="line">下载地址：https:<span class="comment">//archive.apache.org/dist/hadoop/common/</span></span><br></pre></td></tr></table></figure><ol start="2"><li> Cloudera Hadoop </li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">官网地址：https:<span class="comment">//www.cloudera.com/downloads/cdh/5-10-0.html</span></span><br><span class="line"></span><br><span class="line">下载地址：http:<span class="comment">//archive-primary.cloudera.com/cdh5/cdh/5/</span></span><br><span class="line"></span><br><span class="line">（<span class="number">1</span>）<span class="number">2008</span>年成立的Cloudera是最早将Hadoop商用的公司，为合作伙伴提供Hadoop的商用解决方案，主要是包括支持、咨询服务、培训。</span><br><span class="line"></span><br><span class="line">（<span class="number">2</span>）<span class="number">2009</span>年Hadoop的创始人Doug Cutting也加盟Cloudera公司。Cloudera产品主要为CDH，Cloudera Manager，Cloudera Support</span><br><span class="line"></span><br><span class="line">（<span class="number">3</span>    CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全性，稳定性上有所增强。</span><br><span class="line"></span><br><span class="line">（<span class="number">4</span>）Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。Cloudera Support即是对Hadoop的技术支持。</span><br><span class="line"></span><br><span class="line">（<span class="number">5</span>）Cloudera的标价为每年每个节点<span class="number">4000</span>美元。Cloudera开发并贡献了可实时处理大数据的Impala项目。</span><br></pre></td></tr></table></figure><ol start="3"><li> Hortonworks Hadoop</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">官网地址：https:<span class="comment">//hortonworks.com/products/data-center/hdp/</span></span><br><span class="line"></span><br><span class="line">下载地址：https:<span class="comment">//hortonworks.com/downloads/#data-platform</span></span><br><span class="line"></span><br><span class="line">（<span class="number">1</span>）<span class="number">2011</span>年成立的Hortonworks是雅虎与硅谷风投公司Benchmark Capital合资组建。</span><br><span class="line"></span><br><span class="line">（<span class="number">2</span>）公司成立之初就吸纳了大约<span class="number">25</span>名至<span class="number">30</span>名专门研究Hadoop的雅虎工程师，上述工程师均在<span class="number">2005</span>年开始协助雅虎开发Hadoop，贡献了Hadoop80%的代码。</span><br><span class="line"></span><br><span class="line">（<span class="number">3</span>）雅虎工程副总裁、雅虎Hadoop开发团队负责人Eric Baldeschwieler出任Hortonworks的首席执行官。</span><br><span class="line"></span><br><span class="line">（<span class="number">4</span>）Hortonworks的主打产品是Hortonworks Data Platform（HDP），也同样是<span class="number">100</span>%开源的产品，HDP除常见的项目外还包括了Ambari，一款开源的安装和管理系统。</span><br><span class="line"></span><br><span class="line">（<span class="number">5</span>）HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。</span><br><span class="line"></span><br><span class="line">（<span class="number">6</span>）Hortonworks开发了很多增强特性并提交至核心主干，这使得Apache Hadoop能够在包括Window Server和Windows Azure在内的Microsoft Windows平台上本地运行。定价以集群为基础，每<span class="number">10</span>个节点每年为<span class="number">12500</span>美元。</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>其中最常用的就是Apache 开源的hadoop版本. 而且不收费,所以很多公司用的就是他,所以他成为了主流.</p><p>&emsp;     </p><p>该了解的都了解清楚了,所以我们需要在正式的学习hadoop了, 因为hadoop框架有四个模块,每个模块又有不同的功能,所以我们需要了解hadoop的组成…避免文章很长,所以另立一篇文章.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;若是问我最想和谁在一起 我想到的只有你 我既想缠着你 又想放弃你 又想跟你联系 又不想跟你联系 既想慢慢退出你的世界 又怕失去你 终其一生满是遗憾      –来自网易音乐《要命》              &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据是什么?</title>
    <link href="http://xubatian.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%98%AF%E4%BB%80%E4%B9%88/"/>
    <id>http://xubatian.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%98%AF%E4%BB%80%E4%B9%88/</id>
    <published>2022-01-13T05:05:36.000Z</published>
    <updated>2022-01-17T16:46:40.851Z</updated>
    
    <content type="html"><![CDATA[<p>经历了多少委屈 才有一身好脾气。    –来自网易云音乐《习惯》          </p><p>​                  </p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/1.gif" alt="图片"></p><h1 id="大数据概念"><a href="#大数据概念" class="headerlink" title="大数据概念"></a><a href="https://baike.baidu.com/item/%E5%A4%A7%E6%95%B0%E6%8D%AE/1356941">大数据概念</a></h1><p>大数据（Big Data）：指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产.</p><p>说白了大数据就是主要解决海量数据的存储和海量数据的分析计算问题.</p><p>一般数据直接存放在mysql中,通过SQL语言进行分析. 但是数据量特别大的时候到达TB,PB级别的时候,数据再使用mysql等数据库工作就显得不是那么容易了. 这种海量的数据的分析,计算,存储已经是寻常数据库无法完成的了. 所以,大数据孕育而生.大数据所负责的范围是在PB和EB范围居多.</p><p>&emsp;     </p><p>按顺序给出数据存储单位：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">从小到大的存储单位: bit、Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB</span><br><span class="line">1Byte = 8bit  </span><br><span class="line">1K = 1024Byte  </span><br><span class="line">1MB = 1024K</span><br><span class="line">1G = 1024M  </span><br><span class="line">1T = 1024G      </span><br><span class="line">1P = 1024T</span><br><span class="line"></span><br><span class="line">计算机常用的存储单位：</span><br><span class="line"><span class="number">8</span> bit = <span class="number">1</span> Byte 一字节</span><br><span class="line"><span class="number">1024</span> B = <span class="number">1</span> KB （KiloByte） 千字节</span><br><span class="line"><span class="number">1024</span> KB = <span class="number">1</span> MB （MegaByte） 兆字节</span><br><span class="line"><span class="number">1024</span> MB = <span class="number">1</span> GB （GigaByte） 吉字节</span><br><span class="line"><span class="number">1024</span> GB = <span class="number">1</span> TB （TeraByte） 太字节</span><br><span class="line"><span class="number">1024</span> TB = <span class="number">1</span> PB （PetaByte） 拍字节</span><br><span class="line"><span class="number">1024</span> PB = <span class="number">1</span> EB （ExaByte） 艾字节</span><br><span class="line"><span class="number">1024</span> EB = <span class="number">1</span> ZB （ZetaByte） 泽字节</span><br><span class="line"><span class="number">1024</span> ZB = <span class="number">1</span> YB （YottaByte） 尧字节</span><br><span class="line"><span class="number">1024</span> YB = 1BB（Brontobyte）珀字节</span><br><span class="line"><span class="number">1024</span> BB = <span class="number">1</span> NB （NonaByte） 诺字节</span><br><span class="line"><span class="number">1024</span> NB = <span class="number">1</span> DB （DoggaByte）刀字节</span><br></pre></td></tr></table></figure><p>&emsp;     </p><h1 id="大数据应用场景"><a href="#大数据应用场景" class="headerlink" title="大数据应用场景"></a><a href="https://baijiahao.baidu.com/s?id=1707322739345909800&wfr=spider&for=pc">大数据应用场景</a></h1><p>大数据到底在现实生活中有哪些应用场景呢? 说白了就是大数据能干啥? </p><h2 id="电商行业"><a href="#电商行业" class="headerlink" title="电商行业"></a>电商行业</h2><p>不知道大家有没有这样的记忆，你在手机淘宝上搜索了一下衬衫这个商品，在你下一次打开的时候，首页上推荐的绝对有相关产品；你在头条上连着关注了几条疫情相关的信息，那么类似信息就会一直给你推荐。这就是大数据的应用之一。它可以<strong>根据你的的消费习惯为你提供相关产品与服务，而且很精细化</strong>。随着数据量的不断扩大，可以根据特定时间段特定区域等分析出区域消费特征，男女消费特征，消费习惯等等，这样在未来的市场布局中，就可以很有针对性地<strong>预测市场走向，调整销售策略、产品结构及产品备货量等</strong>，创造商业价值</p><h2 id="金融行业"><a href="#金融行业" class="headerlink" title="金融行业"></a>金融行业</h2><p>炒股的人都知道要看K线，<strong>那么K线怎么来的？都是交易过程的一些数据加工而来</strong>，可以说大数据在金融行业的应用非常广泛，行家关注大消息，菜鸟就只能看那些线，看不看得懂另说，这些线都是由<strong>大数据统计</strong>而来。在<strong>交易过程也大都是使用大数据算法进行的。</strong>买卖双方可以根据这些数据以及新闻，决定接下来的几秒内是选择购买还是出售。</p><h2 id="生物技术"><a href="#生物技术" class="headerlink" title="生物技术"></a>生物技术</h2><p>前两天<strong>百度布局苏州，打造生物计算发展新高地</strong>。计算机算，当然要用到数据。用李彦宏自己的话来讲，<strong>生物计算是高度融合的学科。</strong>生物和计算的融合，能够有效利用大量的<strong>生物数据，把药物发现的“大海捞针”变成“按图索骥”</strong>，为人类的生命健康谋福祉。</p><p>借助大数据和人工智能，医生可以检测出不同癌症病人的不同病变，找到个性化的用药，实现<strong>精准医疗</strong>，降低治疗成本。</p><p>大数据能在自身基因技术的多方面发挥作用，如<strong>基因测序和重组方面</strong>，大数据可以将复杂的工程简单化，带来更好的科技成果。</p><h2 id="汽车行业"><a href="#汽车行业" class="headerlink" title="汽车行业"></a>汽车行业</h2><p>说起汽车，不得不提最近互联网巨头的造车潮。小米、华为、360都说要造车，那么哪家更强？<strong>华为、360、腾讯标榜的是不造整车，以技术赋能汽车行业。</strong></p><p>汽车上的<strong>传感器</strong>，随时测量和传递着有关位置、运动、震动、温度、湿度乃至空气中化学物质的变化，<strong>这也是大数据</strong>。</p><p>互联网赋能汽车的软件技术，大多也要通过大数据来实现。如一些操作系统，一些智能云服务等。</p><p>360的周鸿祎明确声明，360 将不会独立造车：“做手机失败之后，我知道自己不能造车，<strong>硬件是好的身体，软件是灵魂</strong>，没有灵魂的肉体是行尸走肉。”</p><p>&emsp;     </p><p>现在，人们越来越多地意识到大数据的价值，把大数据模型系统地应用到公共商业服务中，为政府、企业或个人提供服务；根据用户的查询浏览购买记录来推荐产品……可以说人们的生产生活正在被数字所定义，可以说无数据不存储，无数据不计算，无数据不真相，未来大数据所能发挥的作用更会超越我们的想象。</p><h1 id="大数据特点"><a href="#大数据特点" class="headerlink" title="大数据特点"></a><a href="https://blog.csdn.net/arsaycode/article/details/70847184">大数据特点</a></h1><p>一、Volume：数据量大，包括采集、存储和计算的量都非常大。大数据的起始计量单位至少是P（1000个T）、E（100万个T）或Z（10亿个T）。</p><p>二、Variety：种类和来源多样化。包括结构化、半结构化和非结构化数据，具体表现为网络日志、音频、视频、图片、地理位置信息等等，多类型的数据对数据的处理能力提出了更高的要求。</p><p>三、Value：数据价值密度相对较低，或者说是浪里淘沙却又弥足珍贵。随着互联网以及物联网的广泛应用，信息感知无处不在，信息海量，但价值密度较低，如何结合业务逻辑并通过强大的机器算法来挖掘数据价值，是大数据时代最需要解决的问题。</p><p>四、Velocity：数据增长速度快，处理速度也快，时效性要求高。比如搜索引擎要求几分钟前的新闻能够被用户查询到，个性化推荐算法尽可能要求实时完成推荐。这是大数据区别于传统数据挖掘的显著特征。</p><p>五、Veracity：数据的准确性和可信赖度，即数据的质量。</p><h1 id="大数据发展前景"><a href="#大数据发展前景" class="headerlink" title="大数据发展前景"></a><a href="http://www.npc.gov.cn/npc/c30834/201910/653fc6300310412f841c90972528be67.shtml">大数据发展前景</a></h1><p>随着科技的进步，大数据从科学前沿逐渐深入到各行业。纵观国内外，大数据已经形成产业规模，并上升到国家战略层面，大数据技术和应用呈现纵深发展趋势。面向大数据的云计算技术、大数据计算框架等不断推出，新型大数据挖掘方法和算法大量出现，大数据新模式、新业态层出不穷，传统产业开始利用大数据实现转型升级。</p><p>趋势一：数据的资源化</p><p>趋势二：与云计算的深度结合</p><p>趋势三：科学理论的突破</p><p>趋势四：数据科学和数据联盟的成立</p><p>大数据作为一种重要的战略资产，已经不同程度地渗透到每个行业领域和部门，其深度应用不仅有助于企业经营活动，还有利于推动国民经济发展。它对于推动信息产业创新、大数据存储管理挑战、改变经济社会管理面貌等方面也意义重大。</p><p>大数据的技术发展与物联网、云计算、人工智能等新技术领域的联系将更加紧密，物联网的发展将极大提高数据的获取能力，云计算与人工智能将深刻地融入数据分析体系，融合创新将会不断地涌现和持续深入。</p><p>总体来说，大数据产业发展将迎来快速增长期，创新成为大数据发展主要基调，大数据与各大产业融合将加速，为做大做强数字经济、带动传统产业转型升级提供新动力。</p><h1 id="了解大数据技术生态体系"><a href="#了解大数据技术生态体系" class="headerlink" title="了解大数据技术生态体系"></a>了解大数据技术生态体系</h1><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/bigdata.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>图中涉及的技术名词解释如下：<br>1）Sqoop：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库（MySQL）间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。<br>2）Flume：Flume是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；<br>3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统；<br>4）Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。<br>5）Flink：Flink是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多。<br>6）Oozie：Oozie是一个管理Hadoop作业（job）的工作流程调度管理系统。<br>7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。<br>8）Hive：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。<br>9）ZooKeeper：它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;经历了多少委屈 才有一身好脾气。    –来自网易云音乐《习惯》          &lt;/p&gt;
&lt;p&gt;​                  &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>mapreduce过程中的shuffle机制原理</title>
    <link href="http://xubatian.cn/mapreduce%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84shuffle%E6%9C%BA%E5%88%B6%E5%8E%9F%E7%90%86/"/>
    <id>http://xubatian.cn/mapreduce%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84shuffle%E6%9C%BA%E5%88%B6%E5%8E%9F%E7%90%86/</id>
    <published>2022-01-12T15:09:02.000Z</published>
    <updated>2022-01-17T17:04:24.484Z</updated>
    
    <content type="html"><![CDATA[<p>那个果断删了你的人 却在另一个地方悄悄地关注着你 你认为毫无瓜葛的人 在无数个夜里忍住了一万次 想和你联系的冲动         –来自网易云音乐《再也不会遇见了》</p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_01.gif" width = "" height = "" alt="xubatian的博客" align="center" /><p>前言</p><p>mapreduce过程概括为: InputFormat—-&gt; mapper,shuffle,reducer—&gt;OutputFormat</p><p>此篇文章记录shuffle原理.</p><h1 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a>Shuffle机制</h1><p>什么是shuffle?</p><p>shuffle就是数据落盘的过程.</p><p>hadoop在mapreduce计算过程中,什么阶段会有落盘呢? 在map打散数据之后,reduce聚合数据之前.</p><p>Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle</p><p>以后我们在做一些操作的时候, 能没有reducer的话, 就不写Reducer.<br>因为整个MapReduce任务中, shuffle是最耗费时间的. 因为你Reducer没有shuffle也就没有了. 那么这个程序性能就比较高. </p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/map4.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>其实我们的MapReduce整个阶段我们细细的分可以分为5个阶段.<br>Map——&gt; sort(排序) ——-&gt; copy ——&gt;sort(排序) ——-&gt; Reducer</p><p>首先我们在看shuffle机制之前先了解一下 shuffle过程的前一个过程和后一个过程, 即,mapTask过程和ReduceTask过程.</p><h2 id="通过源码简单了解一下mapTask和ReduceTask"><a href="#通过源码简单了解一下mapTask和ReduceTask" class="headerlink" title="通过源码简单了解一下mapTask和ReduceTask:"></a>通过源码简单了解一下mapTask和ReduceTask:</h2><p>因为你我们的mapreduce程序分为mapTask 和 reduceTask. 在中间的过程就是shuffle过程.所以简单看一下mapTask和reduceTask的源码.</p><h3 id="mapTask"><a href="#mapTask" class="headerlink" title="mapTask"></a>mapTask</h3><p>这是shuffle之前的过程</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/map6.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/oo1.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="ReduceTask"><a href="#ReduceTask" class="headerlink" title="ReduceTask"></a>ReduceTask</h3><p>这是shuffle之后的过程</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/oo2.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/oo3.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="shuffle机制的整个过程图"><a href="#shuffle机制的整个过程图" class="headerlink" title="shuffle机制的整个过程图"></a>shuffle机制的整个过程图</h3><p>shuffle过程是在MapTask方法之后，ReduceTask方法之前的数据处理过程称之为Shuffle。</p><p><strong>shuffle机制展示图:</strong></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/shuffle1.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="对于shuffle机制的解读"><a href="#对于shuffle机制的解读" class="headerlink" title="对于shuffle机制的解读"></a><strong>对于shuffle机制的解读</strong></h3><p>Map方法出来之后先进入到getPartition方法,获取是哪一个分区的.<br>然后进入环形缓冲区.(默认100M,企业开发一般调整到200M,这是优化.) 到达80%溢写.溢写对数据排序. 排序手段是快排. 对key的索引按照字典顺序排 .溢写后产生大量的溢写文件.针对溢写文件进行归并排序. 按照分区放到对应磁盘上等待拉取. reduce拉取对应分区数据放入内存.内存不够放入磁盘. 内存中数据或者磁盘中数据都需要进行归并排序. 排完序后分组. 然后进入到reduce方法.<br>map端的溢写过程产生的溢写文件进行归并,默认是一次归并10个. 但是随着机器性能提高. 这个也可以提高到20 或者30.<br>总体上默认的mapTask的内存是1G. 默认的reduceTask也是1G.<br>通常我们调整做多的是ReduceTask为什么是reduceTask呢?<br>因为他是一个聚合汇总. 因为mapTask默认是128M的数据.<br>而reduceTask是将所有数据都聚合到这里面,数据量相对来说大一些<br>真正开发的时候可以适当调整到4个G左右.<br>yarn单个节点(就是一个服务器)默认内存是8个G,通长也是需要调整的,一般是跟你的集群的节点内存相等,正常是128G.<br>yarn的单个任务(就是处理一件事) 默认内存也是8G. 生产过程这些都是需要调整的. </p><p>优化:<br>环形缓冲区.(默认100M,企业开发一般调整到200M,这是优化.)<br>到达80%溢写.(企业一般调整到90%或者95% 这是优化,调大了减少溢写文件的次数)<br>这样做可以减少溢写次数这就优化了. 对文件进行归并排序前可以进行一次Combiner.前提条件是不影响业务.比如求和,汇总业务不影响. 求平均值就影响.</p><p>Combiner之后进行归并排序. 然后放到对应分区的磁盘上.<br>为了减少磁盘io,即为了减少从map端到reduce端的拉取过程采用压缩.减少磁盘io.  在MapReduce整个过程中,map输入端,map输出端,reduce输出端都可以压缩. 输入端需要注意切片,输入端谁支持切片呢?那种压缩支持切片呢?LZO支持切片.LZO里面需要额外的建立索引. 如果LZO里面没有建立索引,那就不支持切分.<br>reduce默认拉取磁盘数据是5个.但是我们可以增大到10个或者20个.前提条件是机器性能和内存.</p><p>通常map输出端的压缩使用snappy或者LZO</p><h3 id="对于shuffle机制中的缓存区解释图"><a href="#对于shuffle机制中的缓存区解释图" class="headerlink" title="对于shuffle机制中的缓存区解释图"></a><strong>对于shuffle机制中的缓存区解释图</strong></h3><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/shuffle2.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>以后我们在做一些操作的时候, 能没有reducer的话, 就不写Reducer.<br>因为整个MapReduce任务中, shuffle是最耗费时间的. 因为你Reducer没有shuffle也就没有了. 那么这个程序性能就比较高. </p><h3 id="Shuffle过程源码解读"><a href="#Shuffle过程源码解读" class="headerlink" title="Shuffle过程源码解读"></a>Shuffle过程源码解读</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">1.根据ReduceTask的个数决定获取哪个输出对象。</span><br><span class="line">  new NewDirectOutputCollector(taskContext, job, umbilical, reporter); ==&gt;ReduceTask为0 </span><br><span class="line">  output = new NewOutputCollector(taskContext, job, umbilical, reporter);</span><br><span class="line">  获取collector</span><br><span class="line">  [1]. collector = createSortingCollector(job, reporter); 创建一个输出对象，收集map输出的kv</span><br><span class="line">① Class&lt;?&gt;[] collectorClasses = job.getClasses(</span><br><span class="line">           JobContext.MAP_OUTPUT_COLLECTOR_CLASS_ATTR, MapOutputBuffer.class); </span><br><span class="line">   获取到collector的类型</span><br><span class="line">② MapOutputCollector&lt;KEY, VALUE&gt; collector = ReflectionUtils.newInstance(subclazz, job);</span><br><span class="line">   获取到collector的对象</span><br><span class="line">③ collector.init(context); 调用init方法</span><br><span class="line">   (1). job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);</span><br><span class="line">                job.getInt(JobContext.IO_SORT_MB, 100);</span><br><span class="line">获取缓冲区的大小(100MB) 以及 溢写百分比 (80%). </span><br><span class="line">           (2). sorter = ReflectionUtils.newInstance(job.getClass(&quot;map.sort.class&quot;,</span><br><span class="line">QuickSort.class, IndexedSorter.class), job);</span><br><span class="line">获取到排序对象，默认用的是QuickSort</span><br><span class="line">   (3).comparator = job.getOutputKeyComparator();  获取分组比较器</span><br><span class="line">       a. WritableComparator.get(getMapOutputKeyClass().asSubclass(WritableComparable.class), this);</span><br><span class="line">   (4). combiner</span><br><span class="line">   (5). spillThread.start();  启动溢写线程</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">   拿到collector之后</span><br><span class="line">   [2].  if (partitions &gt; 1) &#123;</span><br><span class="line">partitioner = (org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;)</span><br><span class="line">ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span><br><span class="line">         &#125; else &#123;</span><br><span class="line">                partitioner = new org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;() &#123;</span><br><span class="line">               @Override</span><br><span class="line">                public int getPartition(K key, V value, int numPartitions) &#123;</span><br><span class="line">                    return partitions - 1;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line">         &#125;</span><br><span class="line"> 获取到分区器, 计算每个KV对应的分区号，溢写的时候将kv写到对应的分区文件中.</span><br></pre></td></tr></table></figure><h3 id="Shuffle过程源码解读图示"><a href="#Shuffle过程源码解读图示" class="headerlink" title="Shuffle过程源码解读图示"></a>Shuffle过程源码解读图示</h3><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_79.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_80.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_81.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_82.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_83.png" width = "" height = "" alt="xubatian的博客" align="center" /><p><strong>进入collector.init(context)里面.</strong></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_84.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_85.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_86.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_87.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_88.png" width = "" height = "" alt="xubatian的博客" align="center" /><p><strong>获取到分区器, 计算每个KV对应的分区号，溢写的时候将kv写到对应的分区文件中.</strong></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_89.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>源码: (上面部分只是准备过程, 这里开始是执行过程)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">2. Mapper方法执行，写出去KV ,collector收集， 达到阈值， 溢写， 最后合并. </span><br><span class="line">   [1]. context.write(k, v);  Mapper中的map方法中写出KV</span><br><span class="line">   [2]. mapContext.write(key, value);</span><br><span class="line">   [3]. output.write(key, value);</span><br><span class="line">   [4]. collector.collect(key, value, partitioner.getPartition(key, value, partitions));</span><br><span class="line">① startSpill(); 当满足溢写条件后，开始溢写</span><br><span class="line">   (1). spillReady.signal();  发信号给溢写线程，让溢写线程开始工作,实际就是调用</span><br><span class="line">sortAndSpill()方法进行排序和溢写. </span><br><span class="line">           (2). 在sortAndSpill()方法中：</span><br><span class="line">a. final Path filename = mapOutputFile.getSpillFileForWrite(numSpills, size);</span><br><span class="line">   获取到溢写的文件的位置</span><br><span class="line">    D:/tmp/hadoopAdministrator/mapred/local/localRunner/Administrator/jobcache/job_local685727973_0001/attempt_local685727973_0001_m_000000_0/output/spill0.out</span><br><span class="line">        </span><br><span class="line">b. sorter.sort(MapOutputBuffer.this, mstart, mend, reporter); 溢写之前先排序</span><br><span class="line"></span><br><span class="line">c. 生成溢写文件 ，并溢写   spill0.out  splill1.out .....</span><br><span class="line">d. 如果存储index的内存达到阈值，也会溢写到磁盘中。 </span><br><span class="line"></span><br><span class="line">       </span><br><span class="line">  ② 当map端所有的kv全部都写出后， 会触发最后一次溢写， 之后会进行合并. </span><br><span class="line">   合并完成以后，最终会保留两个文件</span><br><span class="line">   file.out  ==&gt;   kv</span><br><span class="line">   file.out.index  == &gt;kv的index</span><br></pre></td></tr></table></figure><p><strong>如下图只是一个mapTask的过程, 假如有多个mapTask, 每一个mapTask最终都会去生成file.out或者file.out.index文件</strong></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_90.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="收集器源码"><a href="#收集器源码" class="headerlink" title="收集器源码"></a><strong>收集器源码</strong></h3><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_61.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="shuffle完成后-从Map写出来的数据过程图"><a href="#shuffle完成后-从Map写出来的数据过程图" class="headerlink" title="shuffle完成后,从Map写出来的数据过程图"></a>shuffle完成后,从Map写出来的数据过程图</h3><p>mapreduce过程概括为: InputFormat—-&gt; mapper,shuffle,reducer—&gt;OutputFormat</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/pp1.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="Partition分区"><a href="#Partition分区" class="headerlink" title="Partition分区"></a>Partition分区</h3><p>什么是partition分区呢? 他的作用使用么?</p><h4 id="1、问题引出"><a href="#1、问题引出" class="headerlink" title="1、问题引出"></a>1、问题引出</h4><p>要求将统计结果照条件输出到不同文件中（分区）比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区）</p><h4 id="2、默认Partitioner分区"><a href="#2、默认Partitioner分区" class="headerlink" title="2、默认Partitioner分区"></a>2、默认Partitioner分区</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;</span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numReduceTasks)</span></span></span><br><span class="line"><span class="function"><span class="title">return</span>  <span class="params">(key.hashCode()</span> &amp; Integer.MAX_VALUE) % numReduceTasks </span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>默认分区是根据key的hashCode对ReduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。</p><h4 id="3、自定义Partitioner步骤及代码"><a href="#3、自定义Partitioner步骤及代码" class="headerlink" title="3、自定义Partitioner步骤及代码"></a>3、自定义Partitioner步骤及代码</h4><p><strong>案例代码</strong>:<a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/Partition">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/Partition</a></p><p>（1）自定义类继承Partitioner，重写getPartition()方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, FlowBean value, <span class="keyword">int</span> numPartitions)</span></span></span><br><span class="line"><span class="function"><span class="comment">// 控制分区代码逻辑</span></span></span><br><span class="line"><span class="function">        ... ...</span></span><br><span class="line"><span class="function">return partition</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（2）在Job驱动中，设置自定义Partitioner</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setPartitionerClass(CustomPartitioner.class);</span><br></pre></td></tr></table></figure><p>（3）自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h4 id="4、分区总结"><a href="#4、分区总结" class="headerlink" title="4、分区总结"></a>4、分区总结</h4><p>（1）如果ReduceTask的数量&gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx；<br>（2）如果1&lt;ReduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception；<br>（3）如果ReduceTask的数量=1，则不管MapTask端输出多少个分区文件,最终结果都交给这一个ReduceTask，最终也就只会产生一个          结果文件part-r-00000；<br>（4）分区号必须从零开始，逐一累加。</p><h4 id="5、案例分析"><a href="#5、案例分析" class="headerlink" title="5、案例分析"></a>5、案例分析</h4><p>例如：假设自定义分区数为5，则</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(1)job.setNumReduceTasks(1); 会正常运行，只不过会产生一个输出文件</span><br><span class="line">(2)job.setNumReduceTasks(2); 会报错</span><br><span class="line">(3）job.setNumReduceTasks(6); 大于5，程序会正常运行，会产生空文件</span><br></pre></td></tr></table></figure><h4 id="对于partition分区的总结"><a href="#对于partition分区的总结" class="headerlink" title="对于partition分区的总结"></a>对于partition分区的总结</h4><p>块(block): 就是存到hdfs的时候是以什么块为单位.<br>切片就是MapReduce程序在处理你每一块数据的时候生成的逻辑上的切片信息.<br><strong>分区就是你的MapReduce处理完我这个数据后,最终往磁盘上输出结果的时候,我最终要输出几个文件</strong>. 比如说我要输出3个文件,那也就代  表着我有三个分区.  如果我输出一个文件,就代表只有一个分区.  所以分区简单理解为将来MapReduce处理完成以后他要帮我输出几个文件就是几个分区.<br>      比如说我一堆单词, a-q ,q-p各自放在一个文件中. 那么这两个文件就必须有两个分区来完成.  说白了这两个文件就是两个分区.</p><p>如图展示什么是partition分区:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_62.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>对于一个wordCount案例来讲, 我希望将统计的结果输出到两个文件中, 但是我没有说什么样的分区给到一分区, 什么样的分区给带二分区. 也就是说没有明确告诉那个k,v按照什么样的条件进入什么样的分区呢? 如图设置两个分区:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_63.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_64.png" width = "" height = "" alt="xubatian的博客" align="center" /> <p>其实他是根据我们的hash算法来帮你区分的. 他会拿上你每一个key的hashcode值去对我的reduce的个数取模,上面我的reduce是2, 任何数字对reduce取模只有可能是0和1.  所以最后我的分区是00000, 00001 即0号分区和1号分区. 简单理解就是我的使用key的hashcode对reduce个数取模, 如果得到是0就往0号分区放. 得到是1就往1号分区放. 所以他默认情况下用的是hash运算. 这就是默认的分区器Partition. 如下图:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_65.png" width = "" height = "" alt="xubatian的博客" align="center" /><h4 id="Partitioner分区器源码解读"><a href="#Partitioner分区器源码解读" class="headerlink" title="Partitioner分区器源码解读"></a>Partitioner分区器源码解读</h4><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_66.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>Partitioner: 分区器<br>    默认的分区器:  HashPartitioner ,通过key的hashcode值对ReduceTasks的个数取余确定去往哪个分区.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">如何确定分区的:</span><br><span class="line">      partitions = jobContext.getNumReduceTasks();</span><br><span class="line">      <span class="keyword">if</span> (partitions &gt; <span class="number">1</span>) &#123;</span><br><span class="line">partitioner = (org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;)</span><br><span class="line">  ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">partitioner = <span class="keyword">new</span> org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;() &#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> partitions - <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_67.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="WritableComparable排序"><a href="#WritableComparable排序" class="headerlink" title="WritableComparable排序"></a>WritableComparable排序</h3><p>溢写到磁盘需要sort排序, 合并成大文件需要排序</p><h4 id="排序概述"><a href="#排序概述" class="headerlink" title="排序概述"></a>排序概述</h4><p>排序是MapReduce框架中最重要的操作之一. MapTask和ReduceTask均会对数据按照key进行排序。该操作属于<br>Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。<br>默认排序是按照排序，且实现该排序的方法是快速排序。</p><p>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。</p><p>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_103.png" width = "" height = "" alt="xubatian的博客" align="center" /><h4 id="排序分类"><a href="#排序分类" class="headerlink" title="排序分类"></a>排序分类</h4><p>（1）部分排序<br>            MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序。<br>（2）全排序<br>            最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在<br>            处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构。<br>（3）辅助排序：(GroupingComparator分组)<br>            在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入            到同一个reduce方法时，可以采用分组排序。<br>（4）二次排序<br>            在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。</p><h4 id="自定义排序WritableComparable及代码"><a href="#自定义排序WritableComparable及代码" class="headerlink" title="自定义排序WritableComparable及代码"></a>自定义排序WritableComparable及代码</h4><p><strong>案例代码:</strong> <a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/WritableComparable/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/WritableComparable/</a></p><p>原理分析<br>bean对象做为key传输，需要实现WritableComparable接口重写compareTo方法，就可以实现排序。</p><h3 id="Combiner合并"><a href="#Combiner合并" class="headerlink" title="Combiner合并"></a>Combiner合并</h3><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_104.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>Combiner虽然继承Reducer,但是他是在mapTask过程中运行的, 而Reducer是在reduceTask过程中运行的.</p><p>（1）Combiner是MR程序中Mapper和Reducer之外的一种组件。<br>（2）Combiner组件的父类就是Reducer。<br>（3）Combiner和Reducer的区别在于运行的位置<br>                    <strong>Combiner是在每一个MapTask所在的节点运行</strong>;<br>                    <strong>Reducer是接收全局所有Mapper的输出结果</strong>；<br>（4）Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量。<br>（5）Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出key,value应该跟Reducer的输入key,value类型要           对应起来。</p><p>如下所示:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Mapper                                     Reducer</span><br><span class="line"><span class="number">3</span> <span class="number">5</span> <span class="number">7</span> -&gt;(<span class="number">3</span>+<span class="number">5</span>+<span class="number">7</span>)/<span class="number">3</span>=<span class="number">5</span>                 (<span class="number">3</span>+<span class="number">5</span>+<span class="number">7</span>+<span class="number">2</span>+<span class="number">6</span>)/<span class="number">5</span>=<span class="number">23</span>/<span class="number">5</span>       不等于     (<span class="number">5</span>+<span class="number">4</span>)/<span class="number">2</span>=<span class="number">9</span>/<span class="number">2</span></span><br><span class="line"><span class="number">2</span> <span class="number">6</span> -&gt;(<span class="number">2</span>+<span class="number">6</span>)/<span class="number">2</span>=<span class="number">4</span></span><br></pre></td></tr></table></figure><p>（  6）自定义Combiner实现步骤</p><p><strong>代码案例</strong>:<a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/combiner">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/combiner</a></p><p>​            (a) 自定义一个Combiner继承Reducer，重写Reduce方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>,<span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 1 汇总操作</span></span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(IntWritable v :values)&#123;</span><br><span class="line">          count += v.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 2 写出</span></span><br><span class="line">        context.write(key, <span class="keyword">new</span> IntWritable(count));</span><br><span class="line">      &#125;</span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure><p>​              (b)  在Job驱动类中设置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(WordcountCombiner.class);</span><br></pre></td></tr></table></figure><p> 对于运行结果展示:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_68.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_69.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>mapreduce过程概括为: InputFormat—-&gt; mapper,shuffle,reducer—&gt;OutputFormat</p><p>最后就是计算好的结果输出问题了.</p><h3 id="OutputFormat数据输出"><a href="#OutputFormat数据输出" class="headerlink" title="OutputFormat数据输出"></a>OutputFormat数据输出</h3><p>和InputFormat一样,OutputFormat也有许多接口实现类.</p><h4 id="OutputFormat接口实现类"><a href="#OutputFormat接口实现类" class="headerlink" title="OutputFormat接口实现类"></a>OutputFormat接口实现类</h4><p>OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了OutputFormat接口。下面我们介绍几种常见的OutputFormat实现类。</p><p>1.文本输出TextOutputFormat<br>            默认的输出格式是TextOutputFormat,它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用toString()方法把它们转换为字符串。</p><p>2.SequenceFileOutputFormat<br>            将SequenceFileOutputFormat输出作为后续 MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑,很容易被压缩。</p><ol start="3"><li>自定义OutputFormat<br>根据用户需求，自定义实现输出。</li></ol><h4 id="自定义OutputFormat"><a href="#自定义OutputFormat" class="headerlink" title="自定义OutputFormat"></a>自定义OutputFormat</h4><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_105.png" width = "700" height = "350" alt="xubatian的博客" align="center" /><p><strong>案例代码</strong>: <a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/OutputFormat/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/hadoop/OutputFormat/</a></p><h1 id="MapReduce开发总结"><a href="#MapReduce开发总结" class="headerlink" title="MapReduce开发总结"></a>MapReduce开发总结</h1><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_111.png" width = "1000" height = "900" alt="xubatian的博客" align="center" />]]></content>
    
    
    <summary type="html">&lt;p&gt;那个果断删了你的人 却在另一个地方悄悄地关注着你 你认为毫无瓜葛的人 在无数个夜里忍住了一万次 想和你联系的冲动         –来自网易云音乐《再也不会遇见了》&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="mapreduce" scheme="http://xubatian.cn/tags/mapreduce/"/>
    
    <category term="shuffle" scheme="http://xubatian.cn/tags/shuffle/"/>
    
  </entry>
  
  <entry>
    <title>hadoop组成模块之MapReduce概述</title>
    <link href="http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BMapReduce%E6%A6%82%E8%BF%B0/"/>
    <id>http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BMapReduce%E6%A6%82%E8%BF%B0/</id>
    <published>2022-01-12T13:12:15.000Z</published>
    <updated>2022-01-17T16:58:24.587Z</updated>
    
    <content type="html"><![CDATA[<p>亲爱的女孩，如果还能跟你心平气和的聊天的话，我希望能告诉你我那些无视和冷笑都是在伪装，可是已经过去了。淡月梨花，借梦来，桥边廊庑。     –来自网易云音乐《任你》</p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_211.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><p>前言</p><p>我们知道大数据面临的痛点就是:</p><p>①数据存储问题.数据超级大,可达TB级别.所以没有任何合适的工具存储这些数据.mysql等数据也无法有效的存储. </p><p>②数据在有效的时间整合出结果,即计算问题.就算能够存储下来,也无法有效的操作这些数据. 即,无法通过类似sql语句查询mysql一样去整合数据得到有效的信息.</p><p>但是我们又说到, hadoop解决了这两个问题. </p><p>首先hadoop解决数据存储问题的模块就是HDFS. 也就是说HDFS就是针对大数据存储问题的一套落地的解决方案. </p><p>那么数据存储了,我需要分析呀.如何取分析呢?使用什么引擎去分析呢?<br>答案就是: MapReduce.   一个模块负责存储, 一个模块负责分析.</p><h1 id="MapReduce概述"><a href="#MapReduce概述" class="headerlink" title="MapReduce概述"></a>MapReduce概述</h1><h2 id="MapReduce定义"><a href="#MapReduce定义" class="headerlink" title="MapReduce定义"></a>MapReduce定义</h2><p>MapReduce是一个分布式运算架，是用户开发“基于Hadoop的数据分析应用”的核心框架。<br>MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。</p><p>&emsp;     </p><h2 id="MapReduce优缺点"><a href="#MapReduce优缺点" class="headerlink" title="MapReduce优缺点"></a>MapReduce优缺点</h2><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_92.png" width = "" height = "" alt="xubatian的博客" align="center" /><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">什么叫离线处理?</span><br><span class="line">他对比的就是实时处理,就像mysql等就可以实时的返回结果;</span><br><span class="line">就是我已经有大量的数据了,你通过hadoop里面的hdfs,mapReduce去做运算,最终经过一段时间的运算给我得出结果,这是要耗费一定时间的,这就叫离线处理</span><br></pre></td></tr></table></figure><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>其实也不能叫缺点,以为他本身设计的时候就没有去考虑这些东西,如果考虑这些东西就可能解决不了大量数据的处理了</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_93.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>静态的就表示,数据已经放好了在这里了,而不是你时不时还往里面添加数据,这是不支持的<br>就是如下图,什么叫有向图(DAG)计算?就是后面应用程序输入的数据是前面数据输出的数据,而mapReduce不擅长做这个,但是也不是不能做. 如果这么做的话会有大量的磁盘io操作. 就是你前面的MapReduce程序完成后结果放在磁盘中,下一个MapReduce程序读取磁盘的结果来操作. 这个过程会发生大量的磁盘io.所以你这个性能是非常低下的</p><h1 id="MapReduce核心思想"><a href="#MapReduce核心思想" class="headerlink" title="MapReduce核心思想"></a>MapReduce核心思想</h1><p>之前map  和 reduce 一个负责分,一个负责合,这是一个很笼统的认识</p><p>现在MapReduce核心编程思想，如图所示:</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mapreduce5.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="对上图MapReduce核心思想解释"><a href="#对上图MapReduce核心思想解释" class="headerlink" title="对上图MapReduce核心思想解释"></a>对上图MapReduce核心思想解释</h2><p>第一个文件200m 我们立马想到,我们存到hdfs中后立马存两块(128M一块). 这里有两个文件,所以有三块数据.在map阶段会读取你的每一块内容,每一块内容都会交给maptask来处理.这个maptask就是map阶段最核心的一个对象. 整个maptask的过程中就是你的map阶段.所有的工作都是围绕着maptask来进行工作的.maptask就是一个任务.是map阶段的一个任务.正常情况下你的每一块数据都会交给maptask来进行处理.在我的maptask里面做什么操作呢?我要把你文件中的每一行数据都给他读出来.<br>1)读数据,并按行处理       (就是我读一行进来)<br>2)按空格切分行内单词     (具体要不要按照行处理看你每个单词与单词之间是否已空格划分)<br>3)K键值对(单词,1)        (hadoop,1)(hive,1)(spark,1)<br>4)将所有的KV键值对中的词,按照单词首字母,分成2个分区溢写到磁盘    (为什么要分成两个分区呢?因为我最终的结果我要的是两个,那我就是将a-z,q-p开头的单词写到不同的文件中)</p><p>最终我的三个maptask中一共六个文件,即分区1(z-p),分区2(q-z)各两个.<br>把三个maptask里面分区1的给一个reduce中,分区2的给一个reduce中.<br>所以在这种情况下他就有两个reducetask,为什么有两个呢? 其实就是由你这个分区来决定的.因为你最终有两个文件,所以我就开启两个reduceTask来处理. 这reduceTask到每一个mapTask生成的文件里面去拷贝数据,比如说分区1里面拷贝到处理a-p的ReduceTask, 分区2拷贝到处理q-z的ReduceTask中.</p><p>Mapreduce程序只能包含一个map阶段和一个reduce阶段.你不能有多个.如果说你的一个MapReduce程序处理完成后还没有得到你想要的结果,那你只能再重新再开一个MapReduce程序接着去处理你这个数据.不能再一个MapReduce中写多个map阶段或者多个reduce阶段程序.</p><p>问题:<br>为什么一个mapTask要处理一块数据?<br>我的kv对处理完成以后如何写到多个分区里面呢?<br>我的reduce阶段如何去拷贝我的多个分区的数据?</p><h2 id="mapreduce核心思想总结"><a href="#mapreduce核心思想总结" class="headerlink" title="mapreduce核心思想总结"></a>mapreduce核心思想总结</h2><p>1）分布式的运算程序往往需要分成至少2个阶段。<br>2）第一个阶段的MapTask并发实例，完全并行运行，互不相干。<br>3）第二个阶段的ReduceTask并发实例互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出。<br>4）MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。<br>总结：分析WordCount数据流走向深入理解MapReduce核心思想。</p><h2 id="MapReduce进程"><a href="#MapReduce进程" class="headerlink" title="MapReduce进程"></a>MapReduce进程</h2><p>一个完整的MapReduce程序在分布式运行时有三类实例进程：<br>1）MrAppMaster：负责整个程序的过程调度及状态协调。<br>2）MapTask:  负责Map阶段的整个数据处理流程。<br>3）ReduceTask:  负责Reduce阶段的整个数据处理流程。</p><h2 id="通过官方WordCount源码查看我们如何取去写MapReduce程序"><a href="#通过官方WordCount源码查看我们如何取去写MapReduce程序" class="headerlink" title="通过官方WordCount源码查看我们如何取去写MapReduce程序"></a>通过官方WordCount源码查看我们如何取去写MapReduce程序</h2><p>采用反编译工具反编译源码，发现WordCount案例有Map类、Reduce类和驱动类。且数据的类型是Hadoop自身封装的序列化类型。</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mapreduce8.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mapreduce9.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="Hadoop常用数据序列化类型"><a href="#Hadoop常用数据序列化类型" class="headerlink" title="Hadoop常用数据序列化类型"></a>Hadoop常用数据序列化类型</h2><p>在mapreduce中用的数据类型都是hadoop重新提供出来的一套数据类型,他没有直接使用java中提供好的8个基本数据类型,因为他认为java定义好的在序列化和反序列化操作时的数据量比较大,所以他自己搞了一套数据类型和一套序列化,实际上特别好记,就是在原java后面加了writable,除了字符串</p><p>​    </p><p>常用的数据类型对应的Hadoop数据序列化类型</p><table><thead><tr><th>Java类型</th><th>Hadoop Writable类型</th></tr></thead><tbody><tr><td>Boolean</td><td>BooleanWritable</td></tr><tr><td>Byte</td><td>ByteWritable</td></tr><tr><td>Int</td><td>IntWritable</td></tr><tr><td>Float</td><td>FloatWritable</td></tr><tr><td>Long</td><td>LongWritable</td></tr><tr><td>Double</td><td>DoubleWritable</td></tr><tr><td>String</td><td><strong>Text</strong></td></tr><tr><td>Map</td><td>MapWritable</td></tr><tr><td>Array</td><td>ArrayWritable</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">&lt;p&gt;亲爱的女孩，如果还能跟你心平气和的聊天的话，我希望能告诉你我那些无视和冷笑都是在伪装，可是已经过去了。淡月梨花，借梦来，桥边廊庑。     –来自网易云音乐《任你》&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="MapReduce" scheme="http://xubatian.cn/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>hadoop组成模块之HDFS分布式存储详解</title>
    <link href="http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BHDFS%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E8%AF%A6%E8%A7%A3/"/>
    <id>http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E4%B9%8BHDFS%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E8%AF%A6%E8%A7%A3/</id>
    <published>2022-01-12T11:06:38.000Z</published>
    <updated>2022-01-17T16:55:13.838Z</updated>
    
    <content type="html"><![CDATA[<p>“你和萤火虫有两个共同点，在我的眼里都会发光，同时，都已经很多年没见了。”    –来自网易云音乐《夜、萤火虫和你》                            </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_209.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><p>前言</p><p>上文了解到了hadoop由四个模块组成: HDFS,MapReduce,Yarn,Common.</p><p>本篇文章具体了解hadoop的HDFS模块.</p><p>什么是HDFS?</p><p>HDFS有什么功能?</p><p>如何掌握HDFS?</p><h1 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h1><h2 id="是什么原因使得HDFS产生的呢"><a href="#是什么原因使得HDFS产生的呢" class="headerlink" title="是什么原因使得HDFS产生的呢?"></a>是什么原因使得HDFS产生的呢?</h2><p>我们知道大数据面临的痛点就是:</p><p>① <strong>数据存储问题.</strong> 数据超级大,可达TB级别.所以没有任何合适的工具存储这些数据.mysql等数据也无法有效的存储. </p><p>② <strong>数据在有效的时间整合出结果,即计算问题</strong>.就算能够存储下来,也无法有效的操作这些数据. 即,无法通过类似sql语句查询mysql一样去整合数据得到有效的信息.</p><p>但是我们又说到, hadoop解决了这两个问题. </p><p>首先hadoop解决数据存储问题的模块就是HDFS. 也就是说HDFS就是针对大数据存储问题的一套落地的解决方案. </p><p>那么为什么连mysql这样的数据库都无法解决的问题,hadoop的HDFS能够解决呢?</p><p>答案就是: 分布式文件存储. 可以理解为, 将十台服务器的磁盘整合成一个磁盘,如果每台服务器是10T,那么,整个HDFS就是100T. 如果后期磁盘不够了,我还可以增加服务器的方式继续扩容.</p><h2 id="HDFS产生背景"><a href="#HDFS产生背景" class="headerlink" title="HDFS产生背景"></a>HDFS产生背景</h2><p>随着数据量越来越大,在一个操作系统存不下所有的数据,那么就分配到更多的操作系统管理的磁盘中,但是不方便管理和维护,迫切需要一种系统来管理多台机器上的文件,这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种。它主要解决什么问题呢?它主要解决的是,当我的是数据量大到一定程度的时候,我把我的数据分布到多台机器上去存储,但是我可以多多台机器上的数据进行统一的管理.</p><p>&emsp;     </p><h2 id="HDFS定义"><a href="#HDFS定义" class="headerlink" title="HDFS定义"></a>HDFS定义</h2><p>HDFS( Hadoop Distributed File System),它是一个文件系统,用于存储文件,通过目录树来定位文件;</p><p>其次,它是分布式的,由很多服务器联合起来实现其功能,集群中的服务器有各自的角色. 因为有很多服务器联合起来是一个HDFS.</p><p>每台服务器上都有HDFS.如何区分谁是主,谁是从呢? 因为有了主从才能更好的管理. 就像董事长管着经理,经理管着员工,这样才能掌控整个大的集团. 毕竟一个角色的人是有限的. 此逻辑在这里也适用.</p><p>&emsp; </p><h2 id="HDFS的使用场景"><a href="#HDFS的使用场景" class="headerlink" title="HDFS的使用场景"></a>HDFS的使用场景</h2><p>适合一次写入,多次读出的场景,且不支持文件的修改,也不支持删除,只支持末尾追加。适合用来做数据分析,并不适合用来做网盘应用.</p><p>Namenode所有元数据信息都是维护在内存中的</p><p>&emsp;     </p><h2 id="HDFS优缺点"><a href="#HDFS优缺点" class="headerlink" title="HDFS优缺点"></a>HDFS优缺点</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/hdfs1.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/hdfs2.png" width = "" height = "" alt="xubatian的博客" align="center" /><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">要知道,namenode给datanode下达指令,但是NameNode具体给datanode下达什么指令是由我们客户端来操作NameNode来决定的;注意在HDFS当中他的文件单位是以块(Block)为单位的存储的.</span><br></pre></td></tr></table></figure><p>&emsp;     </p><h2 id="HDFS组成架构"><a href="#HDFS组成架构" class="headerlink" title="HDFS组成架构"></a>HDFS组成架构</h2><p>上面说了HDFS是有很多服务器联合起来的. 但是需要工作你得有一个领导人.你得分清楚哪一个是领导. 将来数据来了之后我需要存在哪台机器上得由一个小领导来指挥.要知道,namenode给datanode下达指令,但是NameNode具体给datanode下达什么指令是由我们客户端来操作NameNode来决定的;注意在HDFS当中他的文件单位是以块(Block)为单位的</p><p>Namenode具体要给datanode下达什么指令,都是我们客户端来操作的.</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/hdfs3.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>Client:    就是客户端</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">(1)文件切分。文件上传HDFS的时候,Client将文件切分成个个的Bock,然后进行上传;</span></span><br><span class="line"><span class="string">(2)与</span> <span class="string">Namenode交获取文件的位置信息;</span></span><br><span class="line"><span class="string">(3)与</span> <span class="string">Datanode交互,读取或者写入数据;</span></span><br><span class="line"><span class="string">(4)</span> <span class="string">Client提供些命令来管理HDFS,比如</span> <span class="string">Namenode格式化;</span></span><br><span class="line"><span class="string">(5)</span> <span class="string">Client可以通过些命令来访问HDFS,比如对HDFS增删查改操作;</span></span><br></pre></td></tr></table></figure><p>SecondaryNameNode:    并非 NameNode的热备。当 NameNode挂掉的时候,它并不能马上替换 NameNode并提供服务</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>)辅助 NameNode,分担其工作量,比如定期合并 Fsimage和Edits,并推送给 NameNode;</span><br><span class="line">(<span class="number">2</span>)在紧急情况下,可辅助恢复 NameNode</span><br></pre></td></tr></table></figure><p>&emsp;     &emsp;     </p><h2 id="HDFS文件块大小"><a href="#HDFS文件块大小" class="headerlink" title="HDFS文件块大小"></a>HDFS文件块大小</h2><p>注意在HDFS当中他的文件单位是以块(Block)为单位的</p><p>所以 这个这个文件分成几块存到HDFS,一般一块是128M. 如果你的文件是300M,那么他会分成128M+128M+44M. 这三块. </p><p>这三块存到hadoop的HDFS上,可能存在了不同的服务器上.</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/block1.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/block2.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>&emsp;     </p><h2 id="HDFS的Shell操作"><a href="#HDFS的Shell操作" class="headerlink" title="HDFS的Shell操作"></a>HDFS的Shell操作</h2><h2 id="Hadoop命令查看"><a href="#Hadoop命令查看" class="headerlink" title="Hadoop命令查看"></a>Hadoop命令查看</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-3.1.3]$ bin/hadoop fs</span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">[-checksum &lt;src&gt; ...]</span><br><span class="line">[-chgrp [-R] GROUP PATH...]</span><br><span class="line">[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">[-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">[-copyFromLocal [-f] [-p] [-l] [-d] [-t &lt;thread count&gt;] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">[-count [-q] [-h] [-v] [-t [&lt;storage type&gt;]] [-u] [-x] [-e] &lt;path&gt; ...]</span><br><span class="line">[-cp [-f] [-p | -p[topax]] [-d] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">[-df [-h] [&lt;path&gt; ...]]</span><br><span class="line">[-du [-s] [-h] [-v] [-x] &lt;path&gt; ...]</span><br><span class="line">[-expunge]</span><br><span class="line">[-find &lt;path&gt; ... &lt;expression&gt; ...]</span><br><span class="line">[-get [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">[-getfacl [-R] &lt;path&gt;]</span><br><span class="line">[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">[-getmerge [-nl] [-skip-empty-file] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">[-head &lt;file&gt;]</span><br><span class="line">[-help [cmd ...]]</span><br><span class="line">[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [&lt;path&gt; ...]]</span><br><span class="line">[-mkdir [-p] &lt;path&gt; ...]</span><br><span class="line">[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">[-mv &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">[-put [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">[-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ...]</span><br><span class="line">[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span><br><span class="line">[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line">[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">[-stat [format] &lt;path&gt; ...]</span><br><span class="line">[-tail [-f] [-s &lt;sleep interval&gt;] &lt;file&gt;]</span><br><span class="line">[-test -[defsz] &lt;path&gt;]</span><br><span class="line">[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">[-touch [-a] [-m] [-t TIMESTAMP ] [-c] &lt;path&gt; ...]</span><br><span class="line">[-touchz &lt;path&gt; ...]</span><br><span class="line">[-truncate [-w] &lt;length&gt; &lt;path&gt; ...]</span><br><span class="line">[-usage [cmd ...]]</span><br></pre></td></tr></table></figure><p>图示:</p><p>&emsp; <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/mingling.png" width = "" height = "" alt="xubatian的博客" align="center" /></p><h2 id="Hadoop常用命令实操"><a href="#Hadoop常用命令实操" class="headerlink" title="Hadoop常用命令实操"></a>Hadoop常用命令实操</h2><h3 id="启动类命令"><a href="#启动类命令" class="headerlink" title="启动类命令"></a>启动类命令</h3><table><thead><tr><th>功能说明</th><th>命令脚本</th></tr></thead><tbody><tr><td>启动hdfs集群</td><td>sbin/start-dfs.sh</td></tr><tr><td>启动yarn</td><td>sbin/start-yarn.sh</td></tr></tbody></table><h3 id="hadoop-fs-hdfs-dfs-命令"><a href="#hadoop-fs-hdfs-dfs-命令" class="headerlink" title="hadoop fs/hdfs dfs 命令"></a>hadoop fs/hdfs dfs 命令</h3><table><thead><tr><th>功能说明</th><th>命令</th></tr></thead><tbody><tr><td>创建目录</td><td>hdfs dfs -mkdir -p /data/flink</td></tr><tr><td>显示目录</td><td>hdfs dfs -ls /</td></tr><tr><td>从HDFS拷贝到本地</td><td>hdfs dfs -copyToLocal /data/data.txt ./</td></tr><tr><td>文件上传到集群(从本地)</td><td>hhdfs dfs -copyFromLocal data.txt /</td></tr><tr><td>文件下载</td><td>hdfs dfs -get /data/flink</td></tr><tr><td>删除集群的文件</td><td>hdfs dfs -rm /data/flink</td></tr><tr><td>删除文件夹</td><td>hdfs dfs -rm -r -skipTrash /data</td></tr><tr><td>从本地剪切粘贴到HDFS</td><td>hdfs dfs  -moveFromLocal data.txt /data/</td></tr><tr><td>追加一个文件到已经存在的文件末尾hdfs dfs -appendToFile data1.txt /data/data.txt</td><td></td></tr><tr><td>显示文件内容</td><td>hdfs dfs -cat data.txt</td></tr><tr><td>修改文件所属权限</td><td>hdfs dfs  -chmod  777 xxx.sh</td></tr><tr><td>修改文件所属用户组</td><td>hdfs dfs  -chown  root:root data.txt</td></tr><tr><td>从HDFS的一个路径拷贝到HDFS的另一个路径</td><td>hdfs dfs -cp data.txt /data1.txt</td></tr><tr><td>在HDFS目录中移动文件</td><td>hdfs dfs -mv data.txt /opt/</td></tr><tr><td>合并下载多个文件</td><td>hdfs dfs  -getmerge /data/* ./data_merge.txt</td></tr><tr><td>hadoop fs -put</td><td>等同于copyFromLocal</td></tr><tr><td>显示一个文件的末尾</td><td>hdfs dfs -tail data.txt</td></tr><tr><td>删除文件或文件夹</td><td>hdfs dfs -rm /data/data.txt</td></tr><tr><td>删除空目录</td><td>hdfs dfs -rmdir /data</td></tr><tr><td>统计文件夹的大小信息</td><td>hdfs dfs -s -h /data</td></tr><tr><td>统计文件夹下的文件大小信息</td><td>hdfs dfs  -h /data</td></tr><tr><td>设置HDFS中文件的副本数量</td><td>hdfs dfs -setrep 3 /data/data.txt</td></tr></tbody></table><h3 id="yarn命令"><a href="#yarn命令" class="headerlink" title="yarn命令"></a>yarn命令</h3><table><thead><tr><th>功能说明</th><th align="left">命令</th></tr></thead><tbody><tr><td>查看正在运行的yarn任务列表</td><td align="left">yarn application -list appID</td></tr><tr><td>kill掉指定id的yarn任务</td><td align="left">yarn application -kill appID</td></tr><tr><td>查看任务日志信息</td><td align="left">yarn logs -applicationId appID</td></tr></tbody></table><p>&emsp; </p><h1 id="HDFS的API操作"><a href="#HDFS的API操作" class="headerlink" title="HDFS的API操作"></a>HDFS的API操作</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.alibaba.hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.*;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> 上白书妖</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2020/2/26 21:48</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Desription</span>:获取客户端的连接对象,操作hadoop集群</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsClient</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     创建目录</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testMkdirs</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置在集群上运行</span></span><br><span class="line">        <span class="comment">// configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop102:9000&quot;);</span></span><br><span class="line">        <span class="comment">// FileSystem fs = FileSystem.get(configuration);</span></span><br><span class="line"></span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span> );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 创建目录</span></span><br><span class="line">        Boolean result = fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">&quot;/1108/xw/shangbaishuyao&quot;</span>));</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;result:&quot;</span> + result);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    HDFS文件上传（测试参数优先级）</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        configuration.set(<span class="string">&quot;dfs.replication&quot;</span>, <span class="string">&quot;2&quot;</span>);</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 上传文件</span></span><br><span class="line">        fs.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">&quot;H:&quot;</span>+ File.separator+<span class="string">&quot;hello1.txt&quot;</span>), <span class="keyword">new</span> Path(<span class="string">&quot;/bigdata0615&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        fs.close();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;over&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 文件下载</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 执行下载操作</span></span><br><span class="line">        <span class="comment">// boolean delSrc 指是否将原文件删除</span></span><br><span class="line">        <span class="comment">// Path src 指要下载的文件路径</span></span><br><span class="line">        <span class="comment">// Path dst 指将文件下载到的路径</span></span><br><span class="line">        <span class="comment">// boolean useRawLocalFileSystem 是否开启文件校验</span></span><br><span class="line">        fs.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">&quot;/banzhang.txt&quot;</span>), <span class="keyword">new</span> Path(<span class="string">&quot;H:/banhua.txt&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        fs.close();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;over&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    HDFS文件夹删除</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span>  <span class="keyword">void</span> <span class="title">testDelete</span><span class="params">()</span> <span class="keyword">throws</span>  IOException,InterruptedException,URISyntaxException</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 执行删除</span></span><br><span class="line">        fs.delete(<span class="keyword">new</span> Path(<span class="string">&quot;/1108/&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     HDFS文件详情查看</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 获取文件详情</span></span><br><span class="line">        RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">&quot;/&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(listFiles.hasNext())&#123;</span><br><span class="line">            LocatedFileStatus status = listFiles.next();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 输出详情</span></span><br><span class="line">            <span class="comment">// 文件名称</span></span><br><span class="line">            System.out.println(<span class="string">&quot;文件名:&quot;</span>+status.getPath().getName());</span><br><span class="line">            <span class="comment">// 长度</span></span><br><span class="line">            System.out.println(<span class="string">&quot;文件长度:&quot;</span>+status.getLen());</span><br><span class="line">            <span class="comment">// 权限</span></span><br><span class="line">            System.out.println(<span class="string">&quot;文件长度:&quot;</span>+status.getPermission());</span><br><span class="line">            <span class="comment">// 分组</span></span><br><span class="line">            System.out.println(<span class="string">&quot;文件分组:&quot;</span>+status.getGroup());</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取存储的块信息</span></span><br><span class="line">            BlockLocation[] blockLocations = status.getBlockLocations();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (BlockLocation blockLocation : blockLocations) &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 获取块存储的主机节点</span></span><br><span class="line">                String[] hosts = blockLocation.getHosts();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">                    System.out.println(<span class="string">&quot;文件块所在的主机节点:&quot;</span>+host);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            System.out.println(<span class="string">&quot;-----------班长的分割线----------&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 3 关闭资源</span></span><br><span class="line">             fs.close();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;over&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     HDFS文件和文件夹判断,如果要判断其子子孙孙的话,就要用到递归</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListStatus</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件配置信息</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;========================递归判断其子子孙孙是文件还是文件夹========================&quot;</span>);</span><br><span class="line">        listDirectoryAndFile(<span class="string">&quot;/&quot;</span>,fs);</span><br><span class="line">        System.out.println(<span class="string">&quot;==================================递归判断完毕====================================&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 判断是文件还是文件夹</span></span><br><span class="line">        FileStatus[] listStatus = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">&quot;/&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (FileStatus fileStatus : listStatus) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 如果是文件</span></span><br><span class="line">            <span class="keyword">if</span> (fileStatus.isFile())</span><br><span class="line">            &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;文件:&quot;</span>+fileStatus.getPath().getName());</span><br><span class="line">            &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;目录:&quot;</span>+fileStatus.getPath().getName());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        fs.close();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;运行结束&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     需求: 指定一个目录,递归查看多有子目录,及文件</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listDirectoryAndFile</span><span class="params">(String path , FileSystem fs)</span> <span class="keyword">throws</span> IOException</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//处理当前目录下的子目录及文件</span></span><br><span class="line">        FileStatus[] fileStatuses = fs.listStatus(<span class="keyword">new</span> Path(path));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> ( fileStatus.isFile())</span><br><span class="line">            &#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (path.equals(<span class="string">&quot;/&quot;</span>))</span><br><span class="line">                &#123;</span><br><span class="line">                    System.out.println(<span class="string">&quot;文件:&quot;</span> + path +fileStatus.getPath().getName());</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    System.out.println(<span class="string">&quot;文件:&quot;</span> + path + <span class="string">&quot;/&quot;</span> + fileStatus.getPath().getName());</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//文件</span></span><br><span class="line">                System.out.println(<span class="string">&quot;文件:&quot;</span> + path+ <span class="string">&quot;/&quot;</span>+fileStatus.getPath().getName());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">//获取整个路径 即 :  hdfs://hadoop102:9000/bigdata0615</span></span><br><span class="line">                String currentpath = fileStatus.getPath().toString().substring(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>.length());</span><br><span class="line">                <span class="comment">//System.out.println(fileStatus.getPath());</span></span><br><span class="line">                <span class="comment">//目录</span></span><br><span class="line">                <span class="comment">//System.out.println(&quot;目录:&quot; + fileStatus.getPath().getName());</span></span><br><span class="line">                System.out.println(<span class="string">&quot;目录:&quot;</span> +currentpath);</span><br><span class="line"></span><br><span class="line">                <span class="comment">//递归显示当前目录下的子目录及文件</span></span><br><span class="line">                listDirectoryAndFile(currentpath,fs);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     HDFS文件名更改</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testRename</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 修改文件名称</span></span><br><span class="line">        fs.rename(<span class="keyword">new</span> Path(<span class="string">&quot;/shangbaishuyao.txt&quot;</span>), <span class="keyword">new</span> Path(<span class="string">&quot;/shangbaishuyao.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> <span class="comment">/*</span></span><br><span class="line"><span class="comment">     HDFS文件上传</span></span><br><span class="line"><span class="comment">     需求：把本地H盘上的shangbaishuyao.txt文件上传到HDFS根目录</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">putFileToHDFS</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 创建输入流</span></span><br><span class="line">        FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(<span class="string">&quot;H:&quot;</span>+File.separator+<span class="string">&quot;shangbaishuyao.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 获取输出流</span></span><br><span class="line">        FSDataOutputStream fos = fs.create(<span class="keyword">new</span> Path(<span class="string">&quot;/shangbaishuyao.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 流对拷</span></span><br><span class="line">        IOUtils.copyBytes(fis, fos, configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 关闭资源</span></span><br><span class="line">        IOUtils.closeStream(fos);</span><br><span class="line">        IOUtils.closeStream(fis);</span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    定位文件读取</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    需求：分块读取HDFS上的大文件，比如根目录下的/hadoop-2.7.2.tar.gz</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    （1）下载第一块</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek1</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 获取输入流</span></span><br><span class="line">        FSDataInputStream fis = fs.open(<span class="keyword">new</span> Path(<span class="string">&quot;/hadoop-2.7.2.tar.gz&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 创建输出流</span></span><br><span class="line">        FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">&quot;H:/hadoop-2.7.2.tar.gz.part1&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 流的拷贝</span></span><br><span class="line">        <span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i =<span class="number">0</span> ; i &lt; <span class="number">1024</span> * <span class="number">128</span>; i++)&#123;</span><br><span class="line">            fis.read(buf);</span><br><span class="line">            fos.write(buf);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5关闭资源</span></span><br><span class="line">        IOUtils.closeStream(fis);</span><br><span class="line">        IOUtils.closeStream(fos);</span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    （2）下载第二块</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek2</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>), configuration, <span class="string">&quot;shangbaishuyao&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 打开输入流</span></span><br><span class="line">        FSDataInputStream fis = fs.open(<span class="keyword">new</span> Path(<span class="string">&quot;/hadoop-2.7.2.tar.gz&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 定位输入数据位置</span></span><br><span class="line">        fis.seek(<span class="number">1024</span>*<span class="number">1024</span>*<span class="number">128</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 创建输出流</span></span><br><span class="line">        FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">&quot;H:/hadoop-2.7.2.tar.gz.part2&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 流的对拷</span></span><br><span class="line">        IOUtils.copyBytes(fis, fos, configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 关闭资源</span></span><br><span class="line">        IOUtils.closeStream(fis);</span><br><span class="line">        IOUtils.closeStream(fos);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">        （3）合并文件</span></span><br><span class="line"><span class="comment">                在Window命令窗口中进入到目录H:\，然后执行如下命令，对数据进行合并</span></span><br><span class="line"><span class="comment">                type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1</span></span><br><span class="line"><span class="comment">                合并完成后，将hadoop-2.7.2.tar.gz.part1重新命名为hadoop-2.7.2.tar.gz。解压发现该tar包非常完整。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>&emsp; </p><h1 id="HDFS的读写流程"><a href="#HDFS的读写流程" class="headerlink" title="HDFS的读写流程"></a>HDFS的读写流程</h1><h2 id="HDFS写数据流程-剖析文件写入"><a href="#HDFS写数据流程-剖析文件写入" class="headerlink" title="HDFS写数据流程(剖析文件写入)"></a>HDFS写数据流程(剖析文件写入)</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/hdfswrite.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>Bytebuffer 缓存</p><p>1）客户端通过Distributed FileSystem(分布式文件系统)模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。<br>2）NameNode返回是否可以上传。<br>3）客户端请求第一个 Block(块)上传到哪几个DataNode服务器上。<br>4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。<br>5）客户端通过FSDataOutputStream(拿到输出流)模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。(FSDataOutputStream，这个类重载了很多write方法，用于写入很多类型的数据：比如字节数组，long，int，char等等。)<br>6）dn1、dn2、dn3逐级应答客户端。<br>7）客户端开始往dn1上传第一个Block(128M)（先从磁盘读取数据放到一个本地内存缓存），他不可能一次性传128M,他是以Packet(数据包)为单位的形式传,(丢包就是这种形式,有部分数据没收到)，dn1收到一个Packet就会先放进他的缓存里面然后落盘,存到内存的目的是因为他还要传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答,假如我成功收到了,我放在队列当中的东西会被响应的。<br>8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</p><p>&emsp; </p><h2 id="HDFS读数据流程"><a href="#HDFS读数据流程" class="headerlink" title="HDFS读数据流程"></a>HDFS读数据流程</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_98.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。<br>2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。<br>3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet(数据包)为单位来做校验）。<br>4）客户端以Packet(数据包)为单位接收，先在本地缓存，然后写入目标文件。 </p><p>&emsp; </p><h1 id="NameNode和SecondaryNameNode"><a href="#NameNode和SecondaryNameNode" class="headerlink" title="NameNode和SecondaryNameNode"></a>NameNode和SecondaryNameNode</h1><p>对于现在hadoop升级到3.x之后,其实现在namenode已经不太是重点了. 在早期的版本当中很重要,因为在早期的版本中namenode存在一个单点故障. 早期的版本中没有很好的解决方案,只能有secondarynamenode来进行恢复大部分的数据.<br>但是对于现在来说并不是特别重要了. 因为我们有了一个高可用.即hadoopHA.他解决了namenode的一个单点故障的问题. 实际上我们有了高可用之后我们的secondarynamenode我们已经不需要再用他了.</p><p>&emsp; </p><h2 id="NameNode和SecondaryNameNode工作机制"><a href="#NameNode和SecondaryNameNode工作机制" class="headerlink" title="NameNode和SecondaryNameNode工作机制"></a>NameNode和SecondaryNameNode工作机制</h2><p>思考：NameNode中的元数据是存储在哪里的？<br>首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage(镜像文件)。<br>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits(修改记录)文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。<br>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。</p><p>&emsp; </p><p>NameNode和SecondaryNameNode工作机制图示:</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/namenode1.png" width = "" height = "" alt="xubatian的博客" align="center" /><ol><li><p>第一阶段：NameNode启动<br> （1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。<br> （2）客户端对元数据进行增删改的请求。<br> （3）NameNode记录操作日志，更新滚动日志。<br> （4）NameNode在内存中对元数据进行增删改。</p></li><li><p>第二阶段：Secondary NameNode工作<br> （1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。<br> （2）Secondary NameNode请求执行CheckPoint。<br> （3）NameNode滚动正在写的Edits日志。<br> （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。<br> （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。<br> （6）生成新的镜像文件fsimage.chkpoint。<br> （7）拷贝fsimage.chkpoint到NameNode。<br> （8）NameNode将fsimage.chkpoint重新命名成fsimage。</p><p> &emsp; </p><p> NN和2NN工作机制详解：</p></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Fsimage：NameNode内存中元数据序列化后形成的文件。</span><br><span class="line">Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。</span><br><span class="line">NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。</span><br><span class="line">由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。</span><br><span class="line">SecondaryNameNode首先会询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中</span><br></pre></td></tr></table></figure><h2 id="总结NameNode和secondNameNode关系"><a href="#总结NameNode和secondNameNode关系" class="headerlink" title="总结NameNode和secondNameNode关系?"></a>总结NameNode和secondNameNode关系?</h2><p> secondNameNode不是NameNode的热备. 因为他不能替换NameNode去干活. secondNameNode只是辅助namenode去工作. 比如说合并编辑日志.  所以说,当我的NameNode挂掉之后, 我的secondNameNode只是保证我的数据不丢失,不能保证我的机器能正常干活.</p><p>那我的高可用如何去解决这个问题呢? 那就是弄两个NameNode.</p><p>但是两个NameNode不能同时进行工作. </p><p>所以我们一个是作为active活跃状态. 一个作为standby时刻准备着.</p><p>我们的active活跃状态的NameNode是正在对外提供服务的. 那就意味这将来客户端的读写请求操作都要经过active活跃状态的NameNode. </p><p>如果说我们一个写请求,正在操作活跃状态的NameNode, 但是突然挂掉了.另一台standby状态的NameNode如何和active状态的保持一致呢? </p><p>即 ,如何实现active状态的NameNode和standby状态的NameNode数据同步问题?</p><p>这个时候光靠两台NameNode,即一台active状态,一台standby状态. 已经不行了. 这时候就有一个叫journalNode来了. 它实际上也是一个进程. 他和NameNode,DataNode,secondNameNode一样的, 也是一个进程. 他就是用来完成一台active状态的NameNode和一台standby状态的NameNode,这来两台数据同步性的中间进程.    将来我客户端操作active(活跃)状态的NameNode时候, 不仅在内存中修改. 还会写自己的编辑日志, 并且要往journalNode中去写一份. 注意.active(活跃)状态的NameNode往journalnode中去写, standby状态的NameNode往journalName里面去读.这就解决数据同步性的问题.</p><p>假如说我这个active状态的NameNode性能超级强悍. 他一直都不会出现问题. 这个时候我们客户端一致对Actvie状态的NameNode做修改的操作. 那么他的编辑日志会越来越大. 当你达到一定程度, 我们是不是得合并编辑日志呢. 我们合并编辑日志是secondNameNode做的. 但是在hadoopHA高可用情况下, 有了standby状态的NameNode时候,就不需要配置secondNameNode了. 因为我们的standby状态的NameNode就可以完成合并编辑日志的功能.而且当active状态的NameNode出现故障的时候, 我们standby状态的NameNode立马能提供服务.</p><p>实际上,NameNode,secondNameNode,等都是对应的一个java类. </p><h2 id="Fsimage和Edits解析"><a href="#Fsimage和Edits解析" class="headerlink" title="Fsimage和Edits解析"></a>Fsimage和Edits解析</h2><h3 id="Fsimage和Edits概念"><a href="#Fsimage和Edits概念" class="headerlink" title="Fsimage和Edits概念"></a>Fsimage和Edits概念</h3><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/edits.png" width = "" height = "" alt="xubatian的博客" align="center" /><h3 id="oiv查看Fsimage文件"><a href="#oiv查看Fsimage文件" class="headerlink" title="oiv查看Fsimage文件"></a>oiv查看Fsimage文件</h3><p>（1）查看oiv和oev命令,查看镜像文件用oiv,查看编辑日志用oev</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 current]$ hdfs</span><br><span class="line">oiv            apply the offline fsimage viewer to an fsimage</span><br><span class="line">oev            apply the offline edits viewer to an edits file</span><br><span class="line">[shangbaishuyao@Hadoop102 current]$ hdfs oiv -p xml -i fsimage_0000000000000000125 -o ./fsimage125.xml</span><br><span class="line">[shangbaishuyao@Hadoop102 current]$ cat fsimage125.xml </span><br></pre></td></tr></table></figure><p>（2）基本语法<br>hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</p><p>思考：可以看出，Fsimage中没有记录块所对应DataNode，为什么？<br>在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报。</p><p>&emsp; </p><h3 id="oev查看Edits文件"><a href="#oev查看Edits文件" class="headerlink" title="oev查看Edits文件"></a>oev查看Edits文件</h3><p>1）基本语法<br>hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径</p><p>2）案例实操</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/edits.xml</span><br></pre></td></tr></table></figure><p>思考：NameNode如何确定下次开机启动的时候合并哪些Edits？</p><p>&emsp; </p><h3 id="CheckPoint-检查点-时间设置"><a href="#CheckPoint-检查点-时间设置" class="headerlink" title="CheckPoint(检查点)时间设置"></a>CheckPoint(检查点)时间设置</h3><p>1）通常情况下，SecondaryNameNode每隔一小时执行一次。<br>    [hdfs-default.xml]</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;3600&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>2）一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1000000&lt;/value&gt;</span><br><span class="line">&lt;description&gt;操作动作次数&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;60&lt;/value&gt;</span><br><span class="line">&lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt;</span><br><span class="line">&lt;/property &gt;</span><br></pre></td></tr></table></figure><h3 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h3><p>NameNode故障后，可以采用如下两种方法恢复数据。<br>    方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录；</p><ol><li><p>kill -9 NameNode进程</p></li><li><p>删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</p></li></ol>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</span><br></pre></td></tr></table></figure><ol start="3"><li>拷贝SecondaryNameNode中数据到原NameNode存储数据目录</li></ol>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/</span><br></pre></td></tr></table></figure><ol start="4"><li>重新启动NameNode</li></ol>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure><p>  方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。<br>  1.修改hdfs-site.xml中的</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;120&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><ol start="2"><li><p>kill -9 NameNode进程</p></li><li><p>删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</span><br></pre></td></tr></table></figure></li><li><p>如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./</span><br><span class="line">[shangbaishuyao@hadoop102 namesecondary]$ rm -rf in_use.lock</span><br><span class="line">[shangbaishuyao@hadoop102 dfs]$ pwd</span><br><span class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs</span><br><span class="line">[shangbaishuyao@hadoop102 dfs]$ ls</span><br><span class="line">data  name  namesecondary</span><br></pre></td></tr></table></figure><p>导入检查点数据（等待一会ctrl+c结束掉）<br>[shangbaishuyao@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint</p></li><li><p>启动NameNode<br>[shangbaishuyao@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</p><p>&emsp; </p></li></ol><h3 id="Hadoop集群安全模式"><a href="#Hadoop集群安全模式" class="headerlink" title="Hadoop集群安全模式"></a>Hadoop集群安全模式</h3><p>概述:</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/namenodeSafe.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>基本语法:</p><p>集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。<br>（1）bin/hdfs dfsadmin -safemode get        （功能描述：查看安全模式状态）<br>（2）bin/hdfs dfsadmin -safemode enter      （功能描述：进入安全模式状态）<br>（3）bin/hdfs dfsadmin -safemode leave    （功能描述：离开安全模式状态）<br>（4）bin/hdfs dfsadmin -safemode wait    （功能描述：等待安全模式状态）</p><p>&emsp; </p><h3 id="NameNode多目录配置"><a href="#NameNode多目录配置" class="headerlink" title="NameNode多目录配置"></a>NameNode多目录配置</h3><ol><li><p>NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性</p></li><li><p>   具体配置如下    </p></li></ol><pre><code><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   (<span class="number">1</span>) 在hdfs-site.xml文件中增加如下内容,引用了hdfs-core.xml的hadoop.tmp.dir配置</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;file:<span class="comment">///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;</span></span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> (<span class="number">2</span>) 停止集群，删除data和logs中所有数据。</span><br><span class="line">[shangbaishuyao<span class="meta">@hadoop102</span> hadoop-<span class="number">2.7</span><span class="number">.2</span>]$ rm -rf data/ logs/</span><br><span class="line">[shangbaishuyao<span class="meta">@hadoop103</span> hadoop-<span class="number">2.7</span><span class="number">.2</span>]$ rm -rf data/ logs/</span><br><span class="line">[shangbaishuyao<span class="meta">@hadoop104</span> hadoop-<span class="number">2.7</span><span class="number">.2</span>]$ rm -rf data/ logs/</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> (<span class="number">3</span>) 格式化集群并启动。</span><br><span class="line">[shangbaishuyao<span class="meta">@hadoop102</span> hadoop-<span class="number">2.7</span><span class="number">.2</span>]$ bin/hdfs namenode –format</span><br><span class="line">[shangbaishuyao<span class="meta">@hadoop102</span> hadoop-<span class="number">2.7</span><span class="number">.2</span>]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> (<span class="number">4</span>) 查看结果</span><br><span class="line">[shangbaishuyao<span class="meta">@hadoop102</span> dfs]$ ll</span><br><span class="line">总用量 <span class="number">12</span></span><br><span class="line">drwx------. <span class="number">3</span> shangbaishuyao shangbaishuyao <span class="number">4096</span> <span class="number">12</span>月 <span class="number">11</span> 08:<span class="number">03</span> data</span><br><span class="line">drwxrwxr-x. <span class="number">3</span> shangbaishuyao shangbaishuyao <span class="number">4096</span> <span class="number">12</span>月 <span class="number">11</span> 08:<span class="number">03</span> name1</span><br><span class="line">drwxrwxr-x. <span class="number">3</span> shangbaishuyao shangbaishuyao <span class="number">4096</span> <span class="number">12</span>月 <span class="number">11</span> 08:<span class="number">03</span> name2</span><br></pre></td></tr></table></figure></code></pre><p>&emsp; </p><h2 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/datanode.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的       校验和，以及时间戳。<br>2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。<br>3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟30秒没有收到某个DataNode的心跳，则认为该节点不可用。<br>4）集群运行中可以安全加入和退出一些机器。</p><h3 id="数据完整性-如何保存存储的数据是完整的"><a href="#数据完整性-如何保存存储的数据是完整的" class="headerlink" title="数据完整性(如何保存存储的数据是完整的?)"></a>数据完整性(如何保存存储的数据是完整的?)</h3><p>思考：如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号（1）和绿灯信号（0），但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理DataNode节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？<br>如下是DataNode节点保证数据完整性的方法。<br>1）当DataNode读取Block的时候，它会计算CheckSum(校验和)。校验和,就是经过一定的算法的到一个数据的结果,这个结果就是校验和;   一个数据在上传前经过算法变成校验盒,然后再把数据放到hdfs上,经过算法变成校验盒,如果前后两个校验盒相同,则说明数据完整,数据没发生破坏对于银行等机构,数据完整性是相当重要的<br>2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。<br>3）Client读取其他DataNode上的Block。<br>4）DataNode在其文件创建后周期验证CheckSum，如图所示。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/shuju.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>&emsp; </p><h3 id="掉线时限参数设置-10分钟30秒"><a href="#掉线时限参数设置-10分钟30秒" class="headerlink" title="掉线时限参数设置 10分钟30秒"></a>掉线时限参数设置 10分钟30秒</h3><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/test/diaoxian.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval(心跳核查间隔)的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;300000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h2 id="HDFS-Federation架构设计"><a href="#HDFS-Federation架构设计" class="headerlink" title="HDFS Federation架构设计"></a>HDFS Federation架构设计</h2><p>NameNode架构的局限性（内存瓶颈, 毕竟是一台服务器嘛, 内存在高也就那么多）<br>（1）Namespace（命名空间）的限制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">由于NameNode在内存中存储所有的元数据（metadata），因此单个NameNode所能存储的对象（文件+块）数目受到NameNode  所在JVM的heap size(堆大小)的限制。50G的heap(堆)能够存储20亿（200million）个对象，这20亿个对象支持4000个DataNode，12PB的存储（假设文件平均大小为40MB）。随着数据的飞速增长，存储的需求也随之增长。单个DataNode从4T增长到36T，集群的尺寸增长到8000个DataNode。存储的需求从12PB增长到大于100PB。</span><br></pre></td></tr></table></figure><p>（2）隔离问题</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">由于HDFS仅有一个NameNode，无法隔离各个程序，因此HDFS上的一个实验程序就很有可能影响整个HDFS上运行的程序。</span><br></pre></td></tr></table></figure><p>（3）性能的瓶颈</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">由于是单个NameNode的HDFS架构，因此整个HDFS文件系统的吞吐量受限于单个NameNode的吞吐量。</span><br></pre></td></tr></table></figure><ol><li><p>HDFS Federation架构设计，如图所示<br>能不能有多个NameNode</p><table><thead><tr><th>NameNode</th><th>NameNode</th><th>NameNode</th></tr></thead><tbody><tr><td>元数据</td><td>元数据</td><td>元数据</td></tr><tr><td>Log</td><td>machine</td><td>电商数据/话单数据</td></tr></tbody></table></li></ol><p>HDFS Federation架构设计图:</p><p>​                <img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@master/aliyun/www.xubatian.cn_73.png" width = "" height = "" alt="xubatian的博客" align="center" /></p><p>​        </p><h2 id="HDFS-Federation应用思考"><a href="#HDFS-Federation应用思考" class="headerlink" title="HDFS Federation应用思考"></a>HDFS Federation应用思考</h2><p>不同应用可以使用不同NameNode进行数据管理<br>        图片业务、爬虫业务、日志审计业务<br>Hadoop生态系统中，不同的框架使用不同的NameNode进行管理NameSpace。（隔离性）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;“你和萤火虫有两个共同点，在我的眼里都会发光，同时，都已经很多年没见了。”    –来自网易云音乐《夜、萤火虫和你》                            &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
    <category term="HDFS" scheme="http://xubatian.cn/tags/HDFS/"/>
    
    <category term="Namenode" scheme="http://xubatian.cn/tags/Namenode/"/>
    
    <category term="DataNode" scheme="http://xubatian.cn/tags/DataNode/"/>
    
    <category term="SecondaryNameNode" scheme="http://xubatian.cn/tags/SecondaryNameNode/"/>
    
  </entry>
  
  <entry>
    <title>hadoop组成模块及各模块的简介</title>
    <link href="http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E5%8F%8A%E5%90%84%E6%A8%A1%E5%9D%97%E7%9A%84%E7%AE%80%E4%BB%8B/"/>
    <id>http://xubatian.cn/hadoop%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97%E5%8F%8A%E5%90%84%E6%A8%A1%E5%9D%97%E7%9A%84%E7%AE%80%E4%BB%8B/</id>
    <published>2022-01-12T05:56:07.000Z</published>
    <updated>2022-01-17T16:51:50.461Z</updated>
    
    <content type="html"><![CDATA[<p> 太阳在坠落 海浪在发愁 你在干什么       –来自网易云音乐《白墙》                            </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_208.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><p>&emsp;</p><p>前言<br>前面了解到了什么是大数据, 学习大数据为什么要学习hadoop, hadoop是什么.<br>现在我们需要具体的了解hadoop.<br>此篇文章具体了解hadoop的组成.</p><h1 id="从官网看hadoop组成结构"><a href="#从官网看hadoop组成结构" class="headerlink" title="从官网看hadoop组成结构"></a>从官网看hadoop组成结构</h1><p>官网地址:  <a href="https://hadoop.apache.org/">https://hadoop.apache.org/</a></p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/hadoopguangwang1.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>由上图官网可知, hadoop可以分为四个模块: HDFS,MapReduce,Yarn,Common. </p><p>那么hadoop从发行版本,即hadoop1.X 到现如今的 hadoop3.x,他都是这四个模块没有变化吗? </p><h1 id="Hadoop各个版本的不同模块及负责的功能变化"><a href="#Hadoop各个版本的不同模块及负责的功能变化" class="headerlink" title="Hadoop各个版本的不同模块及负责的功能变化"></a>Hadoop各个版本的不同模块及负责的功能变化</h1><p>&emsp;</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/hadoop4.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>由图可知, hadoop最开始是只有三个模块的,及 common,hdfs,mapreduce. 最开始是将计算和资源调度都由mapreduce掌控.演化到hadoop2.x之后才增加了yarn模块,将原本mapreduce掌控计算和资源调度的工作转变成了 mapreduce只负责计算. yarn负责资源调度的方式. 一直沿用至今hadoop3.x</p><p>&emsp;</p><h1 id="Hadoop四个模块具体的功能如何理解呢"><a href="#Hadoop四个模块具体的功能如何理解呢" class="headerlink" title="Hadoop四个模块具体的功能如何理解呢?"></a>Hadoop四个模块具体的功能如何理解呢?</h1><h2 id="①-HDFS架构"><a href="#①-HDFS架构" class="headerlink" title="① HDFS架构"></a>① HDFS架构</h2><p>Hadoop Distributed File System，简称HDFS，是一个分布式文件系统</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/hdfs.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>HDFS 直接hadoop存储的模块名称,只是一个笼统的称呼,叫做分布式文件存储系统. 到底是如何存储数据的,什么样的方式存取.这就涉及到具体的HDFS的一些概念了.</p><p>NameNode:只是存文件的相关信息<br>DataNode:负责真正的存储你的文件<br>Secondary NameNode:帮助NameNode的, 此处只是简单了解.</p><p>这些都是HDFS这个分布式文件存储系统里面具体负责那些功能的一些名称.</p><p>&emsp;</p><h2 id="②YARN架构"><a href="#②YARN架构" class="headerlink" title="②YARN架构"></a>②YARN架构</h2><p>Yet Another Resource Negotiator简称YARN ，另一种资源协调者，是Hadoop的资源管理器。</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/yarn.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_96.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>上面两个图都是yarn架构,表达的内容一样.</p><p>Yarn由四部分组成: ResourceManager,NodeManager,ApplicationMaster,Container<br>ResourceManager是整个集群的老大,NodeManager集群中每一个节点的老大.<br>Container:假设我在我当前的NodeManager里面我跑了一个任务,这一个任务我们把它称之为一个job,我跑这个任务,我的applicationmaster会根据我这个任务的一些情况,帮我去申请一些资源.比如我申请一个G的cpu,申请了一个G的内存等.我把申请下来的资源我要通过container给他做一个抽象.所以说container它封装了某个节点上的多维度资源.比如说你的内存,你的cpu,你的磁盘等等.封装起来的目的就是不要让别的job来侵占我这个job的资源.就是我申请好了资源封装完成后,这个资源是不允许给别人用的.<br>除此之外,我们还可以通过这个container理解到我这个job运行时的资源消耗是什么情况等等.</p><p>ResourceManager是整个集群的老大<br>NodeManager是集群节点的老大</p><p>&emsp;</p><h2 id="③MapReduce架构"><a href="#③MapReduce架构" class="headerlink" title="③MapReduce架构"></a>③MapReduce架构</h2><p>MapReduce将计算过程分为两个阶段：Map和Reduce</p><p>1）Map阶段并行处理输入数据</p><p>2）Reduce阶段对Map结果进行汇总</p><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/mr.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="④Common模块"><a href="#④Common模块" class="headerlink" title="④Common模块"></a>④Common模块</h2><p>Hadoop-common是指支持Hadoop模块的常用实用程序和库的集合.类似Java的公共类. 学习hadoop的时候设计Common不多.看源码的时候遇到的比较多.此处暂不介绍.</p><p>&emsp;</p><h2 id="⑤HDFS、YARN、MapReduce三者关系"><a href="#⑤HDFS、YARN、MapReduce三者关系" class="headerlink" title="⑤HDFS、YARN、MapReduce三者关系"></a>⑤HDFS、YARN、MapReduce三者关系</h2><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/hadoop/hmy.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>&emsp;</p><p>此处先简要了解hadoop四个模块的组成. 后面会一一详解.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt; 太阳在坠落 海浪在发愁 你在干什么       –来自网易云音乐《白墙》                            &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://xubatian.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Linux命令收集</title>
    <link href="http://xubatian.cn/Linux%E5%91%BD%E4%BB%A4%E6%94%B6%E9%9B%86/"/>
    <id>http://xubatian.cn/Linux%E5%91%BD%E4%BB%A4%E6%94%B6%E9%9B%86/</id>
    <published>2022-01-11T15:43:31.000Z</published>
    <updated>2022-01-17T17:00:55.288Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;对于常用的Linux命令进行持续收集。持续更新中…</p><span id="more"></span><img src="https://cdn.jsdelivr.net/gh/ShangBaiShuYao/CDN@v1.0/images/2.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="Linux（vi-vim）"><a href="#Linux（vi-vim）" class="headerlink" title="Linux（vi/vim）"></a>Linux（vi/vim）</h1><h2 id="一般模式"><a href="#一般模式" class="headerlink" title="一般模式"></a>一般模式</h2><table><thead><tr><th>语法</th><th>功能描述</th></tr></thead><tbody><tr><td>yy</td><td>复制光标当前一行</td></tr><tr><td>y数字y</td><td>复制一段（从第几行到第几行）</td></tr><tr><td>p</td><td>箭头移动到目的行粘贴</td></tr><tr><td>u</td><td>撤销上一步</td></tr><tr><td>dd</td><td>删除光标当前行</td></tr><tr><td>d数字d</td><td>删除光标（含）后多少行</td></tr><tr><td>x</td><td>删除一个字母，相当于del</td></tr><tr><td>X</td><td>删除一个字母，相当于Backspace</td></tr><tr><td>yw</td><td>复制一个词</td></tr><tr><td>dw</td><td>删除一个词</td></tr><tr><td>shift+^</td><td>移动到行头</td></tr><tr><td>shift+$</td><td>移动到行尾</td></tr><tr><td>1+shift+g</td><td>移动到页头，数字</td></tr><tr><td>shift+g</td><td>移动到页尾</td></tr><tr><td>数字N+shift+g</td><td>移动到目标行</td></tr></tbody></table><h2 id="编辑模式"><a href="#编辑模式" class="headerlink" title="编辑模式"></a>编辑模式</h2><table><thead><tr><th>按键</th><th>功能</th></tr></thead><tbody><tr><td>i</td><td>当前光标前</td></tr><tr><td>a</td><td>当前光标后</td></tr><tr><td>o</td><td>当前光标行的下一行</td></tr><tr><td>I</td><td>光标所在行最前</td></tr><tr><td>A</td><td>光标所在行最后</td></tr><tr><td>O</td><td>当前光标行的上一行</td></tr></tbody></table><h2 id="指令模式"><a href="#指令模式" class="headerlink" title="指令模式"></a>指令模式</h2><table><thead><tr><th>命令</th><th>功能</th></tr></thead><tbody><tr><td>:w</td><td>保存</td></tr><tr><td>:q</td><td>退出</td></tr><tr><td>:!</td><td>强制执行</td></tr><tr><td>/要查找的词</td><td>n 查找下一个，N 往上查找</td></tr><tr><td>? 要查找的词</td><td>n是查找上一个，shift+n是往下查找</td></tr><tr><td>:set nu</td><td>显示行号</td></tr><tr><td>:set nonu</td><td>关闭行号</td></tr></tbody></table><p>&emsp;</p><h2 id="压缩和解压"><a href="#压缩和解压" class="headerlink" title="压缩和解压"></a>压缩和解压</h2><h4 id="gzip-gunzip-压缩"><a href="#gzip-gunzip-压缩" class="headerlink" title="gzip/gunzip 压缩"></a>gzip/gunzip 压缩</h4><p>（1）只能压缩文件不能压缩目录</p><p>（2）不保留原来的文件</p><p>gzip压缩：gzip hello.txt</p><p>gunzip解压缩文件：gunzip hello.txt.gz</p><p>&emsp;</p><h4 id="zip-unzip-压缩"><a href="#zip-unzip-压缩" class="headerlink" title="zip/unzip 压缩"></a>zip/unzip 压缩</h4><p>可以压缩目录且保留源文件</p><p>zip压缩（压缩 1.txt 和2.txt，压缩后的名称为mypackage.zip）：zip hello.zip hello.txt world.txt</p><p>unzip解压：unzip hello.zip</p><p>unzip解压到指定目录：unzip hello.zip -d /opt</p><p>&emsp;</p><h4 id="tar-打包"><a href="#tar-打包" class="headerlink" title="tar 打包"></a>tar 打包</h4><p>tar压缩多个文件：tar -zcvf hello.txt world.txt</p><p>tar压缩目录：tar -zcvf hello.tar.gz opt/</p><p>tar解压到当前目录：tar -zxvf hello.tar.gz</p><p>tar解压到指定目录：tar -zxvf hello.tar.gz -C /opt</p><p>&emsp;</p><h3 id="RPM"><a href="#RPM" class="headerlink" title="RPM"></a>RPM</h3><p>RPM查询命令：rpm -qa |grep firefox</p><p>RPM卸载命令：</p><p>rpm -e xxxxxx</p><p>rpm -e –nodeps xxxxxx（不检查依赖）</p><p>RPM安装命令：</p><p>rpm -ivh xxxxxx.rpm</p><p>rpm -ivh –nodeps fxxxxxx.rpm（–nodeps，不检测依赖进度）</p><p>&emsp;</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-i</td><td>-i=install，安装</td></tr><tr><td>-v</td><td>-v=verbose，显示详细信息</td></tr><tr><td>-h</td><td>-h=hash，进度条</td></tr><tr><td>–nodeps</td><td>–nodeps，不检测依赖进度</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;nbsp;对于常用的Linux命令进行持续收集。持续更新中…&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Linux" scheme="http://xubatian.cn/tags/Linux/"/>
    
  </entry>
  
</feed>
