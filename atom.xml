<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>我的梦想是星辰大海</title>
  
  <subtitle>知识源于积累,登峰造极源于自律</subtitle>
  <link href="http://xubatian.cn/atom.xml" rel="self"/>
  
  <link href="http://xubatian.cn/"/>
  <updated>2022-02-10T02:25:25.113Z</updated>
  <id>http://xubatian.cn/</id>
  
  <author>
    <name>xubatian</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Flink读取无界流数据计算过程演示</title>
    <link href="http://xubatian.cn/Flink%E8%AF%BB%E5%8F%96%E6%97%A0%E7%95%8C%E6%B5%81%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E6%BC%94%E7%A4%BA/"/>
    <id>http://xubatian.cn/Flink%E8%AF%BB%E5%8F%96%E6%97%A0%E7%95%8C%E6%B5%81%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E6%BC%94%E7%A4%BA/</id>
    <published>2022-02-10T02:07:21.000Z</published>
    <updated>2022-02-10T02:25:25.113Z</updated>
    
    <content type="html"><![CDATA[<p>Flink有界流式读取文本数据计算过程演示</p><span id="more"></span><div style="position: relative; padding: 30% 45%;"><iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/2022-02-10_10-12-09.mp4" frameborder="no" scrolling="no"></iframe></div> ]]></content>
    
    
    <summary type="html">&lt;p&gt;Flink有界流式读取文本数据计算过程演示&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink有界流式读取文本数据计算过程演示</title>
    <link href="http://xubatian.cn/Flink%E6%B5%81%E5%BC%8F%E8%AF%BB%E5%8F%96%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E6%BC%94%E7%A4%BA/"/>
    <id>http://xubatian.cn/Flink%E6%B5%81%E5%BC%8F%E8%AF%BB%E5%8F%96%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E6%BC%94%E7%A4%BA/</id>
    <published>2022-02-07T16:22:27.000Z</published>
    <updated>2022-02-10T02:17:20.844Z</updated>
    
    <content type="html"><![CDATA[<p>Flink流式读取文本数据计算过程演示. 文本数据总有一刻能读的完,所以他是有界的. 无界流读的是kafka的数据.</p><p>Flink是懒加载的,第一遍会检测整体代码.</p><p>并行度设置为1,就是单线程执行,所以Flink是一行一行读取文本数据的, 读一行计算一行. </p><p>sum算子是有状态的. 所以他的历史数据是保存在sum算子里面.sum算子做聚合计算的,他是一个有状态的算子.</p><p>批处理最终是输出一次,而流处理来一条计算一条.所以他保留了状态.方便后面来一条和前面对比进行计算.</p><span id="more"></span><p>源码地址: <a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.14.3-Demo/src/main/java/www/xubatian/cn/FlinkDemo01/Flink_WordCount_Bounded.java">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.14.3-Demo/src/main/java/www/xubatian/cn/FlinkDemo01/Flink_WordCount_Bounded.java</a></p><p>结果演示:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220210091224.png"></p><div style="position: relative; padding: 30% 45%;"><iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/2022-02-08_00-18-10.mp4" frameborder="no" scrolling="no"></iframe></div> ]]></content>
    
    
    <summary type="html">&lt;p&gt;Flink流式读取文本数据计算过程演示. 文本数据总有一刻能读的完,所以他是有界的. 无界流读的是kafka的数据.&lt;/p&gt;
&lt;p&gt;Flink是懒加载的,第一遍会检测整体代码.&lt;/p&gt;
&lt;p&gt;并行度设置为1,就是单线程执行,所以Flink是一行一行读取文本数据的, 读一行计算一行. &lt;/p&gt;
&lt;p&gt;sum算子是有状态的. 所以他的历史数据是保存在sum算子里面.sum算子做聚合计算的,他是一个有状态的算子.&lt;/p&gt;
&lt;p&gt;批处理最终是输出一次,而流处理来一条计算一条.所以他保留了状态.方便后面来一条和前面对比进行计算.&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Mybatis介绍之缓存</title>
    <link href="http://xubatian.cn/Mybatis%E4%BB%8B%E7%BB%8D%E4%B9%8B%E7%BC%93%E5%AD%98/"/>
    <id>http://xubatian.cn/Mybatis%E4%BB%8B%E7%BB%8D%E4%B9%8B%E7%BC%93%E5%AD%98/</id>
    <published>2022-02-07T14:25:10.000Z</published>
    <updated>2022-02-07T15:39:35.907Z</updated>
    
    <content type="html"><![CDATA[<p>Mybatis中有一级缓存和二级缓存，默认情况下一级缓存是开启的，而且是不能关闭的。一级缓存是指SqlSession级别的缓存，当在同一个SqlSession中进行相同的SQL语句查询时，第二次以后的查询不会从数据库查询，而是直接从缓存中获取，一级缓存最多缓存1024条SQL。二级缓存是指可以跨SqlSession的缓存。 </p><p>​    Mybatis中进行SQL查询是通过org.apache.ibatis.executor.Executor接口进行的，总体来讲，它一共有两类实现，一类是BaseExecutor，一类是CachingExecutor。前者是非启用二级缓存时使用的，而后者是采用的装饰器模式，在启用了二级缓存时使用，当二级缓存没有命中时，底层还是通过BaseExecutor来实现的。</p> <span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207233751.png"></p><h1 id="Mybatis介绍之缓存"><a href="#Mybatis介绍之缓存" class="headerlink" title="Mybatis介绍之缓存"></a>Mybatis介绍之缓存</h1><h2 id="一级缓存"><a href="#一级缓存" class="headerlink" title="一级缓存"></a>一级缓存</h2><p> 一级缓存是默认启用的，在BaseExecutor的query()方法中实现，底层默认使用的是PerpetualCache实现，PerpetualCache采用HashMap存储数据。一级缓存会在进行增、删、改操作时进行清除。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;E&gt; <span class="function">List&lt;E&gt; <span class="title">query</span><span class="params">(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql)</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">    ErrorContext.instance().resource(ms.getResource()).activity(<span class="string">&quot;executing a query&quot;</span>).object(ms.getId());</span><br><span class="line">    <span class="keyword">if</span> (closed) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> ExecutorException(<span class="string">&quot;Executor was closed.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (queryStack == <span class="number">0</span> &amp;&amp; ms.isFlushCacheRequired()) &#123;</span><br><span class="line">      clearLocalCache();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    List&lt;E&gt; list;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      queryStack++;</span><br><span class="line">      list = resultHandler == <span class="keyword">null</span> ? (List&lt;E&gt;) localCache.getObject(key) : <span class="keyword">null</span>;</span><br><span class="line">      <span class="keyword">if</span> (list != <span class="keyword">null</span>) &#123;</span><br><span class="line">        handleLocallyCachedOutputParameters(ms, key, parameter, boundSql);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      queryStack--;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (queryStack == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">for</span> (DeferredLoad deferredLoad : deferredLoads) &#123;</span><br><span class="line">        deferredLoad.load();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// issue #601</span></span><br><span class="line">      deferredLoads.clear();</span><br><span class="line">      <span class="keyword">if</span> (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) &#123;</span><br><span class="line">        <span class="comment">// issue #482</span></span><br><span class="line">        clearLocalCache();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> list;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>一级缓存的范围有SESSION和STATEMENT两种，默认是SESSION，如果我们不需要使用一级缓存，那么我们可以把一级缓存的范围指定为STATEMENT，这样每次执行完一个Mapper语句后都会将一级缓存清除。如果只是需要对某一条select语句禁用一级缓存，则可以在对应的select元素上加上flushCache=”true”。如果需要更改一级缓存的范围，请在Mybatis的配置文件中，在<settings>下通过localCacheScope指定。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;setting name=&quot;localCacheScope&quot; value=&quot;SESSION&quot;/&gt;</span><br></pre></td></tr></table></figure><p>为了验证一级缓存，我们进行如下测试，在testCache1中，我们通过同一个SqlSession查询了两次一样的SQL，第二次不会发送SQL。在testCache2中，我们也是查询了两次一样的SQL，但是它们是不同的SqlSession，结果会发送两次SQL请求。需要注意的是当Mybatis整合Spring后，直接通过Spring注入Mapper的形式，如果不是在同一个事务中每个Mapper的每次查询操作都对应一个全新的SqlSession实例，这个时候就不会有一级缓存的命中，如有需要可以启用二级缓存。而在同一个事务中时共用的就是同一个SqlSession。这块有兴趣的朋友可以去查看MapperFactoryBean的源码，其父类SqlSessionDaoSupport在设置SqlSessionFactory或设置SqlSessionTemplate时的逻辑。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 默认是有一级缓存的，一级缓存只针对于使用同一个SqlSession的情况。&lt;br/&gt;</span></span><br><span class="line"><span class="comment">  * 注意：当使用Spring整合后的Mybatis，不在同一个事务中的Mapper接口对应的操作也是没有一级缓存的，因为它们是对应不同的SqlSession。在本示例中如需要下面的第二个语句可使用一级缓存，需要testCache()方法在一个事务中，使用<span class="doctag">@Transactional</span>标注。</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="meta">@Test</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCache</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    PersonMapper mapper = session.getMapper(PersonMapper.class);</span><br><span class="line">    mapper.findById(<span class="number">5L</span>);</span><br><span class="line">    mapper.findById(<span class="number">5L</span>);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="meta">@Test</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCache2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    SqlSession session1 = <span class="keyword">this</span>.sessionFactory.openSession();</span><br><span class="line">    SqlSession session2 = <span class="keyword">this</span>.sessionFactory.openSession();</span><br><span class="line">    session1.getMapper(PersonMapper.class).findById(<span class="number">5L</span>);</span><br><span class="line">    session2.getMapper(PersonMapper.class).findById(<span class="number">5L</span>);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h2 id="二级缓存"><a href="#二级缓存" class="headerlink" title="二级缓存"></a>二级缓存</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>二级缓存是默认启用的，如想取消，则可以通过Mybatis配置文件中的<settings>元素下的子元素<setting>来指定cacheEnabled为false。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;settings&gt;</span><br><span class="line">    &lt;setting name=<span class="string">&quot;cacheEnabled&quot;</span> value=<span class="string">&quot;false&quot;</span> /&gt;</span><br><span class="line"> &lt;/settings&gt;</span><br></pre></td></tr></table></figure><p>cacheEnabled默认是启用的，只有在该值为true的时候，底层使用的Executor才是支持二级缓存的CachingExecutor。具体可参考Mybatis的核心配置类org.apache.ibatis.session.Configuration的newExecutor方法实现。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Executor <span class="title">newExecutor</span><span class="params">(Transaction transaction, ExecutorType executorType)</span> </span>&#123;</span><br><span class="line">   executorType = executorType == <span class="keyword">null</span> ? defaultExecutorType : executorType;</span><br><span class="line">   executorType = executorType == <span class="keyword">null</span> ? ExecutorType.SIMPLE : executorType;</span><br><span class="line">   Executor executor;</span><br><span class="line">   <span class="keyword">if</span> (ExecutorType.BATCH == executorType) &#123;</span><br><span class="line">     executor = <span class="keyword">new</span> BatchExecutor(<span class="keyword">this</span>, transaction);</span><br><span class="line">   &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ExecutorType.REUSE == executorType) &#123;</span><br><span class="line">     executor = <span class="keyword">new</span> ReuseExecutor(<span class="keyword">this</span>, transaction);</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     executor = <span class="keyword">new</span> SimpleExecutor(<span class="keyword">this</span>, transaction);</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">if</span> (cacheEnabled) &#123;</span><br><span class="line">     executor = <span class="keyword">new</span> CachingExecutor(executor);</span><br><span class="line">   &#125;</span><br><span class="line">   executor = (Executor) interceptorChain.pluginAll(executor);</span><br><span class="line">   <span class="keyword">return</span> executor;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>要使用二级缓存除了上面一个配置外，我们还需要在我们对应的Mapper.xml文件中定义需要使用的cache，具体可以参考CachingExecutor的以下实现，其中使用的cache就是我们在对应的Mapper.xml中定义的cache。还有一个条件就是需要当前的查询语句是配置了使用cache的，即下面源码的useCache()是返回true的，默认情况下所有select语句的useCache都是true，如果我们在启用了二级缓存后，有某个查询语句是我们不想缓存的，则可以通过指定其useCache为false来达到对应的效果。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">  public &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql)</span><br><span class="line">      throws SQLException &#123;</span><br><span class="line">    Cache cache = ms.getCache();</span><br><span class="line">    if (cache != null) &#123;</span><br><span class="line">      flushCacheIfRequired(ms);</span><br><span class="line">      if (ms.isUseCache() &amp;&amp; resultHandler == null) &#123;</span><br><span class="line">        ensureNoOutParams(ms, parameterObject, boundSql);</span><br><span class="line">        @SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line">        List&lt;E&gt; list = (List&lt;E&gt;) tcm.getObject(cache, key);</span><br><span class="line">        if (list == null) &#123;</span><br><span class="line">          list = delegate.&lt;E&gt; query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);</span><br><span class="line">          tcm.putObject(cache, key, list); // issue #578 and #116</span><br><span class="line">        &#125;</span><br><span class="line">        return list;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return delegate.&lt;E&gt; query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="cache定义"><a href="#cache定义" class="headerlink" title="cache定义"></a>cache定义</h3><p> 刚刚说了我们要想使用二级缓存，是需要在对应的Mapper.xml文件中定义其中的查询语句需要使用哪个cache来缓存数据的。这有两种方式可以定义，一种是通过cache元素定义，一种是通过cache-ref元素来定义。但是需要注意的是对于同一个Mapper来讲，它只能使用一个Cache，当同时使用了<cache>和<cache-ref>时使用<cache>定义的优先级更高。Mapper使用的Cache是与我们的Mapper对应的namespace绑定的，一个namespace最多只会有一个Cache与其绑定。</p><h3 id="cache元素定义"><a href="#cache元素定义" class="headerlink" title="cache元素定义"></a>cache元素定义</h3><p> 使用cache元素来定义使用的Cache时，最简单的做法是直接在对应的Mapper.xml文件中指定一个空的<cache/>元素，这个时候Mybatis会按照默认配置创建一个Cache对象，准备的说是PerpetualCache对象，更准确的说是LruCache对象（底层用了装饰器模式）。具体可以参考XMLMapperBuilder中的cacheElement()方法中解析cache元素的逻辑。空cache元素定义会生成一个采用最近最少使用算法最多只能存储1024个元素的缓存，而且是可读写的缓存，即该缓存是全局共享的，任何一个线程在拿到缓存结果后对数据的修改都将影响其它线程获取的缓存结果，因为它们是共享的，同一个对象。</p><p>​     cache元素可指定如下属性，每种属性的指定都是针对都是针对底层Cache的一种装饰，采用的是装饰器的模式。</p><p>Ø <strong>blocking</strong>：默认为false，当指定为true时将采用BlockingCache进行封装，blocking，阻塞的意思，使用BlockingCache会在查询缓存时锁住对应的Key，如果缓存命中了则会释放对应的锁，否则会在查询数据库以后再释放锁，这样可以阻止并发情况下多个线程同时查询数据，详情可参考BlockingCache的源码。</p><p>Ø <strong>eviction</strong>：eviction，驱逐的意思。也就是元素驱逐算法，默认是LRU，对应的就是LruCache，其默认只保存1024个Key，超出时按照最近最少使用算法进行驱逐，详情请参考LruCache的源码。如果想使用自己的算法，则可以将该值指定为自己的驱逐算法实现类，只需要自己的类实现Mybatis的Cache接口即可。除了LRU以外，系统还提供了FIFO（先进先出，对应FifoCache）、SOFT（采用软引用存储Value，便于垃圾回收，对应SoftCache）和WEAK（采用弱引用存储Value，便于垃圾回收，对应WeakCache）这三种策略。</p><p>Ø <strong>flushInterval</strong>：清空缓存的时间间隔，单位是毫秒，默认是不会清空的。当指定了该值时会再用ScheduleCache包装一次，其会在每次对缓存进行操作时判断距离最近一次清空缓存的时间是否超过了flushInterval指定的时间，如果超出了，则清空当前的缓存，详情可参考ScheduleCache的实现。</p><p>Ø <strong>readOnly</strong>：是否只读，默认为false。当指定为false时，底层会用SerializedCache包装一次，其会在写缓存的时候将缓存对象进行序列化，然后在读缓存的时候进行反序列化，这样每次读到的都将是一个新的对象，即使你更改了读取到的结果，也不会影响原来缓存的对象，即非只读，你每次拿到这个缓存结果都可以进行修改，而不会影响原来的缓存结果；当指定为true时那就是每次获取的都是同一个引用，对其修改会影响后续的缓存数据获取，这种情况下是不建议对获取到的缓存结果进行更改，意为只读。这是Mybatis二级缓存读写和只读的定义，可能与我们通常情况下的只读和读写意义有点不同。每次都进行序列化和反序列化无疑会影响性能，但是这样的缓存结果更安全，不会被随意更改，具体可根据实际情况进行选择。详情可参考SerializedCache的源码。</p><p>Ø <strong>size</strong>：用来指定缓存中最多保存的Key的数量。其是针对LruCache而言的，LruCache默认只存储最多1024个Key，可通过该属性来改变默认值，当然，如果你通过eviction指定了自己的驱逐算法，同时自己的实现里面也有setSize方法，那么也可以通过cache的size属性给自定义的驱逐算法里面的size赋值。</p><p>Ø <strong>type</strong>：type属性用来指定当前底层缓存实现类，默认是PerpetualCache，如果我们想使用自定义的Cache，则可以通过该属性来指定，对应的值是我们自定义的Cache的全路径名称。</p><h3 id="cache-ref元素定义"><a href="#cache-ref元素定义" class="headerlink" title="cache-ref元素定义"></a>cache-ref元素定义</h3><p>cache-ref元素可以用来指定其它Mapper.xml中定义的Cache，有的时候可能我们多个不同的Mapper需要共享同一个缓存的，是希望在MapperA中缓存的内容在MapperB中可以直接命中的，这个时候我们就可以考虑使用cache-ref，这种场景只需要保证它们的缓存的Key是一致的即可命中，二级缓存的Key是通过Executor接口的createCacheKey()方法生成的，其实现基本都是BaseExecutor，源码如下。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> CacheKey <span class="title">createCacheKey</span><span class="params">(MappedStatement ms, Object parameterObject, RowBounds rowBounds, BoundSql boundSql)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">if</span> (closed) &#123;</span><br><span class="line">     <span class="keyword">throw</span> <span class="keyword">new</span> ExecutorException(<span class="string">&quot;Executor was closed.&quot;</span>);</span><br><span class="line">   &#125;</span><br><span class="line">   CacheKey cacheKey = <span class="keyword">new</span> CacheKey();</span><br><span class="line">   cacheKey.update(ms.getId());</span><br><span class="line">   cacheKey.update(Integer.valueOf(rowBounds.getOffset()));</span><br><span class="line">   cacheKey.update(Integer.valueOf(rowBounds.getLimit()));</span><br><span class="line">   cacheKey.update(boundSql.getSql());</span><br><span class="line"></span><br><span class="line">   List&lt;ParameterMapping&gt; parameterMappings = boundSql.getParameterMappings();</span><br><span class="line"></span><br><span class="line">   TypeHandlerRegistry typeHandlerRegistry = ms.getConfiguration().getTypeHandlerRegistry();</span><br><span class="line">   <span class="comment">// mimic DefaultParameterHandler logic</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; parameterMappings.size(); i++) &#123;</span><br><span class="line">     ParameterMapping parameterMapping = parameterMappings.get(i);</span><br><span class="line">     <span class="keyword">if</span> (parameterMapping.getMode() != ParameterMode.OUT) &#123;</span><br><span class="line">       Object value;</span><br><span class="line">       String propertyName = parameterMapping.getProperty();</span><br><span class="line">       <span class="keyword">if</span> (boundSql.hasAdditionalParameter(propertyName)) &#123;</span><br><span class="line">         value = boundSql.getAdditionalParameter(propertyName);</span><br><span class="line">       &#125; <span class="keyword">else</span> <span class="keyword">if</span> (parameterObject == <span class="keyword">null</span>) &#123;</span><br><span class="line">         value = <span class="keyword">null</span>;</span><br><span class="line">       &#125; <span class="keyword">else</span> <span class="keyword">if</span>(typeHandlerRegistry.hasTypeHandler(parameterObject.getClass())) &#123;</span><br><span class="line">         value = parameterObject;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         MetaObject metaObject = configuration.newMetaObject(parameterObject);</span><br><span class="line">         value = metaObject.getValue(propertyName);</span><br><span class="line">       &#125;</span><br><span class="line">       cacheKey.update(value);</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">if</span> (configuration.getEnvironment() != <span class="keyword">null</span>) &#123;</span><br><span class="line">     <span class="comment">// issue #176</span></span><br><span class="line">     cacheKey.update(configuration.getEnvironment().getId());</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">return</span> cacheKey;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>打个比方我想在PersonMapper.xml中的查询都使用在UserMapper.xml中定义的Cache，则可以通过cache-ref元素的namespace属性指定需要引用的Cache所在的namespace，即UserMapper.xml中的定义的namespace，假设在UserMapper.xml中定义的namespace是com.elim.learn.mybatis.dao.UserMapper，则在PersonMapper.xml的cache-ref应该定义如下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;cache-ref namespace=&quot;com.elim.learn.mybatis.dao.UserMapper&quot;/&gt;</span><br></pre></td></tr></table></figure><h3 id="自定义cache"><a href="#自定义cache" class="headerlink" title="自定义cache"></a>自定义cache</h3><p> 前面提到Mybatis的Cache默认会使用PerpetualCache存储数据，如果我们不想按照它的逻辑实现，或者我们想使用其它缓存框架来实现，比如使用Ehcache、Redis等，这个时候我们就可以使用自己的Cache实现，Mybatis是给我们留有对应的接口，允许我们进行自定义的。要想实现自定义的Cache我们必须定义一个自己的类来实现Mybatis提供的Cache接口，实现对应的接口方法。注意，自定义的Cache必须包含一个接收一个String参数的构造方法，这个参数就是Cache的ID，详情请参考Mybatis初始化Cache的过程，对应XMLMapperBuilder的cacheElement()方法。以下是一个简单的MyCache的实现。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">publicclass MyCache implements Cache &#123;</span><br><span class="line">   <span class="keyword">private</span> String id;</span><br><span class="line">   <span class="keyword">private</span> String name;<span class="comment">//Name，故意加这么一个属性，以方便演示给自定义Cache的属性设值</span></span><br><span class="line">   <span class="keyword">private</span> Map&lt;Object, Object&gt; cache = <span class="keyword">new</span> HashMap&lt;Object, Object&gt;();</span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 构造方法。自定义的Cache实现一定要有一个id参数</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@param</span> id</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="title">MyCache</span><span class="params">(String id)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.id = id;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">this</span>.id;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">putObject</span><span class="params">(Object key, Object value)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.cache.put(key, value);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> Object <span class="title">getObject</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">this</span>.cache.get(key);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> Object <span class="title">removeObject</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">this</span>.cache.remove(key);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.cache.clear();</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getSize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">this</span>.cache.size();</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> ReadWriteLock <span class="title">getReadWriteLock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@return</span> the name</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> name;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@param</span> name the name to set</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.name = name;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 定义了自己的Cache实现类后我们就可以在需要使用它的Mapper.xml文件中通过<cache>标签的type属性来指定我们需要使用的Cache。如果我们的自定义Cache是需要指定参数的，则可以通过<cache>标签的子标签<property>来指定对应的参数，Mybatis在解析的时候会调用指定属性对应的set方法。针对于上面的自定义Cache，我们的配置如下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;cache type=&quot;com.elim.learn.mybatis.cache.MyCache&quot;&gt;</span><br><span class="line">      &lt;property name=&quot;name&quot; value=&quot;调用setName()方法需要传递的参数值&quot;/&gt;</span><br><span class="line">   &lt;/cache&gt;</span><br></pre></td></tr></table></figure><p>圆角矩形：注意：如果我们使用了自定义的Cache，那么cache标签的其它属性，如size、eviction等都不会对自定义的Cache起作用，也就是说不会自动对自定义的Cache进行包装，如果需要使用自定义的Cache，同时又希望使用Mybatis自带的那些Cache包装类，则可以在自定义的Cache中自己进行包装。</p><h3 id="缓存的清除"><a href="#缓存的清除" class="headerlink" title="缓存的清除"></a>缓存的清除</h3><p>二级缓存默认是会在执行update、insert和delete语句时进行清空的，具体可以参考CachingExecutor的update()实现。如果我们不希望在执行某一条更新语句时清空对应的二级缓存，那么我们可以在对应的语句上指定flushCache属性等于false。如果只是某一条select语句不希望使用二级缓存和一级缓存，则也可以在对应的select元素上加上flushCache=”true”。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;insert id=&quot;delete&quot; parameterType=&quot;java.lang.Long&quot;flushCache=&quot;false&quot;&gt;</span><br><span class="line">    delete t_person where id=#&#123;id&#125;</span><br><span class="line"> &lt;/insert&gt;</span><br></pre></td></tr></table></figure><h3 id="自己操作Cache"><a href="#自己操作Cache" class="headerlink" title="自己操作Cache"></a>自己操作Cache</h3><p>Mybatis中创建的二级缓存都会交给Configuration进行管理，Configuration类是Mybatis的核心类，里面包含了各种Mybatis资源的管理，其可以很方便的通过SqlSession、SqlSessionFactory获取，如有需要我们可以直接通过它来操作我们的Cache。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  @Test</span><br><span class="line">   public void testGetCache() &#123;</span><br><span class="line">      Configuration configuration = this.session.getConfiguration();</span><br><span class="line">//    this.sessionFactory.getConfiguration();</span><br><span class="line">      Collection&lt;Cache&gt; caches = configuration.getCaches();</span><br><span class="line">      System.out.println(caches);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>针对二级缓存进行了以下测试，获取两个不同的SqlSession执行两条相同的SQL，在未指定Cache时Mybatis将查询两次数据库，在指定了Cache时Mybatis只查询了一次数据库，第二次是从缓存中拿的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">  public void testCache2() &#123;</span><br><span class="line">     SqlSession session1 = this.sessionFactory.openSession();</span><br><span class="line">     SqlSession session2 = this.sessionFactory.openSession();</span><br><span class="line">     session1.getMapper(PersonMapper.class).findById(5L);</span><br><span class="line">     session1.commit();</span><br><span class="line">     session2.getMapper(PersonMapper.class).findById(5L);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p> 注意在上面的代码中，我在session1执行完对应的SQL后调用了session1的commit()方法，即提交了它的事务，这样我们在第二次查询的时候才会缓存命中，才不会查询数据库，否则就会连着查询两次数据库。这是因为在CachingExecutor中Mybatis在查询的过程中又在原来Cache的基础上包装了TransactionalCache，这个Cache只会在事务提交后才真正的写入缓存，所以在上面的示例中，如果session1执行完SQL后没有马上commit就紧接着用session2执行SQL，虽然session1查询时没有缓存命中，但是此时写入缓存操作还没有进行，session2再查询的时候也就不会缓存命中了。</p><p><strong>参考文档</strong></p><p><a href="http://www.mybatis.org/mybatis-3/zh/sqlmap-xml.html#cache">http://www.mybatis.org/mybatis-3/zh/sqlmap-xml.html#cache</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Mybatis中有一级缓存和二级缓存，默认情况下一级缓存是开启的，而且是不能关闭的。一级缓存是指SqlSession级别的缓存，当在同一个SqlSession中进行相同的SQL语句查询时，第二次以后的查询不会从数据库查询，而是直接从缓存中获取，一级缓存最多缓存1024条SQL。二级缓存是指可以跨SqlSession的缓存。 &lt;/p&gt;
&lt;p&gt;​    Mybatis中进行SQL查询是通过org.apache.ibatis.executor.Executor接口进行的，总体来讲，它一共有两类实现，一类是BaseExecutor，一类是CachingExecutor。前者是非启用二级缓存时使用的，而后者是采用的装饰器模式，在启用了二级缓存时使用，当二级缓存没有命中时，底层还是通过BaseExecutor来实现的。&lt;/p&gt;</summary>
    
    
    
    <category term="Java" scheme="http://xubatian.cn/categories/Java/"/>
    
    
    <category term="Java" scheme="http://xubatian.cn/tags/Java/"/>
    
    <category term="Mybatis" scheme="http://xubatian.cn/tags/Mybatis/"/>
    
  </entry>
  
  <entry>
    <title>推荐一款Mybatis分页插件</title>
    <link href="http://xubatian.cn/%E6%8E%A8%E8%8D%90%E4%B8%80%E6%AC%BEMybatis%E5%88%86%E9%A1%B5%E6%8F%92%E4%BB%B6/"/>
    <id>http://xubatian.cn/%E6%8E%A8%E8%8D%90%E4%B8%80%E6%AC%BEMybatis%E5%88%86%E9%A1%B5%E6%8F%92%E4%BB%B6/</id>
    <published>2022-02-07T13:52:43.000Z</published>
    <updated>2022-02-07T14:17:20.428Z</updated>
    
    <content type="html"><![CDATA[<p>介绍Mybatis的插件，以及如何通过Mybatis的插件功能实现一个自定义的分页插件。前段时间遇到了一款开源的Mybatis分页插件，叫<code>PageHelper</code>，github地址是 <a href="https://github.com/pagehelper/Mybatis-PageHelper">https://github.com/pagehelper/Mybatis-PageHelper</a>   其原理是通过<code>ThreadLocal</code>来存放分页信息，从而可以做到在Service层实现无侵入性的Mybatis分页。笔者感觉还不错，所以特意发博文记录一下，并推荐给大家。</p><span id="more"></span><h1 id="简单示例"><a href="#简单示例" class="headerlink" title="简单示例"></a>简单示例</h1><p>以下是使用<code>PageHelper</code>进行分页的一个简单的示例，更多详细的内容，请大家参数上面提供的<a href="https://github.com/pagehelper/Mybatis-PageHelper">github地址</a>。</p><h2 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h2><p>笔者使用的是Maven，添加依赖如下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;pagehelper&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;4.1.6&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h2 id="注册Mybatis-Plugin"><a href="#注册Mybatis-Plugin" class="headerlink" title="注册Mybatis Plugin"></a>注册Mybatis Plugin</h2><p>跟其它Mybatis Plugin一样，我们需要在Mybatis的配置文件中注册需要使用的Plugin，<code>PageHelper</code>中对应的Plugin实现类就是<code>com.github.pagehelper.PageHelper</code>自身。顺便说一句，Mybatis的Plugin我们说是Plugin，实际上对应的却是<code>org.apache.ibatis.plugin.Interceptor</code>接口，因为<code>Interceptor</code>的核心是其中的<code>plugin(Object target)</code>方法，而对于<code>plugin(Object target)</code>方法的实现，我们在需要对对应的对象进行拦截时会通过<code>org.apache.ibatis.plugin.Plugin</code>的静态方法<code>wrap(Object target, Interceptor interceptor)</code>返回一个代理对象，而方法入参就是当前的<code>Interceptor</code>实现类。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;plugins&gt;  </span><br><span class="line">   &lt;plugin interceptor=&quot;com.github.pagehelper.PageHelper&quot;/&gt;  </span><br><span class="line">&lt;/plugins&gt;</span><br></pre></td></tr></table></figure><h2 id="使用PageHelper"><a href="#使用PageHelper" class="headerlink" title="使用PageHelper"></a>使用PageHelper</h2><p><code>PageHelper</code>拦截的是<code>org.apache.ibatis.executor.Executor</code>的<code>query</code>方法，其传参的核心原理是通过<code>ThreadLocal</code>进行的。当我们需要对某个查询进行分页查询时，我们可以在调用Mapper进行查询前调用一次<code>PageHelper.startPage(..)</code>，这样<code>PageHelper</code>会把分页信息存入一个<code>ThreadLocal</code>变量中。在拦截到<code>Executor</code>的<code>query</code>方法执行时会从对应的<code>ThreadLocal</code>中获取分页信息，获取到了，则进行分页处理，处理完了后又会把<code>ThreadLocal</code>中的分页信息清理掉，以便不影响下一次的查询操作。所以当我们使用了<code>PageHelper.startPage(..)</code>后，每次将对最近一次的查询进行分页查询，如果下一次查询还需要进行分页查询，需要重新进行一次<code>PageHelper.startPage(..)</code>。这样就做到了在引入了分页后可以对原来的查询代码没有任何的侵入性。此外，在进行分页查询时，我们的返回结果一般是一个<code>java.util.List</code>，<code>PageHelper</code>分页查询后的结果会变成<code>com.github.pagehelper.Page</code>类型，其继承了<code>java.util.ArrayList</code>，所以不会对我们的方法声明造成影响。<code>com.github.pagehelper.Page</code>中包含有返回结果的分页信息，包括总记录数，总的分页数等信息，所以一般我们需要把返回结果强转为<code>com.github.pagehelper.Page</code>类型。以下是一个简单的使用<code>PageHelper</code>进行分页查询的示例代码。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">public class PageHelperTest &#123;</span><br><span class="line"></span><br><span class="line">private static SqlSessionFactory sqlSessionFactory;</span><br><span class="line">private SqlSession session;</span><br><span class="line"></span><br><span class="line">@BeforeClass</span><br><span class="line">public static void beforeClass() throws IOException &#123;</span><br><span class="line">InputStream is = Resources.getResourceAsStream(&quot;mybatis-config-single.xml&quot;);</span><br><span class="line">sqlSessionFactory = new SqlSessionFactoryBuilder().build(is);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Before</span><br><span class="line">public void before() &#123;</span><br><span class="line">this.session = sqlSessionFactory.openSession();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@After</span><br><span class="line">public void after() &#123;</span><br><span class="line">this.session.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Test</span><br><span class="line">public void test() &#123;</span><br><span class="line">int pageNum = 2;//页码，从1开始</span><br><span class="line">int pageSize = 10;//每页记录数</span><br><span class="line">PageHelper.startPage(pageNum, pageSize);//指定开始分页</span><br><span class="line">UserMapper userMapper = this.session.getMapper(UserMapper.class);</span><br><span class="line">List&lt;User&gt; all = userMapper.findAll();</span><br><span class="line">Page&lt;User&gt; page = (Page&lt;User&gt;) all;</span><br><span class="line">System.out.println(page.getPages());</span><br><span class="line">System.out.println(page);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上是通过<code>PageHelper.startPage(..)</code>传递分页信息的示例，其实<code>PageHelper</code>还支持Mapper参数传递分页信息等其它用法。关于<code>PageHelper</code>的更多用法和配置信息等请参考该项目的GitHub<a href="https://github.com/pagehelper/Mybatis-PageHelper">官方文档</a>。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;介绍Mybatis的插件，以及如何通过Mybatis的插件功能实现一个自定义的分页插件。前段时间遇到了一款开源的Mybatis分页插件，叫&lt;code&gt;PageHelper&lt;/code&gt;，github地址是 &lt;a href=&quot;https://github.com/pagehelper/Mybatis-PageHelper&quot;&gt;https://github.com/pagehelper/Mybatis-PageHelper&lt;/a&gt;   其原理是通过&lt;code&gt;ThreadLocal&lt;/code&gt;来存放分页信息，从而可以做到在Service层实现无侵入性的Mybatis分页。笔者感觉还不错，所以特意发博文记录一下，并推荐给大家。&lt;/p&gt;</summary>
    
    
    
    <category term="Java" scheme="http://xubatian.cn/categories/Java/"/>
    
    
    <category term="Java" scheme="http://xubatian.cn/tags/Java/"/>
    
    <category term="Mybatis" scheme="http://xubatian.cn/tags/Mybatis/"/>
    
  </entry>
  
  <entry>
    <title>mybatis简介</title>
    <link href="http://xubatian.cn/mybatis%E7%AE%80%E4%BB%8B/"/>
    <id>http://xubatian.cn/mybatis%E7%AE%80%E4%BB%8B/</id>
    <published>2022-02-07T13:41:56.000Z</published>
    <updated>2022-02-07T13:51:26.032Z</updated>
    
    <content type="html"><![CDATA[<p>Mybatis框架简介</p><span id="more"></span><h1 id="MyBatis介绍"><a href="#MyBatis介绍" class="headerlink" title="MyBatis介绍"></a>MyBatis介绍</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>原是apache的一个开源项目iBatis，2010年6月这个项目由apache software foundation 迁移到了google code，随着开发团队转投Google Code旗下，ibatis3.x正式更名为Mybatis ，代码于2013年11月迁移到Github。<br>相对Hibernate和ApacheOJB等“一站式”ORM（Object Mapping）解决方案而言，ibatis 是一种“半自动化”的ORM实现。Relational<br>无论 Hibernate还是Apache OJB，都对数据库结构提供了较为完整的封装，提供了从POJO到数据库表的全套映射机制。程序员往往只需定义好了POJO 到数据库表的映射关系，即可通过 Hibernate或者OJB 提供的方法完成持久层操作。程序员甚至不需要对 SQL 的熟练掌握，Hibernate/OJB 会根据制定的存储逻辑，自动生成对应的 SQL 并调用 JDBC 接口加以执行。</p><h2 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a>官方文档</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207214544.png" alt="博客:www.xubatian.cn"></p><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p>1）支持自定义SQL、存储过程、及高级映射<br>2）实现自动对SQL的参数设置<br>3）实现自动对结果集进行解析和封装<br>4）通过XML或者注解进行配置和映射，大大减少代码量<br>5）数据源的连接信息通过配置文件进行配置</p><p>可以发现，MyBatis是对JDBC进行了简单的封装，帮助用户进行SQL参数的自动设置，以及结果集与Java对象的自动映射。与Hibernate相比，配置更加简单、灵活、执行效率高。但是正因为此，所以没有实现完全自动化，需要手写SQL，这是优点也是缺点。</p><p>因此，对性能要求较高的电商类项目，一般会使用MyBatis，而对与业务逻辑复杂，不太在乎执行效率的传统行业，一般会使用Hibernate</p><h2 id="Mybaits整体架构"><a href="#Mybaits整体架构" class="headerlink" title="Mybaits整体架构"></a>Mybaits整体架构</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207214710.png" alt="博客:www.xubatian.cn"></p><p>1、配置文件<br>全局配置文件：mybatis-config.xmlhibernate.cfg.xml，作用：配置数据源，引入映射文件<br>映射文件：XxMapper.xmlxx.hbm.xml，作用：配置sql语句、参数、结果集封装类型等</p><p>2、SqlSessionFactory<br>相当于Hibernate的SessionFactory，作用：获取SqlSession<br>通过newSqlSessionFactoryBuilder().build(inputStream)来构建，inputStream：读取配置文件的IO流</p><p>3、SqlSession<br>相当于Hibernate的Session，作用：执行CRUD操作</p><p>4、Executor<br>执行器，SqlSession通过调用它来完成具体的CRUD<br>它是一个接口，提供了两种实现：缓存的实现、数据库的实现</p><p>5、Mapped Statement<br>在映射文件里面配置，包含3部分内容：<br>具体的sql，sql执行所需的参数类型，sql执行结果的封装类型<br>参数类型和结果集封装类型包括3种：<br>HashMap，基本数据类型，pojo</p><p>…….未完待续</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Mybatis框架简介&lt;/p&gt;</summary>
    
    
    
    <category term="Java" scheme="http://xubatian.cn/categories/Java/"/>
    
    
    <category term="Java" scheme="http://xubatian.cn/tags/Java/"/>
    
    <category term="Mybatis" scheme="http://xubatian.cn/tags/Mybatis/"/>
    
  </entry>
  
  <entry>
    <title>Flink使用idea将匿名内部类替换为lambda表达会擦除泛型</title>
    <link href="http://xubatian.cn/Flink%E4%BD%BF%E7%94%A8idea%E5%B0%86%E5%8C%BF%E5%90%8D%E5%86%85%E9%83%A8%E7%B1%BB%E6%9B%BF%E6%8D%A2%E4%B8%BAlambda%E8%A1%A8%E8%BE%BE%E4%BC%9A%E6%93%A6%E9%99%A4%E6%B3%9B%E5%9E%8B/"/>
    <id>http://xubatian.cn/Flink%E4%BD%BF%E7%94%A8idea%E5%B0%86%E5%8C%BF%E5%90%8D%E5%86%85%E9%83%A8%E7%B1%BB%E6%9B%BF%E6%8D%A2%E4%B8%BAlambda%E8%A1%A8%E8%BE%BE%E4%BC%9A%E6%93%A6%E9%99%A4%E6%B3%9B%E5%9E%8B/</id>
    <published>2022-02-07T06:12:57.000Z</published>
    <updated>2022-02-07T09:16:00.327Z</updated>
    
    <content type="html"><![CDATA[<p>Flink代码使用IDEA将new匿名内部类替换为lambda表达式运行会报错,因为替换后会擦除泛型</p><p>测试Flink1.12.0没有出现返回值类型报错问题. 不知道是否是Flink高版本结局了此问题还是idea高版本解决了此问题.</p><span id="more"></span><h2 id="源码地址-™"><a href="#源码地址-™" class="headerlink" title="源码地址:™"></a>源码地址:™</h2><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo01/Flink_WordCount_Bounded.java">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/blob/main/flink-1.12.0-Demo/src/main/java/com/shangbaishuyao/demo/FlinkDemo01/Flink_WordCount_Bounded.java</a></p><h2 id="报错类型示例"><a href="#报错类型示例" class="headerlink" title="报错类型示例"></a>报错类型示例</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207141902.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207141937.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207142024.png" alt="博客:www.xubatian.cn"></p><h2 id="返回值类型报错"><a href="#返回值类型报错" class="headerlink" title="返回值类型报错"></a>返回值类型报错</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207142213.png" alt="博客:www.xubatian.cn"></p><h2 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式:"></a>解决方式:</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207142525.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207144407.png" alt="博客:www.xubatian.cn"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Flink代码使用IDEA将new匿名内部类替换为lambda表达式运行会报错,因为替换后会擦除泛型&lt;/p&gt;
&lt;p&gt;测试Flink1.12.0没有出现返回值类型报错问题. 不知道是否是Flink高版本结局了此问题还是idea高版本解决了此问题.&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink流处理案例</title>
    <link href="http://xubatian.cn/Flink%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/"/>
    <id>http://xubatian.cn/Flink%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/</id>
    <published>2022-01-31T09:02:26.000Z</published>
    <updated>2022-02-07T06:01:54.853Z</updated>
    
    <content type="html"><![CDATA[<p>简单Flink流处理案例</p><p>①自定义flatmap操作</p><p>② new 匿名内部类接口 操作</p><p>③简单的算子运用: FlatMap压平, Map转换,Tuple2二元组.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在Java中,二元组Tuple就是<span class="number">2</span>,即Tuple2;三元组就是<span class="number">3</span>,即Tuple3</span><br><span class="line">元组索引从<span class="number">0</span>开始</span><br></pre></td></tr></table></figure><span id="more"></span><h2 id="源代码地址"><a href="#源代码地址" class="headerlink" title="源代码地址"></a>源代码地址</h2><p><a href="https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/tree/main/flink-1.12.0-Demo">https://github.com/ShangBaiShuYao/flink-learning-from-zhisheng/tree/main/flink-1.12.0-Demo</a></p><h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220131173631.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207125512.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220131173834.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220131174208.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220131180631.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207130501.png" alt="博客:www.xubatian.cn"></p><h2 id="补充另一种写法-new-匿名内部类接口"><a href="#补充另一种写法-new-匿名内部类接口" class="headerlink" title="补充另一种写法: new 匿名内部类接口"></a>补充另一种写法: new 匿名内部类接口</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207131308.png" alt="博客:www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220207135330.png" alt="博客:www.xubatian.cn"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;简单Flink流处理案例&lt;/p&gt;
&lt;p&gt;①自定义flatmap操作&lt;/p&gt;
&lt;p&gt;② new 匿名内部类接口 操作&lt;/p&gt;
&lt;p&gt;③简单的算子运用: FlatMap压平, Map转换,Tuple2二元组.&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;在Java中,二元组Tuple就是&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,即Tuple2;三元组就是&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,即Tuple3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;元组索引从&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;开始&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>《说书人》朱广权+尼格买提版</title>
    <link href="http://xubatian.cn/%E8%AF%B4%E4%B9%A6%E4%BA%BA/"/>
    <id>http://xubatian.cn/%E8%AF%B4%E4%B9%A6%E4%BA%BA/</id>
    <published>2022-01-28T14:33:41.000Z</published>
    <updated>2022-02-07T16:23:15.288Z</updated>
    
    <content type="html"><![CDATA[<p>侠义多是平凡辈 , 无须仗剑走天涯。——朱广权</p><span id="more"></span><div style="position: relative; padding: 30% 45%;"><iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/%E3%80%90%E5%89%8D%E6%96%B9%E9%AB%98%E7%87%83%E3%80%91%E5%BD%93%E5%94%A2%E5%91%90%E9%81%87%E8%A7%81%E7%94%B5%E9%9F%B3%EF%BC%81%E6%9C%B1%E5%B9%BF%E6%9D%83-%E5%B0%BC%E6%A0%BC%E4%B9%B0%E6%8F%90%E7%89%88%E3%80%8A%E8%AF%B4%E4%B9%A6%E4%BA%BA%E3%80%8B.mp4" frameborder="no" scrolling="no"></iframe></div> ]]></content>
    
    
    <summary type="html">&lt;p&gt;侠义多是平凡辈 , 无须仗剑走天涯。——朱广权&lt;/p&gt;</summary>
    
    
    
    <category term="轻松一刻" scheme="http://xubatian.cn/categories/%E8%BD%BB%E6%9D%BE%E4%B8%80%E5%88%BB/"/>
    
    
    <category term="轻松一刻" scheme="http://xubatian.cn/tags/%E8%BD%BB%E6%9D%BE%E4%B8%80%E5%88%BB/"/>
    
  </entry>
  
  <entry>
    <title>到底是什么使我心态不稳?</title>
    <link href="http://xubatian.cn/%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%E6%98%AF%E6%88%91%E5%BF%83%E6%80%81%E4%B8%8D%E7%A8%B3/"/>
    <id>http://xubatian.cn/%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%E6%98%AF%E6%88%91%E5%BF%83%E6%80%81%E4%B8%8D%E7%A8%B3/</id>
    <published>2022-01-28T05:47:56.000Z</published>
    <updated>2022-01-28T07:46:01.670Z</updated>
    
    <content type="html"><![CDATA[<p>编译spark源码的三天时间,我到底经历了什么?</p><span id="more"></span><p>原本我是打算使用本地进行编译的…结果撑了一天,不行了.一大波bug正在赶来….</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128135223.png" alt="博客: www.xubatian.cn"></p><p>这种bug,我解决了不下二十个. 最关键的是,这种报错都是些jar包下载不下来….</p><div align="center">    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128135425.png" alt="博客: www.xubatian.cn"></img></div><p>想想还是算了吧,换服务器编译吧… 在准备了一堆maven,scala等一堆环境变量之后,终于走上了编译之路.</p><p>打死我都没想,这条路黑暗了我两天美好的人生….</p><p>再哭一下…..</p><div align="center">    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128135700.png" alt="博客: www.xubatian.cn"></img></div><p>—————————————————————– 以下是我最常遇到的bug  , 经常光顾我 ,也是老熟人了  ———————————————————–</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128135009.png" alt="博客: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128135024.png" alt="博客: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128140050.png" alt="博客: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128144435.png" alt="博客: www.xubatian.cn"></p><p>阳光总在风雨后…</p><p>历经三天的折磨,终于编译出了合适我hadoop版本的spark…..</p><p>此处咧嘴大笑….</p><div align="center">    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/20220128141051.png" alt="博客: www.xubatian.cn"></img></div><div align="center">    <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/tempImage1643355914082.gif" alt="博客: www.xubatian.cn"></img></div><p>炫耀版的展示一下….嘻嘻…</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/www.xubatian.cn_400.png" alt="博客: www.xubatian.cn"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;编译spark源码的三天时间,我到底经历了什么?&lt;/p&gt;</summary>
    
    
    
    <category term="动态" scheme="http://xubatian.cn/categories/%E5%8A%A8%E6%80%81/"/>
    
    
    <category term="动态" scheme="http://xubatian.cn/tags/%E5%8A%A8%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>streamx源码编译及安装部署-本地编译(推荐)</title>
    <link href="http://xubatian.cn/streamx%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-%E6%9C%AC%E5%9C%B0%E7%BC%96%E8%AF%91(%E6%8E%A8%E8%8D%90)/"/>
    <id>http://xubatian.cn/streamx%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-%E6%9C%AC%E5%9C%B0%E7%BC%96%E8%AF%91(%E6%8E%A8%E8%8D%90)/</id>
    <published>2022-01-22T12:21:54.000Z</published>
    <updated>2022-01-23T03:05:51.541Z</updated>
    
    <content type="html"><![CDATA[<p>锚定既定奋斗目标，意气风发走向未来。——人民日报</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_394.jpg" alt="blog: www.xubatian.cn"></p><p>在服务器端进行了streamx编译的那文章也说了,我没有flink,hadoop 的配置,所以我重新进行了streamx的源码编译,版本依旧是streamx-1.2.0 稳定版本</p><p><strong>现在编译的是Flink版本为1.14.3 , hadoop版本为3.1.3.</strong></p><h1 id="源码编译的前提条件"><a href="#源码编译的前提条件" class="headerlink" title="源码编译的前提条件"></a>源码编译的前提条件</h1><p>我使用的是maven 3.8.3版本. node js , jdk1.8.3 ,npm. </p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/20220122203333.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/20220122203432.png" alt="blog: www.xubatian.cn"></p><h1 id="streamx源码编译"><a href="#streamx源码编译" class="headerlink" title="streamx源码编译"></a>streamx源码编译</h1><h2 id="从官网下载streamx稳定版本"><a href="#从官网下载streamx稳定版本" class="headerlink" title="从官网下载streamx稳定版本"></a>从官网下载streamx稳定版本</h2><p>官网地址: <a href="https://www.streamxhub.com/#">https://www.streamxhub.com/#</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_391.png" alt="blog: www.xubatian.cn"></p><p>github地址: <a href="https://github.com/streamxhub/streamx">https://github.com/streamxhub/streamx</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_390.png" alt="blog: www.xubatian.cn"></p><h2 id="修改streamx的相关版本"><a href="#修改streamx的相关版本" class="headerlink" title="修改streamx的相关版本"></a>修改streamx的相关版本</h2><p>修改为公司hadoop,flink,spark等相符合的大数据组件版本. </p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122201007.png" alt="blog: www.xubatian.cn"></p><h2 id="编译streamx源码"><a href="#编译streamx源码" class="headerlink" title="编译streamx源码"></a>编译streamx源码</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/20220122203910.png" alt="blog: www.xubatian.cn"></p><h2 id="报错及解决方式"><a href="#报错及解决方式" class="headerlink" title="报错及解决方式"></a>报错及解决方式</h2><p>常常报错的问题就是 jar包下载不下来. 或者maven镜像无法来取jar包. </p><p>解决方式:</p><p>① 看看是那个jar包下载不下来. </p><p>② 复制该jar名称,去maven中央仓库直接下载版本相同的jar</p><p>maven中央仓库地址: <a href="https://mvnrepository.com/">https://mvnrepository.com/</a> </p><p>注: 也可以直接放到谷歌浏览器上直接搜索.</p><p>③ 删除之前编译残留的文件,将下载好的jar包拷贝到本地maven仓库(注意:一定要放到指定的文件夹下)</p><p>④ 然后重新编译</p><h3 id="如下是示例"><a href="#如下是示例" class="headerlink" title="如下是示例:"></a>如下是示例:</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122194838.png" alt="blog: www.xubatian.cn"></p><h3 id="查看本地maven仓库-删除全部残余文件"><a href="#查看本地maven仓库-删除全部残余文件" class="headerlink" title="查看本地maven仓库,删除全部残余文件"></a>查看本地maven仓库,删除全部残余文件</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122195038.png" alt="blog: www.xubatian.cn"></p><h3 id="下载版本一样的jar包"><a href="#下载版本一样的jar包" class="headerlink" title="下载版本一样的jar包"></a>下载版本一样的jar包</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122195126.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122195208.png" alt="blog: www.xubatian.cn"></p><p>将下载好的jar拷贝至本地maven仓库</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122195342.png" alt="blog: www.xubatian.cn"></p><h3 id="重新编译"><a href="#重新编译" class="headerlink" title="重新编译"></a>重新编译</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/20220122203910.png" alt="blog: www.xubatian.cn"></p><h2 id="反复经过N次上述行为后-恭喜你-成功了"><a href="#反复经过N次上述行为后-恭喜你-成功了" class="headerlink" title="反复经过N次上述行为后,恭喜你,成功了"></a>反复经过N次上述行为后,恭喜你,成功了</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122200810.png" alt="blog: www.xubatian.cn"></p><h2 id="在此目录下的压缩包拷贝至服务器上解压"><a href="#在此目录下的压缩包拷贝至服务器上解压" class="headerlink" title="在此目录下的压缩包拷贝至服务器上解压"></a>在此目录下的压缩包拷贝至服务器上解压</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122201341.png" alt="blog: www.xubatian.cn"></p><p>后缀是macOS的是我在本地进行编译的. 后缀是Linux的是我在服务器端编译的.</p><p>二者的区别就是 macOS端的我修改了hadoop版本为3.1.3 ,Flink版本为1.14.3</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/20220122205512.png" alt="blog: www.xubatian.cn"></p><h2 id="streamx部署"><a href="#streamx部署" class="headerlink" title="streamx部署"></a>streamx部署</h2><p>请看我的另一篇文章: <a href="https://www.xubatian.cn/streamx%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E7%BC%96%E8%AF%91/">streamx源码编译及安装部署-服务器端编译</a></p><h2 id="启动streamx"><a href="#启动streamx" class="headerlink" title="启动streamx"></a>启动streamx</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122201928.png" alt="blog: www.xubatian.cn"></p><h2 id="访问Web页面"><a href="#访问Web页面" class="headerlink" title="访问Web页面"></a>访问Web页面</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122202012.png" alt="blog: www.xubatian.cn"></p><h2 id="编译好的streamx存放地址"><a href="#编译好的streamx存放地址" class="headerlink" title="编译好的streamx存放地址"></a>编译好的streamx存放地址</h2><p>streamx1.12.0 源码默认配置 直接编译 编译后的压缩包为: streamx-console-service-1.2.0-Linux-bin.tar.gz<br>streamx1.12.0 源码,修改hadoop版本为3.1.3, flink版本为1.14.3 编译后的压缩包为: streamx-console-service-1.2.0-macOS-bin.tar.gz<br>两个压缩包都方式这里,链接: <a href="https://pan.baidu.com/s/1M4R0K3rOzNZOdilnJiduFw">https://pan.baidu.com/s/1M4R0K3rOzNZOdilnJiduFw</a> 提取码: 2olc</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;锚定既定奋斗目标，意气风发走向未来。——人民日报&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="streamx" scheme="http://xubatian.cn/tags/streamx/"/>
    
  </entry>
  
  <entry>
    <title>streamx源码编译及安装部署-服务器端编译</title>
    <link href="http://xubatian.cn/streamx%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E7%BC%96%E8%AF%91/"/>
    <id>http://xubatian.cn/streamx%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E7%BC%96%E8%AF%91/</id>
    <published>2022-01-22T06:38:59.000Z</published>
    <updated>2022-01-23T03:05:56.394Z</updated>
    
    <content type="html"><![CDATA[<p>遇到问题，改变苛求别人的惯性，重新塑造思考问题的方式。换个角度看世界，换个方向看问题，就会豁然开朗。——人民日报</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_387.jpg" alt="blog: www.xubatian.cn"></p><h1 id="什么是streamx"><a href="#什么是streamx" class="headerlink" title="什么是streamx"></a>什么是streamx</h1><p> 大数据技术如今发展的如火如荼，已经呈现百花齐放欣欣向荣的景象，实时处理流域 Apache Spark 和 Apache Flink 更是一个伟大的进步，尤其是 Apache Flink 被普遍认为是下一代大数据流计算引擎， 我们在使用 Flink 时发现从编程模型， 启动配置到运维管理都有很多可以抽象共用的地方， 我们将一些好的经验固化下来并结合业内的最佳实践， 通过不断努力终于诞生了今天的框架 —— StreamX， 项目的初衷是 —— 让 Flink 开发更简单， 使用 StreamX 开发，可以极大降低学习成本和开发门槛， 让开发者只用关心最核心的业务， StreamX 规范了项目的配置，鼓励函数式编程，定义了最佳的编程方式，提供了一系列开箱即用的 Connectors ，标准化了配置、开发、测试、部署、监控、运维的整个过程， 提供 Scala 和 Java 两套api， 其最终目的是打造一个一站式大数据平台，流批一体，湖仓一体的解决方案.</p><h1 id="源码编译的前提条件"><a href="#源码编译的前提条件" class="headerlink" title="源码编译的前提条件"></a>源码编译的前提条件</h1><p>我使用的是CentOS Linux release 7.5.1804 (Core).  mysql5.7. 以及maven 3.8.3版本. node js 和 jdk1.8.3 .最少2个多G的磁盘空间</p><p>   <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122150459.png" alt="blog: www.xubatian.cn"></p><h2 id="安装node-js"><a href="#安装node-js" class="headerlink" title="安装node-js"></a>安装node-js</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ wget https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz</span><br></pre></td></tr></table></figure><h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ tar xf node-v16.13.1-linux-x64.tar.xz </span><br></pre></td></tr></table></figure><h3 id="进入解压目录"><a href="#进入解压目录" class="headerlink" title="进入解压目录"></a>进入解压目录</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ cd node-v16.13.1-linux-x64</span><br></pre></td></tr></table></figure><h3 id="修改Linux环境变量"><a href="#修改Linux环境变量" class="headerlink" title="修改Linux环境变量"></a>修改Linux环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ vim /etc/profile.d/shangbaishuyao_configurationfile.sh</span><br><span class="line"><span class="meta">#</span><span class="bash">JAVA_HOME 加<span class="built_in">export</span>是对全局有效,相当于对外暴露一个接口</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">HADOOP_HOME</span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">KAFKA_HOME</span></span><br><span class="line">export KAFKA_HOME=/opt/module/kafka_2.11-2.4.1</span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">HIVE_HOME</span></span><br><span class="line">export HIVE_HOME=/opt/module/hive-3.1.2</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">MAVEN_HOME</span></span><br><span class="line">export MAVEN_HOME=/opt/module/maven-3.8.3</span><br><span class="line">export MAVEN_HOME</span><br><span class="line">export PATH=$PATH:$MAVEN_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> SPARK_HOME</span></span><br><span class="line">export SPARK_HOME=/opt/module/spark-3.0.0-hadoop3.2</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> NODE_JS</span></span><br><span class="line">export NODE_HOME=/opt/module/node-v16.13.1-linux-x64</span><br><span class="line">export PATH=$PATH:$NODE_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">HBASE_HOME</span></span><br><span class="line">export HBASE_HOME=/opt/module/hbase-2.0.5</span><br><span class="line">export PATH=$PATH:$HBASE_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">FLINK_HOME</span></span><br><span class="line">export FLINK_HOME=/opt/module/flink-1.14.3</span><br><span class="line">export PATH=$PATH:$FLINK_HOME/bin</span><br><span class="line"></span><br><span class="line">export HADOOP_CLASSPATH=`hadoop classpath`</span><br><span class="line">[shangbaishuyao@hadoop102 module]$</span><br></pre></td></tr></table></figure><h3 id="刷新环境变量"><a href="#刷新环境变量" class="headerlink" title="刷新环境变量"></a>刷新环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ source /etc/profile.d/shangbaishuyao_configurationfile.sh</span><br></pre></td></tr></table></figure><h3 id="查看是否安装成功"><a href="#查看是否安装成功" class="headerlink" title="查看是否安装成功"></a>查看是否安装成功</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]$ node -v</span><br><span class="line">v16.13.1</span><br></pre></td></tr></table></figure><h2 id="安装maven"><a href="#安装maven" class="headerlink" title="安装maven"></a>安装maven</h2><p>网上例子很多,此处略.</p><p>参考文章: <a href="https://www.cnblogs.com/freeweb/p/5241013.html">https://www.cnblogs.com/freeweb/p/5241013.html</a></p><h2 id="安装npm"><a href="#安装npm" class="headerlink" title="安装npm"></a>安装npm</h2><p>直接安装好node.js就有npm命令了,此处略. 有个问题是. 我在编译streamx的时候因为npm版本过低所以失败三次. 所以我升级了npm命令为: npm install -g npm</p><h2 id="安装JDK"><a href="#安装JDK" class="headerlink" title="安装JDK"></a>安装JDK</h2><p>网上例子很多,此处略.</p><h2 id="安装mysql"><a href="#安装mysql" class="headerlink" title="安装mysql"></a>安装mysql</h2><p>网上例子很多,此处略.</p><h3 id="进入mysql修改配置"><a href="#进入mysql修改配置" class="headerlink" title="进入mysql修改配置"></a>进入mysql修改配置</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 streamx-release-1.2.0]$ vim /etc/my.cnf</span><br><span class="line"></span><br><span class="line">#streamx</span><br><span class="line">port=3306</span><br><span class="line">bind-address=0.0.0.0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122153152.png" alt="blog: www.xubatian.cn"></p><h3 id="重新启动mysql"><a href="#重新启动mysql" class="headerlink" title="重新启动mysql"></a>重新启动mysql</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql restart</span><br></pre></td></tr></table></figure><h1 id="streamx源码编译"><a href="#streamx源码编译" class="headerlink" title="streamx源码编译"></a>streamx源码编译</h1><h2 id="从官网下载streamx稳定版本"><a href="#从官网下载streamx稳定版本" class="headerlink" title="从官网下载streamx稳定版本"></a>从官网下载streamx稳定版本</h2><p>官网地址: <a href="https://www.streamxhub.com/#">https://www.streamxhub.com/#</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_391.png" alt="blog: www.xubatian.cn"></p><p>github地址: <a href="https://github.com/streamxhub/streamx">https://github.com/streamxhub/streamx</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_390.png" alt="blog: www.xubatian.cn"></p><h2 id="修改streamx的相关版本"><a href="#修改streamx的相关版本" class="headerlink" title="修改streamx的相关版本"></a>修改streamx的相关版本</h2><p>修改为公司hadoop,flink,spark等相符合的大数据组件版本. </p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_392.png" alt="blog: www.xubatian.cn"></p><p>然后将压缩包上传到服务器上并解压.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ unzip streamx-release-1.2.0.zip -d /opt/module/</span><br></pre></td></tr></table></figure><p>进入解压目录编译源码,1.2.0默认flink版本为1.4,如需更改修改pom.xml再进行编译。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 streamx-release-1.2.0]$ mvn clean install -DskipTests -Denv=prod</span><br></pre></td></tr></table></figure><p>等待……</p><p>最后成功!</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_376.png" alt="blog: www.xubatian.cn"></p><h2 id="进行解压编译成功后的压缩包"><a href="#进行解压编译成功后的压缩包" class="headerlink" title="进行解压编译成功后的压缩包"></a>进行解压编译成功后的压缩包</h2><p>编译后在/opt/module/streamx-release-1.2.0/streamx-console/streamx-console-service/target目录会有对应tar包</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_380.png" alt="blog: www.xubatian.cn"></p><h2 id="进入解压后的目录"><a href="#进入解压后的目录" class="headerlink" title="进入解压后的目录"></a>进入解压后的目录</h2><p>进入到对应目录，修改配置文件，需要使用mysql地址来存储数据。</p><p>注意：数据库不会自动创建，需要手动创建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ cd streamx-console-service-1.2.0/</span><br><span class="line">[shangbaishuyao@hadoop102 streamx-console-service-1.2.0]$ vim conf/application.yml</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_381.png" alt="blog: www.xubatian.cn"></p><p>手动创建streamx的数据库</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/20220122153945.png" alt="blog: www.xubatian.cn"></p><h2 id="启动streamx"><a href="#启动streamx" class="headerlink" title="启动streamx"></a>启动streamx</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 streamx-release-1.2.0]$ bin/startup.sh </span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_382.png" alt="blog: www.xubatian.cn"></p><h2 id="查看是否启动成功"><a href="#查看是否启动成功" class="headerlink" title="查看是否启动成功"></a>查看是否启动成功</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_384.png" alt="blog: www.xubatian.cn"></p><p>如果没有streamXconsole说明出现错误. 去logs里面查看具体错误.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_383.png" alt="blog: www.xubatian.cn"></p><h2 id="使用浏览器访问streamx"><a href="#使用浏览器访问streamx" class="headerlink" title="使用浏览器访问streamx"></a>使用浏览器访问streamx</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_385.png" alt="blog: www.xubatian.cn"></p><p>账号为: admin 密码为: streamx</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_386.png" alt="blog: www.xubatian.cn"></p><h2 id="编译好的streamx存放地址"><a href="#编译好的streamx存放地址" class="headerlink" title="编译好的streamx存放地址"></a>编译好的streamx存放地址</h2><p>streamx1.12.0 源码默认配置 直接编译 编译后的压缩包为: streamx-console-service-1.2.0-Linux-bin.tar.gz<br>streamx1.12.0 源码,修改hadoop版本为3.1.3, flink版本为1.14.3 编译后的压缩包为: streamx-console-service-1.2.0-macOS-bin.tar.gz<br>两个压缩包都方式这里,链接: <a href="https://pan.baidu.com/s/1M4R0K3rOzNZOdilnJiduFw">https://pan.baidu.com/s/1M4R0K3rOzNZOdilnJiduFw</a> 提取码: 2olc</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;遇到问题，改变苛求别人的惯性，重新塑造思考问题的方式。换个角度看世界，换个方向看问题，就会豁然开朗。——人民日报&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="streamx" scheme="http://xubatian.cn/tags/streamx/"/>
    
  </entry>
  
  <entry>
    <title>Flink部署</title>
    <link href="http://xubatian.cn/Flink%E9%83%A8%E7%BD%B2/"/>
    <id>http://xubatian.cn/Flink%E9%83%A8%E7%BD%B2/</id>
    <published>2022-01-20T21:39:16.000Z</published>
    <updated>2022-01-23T02:58:21.611Z</updated>
    
    <content type="html"><![CDATA[<p>所处的位置不同，看到的风景和思考的问题也有所不同。——人民日报</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_360.jpg" alt="blog: www.xubatian.cn"></p><h1 id="Standalone模式Flink自带的"><a href="#Standalone模式Flink自带的" class="headerlink" title="Standalone模式Flink自带的"></a>Standalone模式Flink自带的</h1><p>首先运行我们standalone的环境    </p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_361.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_362.png" alt="blog: www.xubatian.cn"></p><p>解压缩  <strong>flink-1.7.2-bin-hadoop27-scala_2.11.tgz</strong>(如果你两个模式都想试一下就这个压缩包,实际上你要真正搭建独立模式只需要flink-1.7.2后面不需要接hadoop27-scala_2.11的压缩包)，进入conf目录中。</p><p>1）修改 flink/conf/flink-conf.yaml 文件：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_363.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_364.png"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_365.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">为了更好地看清图片内容,我复制出来这两行:</span><br><span class="line"><span class="meta">#</span><span class="bash"> The heap size <span class="keyword">for</span> the TaskManager JVM</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#整个TaskManager的内存大小</span></span></span><br><span class="line">taskmanager.heap.size: 1024m    </span><br><span class="line"></span><br><span class="line">下面是,这一个G的内存可以同时允许你同时并行运行多少个Task,每个task会占用一个插槽</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#插槽即可理解为流水槽,你的水从流水槽上流出去,这个slot就好比是两个木板上的流水槽,这个slot是TaskManager的</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#TaskManager说白了就暂时是我们的worker节点,实际上来说你也可以看成是executor节点也可以.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#但是这里和saprk的standalone去类比的话这就矛盾了,因为flink的TaskManager在这里有相当于executor,也相当</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#于worker.   spark的standalone模式中,一个worker下面可以有多个executor,每一个executor并行可以运行两个</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#以上的任务吗?可以的. 所以我们spark独立模式理解起来有点难度.他是一个worker下面有一个executor,executor下面</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#又可能会运行多个task,也可能只运行一个task.这对初学者不好理解.而这里我们的flink将他简化了,他把worker这</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 一层去掉了,worker这一层就是taskmanager,就还好比是executor一样.这个executor上到底运行了几个task是由</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># slot来决定的.即由插槽来决定的. 这里的插槽设置为1,表示每一个TaskManager上有多少个slot.这里是有1个slot.这就意味着</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#我这个taskmanager上只能同时运行一个任务.你如果想要增加我们的并行度,就必须修改插槽数量.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> taskmanager.numberOfTaskSlots: 1</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改为3个插槽,表示每一个TaskManager里面有三个插槽,这三个插槽可以同时允许运行三个并行度,这三个并行度可以是不一样的广告</span></span><br><span class="line">taskmanager.numberOfTaskSlots: 3</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The parallelism used <span class="keyword">for</span> programs that did not specify and other parallelism.</span></span><br></pre></td></tr></table></figure><p>就只需要配置下面两个文件就可以了</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_366.png" alt="blog: www.xubatian.cn"></p><p>2）修改 /conf/slave文件：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_367.png" alt="blog: www.xubatian.cn"></p><p>3）分发给另外两台机子：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_368.png" alt="blog: www.xubatian.cn"></p><p>4）启动：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_369.png" alt="blog: www.xubatian.cn"></p><p>访问<a href="http://localhost:8081可以对flink集群和任务进行监控管理">http://localhost:8081可以对flink集群和任务进行监控管理</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_370.png" alt="blog: www.xubatian.cn"></p><h2 id="提交任务"><a href="#提交任务" class="headerlink" title="提交任务"></a>提交任务</h2>]]></content>
    
    
    <summary type="html">&lt;p&gt;所处的位置不同，看到的风景和思考的问题也有所不同。——人民日报&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink简介</title>
    <link href="http://xubatian.cn/Flink%E7%AE%80%E4%BB%8B/"/>
    <id>http://xubatian.cn/Flink%E7%AE%80%E4%BB%8B/</id>
    <published>2022-01-19T07:57:22.000Z</published>
    <updated>2022-01-23T02:58:21.611Z</updated>
    
    <content type="html"><![CDATA[<p>一旦时机到来，我们要能迅速地发现时机、把握时机，不犹豫，不踌躇，乘风而起，破万里浪。——人民日报</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_347.jpg" alt="blog: www.xubatian.cn"></p><h1 id="Flink简介"><a href="#Flink简介" class="headerlink" title="Flink简介"></a>Flink简介</h1><p>Flink其中一半是java语言开发的,另一半是scala语言开发的;spark的源码是scala语言开发的.但是scala是基于jvm的,所有的语法直接照着jvm调就可以了    </p><p>大数据中比较重要的框架,Hadoop(mapreduce),Spark,Flink,只有这三个才叫计算框架.<br>Flink,不管你是开发流式计算也好,还是做离线计算也好,还是做批量计算也好,他就是那一套代码,他不像Spark,Spark开发流式计算用的是SparkStreaming,用批量计算使用RDD,sparkCore<br>Flink有一套代码,但是他有很多套API<br>Flink有中文版,你发现Apache顶级项目有中文版的有那几个?他们的特点是什么?<br>因为他们50%以上的代码都是由中国人贡献的,Flink也一样,50%以上的代码是由中国国内贡献的</p><p>Flink和spark类似,他们的数据都是在内存中直接计算的,甚至他的状态都是存在内存中的 </p><p>Flink是默认就是有状态的计算,Flink中没有无状态这个说法但是spark中有无状态这种说法</p><p>flink的kappa架构怎么实现: <a href="https://www.jianshu.com/p/5f5736656bd5">https://www.jianshu.com/p/5f5736656bd5</a><br>Flink数据倾斜问题: <a href="https://www.cnblogs.com/qiu-hua/p/14056747.html">https://www.cnblogs.com/qiu-hua/p/14056747.html</a><br>                  <a href="https://www.cnblogs.com/Christbao/p/13569616.html">https://www.cnblogs.com/Christbao/p/13569616.html</a></p><p>但是实际上大数据量经常出现，一个 Flink 作业包含 200 个 Task 节点，其中有 199 个节点可以在很短的时间内完成计算。但是有一个节点执行时间远超其他结果，并且随着数据量的持续增加，导致该计算节点挂掉，从而整个任务失败重启。<br>我们可以在 Flink 的管理界面中看到任务的某一个 Task 数据量远超其他节点。<br>Flink 任务出现数据倾斜的直观表现是任务节点频繁出现反压，但是增加并行度后并不能解决问题；部分节点出现 OOM 异常，是因为大量的数据集中在某个节点上，导致该节点内存被爆，任务失败重启。</p><h2 id="初识Flink"><a href="#初识Flink" class="headerlink" title="初识Flink"></a>初识Flink</h2><p>Flink起源于Stratosphere项目，Stratosphere是在2010~2014年由3所地处柏林的大学和欧洲的一些其他的大学共同进行的研究项目，2014年4月Stratosphere的代码被复制并捐赠给了Apache软件基金会，参加这个孵化项目的初始成员是Stratosphere系统的核心开发人员，2014年12月，Flink一跃成为Apache软件基金会的顶级项目。<br>在德语中，Flink一词表示快速和灵巧，项目采用一只松鼠的彩色图案作为logo，这不仅是因为松鼠具有快速和灵巧的特点，还因为柏林的松鼠有一种迷人的红棕色，而Flink的松鼠logo拥有可爱的尾巴，尾巴的颜色与Apache软件基金会的logo颜色相呼应，也就是说，这是一只Apache风格的松鼠。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_348.png" alt="blog: www.xubatian.cn">Flink项目的理念是：“Apache Flink是为分布式、高性能、随时可用以及准确的流处理应用程序打造的开源流处理框架”。</p><p>Apache Flink是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。Flink被设计在所有常见的集群环境中运行，以内存执行速度和任意规模来执行计算。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img/www.xubatian.cn_349.png" alt="blog: www.xubatian.cn"></p><p>博主解析图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_350.png" alt="blog: www.xubatian.cn"></p><h2 id="Flink的重要特点"><a href="#Flink的重要特点" class="headerlink" title="Flink的重要特点"></a>Flink的重要特点</h2><h3 id="事件驱动型-Event-driven"><a href="#事件驱动型-Event-driven" class="headerlink" title="事件驱动型(Event-driven)"></a>事件驱动型(Event-driven)</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_351.png" alt="blog: www.xubatian.cn"></p><p>事件驱动型应用是一类具有状态的应用，它从一个或多个事件流提取数据(其实就是他可以从多个源中读取数据)，并根据到来的事件触发计算(就是来一条数据立马计算不等待(计算是根据业务来的,可以做聚合计算等))、状态更新或其他外部动作。比较典型的就是以kafka为代表的消息队列几乎都是事件驱动型应用。</p><p>与之不同的就是SparkStreaming微批次，如图：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_352.png" alt="blog: www.xubatian.cn"></p><p>事件驱动型：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_353.png" alt="blog: www.xubatian.cn"></p><p>博主解析图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_354.png" alt="blog: www.xubatian.cn"></p><h3 id="事件驱动型应用的优势？"><a href="#事件驱动型应用的优势？" class="headerlink" title="事件驱动型应用的优势？"></a>事件驱动型应用的优势？</h3><p>事件驱动型应用无须查询远程数据库，本地数据访问使得它具有更高的吞吐和更低的延迟。而由于定期向远程持久化存储的 checkpoint 工作可以异步、增量式完成，因此对于正常事件处理的影响甚微。事件驱动型应用的优势不仅限于本地数据访问。传统分层架构下，通常多个应用会共享同一个数据库，因而任何对数据库自身的更改（例如：由应用更新或服务扩容导致数据布局发生改变）都需要谨慎协调。反观事件驱动型应用，由于只需考虑自身数据，因此在更改数据表示或服务扩容时所需的协调工作将大大减少</p><h3 id="流与批的世界观"><a href="#流与批的世界观" class="headerlink" title="流与批的世界观"></a>流与批的世界观</h3><p>有界和无界分别对应的就是批处理和流处理<br><strong>批处理</strong>(就是所谓的有界数据)的特点是有界、持久、大量，非常适合需要访问全套记录才能完成的计算工作，一般用于离线统计。<br><strong>流处理</strong>(就是所谓的无界数据)的特点是无界、实时,  无需针对整个数据集执行操作，而是对通过系统传输的每个数据项执行操作，一般用于实时统计。<br>在spark的世界观中，一切都是由批次组成的，离线数据是一个大批次，而实时数据是由一个一个无限的小批次组成的。<br>而在flink的世界观中，一切都是由流组成的，离线数据是有界限的流，实时数据是一个没有界限的流，这就是所谓的有界流和无界流。</p><p><strong>无界数据流</strong>：无界数据流有一个开始但是没有结束，它们不会在生成时终止并提供数据，必须连续处理无界流，也就是说必须在获取后立即处理event。对于无界数据流我们无法等待所有数据都到达，因为输入是无界的，并且在任何时间点都不会完成。处理无界数据通常要求以特定顺序（例如事件发生的顺序）获取event，以便能够推断结果完整性。</p><p><strong>有界数据流</strong>：有界数据流有明确定义的开始和结束，可以在执行任何计算之前通过获取所有数据来处理有界流，处理有界流不需要有序获取，因为可以始终对有界数据集进行排序，有界流的处理也称为批处理。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_355.png" alt="blog: www.xubatian.cn"></p><p>这种以流为世界观的架构，获得的最大好处就是具有极低的延迟。</p><p>博主解析图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_356.png" alt="blog: www.xubatian.cn"></p><h3 id="分层API"><a href="#分层API" class="headerlink" title="分层API"></a>分层API</h3><p>(对于我们学flink来说,我们三层都必须得会,经常用到的是中间那层DataStream API)<br>Flink他本质上把批量的数据和流数据都看成是流了,所以他本质上是流处理,所以他也可以做批处理,他和saprk相反,spark把所有的数据都看成是批处理了,但是spark也是可以做流处理的<br>记住: DataStream API做流处理,流处理是无界的  DataSetAPI是做批处理是有界的    </p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_357.png" alt="blog: www.xubatian.cn"></p><p>分层API是Flink根据抽象程度,提供的三种不同的API,所谓抽象程度就是看你封装的程度,如果你不怎么封装,那就是底层的API,稍微封装一下就叫中间的API,封装的很厉害就叫高级API.</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_358.png" alt="blog: www.xubatian.cn"></p><p>最底层级的抽象仅仅提供了有状态流，它将通过过程函数（Process Function）被嵌入到DataStream API中。底层过程函数（Process Function） 与 DataStream API 相集成，使其可以对某些特定的操作进行底层的抽象，它允许用户可以自由地处理来自一个或多个数据流的事件，并使用一致的容错的状态。除此之外，用户可以注册事件时间并处理时间回调，从而使程序可以处理复杂的计算。</p><p>实际上，大多数应用并不需要上述的底层抽象，而是针对核心API（Core APIs） 进行编程，比如DataStream API（有界或无界流数据）以及DataSet API（有界数据集）。这些API为数据处理提供了通用的构建模块，比如由用户定义的多种形式的转换（transformations），连接（joins），聚合（aggregations），窗口操作（windows）等等。DataSet API 为有界数据集提供了额外的支持，例如循环与迭代。这些API处理的数据类型以类（classes）的形式由各自的编程语言所表示。</p><p>Table API 是以表为中心的声明式编程，其中表可能会动态变化（在表达流数据时）。Table API遵循（扩展的）关系模型：表有二维数据结构（schema）（类似于关系数据库中的表），同时API提供可比较的操作，例如select、project、join、group-by、aggregate等。Table API程序声明式地定义了什么逻辑操作应该执行，而不是准确地确定这些操作代码的看上去如何。</p><p>尽管Table API可以通过多种类型的用户自定义函数（UDF）进行扩展，其仍不如核心API更具表达能力，但是使用起来却更加简洁（代码量更少）。除此之外，Table API程序在执行之前会经过内置优化器进行优化。<br>你可以在表与 DataStream/DataSet 之间无缝切换，以允许程序将 Table API 与 DataStream 以及 DataSet 混合使用。</p><p>Flink提供的最高层级的抽象是 SQL 。这一层抽象在语法与表达能力上与 Table API 类似，但是是以SQL查询表达式的形式表现程序。SQL抽象与Table API交互密切，同时SQL查询可以直接在Table API定义的表上执行。</p><p>目前Flink作为批处理还不是主流，不如Spark成熟，所以DataSet使用的并不是很多。Flink Table API和Flink SQL也并不完善，大多都由各大厂商自己定制。所以我们主要学习DataStream API的使用。实际上Flink作为最接近Google DataFlow模型的实现，是流批统一的观点，所以基本上使用DataStream就可以了。<br>Flink几大模块<br>Flink Table &amp; SQL(还没开发完)<br>Flink Gelly(图计算)<br>Flink CEP(复杂事件处理)</p><p>Flink官方给你提供的连接器,虽然都是flink的,但是他来自两个不同的库,一个来自flink的,一个来自Bahir的</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_359.png" alt="blog: www.xubatian.cn"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;一旦时机到来，我们要能迅速地发现时机、把握时机，不犹豫，不踌躇，乘风而起，破万里浪。——人民日报&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Flink" scheme="http://xubatian.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>SparkCore之RDD编程的编程模型</title>
    <link href="http://xubatian.cn/SparkCore%E4%B9%8BRDD%E7%BC%96%E7%A8%8B%E7%9A%84%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/"/>
    <id>http://xubatian.cn/SparkCore%E4%B9%8BRDD%E7%BC%96%E7%A8%8B%E7%9A%84%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/</id>
    <published>2022-01-19T06:50:36.000Z</published>
    <updated>2022-01-23T02:58:21.755Z</updated>
    
    <content type="html"><![CDATA[<p>闲适因为忙碌才获得意义。如果摸鱼成为常态，放松就失去了意义；如果划水占据人生，幸福就会失去方向。               ——人民日报    </p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_340.jpg" alt="blog: www.xubatian.cn"></p><h1 id="RDD编程"><a href="#RDD编程" class="headerlink" title="RDD编程"></a>RDD编程</h1><p>创建RDD ,RDD的转换, RDD的输出</p><h2 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h2><p>在spark中无论是Transformations方法还是Actions方法,我们都要把他们称作算子</p><p>在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的Transformations(转换)定义RDD之后，就可以调用Actions(行动)触发RDD的计算，Action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到Action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。 </p><p>要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行。Driver中定义了一个或多个RDD，并调用RDD上的Action，Executor则执行RDD分区计算任务。</p><p>Actions(行动)算子会真正的去触发job去执行<br>Transformation(转换)算子懒执行<br>所以返回值是RDD类型的是Transformation算子,返回值非RDD类型就是Action算子</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_341.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_342.png" alt="blog: www.xubatian.cn"></p><h2 id="RDD的创建"><a href="#RDD的创建" class="headerlink" title="RDD的创建"></a>RDD的创建</h2><p>在Spark中创建RDD的创建方式可以分为三种：<br>从scala集合中创建RDD；<br>从外部存储创建RDD；<br>从其他RDD创建(这个其实讲的就是转换)。</p><h3 id="从集合中创建"><a href="#从集合中创建" class="headerlink" title="从集合中创建"></a>从集合中创建</h3><p>从集合中创建RDD，Spark主要提供了两种函数：parallelize(并行化)和makeRDD(创建RDD)</p><p>1）使用parallelize()从集合创建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(Array(1,2,3,4,5,6,7,8))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br></pre></td></tr></table></figure><p>2）使用makeRDD()从集合创建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd1 = sc.makeRDD(Array(1,2,3,4,5,6,7,8))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at &lt;console&gt;:24</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_343.png" alt="blog: www.xubatian.cn"></p><h3 id="由外部存储系统的数据集创建"><a href="#由外部存储系统的数据集创建" class="headerlink" title="由外部存储系统的数据集创建"></a>由外部存储系统的数据集创建</h3><p>包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等，</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd2= sc.textFile(&quot;hdfs://hadoop102:9000/RELEASE&quot;)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] = hdfs://hadoop102:9000/RELEASE MapPartitionsRDD[4] at textFile at &lt;console&gt;:24</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="从其他RDD创建"><a href="#从其他RDD创建" class="headerlink" title="从其他RDD创建"></a>从其他RDD创建</h3><p>下面都是…此处略</p><h2 id="RDD的转换"><a href="#RDD的转换" class="headerlink" title="RDD的转换"></a>RDD的转换</h2><p>RDD整体上分为Value类型和Key-Value类型,  K-V形式其实也是value类型<br>eg:   (k,(k,v))是value, 是不是kv? 是,只不过value类型是二元组.<br>如果是单个值, 是value, 如果是形如二元组是K,V.  k,v形式是value类型, 我把整个k,v元组当成整体来看,他就是value类型.  他们两是包含的关系. 所有的RDD都可以看做是value类型, 只过不特殊的我们拎出来,如k-v等等</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_344.png" alt="blog: www.xubatian.cn"></p><h3 id="Value类型"><a href="#Value类型" class="headerlink" title="Value类型"></a>Value类型</h3><p>什么叫做value类型呢?<br>因为他里面传的函数都是操作当前这个RDD里面的元素. 可能是单个元素, 可能是一个分区里面的元素.但是他操作的是里面的数据.<br>什么叫双value类型呢?<br>双value类型它里面传的参数是任意一个RDD.<br>Eg : RDD1.调用一个算子(RDD2)<br>以为之前提过, scala也好,spark也好,他是面向数据处理的. 那这个双value类型就是数学里面的,集合之间的关系. 集合里面有哪些关系呢? 并集, 交叉 ,笛卡尔集</p><h4 id="map-func-案例"><a href="#map-func-案例" class="headerlink" title="map(func)案例"></a>map(func)案例</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. 作用：返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成</span><br><span class="line">2. 需求：创建一个1-10数组的RDD，将所有元素*2形成新的RDD</span><br></pre></td></tr></table></figure><p>  (1)   创建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var source  = sc.parallelize(1 to 10)</span><br><span class="line"></span><br><span class="line">source: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at &lt;console&gt;:24</span><br></pre></td></tr></table></figure><p>（2）打印</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; source.collect()</span><br><span class="line"></span><br><span class="line">res7: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</span><br></pre></td></tr></table></figure><p>（3）将所有元素*2</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val mapadd = source.map(_ * 2)</span><br><span class="line">这个map是算子</span><br><span class="line"></span><br><span class="line">mapadd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at map at &lt;console&gt;:26</span><br></pre></td></tr></table></figure><p>（4）打印最终结果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; mapadd.collect()</span><br><span class="line"></span><br><span class="line">res8: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_345.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_346.png" alt="blog: www.xubatian.cn"></p><h3 id="双Value类型交互"><a href="#双Value类型交互" class="headerlink" title="双Value类型交互"></a>双Value类型交互</h3><p>以后慢慢写……</p><h3 id="Key-Value类型"><a href="#Key-Value类型" class="headerlink" title="Key-Value类型"></a>Key-Value类型</h3><p>以后慢慢写……</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;闲适因为忙碌才获得意义。如果摸鱼成为常态，放松就失去了意义；如果划水占据人生，幸福就会失去方向。               ——人民日报    &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="spark" scheme="http://xubatian.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>SparkCore之RDD概述</title>
    <link href="http://xubatian.cn/SparkCore%E4%B9%8BRDD%E6%A6%82%E8%BF%B0/"/>
    <id>http://xubatian.cn/SparkCore%E4%B9%8BRDD%E6%A6%82%E8%BF%B0/</id>
    <published>2022-01-19T06:22:21.000Z</published>
    <updated>2022-01-23T02:58:21.728Z</updated>
    
    <content type="html"><![CDATA[<p>仰观天宇，时间更加深邃；俯身耕耘，未来无限可能                  ——人民日报    </p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_331.jpg" alt="blog: www.xubatian.cn"></p><p>我们Spark中的stage是按照shuffle来切的.</p><h1 id="RDD概述"><a href="#RDD概述" class="headerlink" title="RDD概述"></a>RDD概述</h1><h2 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h2><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象。代码中是一个抽象类，它代表一个弹性的(分区)、不可变(元素)、可分区、里面的元素可并行计算的集合。</p><p>RDD是抽象类</p><h2 id="RDD的属性"><a href="#RDD的属性" class="headerlink" title="RDD的属性"></a>RDD的属性</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_332.png" alt="blog: www.xubatian.cn"></p><p>（1）一组分区（Partition）,切片和分区是一样的，即数据集的基本组成单位；<br>（2）一个计算每个分区的函数；<br>（3）RDD之间的依赖关系；<br>（4）一个Partitioner，即RDD的分片函数；<br>（5）一个列表，存储存取每个Partition的优先位置（preferred location）。</p><h2 id="RDD特点"><a href="#RDD特点" class="headerlink" title="RDD特点"></a>RDD特点</h2><p>RDD表示只读(不可变性)的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。</p><h2 id="弹性"><a href="#弹性" class="headerlink" title="弹性"></a>弹性</h2><p>存储的弹性：内存与磁盘的自动切换；<br>容错的弹性：数据丢失可以自动恢复；<br>计算的弹性：计算出错重试机制；<br>分片的弹性：可根据需要重新分片。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_333.png" alt="blog: www.xubatian.cn"></p><h2 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h2><p>RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute(计算)函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute(计算)函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute(计算)函数是执行转换逻辑将其他RDD的数据进行转换</p><h2 id="只读"><a href="#只读" class="headerlink" title="只读"></a>只读</h2><p>RDD是只读的（元素不可变），要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。<br>由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了。</p><p>RDD的操作算子包括两类，<br>一类叫做Transformations(转换)，它是用来将RDD进行转化，构建RDD的血缘关系；<br>另一类叫做Actions(行动)，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。</p><h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p>RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，</p><p>依赖包括两种，<br>一种是窄依赖，RDDs之间分区是一一对应的，<br>另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。(一对一,多对多指的是分区) </p><p>这里的一对一,一对多指的是分区</p><p>注意: 依赖和shuffle有关系</p><p>原图:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_334.png" alt="blog: www.xubatian.cn"></p><p>博主解读:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_336.png" alt="blog: www.xubatian.cn"></p><p>父RDD中的全部数据被某一个子RDD的某个分区全部拥有我们叫窄依赖</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_337.png" alt="blog: www.xubatian.cn"></p><h2 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h2><p>如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_338.png" alt="blog: www.xubatian.cn"></p><h2 id="CheckPoint"><a href="#CheckPoint" class="headerlink" title="CheckPoint"></a>CheckPoint</h2><p>切断血缘关系后可以从CheckPoint中拿数据</p><p>虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。</p><p>checkPoint将数据持久化了之后他就会切断血缘关系, 因为他认为你持久化到文件当中后这个数据就不会丢了,这个依赖关系就不需要了,就切断了. 因为你是文件嘛, 而且默认一般存在hdfs中,hdfs又默认有三个副本. 所以他觉得你数据不会丢了. 既然不会丢了,依赖关系就不要了, 因为依赖关系就是防止你数据丢了重新计算做数据恢复的.</p><p>但是你缓存到内存当中,你不能将依赖切断, 因为内存当中数据可能会掉的.他可能还要用这个依赖关系重新做计算的.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_339.png" alt="blog: www.xubatian.cn"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;仰观天宇，时间更加深邃；俯身耕耘，未来无限可能                  ——人民日报    &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="spark" scheme="http://xubatian.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark基础解析</title>
    <link href="http://xubatian.cn/spark%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90/"/>
    <id>http://xubatian.cn/spark%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90/</id>
    <published>2022-01-19T03:09:24.000Z</published>
    <updated>2022-02-07T13:27:08.253Z</updated>
    
    <content type="html"><![CDATA[<p> 征途漫漫，惟有奋斗；梦想成真，惟有实干。                     ——人民日报</p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_308.jpg" alt="blog: www.xubatian.cn"></p><h1 id="Spark概述"><a href="#Spark概述" class="headerlink" title="Spark概述"></a>Spark概述</h1><h2 id="什么是Spark"><a href="#什么是Spark" class="headerlink" title="什么是Spark"></a>什么是Spark</h2><p>1、定义<br>Spark是一种基于内存的快速、通用、可扩展的大数据分析引擎。<br>2、历史<br>2009年诞生于加州大学伯克利分校AMPLab, 项目采用Scala编写;<br>2010年开源;<br>2013年6月成为Apache孵化项目；<br>2014年2月成为Apache顶级项目。</p><p><strong>xubatian解析:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在Scala当中的map ，reduce这些方法在spark当中同样也有这些方法。</span><br><span class="line"></span><br><span class="line">要知道的是Scala的这些方法是面向的是集合当中做处理的，面向的是数据集合，数组等等这些操作。而spark是面对的是海量数据处理的，他面向的数据分析的什么东西呢？叫分布式数据集。Scala处理的数据在一个集合当中，而spark处理的数据可能跨了很多台机器。因为他是用hdfs来存储的。而hdfs存储的时候不是把所有的数据都放在一台机器上的。而是很多台机器上都有。而spark就是同时处理很多台机器上的事情。所以Scala和spark都有map方法，可能功能上都是一样的，都是把里面每一个元素做一个转变。但是他们面向的数据集不一样，spark面向的数据集时RDD。</span><br><span class="line"></span><br><span class="line">SparkStream和kafka做对接, 你kafka过来的还是一行一行的数据.虽然封装成了Dstream,但是他还是一行一行的数据. 你要做分析转换输出等</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Spark内置模块"><a href="#Spark内置模块" class="headerlink" title="Spark内置模块"></a>Spark内置模块</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_309.png" alt="blog: www.xubatian.cn"></p><p><strong>Spark Core</strong>：实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core中还包含了对弹性分布式数据集(Resilient Distributed DataSet，简称RDD)的API定义；</p><p><strong>Spark SQL</strong>：是Spark用来操作结构化数据的程序包。通过Spark SQL，我们可以使用 SQL或者Apache Hive版本的SQL方言(HQL)来查询数据。Spark SQL支持多种数据源，比如Hive表、Parquet以及JSON等；</p><p><strong>Spark Streaming</strong>：是Spark提供的对实时数据进行流式计算的组件。提供了用来操作数据流的API，并且与Spark Core中的 RDD API高度对应；</p><p><strong>Spark MLlib</strong>：提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能；</p><p>集群管理器：Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计 算。为了实现这样的要求，同时获得最大灵活性，Spark支持在各种集群管理器(Cluster Manager)上运行，包括Hadoop YARN、Apache Mesos，以及Spark自带的一个调度器，叫作独立调度器。</p><p> Spark得到了众多大数据公司的支持，这些公司包括Hortonworks、IBM、Intel、Cloudera、MapR、Pivotal、百度、腾讯、京东、携程、优酷土豆。当前百度的Spark已应用于大搜索、直达号、百度大数据等业务；阿里利用GraphX构建了大规模的图计算和图挖掘系统，实现了很多生产系统的推荐算法；腾讯Spark集群达到8000台的规模，是当前已知的世界上最大的Spark集群。</p><h2 id="Spark特点-DAG"><a href="#Spark特点-DAG" class="headerlink" title="Spark特点(DAG)"></a>Spark特点(DAG)</h2><ol><li><p><strong>快</strong><br>与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。计算的中间结果是存在于内存中的。</p></li><li><p><strong>易用</strong><br>Spark支持Java、Python和Scala的API, 还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的Shell,可以非常方便地在这些Shell中使用Spark集群来验证解决问题的方法。</p></li></ol><ol start="3"><li><strong>通用</strong></li></ol><p>  Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理(Spark Streaming) 、机器学习 (Spark MLlib)和图计算(GraphX).这些不同类型的处理都可以在同一个应用中无缝使用。减少了开发和维护的人力成本和部署平台的物力成本。</p><ol start="4"><li><strong>兼容性</strong><br>Spark可以非常方便地与其他的开源产品进行融合。比如, Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。</li></ol><h2 id="博主补充"><a href="#博主补充" class="headerlink" title="博主补充"></a>博主补充</h2><p>Spark实现了高效的DAG执行引擎。DAG是有向无环图即多个任务之间通过内存来做交互。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_310.png" alt="blog: www.xubatian.cn"></p><p>多个任务之间直接通过内存来做交互。它可以直接将他们串联起来。而我们之前可能需要用到MR1，MR2，MR3进行落盘操作。</p><p>另一个spark快的原因是：<br>                对于MR来说，你整个Map任务和Reduce任务是计算的核心。而map任务和reduce任务你用jps能看到进程吗？不能。也看不到spark当中的maptask和reducetask。这也是spark比mr快的一个比较核心的一个点。一个呢，对于hadoop来说他是使用进程来调度的。我启动一个单独的task都是一个单独的进程。你能jps看到的是进程号。而在spark当中启动一个任务他是线程。你说是调用进程快呢还是线程快呢？我线程我可以事先启动好一个线程池。我要的时候去取一下就完了。嘿嘿~~阴险。这也是spark比mr快的一个很重要的一个点。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_311.png" alt="blog: www.xubatian.cn"></p><h1 id="Spark运行模式"><a href="#Spark运行模式" class="headerlink" title="Spark运行模式"></a>Spark运行模式</h1><p>重点local模式,和yarn模式,为什么不掌握spark自己的呢?<br>第一重点:<br>本地模式主要是用于教学和测试.公司当中的一些demo级别的测试也是用本地模式.因为他相对来说,资源消耗等等都比较简单一点.local模式相对来说简单一点.不需要启动很多进程去占用额外的资源<br>第二重点:<br>Yarn模式,这个是应用于公司的生产环境.为什么公司的生产环境会用到yarn模式呢?稍微结合standlone模式思考一下.standlone是spark他自己来管理这一套资源.而公司当中其实并不愿意采用standlone模式,而是采用yarn模式比较多.为什么公司当中不用呢?既然spark自己有一套独立的调度资源系统,那你说他和standlone模式兼容性更好还是yarn模式兼容性更好呢?肯定是standlone。因为这是他自己的。那为什么兼容性更好却不用呢？说明他两又有区别，而且区别在公司当中standlone模式比yarn模式更严重一点。我们的mapreduce是yarn分配资源的，我们学过的tez也是yarn分配资源的。Storm也是yarn分配资源的。如果说我们spark也用yarn分配调度资源有什么好处呢？是不是统一的资源调度呢呀！<br>如果说我们spark当中使用独立的一套呢？会产生资源争抢。因为yarn认为这块资源是我独有的，而spark的standlone也认为这块资源是我独有的，那我分配的时候有可能两个任务就冲突了。但是我交给某一个人统一的安排这个资源，不行就等待，就不会产生资源争抢的问题。这个就是公司当中用yarn模式做的一个点。<br>Yarn模式在生产环境中用的比较多。主要体现在中小型公司。他整个集群资源规模不大，他整个MR任务，spark任务，或者其他任务都是运行在同一套资源上的。如果告诉你你公司比较有钱，你的spark集群是独立的spark集群。那么我们就用spark的standlone模式。<br>但是绝大多数公司他的整个集群都是资源混布的。这就比较依赖与统一的资源管理了。这样就不至于产生资源争抢。<br>我们所讲的几种模式都是在Liunx环境当中开一个shell窗口。类似于之前写的hive，在里面写sql操作。但是实际生产当中，他更多的对于spark来说还是要写代码，打jar包来运行。所以最后是我们写的一个wordcount程序，打jar包来提交到集群上去运行。</p><h2 id="Spark安装地址"><a href="#Spark安装地址" class="headerlink" title="Spark安装地址"></a>Spark安装地址</h2><p>1．官网地址<br><a href="http://spark.apache.org/">http://spark.apache.org</a><br>2．文档查看地址<br><a href="https://spark.apache.org/docs">https://spark.apache.org/docs</a><br>3．下载地址<br><a href="https://spark.apache.org/downloads.html">https://spark.apache.org/downloads.html</a></p><h2 id="集群角色"><a href="#集群角色" class="headerlink" title="集群角色"></a>集群角色</h2><p>注意: 不是standlone模式就没有Master和Worker</p><h3 id="Master和Worker"><a href="#Master和Worker" class="headerlink" title="Master和Worker"></a>Master和Worker</h3><p>Master和Worker:   负责资源的,具体运行,哪个的代码他不管.用户客户端提交代码后,你告诉我分配3G内存,2核CPU我给你分配就完了.    Master和Worker:   是standlone模式所独有的. yarn模式没有. Applicationmaster提交任务前,master和work一定是启动状态.</p><p>1）Master</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Spark特有资源调度系统的Leader。掌管着整个集群的资源信息，类似于Yarn框架中的ResourceManager，主要功能：</span><br><span class="line">（1）监听Worker，看Worker是否正常工作；</span><br><span class="line">（2）Master对Worker、Application等的管理(接收Worker的注册并管理所有的Worker，接收Client提交的application，调度等待的Application并向Worker提交)。</span><br></pre></td></tr></table></figure><p>2）Worker</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Spark特有资源调度系统的Slave(奴隶,随从)，有多个。每个Slave掌管着所在节点的资源信息，类似于Yarn框架中的NodeManager，主要功能：</span><br><span class="line">（1）通过RegisterWorker注册到Master；</span><br><span class="line">（2）定时发送心跳给Master；</span><br><span class="line">（3）根据Master发送的Application配置进程环境，并启动ExecutorBackend(执行Task所需的临时进程)</span><br></pre></td></tr></table></figure><h3 id="Driver和Executor"><a href="#Driver和Executor" class="headerlink" title="Driver和Executor"></a>Driver和Executor</h3><p>Driver和Executor:  负责具体执行的任务.他和具体提执行的任务相关.驱动器和执行器.驱动器是主,执行器是从.M任务的resourcemanager和nodemanager是负责管理资源, 资源申请下来之后他先启动的是Applicationmaster,是当前这个任务的小组长. Driver类似于MR的Applicationmaster . Applicationmaster来了之后,他去执行执行相应的具体的任务.就是mapTask,ReduceTask等.这些task就executer中去运行. Driver和Executer是线程级别的任务.</p><p>1）Driver（驱动器）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Spark的驱动器是执行开发程序中的main方法的线程。它负责开发人员编写的用来创建SparkContext (sc)、创建RDD，以及进行RDD的转化操作和行动操作代码的执行。如果你是用Spark Shell，那么当你启动Spark shell的时候，系统后台自启了一个Spark驱动器程序，就是在Spark shell中预加载的一个叫作 sc的SparkContext对象。如果驱动器程序终止，那么Spark应用也就结束了。</span><br><span class="line">Driver(驱动器)主要负责：</span><br><span class="line">（1）将用户程序代码转化为作业（Job）；</span><br><span class="line">（2）在Executor之间调度任务（Task）；</span><br><span class="line">（3）跟踪Executor的执行情况；</span><br><span class="line">（4）通过UI展示查询运行情况。</span><br></pre></td></tr></table></figure><p>2）Executor（执行器）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Spark Executor是一个工作节点，负责在 Spark 作业(Job)中运行任务(Task)，任务间相互独立。Spark 应用启动时，Executor节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有Executor节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他Executor节点上继续运行。</span><br><span class="line">Executor（执行器）主要负责：</span><br><span class="line">（1）负责运行组成 Spark 应用的任务，并将状态信息返回给驱动器(Driver)程序；</span><br><span class="line">（2）通过自身的块管理器（Block Manager）为用户程序中要求缓存的RDD提供内存式存储。RDD是直接缓存在Executor内的，因此任务可以在运行时充分利用缓存数据加速运算。</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Master和Worker是Spark的守护进程(什么叫守护进程?即一直都在的)，即Spark在特定模式下正常运行所必须的进程。</p><p>Driver和Executor是临时程序，当有具体任务提交到Spark集群才会开启的程序。其实Driver和Executor是线程.  而Master和Worker是进程.</p><h3 id="博主补充-1"><a href="#博主补充-1" class="headerlink" title="博主补充"></a>博主补充</h3><p>Driver(驱动器) 和 Executer(执行器) 有主从关系, Driver是主,Executer是从.<br>Driver可以这样理解,MR中资源准备好了之后,要启一个ApplicationMaster,即当前这个任务的守护者,相当于Driver(驱动器).  ApplicationMaster启动好了之后,启动相应的任务, 如mapTask,ReduceTask等. 具体的一个个Task去运行,这就相当于Executer里面运行的内容</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_312.png" alt="blog: www.xubatian.cn"></p><p>  1.只要你用的是StandLone模式,master和worker将一直都有.如果你是yarn模式就不需要. </p><ol start="2"><li>对于Driver和Executer只有等任务来了才有. 而Driver和Executer,无论本地模式和yarn模式都有, 他和模式没有关系 </li></ol><p>以下是Spark的几个模式, 它运行的位置可能不一样. standlone的Drive和Executor是由master和Worker来决定位置的.<br>如果是yarn模式就有ResourceManager来决定位置.</p><h2 id="Local模式"><a href="#Local模式" class="headerlink" title="Local模式"></a>Local模式</h2><p>本地模式：解压完了就等于安装好了。就和Hadoop一样，解压完了，什么都没改，直接就可以运行jar包了。这个也一样的，直接可以运行jar包</p><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Local模式就是运行在一台计算机上的模式，通常就是用于在本机上练手和测试。它可以通过以下集中方式设置Master。</p><p>local: 所有计算都运行在一个Core当中，没有任何并行计算，通常我们在本机执行些测试代码, 或者练手, 就用这种模式;</p><p>local[K]: 指定使用K个Core来运行计算，比如local[4]就是运行4个Core来执行;</p><p>local[*]:  这种模式直接使用最大Core数。</p><p>master叫资源管理器. 我们统称为master.<br>我们将standlone里面的master, yarn里面的ResourceManager. 以及这里上图的资源管理器都成为master.</p><h3 id="安装使用"><a href="#安装使用" class="headerlink" title="安装使用"></a>安装使用</h3><p>1）上传并解压spark安装包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 sorfware]$ tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/</span><br><span class="line">[shangbaishuyao@hadoop102 module]$ mv spark-2.1.1-bin-hadoop2.7 spark</span><br></pre></td></tr></table></figure><p>2）官方求PI案例(类似java jar)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>（1）基本语法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class &lt;main-class&gt;</span><br><span class="line">--master &lt;master-url&gt; \</span><br><span class="line">--deploy-mode &lt;deploy-mode&gt; \</span><br><span class="line">--conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">... # other options</span><br><span class="line">&lt;application-jar&gt; \        -- jar 包所在路径</span><br><span class="line">[application-arguments]    --大括号表示可选的,有些main方法不需要参数</span><br><span class="line"></span><br><span class="line">===============上面是模板,下面是实例,对比====================</span><br><span class="line"></span><br><span class="line"> bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>（2）参数说明</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">--master 指定Master的地址；</span><br><span class="line">--class: 你的应用的启动类 (如 org.apache.spark.examples.SparkPi)；</span><br><span class="line">--deploy-mode: 是否发布你的驱动到worker节点(cluster) 或者作为一个本地客户端 (client) (default: client)；</span><br><span class="line">--conf: 任意的Spark配置属性， 格式key=value. 如果值包含空格，可以加引号“key=value” ；</span><br><span class="line">application-jar: 打包好的应用jar,包含依赖. 这个URL在集群中全局可见。 比如hdfs:// 共享存储系统， 如果是 file:// path， 那么所有的节点的path都包含同样的jar</span><br><span class="line">application-arguments: 传给main()方法的参数；</span><br><span class="line">--executor-memory 1G 指定每个executor可用内存为1G；</span><br><span class="line">--total-executor-cores 2 指定每个executor使用的cup核数为2个。</span><br></pre></td></tr></table></figure><p>3）结果</p><p>该算法是利用蒙特·卡罗算法求PI</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_313.png" alt="blog: www.xubatian.cn"></p><p>4）准备文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ mkdir input</span><br><span class="line">在input下创建3个文件1.txt和2.txt，并输入以下内容</span><br><span class="line">hello shangbaishuyao</span><br><span class="line">hello spark</span><br></pre></td></tr></table></figure><p>5）启动spark-shell</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-shell</span><br><span class="line">Using Spark&#x27;s default log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line">Setting default log level to &quot;WARN&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">18/09/29 08:50:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">18/09/29 08:50:58 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</span><br><span class="line">Spark context Web UI available at http://192.168.9.102:4040</span><br><span class="line">Spark context available as &#x27;sc&#x27; (master = local[*], app id = local-1538182253312).</span><br><span class="line">Spark session available as &#x27;spark&#x27;.</span><br><span class="line">Welcome to</span><br><span class="line"></span><br><span class="line">      / __/__  ___ _____/ /__</span><br><span class="line">     _\ \/ _ \/ _ `/ __/  &#x27;_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1</span><br><span class="line">       /_/</span><br><span class="line">          </span><br><span class="line">Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)</span><br><span class="line">Type in expressions to have them evaluated.</span><br><span class="line">Type :help for more information.</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span></span><br></pre></td></tr></table></figure><p>6）结果图示</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_314.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_315.png" alt="blog: www.xubatian.cn"></p><p>7）运行WordCount程序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;sc.textFile(&quot;input&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">res0: Array[(String, Int)] = Array((hadoop,6), (oozie,3), (spark,3), (hive,3), (shangbaishuyao,3), (hbase,6))</span><br><span class="line">scala&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可登录hadoop102:4040查看程序运行</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_316.png" alt="blog: www.xubatian.cn"></p><h3 id="提交流程"><a href="#提交流程" class="headerlink" title="提交流程"></a>提交流程</h3><p>1）提交任务分析</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_317.png" alt="blog: www.xubatian.cn"></p><h3 id="博主补充解析"><a href="#博主补充解析" class="headerlink" title="博主补充解析"></a>博主补充解析</h3><p>因为是Local模式,没有master和worker. 我们在提交任务之前,没有启动任何程序. 所以资源管理者就是本身,就是spark-submit,即他自己管理计算,自己管理资源.  正常提交,提交之后就会运行一个Driver. 其实你在起动spark-shell的时候就已经有了这个Driver了. Driver去资源管理者里注册应用程序,然后启动Executor. 至此,这一套就在起动saprk-shell的时候就已经搞好了. 接下来过程就是我们自己写代码了.  就是Executor反向注册到Driver中产生通信. 然后写代码, 如初始化sparkContext, 任务划分,任务调度等. 调度完后给Executor中去运行.</p><h3 id="数据流程"><a href="#数据流程" class="headerlink" title="数据流程"></a>数据流程</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_319.png" alt=" blog: www.xubatian.cn"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">textFile(&quot;input&quot;)：  读取本地文件input文件夹数据；</span><br><span class="line">flatMap(_.spl it(&quot; &quot;))：压平操作，按照空格分割符将一行数据映射成一个个单词；</span><br><span class="line">map((_,1))：对每一个元素操作，将单词映射为元组；</span><br><span class="line">reduceByKey(_+_)：按照key将值进行聚合，相加；</span><br><span class="line">collect：将数据收集到Driver端展示。</span><br></pre></td></tr></table></figure><p><strong>案例分析</strong></p><p> <img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_320.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_321.png" alt="blog: www.xubatian.cn"></p><h2 id="Standalone模式"><a href="#Standalone模式" class="headerlink" title="Standalone模式"></a>Standalone模式</h2><p>Standalone模式有一组进程叫master和worker</p><p>单机模式，这里指的是spark自己来管理整个的计算资源，交给spark来管理了，他也是一个分布式的。但是这个计算资源不跟其他的mapreduce呀或者storm等程序所共用的，他自己来管理的。意思就是说，spark他自己玩自己的。他有一套独立的资源管理系统在里面此模式中有一组进程叫master和worker</p><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_323.png" alt="blog: www.xubatian.cn"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">构建一个由Master+Slave构成的Spark集群，使Spark程序运行在集群中，且有Cluster与Client模式(默认是这种)两种。主要区别在于：Driver程序的运行节点不一样。</span><br><span class="line">Driver是一个线程,是执行我写的程序的main方法,就是执行的spark-submit --class里面的main方法.</span><br><span class="line"></span><br><span class="line">Client模式指什么意思呢? 我们需要执行Spark-submit来提交一个任务. 如果我们采用的是client模式. 那么我们的Driver程序就在当前提交的机器的线程. 这个spark-submit是不是一个进程,这个进程的名字叫spark-submit. 这个线程就运行在进程spark-submit里面. 这是client模式.</span><br><span class="line">而Cluster模式,他这个Driver运行在哪? 他是由master来决定的一个位置. 所以cluster模式和Client模式他们两个区别就在这. </span><br><span class="line">如果生产环境中要用的话, 用的最多的是Cluster模式. 因为 Driver在整个运行过程中,他会和其他节点Executor做通信. 这样就对内存用的比较大了. 这样的话,我们让集群自己去做选择是更好一些. 因为client模式, 你在哪提交的,你的Driver就运行在哪. 很有可能,你提交的地方的这台机器本身资源不足等问题,所以用cluster模式更好一些</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">7077 standlone模式下master的服务端口</span><br><span class="line">8080 standlone模式下master的web端口</span><br><span class="line">4040 Driver的web端口</span><br><span class="line">18080 历史服务端口</span><br><span class="line">8088 ResourceManager的web端口</span><br><span class="line">19888 是MapReduce里面yarn的历史服务端口</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_322.png" alt="blog: www.xubatian.cn"></p><h3 id="安装使用-1"><a href="#安装使用-1" class="headerlink" title="安装使用"></a>安装使用</h3><p>1）进入spark安装目录下的conf文件夹</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ cd spark/conf/</span><br></pre></td></tr></table></figure><p>2）修改配置文件名称</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ mv slaves.template slaves</span><br><span class="line">[shangbaishuyao@hadoop102 conf]$ mv spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure><p>3）修改slave文件，添加work节点</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vim slaves</span><br><span class="line"></span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p>4）修改spark-env.sh文件，添加如下配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vim spark-env.sh</span><br><span class="line"></span><br><span class="line">SPARK_MASTER_HOST=hadoop102</span><br><span class="line">SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure><p>5）分发spark包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ xsync spark/</span><br></pre></td></tr></table></figure><p>6）启动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ sbin/start-all.sh</span><br><span class="line">[shangbaishuyao@hadoop102 spark]$ util.sh </span><br><span class="line">================shangbaishuyao@hadoop102================</span><br><span class="line">3330 Jps</span><br><span class="line">3238 Worker</span><br><span class="line">3163 Master</span><br><span class="line">================shangbaishuyao@hadoop103================</span><br><span class="line">2966 Jps</span><br><span class="line">2908 Worker</span><br><span class="line">================shangbaishuyao@hadoop104================</span><br><span class="line">2978 Worker</span><br><span class="line">3036 Jps</span><br></pre></td></tr></table></figure><p>网页查看：hadoop102:8080<br>注意：如果遇到 “JAVA_HOME not set” 异常，可以在sbin目录下的spark-config.sh 文件中加入如下配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure><p>7）官方求PI案例</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077 \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_324.png" alt="blog: www.xubatian.cn"></p><p>8）启动spark shell</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/spark/bin/spark-shell \</span><br><span class="line">--master spark://hadoop102:7077 \                     </span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--total-executor-cores 2</span><br><span class="line">参数：--master spark://hadoop102:7077指定要连接的集群的master</span><br><span class="line">执行WordCount程序</span><br><span class="line">scala&gt;sc.textFile(&quot;/opt/module/spark/input&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">res0: Array[(String, Int)] = Array((hadoop,6), (oozie,3), (spark,3), (hive,3), (shangbaishuyao,3), (hbase,6))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><h3 id="JobHistoryServer配置-查看历史用的-历史服务器"><a href="#JobHistoryServer配置-查看历史用的-历史服务器" class="headerlink" title="JobHistoryServer配置  (查看历史用的,历史服务器)"></a>JobHistoryServer配置  (查看历史用的,历史服务器)</h3><p>1）修改spark-default.conf.template名称</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ mv spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure><p>2）修改spark-default.conf文件，开启Log  (配置的是写)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vi spark-defaults.conf</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               hdfs://hadoop102:9000/directory</span><br></pre></td></tr></table></figure><p>注意：HDFS上的目录需要提前存在。<br>3）修改spark-env.sh文件，添加如下配置 (配置的是读取)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vi spark-env.sh</span><br><span class="line"></span><br><span class="line">export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=18080</span><br><span class="line">-Dspark.history.retainedApplications=30 </span><br><span class="line">-Dspark.history.fs.logDirectory=hdfs://hadoop102:9000/directory&quot;</span><br></pre></td></tr></table></figure><p>参数描述：<br>spark.eventLog.dir：Application在运行过程中所有的信息均记录在该属性指定的路径下<br>spark.history.ui.port=18080  WEBUI访问的端口号为18080<br>spark.history.fs.logDirectory=hdfs://hadoop102:9000/directory配置了该属性后，在start-history-server.sh时就无需再显式的指定路径，Spark History Server页面只展示该指定路径下的信息<br>spark.history.retainedApplications=30指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除。注意：这个是内存中的应用数，而不是页面上显示的应用数。<br>4）分发配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ xsync spark-defaults.conf</span><br><span class="line">[shangbaishuyao@hadoop102 conf]$ xsync spark-env.sh</span><br></pre></td></tr></table></figure><p>5）启动历史服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ sbin/start-history-server.sh</span><br></pre></td></tr></table></figure><p>6）再次执行任务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077 \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>7）查看历史服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop102:18080</span><br></pre></td></tr></table></figure><h3 id="HA配置"><a href="#HA配置" class="headerlink" title="HA配置"></a>HA配置</h3><p>我们worker有三个宕机一个还能用,但是master只有一个,我发高可用,所以我们要将master依赖zookeeper,由zookeeper来选举master,不能让我们直接指定</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_325.png" alt="blog: www.xubatian.cn"></p><p>1）zookeeper正常安装并启动<br>2）修改spark-env.sh文件，添加如下配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vi spark-env.sh</span><br><span class="line">注释掉如下内容：因为我们的master由zookeeper来选举,不能由我们自己指定了,故注释掉</span><br><span class="line">#SPARK_MASTER_HOST=hadoop102</span><br><span class="line">#SPARK_MASTER_PORT=7077</span><br><span class="line">添加上如下内容：</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS=&quot;</span><br><span class="line">-Dspark.deploy.recoveryMode=ZOOKEEPER </span><br><span class="line">-Dspark.deploy.zookeeper.url=hadoop102,hadoop103,hadoop104 </span><br><span class="line">-Dspark.deploy.zookeeper.dir=/spark&quot;</span><br></pre></td></tr></table></figure><p>3）分发配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ xsync spark-env.sh</span><br></pre></td></tr></table></figure><p>4）在hadoop102上启动全部节点</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ sbin/start-all.sh</span><br></pre></td></tr></table></figure><p>5）在hadoop103上单独启动master节点</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop103 spark]$ sbin/start-master.sh</span><br></pre></td></tr></table></figure><p>6）spark HA集群访问,一般先连接前面的master,前面挂掉了再连接后面的 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/spark/bin/spark-shell \</span><br><span class="line">--master spark://hadoop102:7077,hadoop103:7077 \</span><br><span class="line">--executor-memory 2g \</span><br><span class="line">--total-executor-cores 2</span><br></pre></td></tr></table></figure><h2 id="Yarn模式"><a href="#Yarn模式" class="headerlink" title="Yarn模式"></a>Yarn模式</h2><p>他不需要部署spark集群,我只需要部署yarn集群,因为我所解压的spark只是作为本地客户端,只是提交用,当然你也可以分发, 分发后的目的也就是hadoop102,hadoop103,hadoop104都是可以提交任务而已. 因为yarn模式,我们解压的spark仅仅作为客户端来用的<br>生产环境当中用的最多的一种模式，就是说spark他有一个计算任务。任务呢，我来执行，但是运行任务的CPU,还有内存这些东西交给yarn来管理。交给yarn来管理其实就是交给resourcemanager和nodemanager来管理。</p><h3 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h3><p>Spark客户端直接连接Yarn，不需要额外构建Spark集群。有yarn-client和yarn-cluster两种模式，主要区别在于：Driver程序的运行节点。<br>yarn-client：Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出<br>yarn-cluster：Driver程序运行在由RM（ResourceManager）启动的AM（APPMaster）适用于生产环境。</p><p>Yarn-cluster提交流程图:<br><a href="https://www.cnblogs.com/shi-qi/articles/12174206.html">https://www.cnblogs.com/shi-qi/articles/12174206.html</a></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_326.png" alt="blog: www.xubatian.cn"></p><h3 id="博主解析"><a href="#博主解析" class="headerlink" title="博主解析"></a>博主解析</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_327.png" alt="blog: www.xubatian.cn"></p><h3 id="安装使用-2"><a href="#安装使用-2" class="headerlink" title="安装使用"></a>安装使用</h3><p>1）修改hadoop配置文件yarn-site.xml,添加如下内容<br>[shangbaishuyao@hadoop102 hadoop]$ vi yarn-site.xml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>2）修改spark-env.sh，添加如下配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vi spark-env.sh</span><br><span class="line"></span><br><span class="line">YARN_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop</span><br></pre></td></tr></table></figure><p>3）分发配置文件,只是分发这个配置文件,我spark-yarn要分发吗?不需要.因为我们不需要额外去构建spark集群,yarn是分布式的,而本地的spark-yarn仅仅是做提交任务的客户端,所以<br>Spark-yarn不许要分发</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ xsync /opt/module/hadoop-2.7.2/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure><p>4）执行一个程序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>注意：在提交任务之前需启动HDFS以及YARN集群。</p><p>Yarn 模式读取的文件是HDFS里面的</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_328.png" alt="blog: www.xubatian.cn"></p><h3 id="日志查看"><a href="#日志查看" class="headerlink" title="日志查看"></a>日志查看</h3><p>1）修改配置文件spark-defaults.conf，添加如下内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.yarn.historyServer.address=hadoop102:18080</span><br><span class="line">spark.history.ui.port=18080</span><br></pre></td></tr></table></figure><p>2）重启Spark历史服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ sbin/stop-history-server.sh </span><br><span class="line">stopping org.apache.spark.deploy.history.HistoryServer</span><br><span class="line">[shangbaishuyao@hadoop102 spark]$ sbin/start-history-server.sh </span><br><span class="line">starting org.apache.spark.deploy.history.HistoryServer, logging to /opt/module/spark/logs/spark-shangbaishuyao-org.apache.spark.deploy.history.HistoryServer-1-hadoop102.out</span><br></pre></td></tr></table></figure><p>3）提交任务到Yarn执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 spark]$ bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>4）Web页面查看日志</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_329.png" alt="blog: www.xubatian.cn"></p><h2 id="Mesos模式"><a href="#Mesos模式" class="headerlink" title="Mesos模式"></a>Mesos模式</h2><p>（这种很少用，几乎没有公司在用。Mesos也是apache的一个资源调度框架，就和yarn是类似的东西）</p><p>Spark客户端直接连接Mesos；不需要额外构建Spark集群。国内应用比较少，更多的是运用yarn调度。</p><h2 id="几种模式对比"><a href="#几种模式对比" class="headerlink" title="几种模式对比"></a>几种模式对比</h2><table><thead><tr><th>模式</th><th>Spark安装机器数</th><th>需启动的进程(提交任务前)</th><th>所属者</th></tr></thead><tbody><tr><td>Local</td><td>1</td><td>无</td><td>Spark</td></tr><tr><td>Standalone</td><td>3</td><td>Master及Worker</td><td>Spark</td></tr><tr><td>Yarn</td><td>1</td><td>Yarn及HDFS</td><td>Hadoop</td></tr></tbody></table><h1 id="案例代码"><a href="#案例代码" class="headerlink" title="案例代码"></a>案例代码</h1><p>WordCount案例代码:</p><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/sparkCore/src/main/scala/com/shangbaishuyao/wordCount">https://github.com/ShangBaiShuYao/bigdata/blob/master/sparkCore/src/main/scala/com/shangbaishuyao/wordCount</a></p><p>整个Spark学习案例代码:</p><p><a href="https://github.com/ShangBaiShuYao/bigdata">https://github.com/ShangBaiShuYao/bigdata</a></p><p>调试补充:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_330.png" alt="https://www.xubatian.cn/"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt; 征途漫漫，惟有奋斗；梦想成真，惟有实干。                     ——人民日报&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="spark" scheme="http://xubatian.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>hive常用函数收录</title>
    <link href="http://xubatian.cn/hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E6%94%B6%E5%BD%95/"/>
    <id>http://xubatian.cn/hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E6%94%B6%E5%BD%95/</id>
    <published>2022-01-18T14:43:32.000Z</published>
    <updated>2022-01-23T02:58:21.757Z</updated>
    
    <content type="html"><![CDATA[<p>山再高，往上攀，总能登顶；路再长，走下去，定能到达。       ——人民日报                  </p><p>​                                              </p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_307.jpg" alt="blog: www.xubatian.cn"></p><h1 id="常用日期函数"><a href="#常用日期函数" class="headerlink" title="常用日期函数"></a>常用日期函数</h1><p>unix_timestamp:返回当前或指定时间的时间戳    </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select unix_timestamp();</span><br><span class="line">select unix_timestamp(&quot;2020-10-28&quot;,&#x27;yyyy-MM-dd&#x27;);</span><br></pre></td></tr></table></figure><p>from_unixtime：将时间戳转为日期格式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select from_unixtime(1603843200);</span><br></pre></td></tr></table></figure><p>current_date：当前日期</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select current_date;</span><br></pre></td></tr></table></figure><p>current_timestamp：当前的日期加时间</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select current_timestamp;</span><br></pre></td></tr></table></figure><p>to_date：抽取日期部分</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select to_date(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>year：获取年</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select year(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>month：获取月</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select month(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>day：获取日</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select day(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>hour：获取时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select hour(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>minute：获取分</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select minute(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>second：获取秒</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select second(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>weekofyear：当前时间是一年中的第几周</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select weekofyear(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>dayofmonth：当前时间是一个月中的第几天</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select dayofmonth(&#x27;2020-10-28 12:12:12&#x27;);</span><br></pre></td></tr></table></figure><p>months_between： 两个日期间的月份</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select months_between(&#x27;2020-04-01&#x27;,&#x27;2020-10-28&#x27;);</span><br></pre></td></tr></table></figure><p>add_months：日期加减月</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select add_months(&#x27;2020-10-28&#x27;,-3);</span><br></pre></td></tr></table></figure><p>datediff：两个日期相差的天数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select datediff(&#x27;2020-11-04&#x27;,&#x27;2020-10-28&#x27;);</span><br></pre></td></tr></table></figure><p>date_add：日期加天数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select date_add(&#x27;2020-10-28&#x27;,4);</span><br></pre></td></tr></table></figure><p>date_sub：日期减天数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select date_sub(&#x27;2020-10-28&#x27;,-4);</span><br></pre></td></tr></table></figure><p>last_day：日期的当月的最后一天</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select last_day(&#x27;2020-02-30&#x27;);</span><br></pre></td></tr></table></figure><p>date_format(): 格式化日期</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select date_format(&#x27;2020-10-28 12:12:12&#x27;,&#x27;yyyy/MM/dd HH:mm:ss&#x27;);</span><br></pre></td></tr></table></figure><p>常用取整函数<br>round： 四舍五入</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select round(3.14);</span><br><span class="line">select round(3.54);</span><br></pre></td></tr></table></figure><p>ceil：  向上取整</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select ceil(3.14);</span><br><span class="line">select ceil(3.54);</span><br></pre></td></tr></table></figure><p>floor： 向下取整</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select floor(3.14);</span><br><span class="line">select floor(3.54);</span><br></pre></td></tr></table></figure><h1 id="常用字符串操作函数"><a href="#常用字符串操作函数" class="headerlink" title="常用字符串操作函数"></a>常用字符串操作函数</h1><p>upper： 转大写</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select upper(&#x27;low&#x27;);</span><br></pre></td></tr></table></figure><p>lower： 转小写</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select lower(&#x27;low&#x27;);</span><br></pre></td></tr></table></figure><p>length： 长度</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select length(&quot;shangbaishuyao&quot;);</span><br></pre></td></tr></table></figure><p>trim：  前后去空格</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select trim(&quot; shangbaishuyao &quot;);</span><br></pre></td></tr></table></figure><p>lpad： 向左补齐，到指定长度</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select lpad(&#x27;shangbaishuyao&#x27;,9,&#x27;g&#x27;);</span><br></pre></td></tr></table></figure><p>rpad：  向右补齐，到指定长度</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select rpad(&#x27;shangbaishuyao&#x27;,9,&#x27;g&#x27;);</span><br></pre></td></tr></table></figure><p>regexp_replace：使用正则表达式匹配目标字符串，匹配成功后替换！</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT regexp_replace(&#x27;2020/10/25&#x27;, &#x27;/&#x27;, &#x27;-&#x27;);</span><br></pre></td></tr></table></figure><h1 id="集合操作"><a href="#集合操作" class="headerlink" title="集合操作"></a>集合操作</h1><p>size： 集合中元素的个数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select size(friends) from test3;</span><br></pre></td></tr></table></figure><p>map_keys： 返回map中的key</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select map_keys(children) from test3;</span><br></pre></td></tr></table></figure><p>map_values: 返回map中的value</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select map_values(children) from test3;</span><br></pre></td></tr></table></figure><p>array_contains: 判断array中是否包含某个元素</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select array_contains(friends,&#x27;bingbing&#x27;) from test3;</span><br></pre></td></tr></table></figure><p>sort_array： 将array中的元素排序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select sort_array(friends) from test3;</span><br></pre></td></tr></table></figure><p>grouping_set:多维分析</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;山再高，往上攀，总能登顶；路再长，走下去，定能到达。       ——人民日报                  &lt;/p&gt;
&lt;p&gt;​                                              &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="hive" scheme="http://xubatian.cn/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>hive简介(二)</title>
    <link href="http://xubatian.cn/hive%E7%AE%80%E4%BB%8B(%E4%BA%8C)/"/>
    <id>http://xubatian.cn/hive%E7%AE%80%E4%BB%8B(%E4%BA%8C)/</id>
    <published>2022-01-18T10:35:45.000Z</published>
    <updated>2022-01-23T02:58:21.682Z</updated>
    
    <content type="html"><![CDATA[<p>白衣执甲，逆行而上，以勇气和辛劳诠释了医者仁心，用担当和奉献换来了山河无恙。新时代中国女性可亲、可敬、可爱，她们在热血奋斗中怒放生命，在应对挑战中成就不凡。          ——人民日报                    </p><p>​                                              </p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_268.png" alt="blog: www.xubatian.cn"></p><h1 id="DDL数据定义-创建库-创建表都属于他"><a href="#DDL数据定义-创建库-创建表都属于他" class="headerlink" title="DDL数据定义(创建库,创建表都属于他)"></a>DDL数据定义(创建库,创建表都属于他)</h1><h2 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h2><p>1）创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_269.png" alt="blog: www.xubatian.cn"></p><p>2）避免要创建的数据库已经存在错误，增加if not exists判断。（标准写法）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists</span><br><span class="line">hive (default)&gt; create database if not exists db_hive;</span><br></pre></td></tr></table></figure><p>3）创建一个数据库，指定数据库在HDFS上存放的位置,/db-hive2.db是名字</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive2 location &#x27;/db_hive2.db&#x27;;  放在根目录下</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_270.png" alt="blog: www.xubatian.cn"></p><h2 id="查询数据库"><a href="#查询数据库" class="headerlink" title="查询数据库"></a>查询数据库</h2><h3 id="显示数据库"><a href="#显示数据库" class="headerlink" title="显示数据库"></a>显示数据库</h3><p>1．显示数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure><p>2．过滤显示查询的数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases like &#x27;db_hive*&#x27;;</span><br><span class="line">OK</span><br><span class="line">db_hive</span><br><span class="line">db_hive_1</span><br></pre></td></tr></table></figure><p>4.2.2 查看数据库详情<br>1．显示数据库信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hivehdfs://hadoop102:9000/user/hive/warehouse/db_hive.dbshangbaishuyaoUSER</span><br></pre></td></tr></table></figure><p>上白书妖补充:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_271.png" alt="blog: www.xubatian.cn"></p><p>2．显示数据库详细信息，extended</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hivehdfs://hadoop102:9000/user/hive/warehouse/db_hive.dbshangbaishuyaoUSER</span><br><span class="line">40.3.3 切换当前数据库</span><br><span class="line">hive (default)&gt; use db_hive;</span><br></pre></td></tr></table></figure><ol start="3"><li>切换当前数据库 </li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; use db_hive;</span><br></pre></td></tr></table></figure><h3 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h3><p>用户可以使用ALTER DATABASE (alter database )命令为某个数据库的DBPROPERTIES(dbproperties)设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter database db_hive set dbproperties(&#x27;createtime&#x27;=&#x27;20170830&#x27;);</span><br></pre></td></tr></table></figure><p>在hive中查看修改结果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line">db_name comment location        owner_name      owner_type      parameters</span><br><span class="line">db_hive         hdfs://hadoop102:8020/user/hive/warehouse/db_hive.db    shangbaishuyao USER    &#123;createtime=20170830&#125;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_272.png" alt="blog: www.xubatian.cn"></p><h3 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h3><p>1．删除空数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;drop database db_hive2;</span><br></pre></td></tr></table></figure><p>2．如果删除的数据库不存在，最好采用 if exists判断数据库是否存在</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</span><br><span class="line">hive&gt; drop database if exists db_hive2;</span><br></pre></td></tr></table></figure><p>3．如果数据库不为空，里面有表的话，可以采用cascade命令，强制删除</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</span><br><span class="line">hive&gt; drop database db_hive cascade;</span><br></pre></td></tr></table></figure><h2 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h2><p>1．建表语法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name </span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[COMMENT table_comment] </span><br><span class="line">[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[CLUSTERED BY (col_name, col_name, ...) </span><br><span class="line">[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] </span><br><span class="line">[ROW FORMAT row_format] </span><br><span class="line">[STORED AS file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_273.png" alt="blog: www.xubatian.cn"></p><p>2．字段解释说明<br>（1）CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。<br>（2）EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。<br>（3）COMMENT：为表和列添加注释。<br>（4）PARTITIONED BY创建分区表<br>（5）CLUSTERED BY创建分桶表<br>（6）SORTED BY不常用<br>（7）ROW FORMAT<br>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]<br>        [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]<br>   | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)]<br>用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT 或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。<br>SerDe是Serialize/Deserilize的简称，目的是用于序列化和反序列化。<br>（8）STORED AS指定存储文件类型<br>常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）<br>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。<br>（9）LOCATION ：指定表在HDFS上的存储位置。<br>（10）LIKE允许用户复制现有的表结构，但是不复制数据。</p><h3 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h3><p>也叫内部表,将来删除表中数据会把对应HDFS中存的也删掉,在HDFS上的文件,目录也会删除掉</p><p>1．理论<br>我们默认创建的表都是内部表,外部表的创建需要单独去指定<br>默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。    当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据。</p><p>2．案例实操<br>（1）普通创建表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student2(</span><br><span class="line">id <span class="type">int</span>, name string</span><br><span class="line">) </span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">&#x27;/user/hive/warehouse/student2&#x27;</span>;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_275.png" alt="blog: www.xubatian.cn"></p><p>（2）根据查询结果创建表（查询的结果会添加到新创建的表中）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists student3 as select id, name from student;</span><br></pre></td></tr></table></figure><p>（3）根据已经存在的表结构创建表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists student4 like student;</span><br></pre></td></tr></table></figure><p>补充效果图:  说明这种方式只是按照表结构来创建一张表,里面并没有数据</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_276.png" alt="blog: www.xubatian.cn"></p><p>（4）查询表的类型,更详细的信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE  </span><br><span class="line">由这个就可以看出他是管理表,或者说就是内部表,注意，只要表删了，在hdfs中对应的目录就没了</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_277.png" alt="blog: www.xubatian.cn"></p><h3 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h3><p>hive认为这个表不归我管，所以叫外部表</p><p>1．理论<br>因为表是外部表，所以Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。就是在所对应的hdfs上的文件不会被删除<br>特点就是：删除数据时，如果你是内部表，我就把你数据删了，如果你是外部表，我就不删除你数据<br>2．管理表和外部表的使用场景<br>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。<br>外部表的创建就只需要在创建表的语法上加上external 就是创建的外部表</p><ol start="3"><li>案例实操<br>分别创建部门和员工外部表，并向表中导入数据。</li></ol><p>​      建表语句</p><p>创建部门表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create external  table if not exists default.dept(</span><br><span class="line">deptno int,</span><br><span class="line">dname string,</span><br><span class="line">loc int</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>创建员工表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create external table if not exists default.emp(</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string, </span><br><span class="line">sal double, </span><br><span class="line">comm double,</span><br><span class="line">deptno int)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>查看创建的表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line">dept</span><br><span class="line">emp</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>导入数据</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_278.png" alt="blog: www.xubatian.cn"></p><ol start="4"><li>向外部表中导入数据</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept;</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/emp.txt&#x27; into table default.emp;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_279.png" alt="blog: www.xubatian.cn"></p><p>查询结果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp;</span><br><span class="line">hive (default)&gt; select * from dept;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_280.png" alt="blog: www.xubatian.cn"></p><p>上面看的结构比较乱,如果想看的更清楚可以使用hiveserver2查看:如下图</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_281.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_282.png" alt="blog: www.xubatian.cn"></p><ol start="5"><li>查看表格式化数据</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted dept;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure><h3 id="管理表-内部表-与外部表的互相转换"><a href="#管理表-内部表-与外部表的互相转换" class="headerlink" title="管理表(内部表)与外部表的互相转换"></a>管理表(内部表)与外部表的互相转换</h3><p>（1）查询表的类型</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure><p>（2）修改内部表student2为外部表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table student2 set tblproperties(&#x27;EXTERNAL&#x27;=&#x27;TRUE&#x27;);</span><br></pre></td></tr></table></figure><p>（3）查询表的类型</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure><p>（4）修改外部表student2为内部表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table student2 set tblproperties(&#x27;EXTERNAL&#x27;=&#x27;FALSE&#x27;);</span><br></pre></td></tr></table></figure><p>（5）查询表的类型</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure><p>注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！</p><h2 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h2><p>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。 </p><h3 id="分区表基本操作"><a href="#分区表基本操作" class="headerlink" title="分区表基本操作"></a>分区表基本操作</h3><p>1．引入分区表（需要根据日期对日志进行管理,按月存储,按月分区）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/user/hive/warehouse/log_partition/20170702/20170702.log</span><br><span class="line">/user/hive/warehouse/log_partition/20170703/20170703.log</span><br><span class="line">/user/hive/warehouse/log_partition/20170704/20170704.log</span><br></pre></td></tr></table></figure><p>2．创建分区表语法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition(</span><br><span class="line">deptno int, dname string, loc string )</span><br><span class="line">partitioned by (month string)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>3．加载数据到分区表中</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition partition(month=&#x27;201709&#x27;);</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition partition(month=&#x27;201708&#x27;);</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition partition(month=&#x27;201707’);</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_283.png" alt="blog: www.xubatian.cn"></p><p>4．查询分区表中数据<br>    单分区查询</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month=&#x27;201709&#x27;;</span><br></pre></td></tr></table></figure><p>多分区联合查询</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month=&#x27;201709&#x27;</span><br><span class="line">              union</span><br><span class="line">              select * from dept_partition where month=&#x27;201708&#x27;</span><br><span class="line">              union</span><br><span class="line">              select * from dept_partition where month=&#x27;201707&#x27;;</span><br><span class="line"></span><br><span class="line">_u3.deptno      _u3.dname       _u3.loc _u3.month</span><br><span class="line">10      ACCOUNTING      NEW YORK        201707</span><br><span class="line">10      ACCOUNTING      NEW YORK        201708</span><br><span class="line">10      ACCOUNTING      NEW YORK        201709</span><br><span class="line">20      RESEARCH        DALLAS  201707</span><br><span class="line">20      RESEARCH        DALLAS  201708</span><br><span class="line">20      RESEARCH        DALLAS  201709</span><br><span class="line">30      SALES   CHICAGO 201707</span><br><span class="line">30      SALES   CHICAGO 201708</span><br><span class="line">30      SALES   CHICAGO 201709</span><br><span class="line">40      OPERATIONS      BOSTON  201707</span><br><span class="line">40      OPERATIONS      BOSTON  201708</span><br><span class="line">40      OPERATIONS      BOSTON  201709</span><br></pre></td></tr></table></figure><p>5．单独增加分区<br>    创建单个分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month=&#x27;201706&#x27;) ;</span><br></pre></td></tr></table></figure><p>​    同时创建多个分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month=&#x27;201705&#x27;) partition(month=&#x27;201704&#x27;);</span><br></pre></td></tr></table></figure><p>6．删除分区<br>    删除单个分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month=&#x27;201704&#x27;);</span><br></pre></td></tr></table></figure><p>同时删除多个分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month=&#x27;201705&#x27;), partition (month=&#x27;201706&#x27;);</span><br></pre></td></tr></table></figure><p>7．查看分区表有多少分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show partitions dept_partition;</span><br></pre></td></tr></table></figure><p>8．查看分区表结构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc formatted dept_partition;</span><br><span class="line">#Partition Information          </span><br><span class="line">#col_name              data_type               comment             </span><br><span class="line">month                   string    </span><br></pre></td></tr></table></figure><h3 id="分区表注意事项"><a href="#分区表注意事项" class="headerlink" title="分区表注意事项"></a>分区表注意事项</h3><p>1．创建二级分区表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> dept_partition2(</span><br><span class="line">                 deptno <span class="type">int</span>, dname string, loc string )</span><br><span class="line">                 partitioned <span class="keyword">by</span> (<span class="keyword">month</span> string, <span class="keyword">day</span> string)</span><br><span class="line">                 <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>2．正常的加载数据<br>（1）加载数据到二级分区表中</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table</span><br><span class="line"> default.dept_partition2 partition(month=&#x27;201709&#x27;, day=&#x27;13&#x27;);</span><br></pre></td></tr></table></figure><p>（2）查询分区数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;13&#x27;;</span><br></pre></td></tr></table></figure><p>3．把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_284.png" alt="blog: www.xubatian.cn"></p><p>（1）方式一：上传数据后修复<br>    上传数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=12;  我自己创建分区目录,和分区如何关联?</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br><span class="line">把文件上传到自己创建的的分区目录下</span><br></pre></td></tr></table></figure><p>查询数据（查询不到刚上传的数据）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;12&#x27;;</span><br></pre></td></tr></table></figure><p>执行修复命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; msck repair table dept_partition2;</span><br></pre></td></tr></table></figure><p>再次查询数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;12&#x27;;</span><br></pre></td></tr></table></figure><p>​    （2）方式二：上传数据后添加分区<br>​    上传数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br></pre></td></tr></table></figure><p>​    执行添加分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 add partition(month=&#x27;201709&#x27;, day=&#x27;11&#x27;);</span><br></pre></td></tr></table></figure><p>​    查询数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;11&#x27;;</span><br></pre></td></tr></table></figure><p>（3）方式三：创建文件夹后load数据到分区<br>        创建目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=10;</span><br></pre></td></tr></table></figure><p>上传数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table</span><br><span class="line"> dept_partition2 partition(month=&#x27;201709&#x27;,day=&#x27;10&#x27;);</span><br></pre></td></tr></table></figure><p>查询数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;10&#x27;;</span><br></pre></td></tr></table></figure><h2 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h2><p> 重命名表<br>1．语法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name RENAME TO new_table_name</span><br></pre></td></tr></table></figure><p>2．实操案例</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 rename to dept_partition3;</span><br></pre></td></tr></table></figure><h3 id="实操案例"><a href="#实操案例" class="headerlink" title="实操案例"></a>实操案例</h3><p>（1）查询表结构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><p>（2）添加列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add columns(deptdesc string);</span><br></pre></td></tr></table></figure><p>（3）查询表结构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><p>（4）更新列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition change column deptdesc desc int;</span><br></pre></td></tr></table></figure><p>（5）查询表结构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><p>（6）替换列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition replace columns(deptno string, dname, string, loc string);</span><br></pre></td></tr></table></figure><p>（7）查询表结构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><h2 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; drop table dept_partition;</span><br></pre></td></tr></table></figure><h1 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h1><h2 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h2><h3 id="向表中装载数据（Load）"><a href="#向表中装载数据（Load）" class="headerlink" title="向表中装载数据（Load）"></a>向表中装载数据（Load）</h3><p>1．语法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data [local] inpath &#x27;/opt/module/datas/student.txt&#x27; overwrite | into table student [partition (partcol1=val1,…)];</span><br></pre></td></tr></table></figure><p>（1）load data:表示加载数据<br>（2）local:表示从本地加载数据到hive表；没有写Local则从HDFS加载数据到hive表<br>（3）inpath:表示加载数据的路径<br>（4）overwrite:表示覆盖表中已有数据，没有写overwrite则表示追加<br>（5）into table:表示加载到哪张表<br>（6）student:表示具体的表<br>（7）partition:表示上传到指定分区<br>注意:从本地文件系统中取load数据,他其实是copy的操作.如果你从HDFS系统中去load数据,他其实是剪切的操作.<br>2．实操案例<br>    （0）创建一张表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table student(id string, name string) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>（1）加载本地文件到hive    </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/student.txt&#x27; into table default.student;</span><br></pre></td></tr></table></figure><p>（2）加载HDFS文件到hive中<br>    上传文件到HDFS</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/shangbaishuyao/hive;</span><br></pre></td></tr></table></figure><p>加载HDFS上数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data inpath &#x27;/user/shangbaishuyao/hive/student.txt&#x27; into table default.student;</span><br></pre></td></tr></table></figure><p>（3）加载数据覆盖表中已有的数据<br>上传文件到HDFS</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/shangbaishuyao/hive;</span><br></pre></td></tr></table></figure><p>加载数据覆盖表中已有的数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data inpath &#x27;/user/shangbaishuyao/hive/student.txt&#x27; overwrite into table default.student;</span><br></pre></td></tr></table></figure><p>上传的方式就是不能加””号</p><h3 id="通过查询语句向表中插入数据（Insert）"><a href="#通过查询语句向表中插入数据（Insert）" class="headerlink" title="通过查询语句向表中插入数据（Insert）"></a>通过查询语句向表中插入数据（Insert）</h3><p>1．创建一张分区表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>2．基本插入数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table  student partition(month=&#x27;201709&#x27;) values(1,&#x27;wangwu&#x27;);</span><br></pre></td></tr></table></figure><p>3．基本模式插入（根据单张表查询结果）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table student partition(month=&#x27;201708&#x27;)</span><br><span class="line">             select id, name from student where month=&#x27;201709&#x27;;</span><br></pre></td></tr></table></figure><p>4．多插入模式（根据多张表查询结果）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; from student</span><br><span class="line">               insert overwrite table student partition(month=&#x27;201707&#x27;)</span><br><span class="line">               select id, name where month=&#x27;201709&#x27;</span><br><span class="line">               insert overwrite table student partition(month=&#x27;201706&#x27;)</span><br><span class="line">              select id, name where month=&#x27;201709&#x27;;</span><br></pre></td></tr></table></figure><h3 id="查询语句中创建表并加载数据（As-Select）"><a href="#查询语句中创建表并加载数据（As-Select）" class="headerlink" title="查询语句中创建表并加载数据（As Select）"></a>查询语句中创建表并加载数据（As Select）</h3><p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists student3</span><br><span class="line">as select id, name from student;</span><br></pre></td></tr></table></figure><h3 id="创建表时通过Location指定加载数据路径"><a href="#创建表时通过Location指定加载数据路径" class="headerlink" title="创建表时通过Location指定加载数据路径"></a>创建表时通过Location指定加载数据路径</h3><p>1．创建表，并指定在hdfs上的位置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table if not exists student5(</span><br><span class="line">                 id int, name string  )</span><br><span class="line">                 row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">                 location &#x27;/user/hive/warehouse/student5&#x27;;</span><br></pre></td></tr></table></figure><p>2．上传数据到hdfs上</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hive/warehouse/student5;</span><br></pre></td></tr></table></figure><p>3．查询数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from student5;</span><br></pre></td></tr></table></figure><h3 id="Import数据到指定Hive表中"><a href="#Import数据到指定Hive表中" class="headerlink" title="Import数据到指定Hive表中"></a>Import数据到指定Hive表中</h3><p>注意：先用export导出后，再将数据导入。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; import table student2 partition(month=&#x27;201709&#x27;) from</span><br><span class="line"> &#x27;/user/hive/warehouse/export/student&#x27;;</span><br></pre></td></tr></table></figure><h2 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h2><h3 id="Insert导出"><a href="#Insert导出" class="headerlink" title="Insert导出"></a>Insert导出</h3><p>1．将查询的结果导出到本地</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/datas/export/student&#x27;</span><br><span class="line">            select * from student;</span><br></pre></td></tr></table></figure><p>2．将查询的结果格式化导出到本地</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;insert overwrite local directory &#x27;/opt/module/datas/export/student1&#x27;</span><br><span class="line">           ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;             select * from student;</span><br></pre></td></tr></table></figure><p>3．将查询的结果导出到HDFS上(没有local)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite directory &#x27;/user/shangbaishuyao/student2&#x27;</span><br><span class="line">             ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; </span><br><span class="line">             select * from student;</span><br></pre></td></tr></table></figure><h3 id="Hadoop命令导出到本地"><a href="#Hadoop命令导出到本地" class="headerlink" title="Hadoop命令导出到本地"></a>Hadoop命令导出到本地</h3><p>get就是从hdfs上去下载文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0</span><br><span class="line">/opt/module/datas/export/student3.txt;</span><br></pre></td></tr></table></figure><h3 id="Hive-Shell-命令导出"><a href="#Hive-Shell-命令导出" class="headerlink" title="Hive Shell 命令导出"></a>Hive Shell 命令导出</h3><p>基本语法：（hive -f/-e 执行语句或者脚本 &gt; file）<br>注意:   &gt;   这个是追加号,将他追加到我们的文件里面</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/hive -e &#x27;select * from default.student;&#x27; &gt;</span><br><span class="line"> /opt/module/datas/export/student4.txt;</span><br></pre></td></tr></table></figure><h3 id="Export导出到HDFS上"><a href="#Export导出到HDFS上" class="headerlink" title="Export导出到HDFS上"></a>Export导出到HDFS上</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(defahiveult)&gt; export table default.student to</span><br><span class="line"> &#x27;/user/hive/warehouse/export/student&#x27;;</span><br></pre></td></tr></table></figure><p>说明他不仅把你数据导出来,还把你的元数据也导出来了,说明,也就是只有这两者都具备的的文件才能够导入</p><p>导出的东西有元数据的,但是你student.txt导入的东西也是student.txt,是没有元数据的</p><h3 id="清除表中数据（Truncate）"><a href="#清除表中数据（Truncate）" class="headerlink" title="清除表中数据（Truncate）"></a>清除表中数据（Truncate）</h3><p>注意：Truncate只能删除管理表，不能删除外部表中数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; truncate table student;</span><br></pre></td></tr></table></figure><h1 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h1><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select</a></p><h2 id="查询语句语法"><a href="#查询语句语法" class="headerlink" title="查询语句语法"></a>查询语句语法</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[WITH CommonTableExpression (, CommonTableExpression)*]    (Note: Only available</span><br><span class="line"> starting with Hive 0.13.0)</span><br><span class="line"></span><br><span class="line">SELECT [ALL | DISTINCT] select_expr, select_expr, ...</span><br><span class="line">  FROM table_reference</span><br><span class="line">  [WHERE where_condition]</span><br><span class="line">  [GROUP BY col_list]</span><br><span class="line">  [ORDER BY col_list]</span><br><span class="line">  [CLUSTER BY col_list</span><br><span class="line">    | [DISTRIBUTE BY col_list] [SORT BY col_list]</span><br><span class="line">  ]</span><br><span class="line"> [LIMIT number]</span><br></pre></td></tr></table></figure><h3 id="基本查询（Select…From）"><a href="#基本查询（Select…From）" class="headerlink" title="基本查询（Select…From）"></a>基本查询（Select…From）</h3><h3 id="全表和特定列查询"><a href="#全表和特定列查询" class="headerlink" title="全表和特定列查询"></a>全表和特定列查询</h3><p>1．全表查询</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp;</span><br></pre></td></tr></table></figure><p>2．选择特定列查询</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select empno, ename from emp;</span><br></pre></td></tr></table></figure><p>注意：<br>（1）SQL 语言大小写不敏感。<br>（2）SQL 可以写在一行或者多行<br>（3）关键字不能被缩写也不能分行<br>（4）各子句一般要分行写。<br>（5）使用缩进提高语句的可读性。</p><h3 id="列别名"><a href="#列别名" class="headerlink" title="列别名"></a>列别名</h3><p>1．重命名一个列<br>2．便于计算<br>3．紧跟列名，也可以在列名和别名之间加入关键字‘AS’<br>4．案例实操<br>查询名称和部门</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename AS name, deptno dn from emp;</span><br></pre></td></tr></table></figure><h3 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h3><table><thead><tr><th>运算符</th><th>描述</th></tr></thead><tbody><tr><td>A+B</td><td>A和B 相加</td></tr><tr><td>A-B</td><td>A减去B</td></tr><tr><td>A*B</td><td>A和B 相乘</td></tr><tr><td>A/B</td><td>A除以B</td></tr><tr><td>A%B</td><td>A对B取余</td></tr><tr><td>A&amp;B</td><td>A和B按位取与</td></tr><tr><td>A|B</td><td>A和B按位取或</td></tr><tr><td>A^B</td><td>A和B按位取异或</td></tr><tr><td>~A</td><td>A按位取反</td></tr></tbody></table><h3 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h3><p>1．求总行数（count）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) cnt from emp;</span><br></pre></td></tr></table></figure><p>2．求工资的最大值（max）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select max(sal) max_sal from emp;</span><br></pre></td></tr></table></figure><p>3．求工资的最小值（min）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select min(sal) min_sal from emp;</span><br></pre></td></tr></table></figure><p>4．求工资的总和（sum）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select sum(sal) sum_sal from emp; </span><br></pre></td></tr></table></figure><p>5．求工资的平均值（avg）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select avg(sal) avg_sal from emp;</span><br></pre></td></tr></table></figure><h3 id="Limit语句"><a href="#Limit语句" class="headerlink" title="Limit语句"></a>Limit语句</h3><p>典型的查询会返回多行数据。LIMIT子句用于限制返回的行数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp limit 5;</span><br></pre></td></tr></table></figure><h3 id="Where语句"><a href="#Where语句" class="headerlink" title="Where语句"></a>Where语句</h3><p>1．使用WHERE子句，将不满足条件的行过滤掉<br>2．WHERE子句紧随FROM子句<br>3．案例实操<br>查询出薪水大于1000的所有员工</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal &gt;1000;</span><br></pre></td></tr></table></figure><h3 id="比较运算符（Between-In-Is-Null）"><a href="#比较运算符（Between-In-Is-Null）" class="headerlink" title="比较运算符（Between/In/ Is Null）"></a>比较运算符（Between/In/ Is Null）</h3><p>1）下面表中描述了谓词操作符，这些操作符同样可以用于JOIN…ON和HAVING语句中</p><table><thead><tr><th>操作符</th><th>支持的数据类型</th><th>描述</th></tr></thead><tbody><tr><td>A=B</td><td>基本数据类型</td><td>如果A等于B则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=&gt;B</td><td>基本数据类型</td><td>如果A和B都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL</td></tr><tr><td>A&lt;&gt;B, A!=B</td><td>基本数据类型</td><td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A [NOT] BETWEEN B AND C</td><td>基本数据类型</td><td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A IS NULL</td><td>所有数据类型</td><td>如果A等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>A IS NOT NULL</td><td>所有数据类型</td><td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>IN(数值1, 数值2)</td><td>所有数据类型</td><td>使用 IN运算显示列表中的值</td></tr><tr><td>A [NOT] LIKE B</td><td>STRING 类型</td><td>B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A RLIKE B, A REGEXP B</td><td>STRING 类型</td><td>B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td></tr></tbody></table><p>2）案例实操<br>（1）查询出薪水等于5000的所有员工</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal =5000;</span><br></pre></td></tr></table></figure><p>（2）查询工资在500到1000的员工信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal between 500 and 1000;</span><br></pre></td></tr></table></figure><p>（3）查询comm为空的所有员工信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where comm is null;</span><br></pre></td></tr></table></figure><p>（4）查询工资是1500或5000的员工信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal IN (1500, 5000);</span><br></pre></td></tr></table></figure><h3 id="Like和RLike"><a href="#Like和RLike" class="headerlink" title="Like和RLike"></a>Like和RLike</h3><p>1）使用LIKE运算选择类似的值<br>2）选择条件可以包含字符或数字:<br>% 代表零个或多个字符(任意个字符)。<br>_ 代表一个字符。<br>2）RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_285.png" alt="blog: www.xubatian.cn"></p><p>这个就是匹配我这里面有2和3的数字</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_286.png" alt="blog: www.xubatian.cn"></p><p>这个表示我要严格要求以2开头以3结束的匹配数据</p><p>4）案例实操<br>    （1）查找以2开头薪水的员工信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal LIKE &#x27;2%&#x27;;</span><br></pre></td></tr></table></figure><p>​    （2）查找第二个数值为2的薪水的员工信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal LIKE &#x27;_2%&#x27;;</span><br></pre></td></tr></table></figure><p>​    （3）查找薪水中含有2的员工信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal RLIKE &#x27;[2]&#x27;;</span><br></pre></td></tr></table></figure><h3 id="逻辑运算符（And-Or-Not）"><a href="#逻辑运算符（And-Or-Not）" class="headerlink" title="逻辑运算符（And/Or/Not）"></a>逻辑运算符（And/Or/Not）</h3><table><thead><tr><th>操作符</th><th>含义</th></tr></thead><tbody><tr><td>AND</td><td>逻辑并</td></tr><tr><td>OR</td><td>逻辑或</td></tr><tr><td>NOT</td><td>逻辑否</td></tr></tbody></table><p>案例实操<br>    （1）查询薪水大于1000，部门是30</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal&gt;1000 and dep tno=30;</span><br></pre></td></tr></table></figure><p>​    （2）查询薪水大于1000，或者部门是30</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal&gt;1000 or deptno=30;</span><br></pre></td></tr></table></figure><p>​    （3）查询除了20部门和30部门以外的员工信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where deptno not IN(30, 20);</span><br></pre></td></tr></table></figure><h1 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h1><h2 id="Group-By语句"><a href="#Group-By语句" class="headerlink" title="Group By语句"></a>Group By语句</h2><p>分组?什么时候下会用到分组?<br>把整个一张表拆分成多个数据<br>GROUP BY(分组)语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。<br>案例实操：<br>    （1）计算emp表每个部门的平均工资</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno;</span><br></pre></td></tr></table></figure><p>2）计算emp每个部门中每个岗位的最高薪水</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select t.deptno, t.job, max(t.sal) max_sal from emp t group by t.deptno, t.job;   主标识</span><br></pre></td></tr></table></figure><h2 id="Having语句"><a href="#Having语句" class="headerlink" title="Having语句"></a>Having语句</h2><p>1．having与where不同点</p><p>（1） where针对表中的列发挥作用，查询数据；<br>having针对查询结果中的列发挥作用，筛选数据。<br>（2）where后面不能写分组函数，而having后面可以使用分组函数。<br>（3）having只用于group by分组统计语句。</p><p>2．案例实操<br>（1）求每个部门的平均薪水大于2000的部门<br>求每个部门的平均工资</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select deptno, avg(sal) from emp group by deptno;</span><br></pre></td></tr></table></figure><p>​      求每个部门的平均薪水大于2000的部门</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select deptno, avg(sal) avg_sal from emp group by deptno having</span><br><span class="line"> avg_sal &gt; 2000;</span><br></pre></td></tr></table></figure><h1 id="Join语句"><a href="#Join语句" class="headerlink" title="Join语句"></a>Join语句</h1><h2 id="等值Join"><a href="#等值Join" class="headerlink" title="等值Join"></a>等值Join</h2><p>Hive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接。等值连接就是A表里面的某某字段等于B表里面的某某字段.<br>案例实操<br>（1）根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称；</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno, d.dname from emp e join dept d</span><br><span class="line"> on e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><h2 id="表的别名"><a href="#表的别名" class="headerlink" title="表的别名"></a>表的别名</h2><p>1．好处<br>（1）使用别名可以简化查询。<br>（2）使用表名前缀可以提高执行效率。<br>2．案例实操<br>合并员工表和部门表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno </span><br><span class="line">from emp e join dept d </span><br><span class="line">on e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><h2 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h2><p>内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno</span><br><span class="line"> = d.deptno;</span><br></pre></td></tr></table></figure><h2 id="左外连接"><a href="#左外连接" class="headerlink" title="左外连接"></a>左外连接</h2><p>左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno</span><br><span class="line"> = d.deptno;</span><br></pre></td></tr></table></figure><h2 id="右外连接"><a href="#右外连接" class="headerlink" title="右外连接"></a>右外连接</h2><p>右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno</span><br><span class="line"> = d.deptno;</span><br></pre></td></tr></table></figure><h2 id="满外连接"><a href="#满外连接" class="headerlink" title="满外连接"></a>满外连接</h2><p>​    满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno</span><br><span class="line"> = d.deptno;</span><br></pre></td></tr></table></figure><h2 id="多表连接"><a href="#多表连接" class="headerlink" title="多表连接"></a>多表连接</h2><p>注意：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</p><p>1．创建位置表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists default.location(</span><br><span class="line">loc int,</span><br><span class="line">loc_name string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>2．导入数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/location.txt&#x27; into table default.location;</span><br></pre></td></tr></table></figure><p>3．多表连接查询</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;SELECT e.ename, d.deptno, l.loc_name</span><br><span class="line">FROM   emp e </span><br><span class="line">JOIN   dept d</span><br><span class="line">ON     d.deptno = e.deptno </span><br><span class="line">JOIN   location l</span><br><span class="line">ON     d.loc = l.loc;</span><br></pre></td></tr></table></figure><p>大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l;进行连接操作。<br>注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的。</p><h2 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h2><p>1．笛卡尔集会在下面条件下产生<br>（1）省略连接条件<br>（2）连接条件无效<br>（3）所有表中的所有行互相连接<br>2．案例实操</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select empno, dname from emp, dept;</span><br></pre></td></tr></table></figure><h2 id="连接谓词中不支持or-但是and是支持的"><a href="#连接谓词中不支持or-但是and是支持的" class="headerlink" title="连接谓词中不支持or 但是and是支持的"></a>连接谓词中不支持or 但是and是支持的</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno</span><br><span class="line">= d.deptno or e.ename=d.ename;   错误的</span><br></pre></td></tr></table></figure><h1 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h1><h2 id="全局排序（Order-By）"><a href="#全局排序（Order-By）" class="headerlink" title="全局排序（Order By）"></a>全局排序（Order By）</h2><p>Order By：全局排序，会进入到一个Reducer里面,最终会生成一个结果文件<br>1．使用 ORDER BY 子句排序<br>ASC（ascend）: 升序（默    认）<br>DESC（descend）: 降序<br>2．ORDER BY 子句在SELECT语句的结尾<br>3．案例实操<br>    （1）查询员工信息按工资升序排列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp order by sal;</span><br></pre></td></tr></table></figure><p>​    （2）查询员工信息按工资降序排列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp order by sal desc;</span><br></pre></td></tr></table></figure><h2 id="按照别名排序"><a href="#按照别名排序" class="headerlink" title="按照别名排序"></a>按照别名排序</h2><p>按照员工薪水的2倍排序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename, sal*2 twosal from emp order by twosal;</span><br></pre></td></tr></table></figure><h2 id="多个列排序"><a href="#多个列排序" class="headerlink" title="多个列排序"></a>多个列排序</h2><p>按照部门和工资升序排序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename, deptno, sal from emp order by deptno, sal ;</span><br></pre></td></tr></table></figure><h2 id="每个MapReduce内部排序（Sort-By）"><a href="#每个MapReduce内部排序（Sort-By）" class="headerlink" title="每个MapReduce内部排序（Sort By）"></a>每个MapReduce内部排序（Sort By）</h2><p>什么情况下map进入多个reduce呢?  分区.  </p><p>Sort By：对每个Reducer里面的数据进行排序，对全局结果集来说不是排序。<br>Order by : 是对整个数据进行排序<br>1．设置reduce个数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces=3;</span><br></pre></td></tr></table></figure><p>2．查看设置reduce个数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces;</span><br></pre></td></tr></table></figure><p>3．根据部门编号降序查看员工信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp sort by empno desc;</span><br></pre></td></tr></table></figure><p>4．将查询结果导入到文件中（按照部门编号降序排序）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/datas/sortby-result&#x27;</span><br><span class="line"> select * from emp sort by deptno desc;</span><br></pre></td></tr></table></figure><p>MapReduce里面是按照hash来进行分区的</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_287.png" alt="blog: www.xubatian.cn"></p><p>可以看到,这是随机分配的,是没有规律的,我们一般排序肯定会先指定分区然后在排序,分区的话,一般我们会指定按照什么来分区,然而这里没有,说明这种排序方式不提供指定分区,从而出现了随机分配的现象</p><h2 id="分区排序（Distribute-By）"><a href="#分区排序（Distribute-By）" class="headerlink" title="分区排序（Distribute By）"></a>分区排序（Distribute By）</h2><p>这种方式确确实实分区了,分区之后你在哪个区就用sort by了</p><p>Distribute By：类似MR中partition，进行分区,就是先分好区,分好之后在每个partition里面进行sort by一下，结合sort by使用。</p><p>注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。<br>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><p>Distribute指定正常的分区字段,指定之后就可以正常的分区操作了</p><h2 id="Cluster-By"><a href="#Cluster-By" class="headerlink" title="Cluster By"></a>Cluster By</h2><p>当distribute by和sort by字段相同时，可以使用cluster by方式。<br>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。<br>Sort by可以指定是升序还是降序,但是用cluster by之后就不能指定了,只能是升序<br>1）以下两种写法等价</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp cluster by deptno;</span><br><span class="line">hive (default)&gt; select * from emp distribute by deptno sort by deptno;</span><br></pre></td></tr></table></figure><p>注意：按照部门编号分区，不一定就是固定死的数值，可以是30号和60号部门分到一个分区里面去</p><h1 id="其他常用查询函数"><a href="#其他常用查询函数" class="headerlink" title="其他常用查询函数"></a>其他常用查询函数</h1><h3 id="空字段赋值"><a href="#空字段赋值" class="headerlink" title="空字段赋值"></a>空字段赋值</h3><p>1.函数说明<br>NVL：给值为NULL的数据赋值，它的格式是NVL( string1, replace_with)。<br>它的功能是如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为NULL ，则返回NULL。<br>2.数据准备：采用员工表<br>3.查询：如果员工的comm为NULL，则用-1代替</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select nvl(comm,-1) from emp;</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">20.0</span><br><span class="line">300.0</span><br><span class="line">500.0</span><br><span class="line">-1.0</span><br><span class="line">1400.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br><span class="line">0.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br></pre></td></tr></table></figure><p>4.查询：如果员工的comm为NULL，则用领导id代替</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select nvl(comm,mgr) from emp;</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">20.0</span><br><span class="line">300.0</span><br><span class="line">500.0</span><br><span class="line">7839.0</span><br><span class="line">1400.0</span><br><span class="line">7839.0</span><br><span class="line">7839.0</span><br><span class="line">7566.0</span><br><span class="line">NULL</span><br><span class="line">0.0</span><br><span class="line">7788.0</span><br><span class="line">7698.0</span><br><span class="line">7566.0</span><br></pre></td></tr></table></figure><h3 id="CASE-WHEN-THEN-ELSE-END"><a href="#CASE-WHEN-THEN-ELSE-END" class="headerlink" title="CASE  WHEN  THEN  ELSE  END"></a>CASE  WHEN  THEN  ELSE  END</h3><h1 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h1><p>1．相关函数说明<br>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化<br>CURRENT ROW：当前行<br>n PRECEDING：往前n行数据<br>n FOLLOWING：往后n行数据<br>UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点<br>LAG(col,n)：往前第n行数据<br>LEAD(col,n)：往后第n行数据<br>NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。</p><h1 id="Rank"><a href="#Rank" class="headerlink" title="Rank"></a>Rank</h1><p>1．函数说明<br>RANK() 排序相同时会重复，总数不会变<br>DENSE_RANK() 排序相同时会重复，总数会减少<br>ROW_NUMBER() 会根据顺序计算</p><h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1><h2 id="系统内置函数"><a href="#系统内置函数" class="headerlink" title="系统内置函数"></a>系统内置函数</h2><p>1．查看系统自带的函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show functions;</span><br></pre></td></tr></table></figure><p>2．显示自带的函数的用法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc function upper;</span><br></pre></td></tr></table></figure><p>3．详细显示自带的函数的用法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc function extended upper;</span><br></pre></td></tr></table></figure><h2 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h2><p>1）Hive 自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。<br>2）当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。<br>3）根据用户自定义函数类别分为以下三种：<br>    （1）UDF（User-Defined-Function）<br>        一进一出<br>    （2）UDAF（User-Defined Aggregation Function）<br>        聚集函数，多进一出<br>        类似于：count/max/min<br>    （3）UDTF（User-Defined Table-Generating Functions）<br>        一进多出<br>        如lateral view explode()<br>4）官方文档地址<br><a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a><br>5）编程步骤：<br>    （1）继承org.apache.hadoop.hive.ql.UDF<br>    （2）需要实现evaluate函数；evaluate函数支持重载；<br>    （3）在hive的命令行窗口创建函数<br>        a）添加jar</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar linux_jar_path</span><br></pre></td></tr></table></figure><p>​        b）创建function，</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create [temporary] function [dbname.]function_name AS class_name;</span><br></pre></td></tr></table></figure><p>​    （4）在hive的命令行窗口删除函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Drop [temporary] function [if exists] [dbname.]function_name;</span><br></pre></td></tr></table></figure><p>6）注意事项<br>    （1）UDF必须要有返回类型，可以返回null，但是返回类型不能为void；</p><h2 id="自定义UDF函数"><a href="#自定义UDF函数" class="headerlink" title="自定义UDF函数"></a>自定义UDF函数</h2><p>1．创建一个Maven工程Hive<br>2．导入依赖</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hive-exec&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.2.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure><p>3．创建一个类</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">package com.shangbaishuyao.hive;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line">public class Lower extends UDF &#123;</span><br><span class="line"></span><br><span class="line">public String evaluate (final String s) &#123;</span><br><span class="line"></span><br><span class="line">if (s == null) &#123;</span><br><span class="line">return null;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">return s.toLowerCase();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="4"><li>打成jar包上传到服务器/opt/module/jars/udf.jar</li></ol><p>5．将jar包添加到hive的classpath,</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; add jar /opt/module/datas/udf.jar;</span><br></pre></td></tr></table></figure><p>6.创建临时函数与开发好的java class关联</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create [temporary] function [dbname.]function_name AS class_name;</span><br></pre></td></tr></table></figure><p>注意:如果你加了这个temporary的话,创建的函数就是临时的,窗口关闭就没了<br>如果你不加这个temporary的话,创建的就是永久的函数;function后面指定那个库,as后面关联的是你idea写的类的全类名</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create temporary function mylower as &quot;com.shangbaishuyao.hive.Lower&quot;;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_288.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_289.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_290.png" alt="blog: www.xubatian.cn"></p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_291.png" alt="blog: www.xubatian.cn"></p><p>7．即可在hql中使用自定义的函数strip </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename, mylower(ename) lowername from emp;</span><br></pre></td></tr></table></figure><h2 id="自定义UDTF函数"><a href="#自定义UDTF函数" class="headerlink" title="自定义UDTF函数"></a>自定义UDTF函数</h2><p>1）需求说明<br>自定义一个UDTF实现将一个任意分割符的字符串切割成独立的单词，例如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Line:<span class="string">&quot;hello,world,hadoop,hive&quot;</span></span><br><span class="line"></span><br><span class="line">Myudtf(line, <span class="string">&quot;,&quot;</span>)</span><br><span class="line"></span><br><span class="line">hello</span><br><span class="line">world</span><br><span class="line">hadoop</span><br><span class="line">hive</span><br></pre></td></tr></table></figure><p>2）代码实现</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shangbaishuyao.udtf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUDTF</span> <span class="keyword">extends</span> <span class="title">GenericUDTF</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> ArrayList&lt;String&gt; outList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StructObjectInspector <span class="title">initialize</span><span class="params">(StructObjectInspector argOIs)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">  <span class="comment">//1.定义输出数据的列名和类型</span></span><br><span class="line">    List&lt;String&gt; fieldNames = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    List&lt;ObjectInspector&gt; fieldOIs = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.添加输出数据的列名和类型</span></span><br><span class="line">    fieldNames.add(<span class="string">&quot;lineToWord&quot;</span>);</span><br><span class="line">    fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object[] args)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//1.获取原始数据</span></span><br><span class="line">    String arg = args[<span class="number">0</span>].toString();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.获取数据传入的第二个参数，此处为分隔符</span></span><br><span class="line">    String splitKey = args[<span class="number">1</span>].toString();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.将原始数据按照传入的分隔符进行切分</span></span><br><span class="line">    String[] fields = arg.split(splitKey);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.遍历切分后的结果，并写出</span></span><br><span class="line">    <span class="keyword">for</span> (String field : fields) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//集合为复用的，首先清空集合</span></span><br><span class="line">        outList.clear();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将每一个单词添加至集合</span></span><br><span class="line">        outList.add(field);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将集合内容写出</span></span><br><span class="line">        forward(outList);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>3）打成jar包上传到服务器/opt/module/data/udtf.jar<br>4）将jar包添加到hive的classpath下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; add jar /opt/module/data/udtf.jar;</span><br></pre></td></tr></table></figure><p>5）创建临时函数与开发好的java class关联</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create temporary function myudtf as &quot;com.shangbaishuyao.hive.MyUDTF&quot;;</span><br></pre></td></tr></table></figure><p>6）即可在hql中使用自定义的函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select myudtf(line, &quot;,&quot;) word from words;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_292.png" alt="blog: www.xubatian.cn"></p><h1 id="压缩和存储"><a href="#压缩和存储" class="headerlink" title="压缩和存储"></a>压缩和存储</h1><h2 id="snappy压缩"><a href="#snappy压缩" class="headerlink" title="snappy压缩"></a>snappy压缩</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_293.png" alt="blog: www.xubatian.cn"></p><p>所以hadoop本身是不支持的</p><p>不支持Snappy压缩的,如图,东西很少</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_294.png" alt="blog: www.xubatian.cn"></p><p>而支持Snapp压缩的东西就比较多了</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_295.png" alt="blog: www.xubatian.cn"></p><p>所以我们要把这些东西直接拷贝到我正在使用的hadoop下面把他覆盖一下就可以了</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_296.png" alt="blog: www.xubatian.cn"></p><p>拷贝完查看一下就ok了</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_297.png" alt="blog: www.xubatian.cn"></p><p>然后如果是集群的话,就分发一下</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_298.png" alt="blog: www.xubatian.cn"></p><h1 id="Hadoop压缩配置"><a href="#Hadoop压缩配置" class="headerlink" title="Hadoop压缩配置"></a>Hadoop压缩配置</h1><h2 id="MR支持的压缩编码"><a href="#MR支持的压缩编码" class="headerlink" title="MR支持的压缩编码"></a>MR支持的压缩编码</h2><table><thead><tr><th>压缩格式</th><th>工具</th><th>算法</th><th>文件扩展名</th><th>是否可切分</th></tr></thead><tbody><tr><td>DEFAULT</td><td>无</td><td>DEFAULT</td><td>.deflate</td><td>否</td></tr><tr><td>Gzip</td><td>gzip</td><td>DEFAULT</td><td>.gz</td><td>否</td></tr><tr><td>bzip2</td><td>bzip2</td><td>bzip2</td><td>.bz2</td><td>是</td></tr><tr><td>LZO</td><td>lzop</td><td>LZO</td><td>.lzo</td><td>是</td></tr><tr><td>Snappy</td><td>无</td><td>Snappy</td><td>.snappy</td><td>否</td></tr></tbody></table><p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示:</p><table><thead><tr><th>压缩格式</th><th>对应的编码/解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><p>压缩性能的比较:</p><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr></tbody></table><p><a href="http://google.github.io/snappy/">http://google.github.io/snappy/</a><br>On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.</p><h2 id="压缩参数配置"><a href="#压缩参数配置" class="headerlink" title="压缩参数配置"></a>压缩参数配置</h2><p>要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）</p><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs  （在core-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.Lz4Codec</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec</td><td>org.apache.hadoop.io.compress. DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.type</td><td>RECORD</td><td>reducer输出</td><td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td></tr></tbody></table><h2 id="开启Map输出阶段压缩"><a href="#开启Map输出阶段压缩" class="headerlink" title="开启Map输出阶段压缩"></a>开启Map输出阶段压缩</h2><p>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：<br>案例实操：<br>1．开启hive中间传输数据压缩功能</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set hive.exec.compress.intermediate=true;</span><br></pre></td></tr></table></figure><p>2．开启mapreduce中map输出压缩功能</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.map.output.compress=true;</span><br></pre></td></tr></table></figure><p>3．设置mapreduce中map输出数据的压缩方式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure><p>4．执行查询语句</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(ename) name from emp;</span><br></pre></td></tr></table></figure><p>结果展示:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_299.png" alt="blog: www.xubatian.cn"></p><h2 id="开启Reduce输出阶段压缩"><a href="#开启Reduce输出阶段压缩" class="headerlink" title="开启Reduce输出阶段压缩"></a>开启Reduce输出阶段压缩</h2><p>当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。<br>案例实操：<br>1．开启hive最终输出数据压缩功能</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set hive.exec.compress.output=true;</span><br></pre></td></tr></table></figure><p>2．开启mapreduce最终输出数据压缩</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.output.fileoutputformat.compress=true;</span><br></pre></td></tr></table></figure><p>3．设置mapreduce最终数据输出压缩方式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure><p>4．设置mapreduce最终数据输出压缩为块压缩,以为他默认的是record,所以我们改成块</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK;</span><br></pre></td></tr></table></figure><p>5．测试一下输出结果是否是压缩文件;如果我直接写在hive里面是看不到的<br>所以我们写在文件里面,用下面insert这种方式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory</span><br><span class="line"> &#x27;/opt/modules/datas/distribute-result&#x27; select * from emp distribute by deptno sort by empno desc;</span><br></pre></td></tr></table></figure><p>结果展示:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_300.png" alt="blog: www.xubatian.cn"></p><p>补充:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_300.png" alt="blog: www.xubatian.cn"></p><p>结果图示:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_302.png" alt="blog: www.xubatian.cn"></p><h1 id="文件存储格式"><a href="#文件存储格式" class="headerlink" title="文件存储格式"></a>文件存储格式</h1><p>Hive支持的存储数据的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。</p><h2 id="列式存储和行式存储"><a href="#列式存储和行式存储" class="headerlink" title="列式存储和行式存储"></a>列式存储和行式存储</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_303.png" alt="blog: www.xubatian.cn"></p><p>如图所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p><h3 id="1．行存储的特点"><a href="#1．行存储的特点" class="headerlink" title="1．行存储的特点"></a>1．行存储的特点</h3><p>查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p><h3 id="2．列存储的特点"><a href="#2．列存储的特点" class="headerlink" title="2．列存储的特点"></a>2．列存储的特点</h3><p>因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。<br>TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；<br>ORC和PARQUET是基于列式存储的。</p><h2 id="TextFile格式"><a href="#TextFile格式" class="headerlink" title="TextFile格式"></a>TextFile格式</h2><p>TextFile格式,本身这种格式是不带压缩的,必须要借助其他的压缩来压缩他,压缩时要注意,如果我将来数据要切分的话,我就要用可切分的压缩格式来压缩他<br>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。</p><h2 id="Orc格式"><a href="#Orc格式" class="headerlink" title="Orc格式"></a>Orc格式</h2><p>Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。<br>如图所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于RowGroup概念，不过大小由4MB-&gt;250MB，这样应该能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data(索引)，Row Data(具体的每一行数据)，Stripe Footer：</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_304.png" alt="blog: www.xubatian.cn"></p><p>1）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset。<br>2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。<br>3）Stripe Footer：存的是各个Stream的类型，长度等信息。<br>每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。</p><h2 id="Parquet格式"><a href="#Parquet格式" class="headerlink" title="Parquet格式"></a>Parquet格式</h2><p>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。<br>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。<br>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如图所示。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_306.png" alt="blog: www.xubatian.cn"></p><p>上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。</p><h2 id="存储文件的压缩比总结"><a href="#存储文件的压缩比总结" class="headerlink" title="存储文件的压缩比总结"></a>存储文件的压缩比总结</h2><p>ORC &gt;  Parquet &gt;  textFile</p><p><strong>注意orc是以256兆为单位来存的,其他方式就是按正常的128兆为一个块来存的</strong><br>在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;白衣执甲，逆行而上，以勇气和辛劳诠释了医者仁心，用担当和奉献换来了山河无恙。新时代中国女性可亲、可敬、可爱，她们在热血奋斗中怒放生命，在应对挑战中成就不凡。          ——人民日报                    &lt;/p&gt;
&lt;p&gt;​                                              &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="hive" scheme="http://xubatian.cn/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>hive简介(一)</title>
    <link href="http://xubatian.cn/hive%E7%AE%80%E4%BB%8B(%E4%B8%80)/"/>
    <id>http://xubatian.cn/hive%E7%AE%80%E4%BB%8B(%E4%B8%80)/</id>
    <published>2022-01-18T08:44:24.000Z</published>
    <updated>2022-01-23T02:58:21.777Z</updated>
    
    <content type="html"><![CDATA[<p>涓涓不塞，是为江河；源源不断，是为奋斗；生生不息，是为中国。——人民日报                </p><p>​                                              </p><span id="more"></span><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_239.jpg" alt="blog: www.xubatian.cn"></p><h1 id="Hive入门"><a href="#Hive入门" class="headerlink" title="Hive入门"></a>Hive入门</h1><p>做数据分析用的,因为Hive里面不存数据. hive底层是MapReduce</p><h2 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h2><p>Hive：由Facebook开源用于解决海量结构化日志的数据统计。<br>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。<br>本质是：将HQL转化成MapReduce程序</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_241.png" alt="blog: www.xubatian.cn"></p><p>1）Hive处理的数据存储在HDFS<br>2）Hive分析数据底层的实现是MapReduce<br>3）执行程序运行在Yarn上</p><h2 id="Hive的优缺点"><a href="#Hive的优缺点" class="headerlink" title="Hive的优缺点"></a>Hive的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>)操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。</span><br><span class="line"><span class="number">2</span>)避免了去写MapReduce，减少开发人员的学习成本。</span><br><span class="line"><span class="number">3</span>)Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合。</span><br><span class="line"><span class="number">4</span>)Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。</span><br><span class="line"><span class="number">5</span>)Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</span><br></pre></td></tr></table></figure><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>1．Hive的HQL表达能力有限</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">（<span class="number">1</span>）迭代式算法无法表达(就是原始数据经过运算得到一个数据结果传给下一个在计算以此类推.这就是迭代)</span><br><span class="line">（<span class="number">2</span>）数据挖掘方面不擅长</span><br></pre></td></tr></table></figure><p>2．Hive的效率比较低(以为他通过HQL自动匹配模板,所以模板不能随便修改)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">（<span class="number">1</span>）Hive自动生成的MapReduce作业，通常情况下不够自主化</span><br><span class="line">（<span class="number">2</span>）Hive调优比较困难，粒度较粗(因为他封装了,所以调优比较困难)</span><br></pre></td></tr></table></figure><h2 id="Hive架构原理"><a href="#Hive架构原理" class="headerlink" title="Hive架构原理"></a>Hive架构原理</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_242.png" alt="blog: www.xubatian.cn"></p><p>1．用户接口：Client</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive）</span><br></pre></td></tr></table></figure><p>2．元数据：Metastore</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">元数据包括：表名、表所属的数据库（默认是<span class="keyword">default</span>）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；</span><br><span class="line">默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore</span><br></pre></td></tr></table></figure><p>3．Hadoop</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用HDFS进行存储，使用MapReduce进行计算。</span><br></pre></td></tr></table></figure><p>4．驱动器：Driver</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（<span class="number">1</span>）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</span><br><span class="line">（<span class="number">2</span>）编译器（Physical Plan）：将AST编译生成逻辑执行计划。</span><br><span class="line">（<span class="number">3</span>）优化器（Query Optimizer）：对逻辑执行计划进行优化。</span><br><span class="line">（<span class="number">4</span>）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_244.png" alt="blog: www.xubatian.cn"></p><p>Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。</p><h2 id="Hive和数据库比较"><a href="#Hive和数据库比较" class="headerlink" title="Hive和数据库比较"></a>Hive和数据库比较</h2><p>由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。</p><h3 id="数据存储位置"><a href="#数据存储位置" class="headerlink" title="数据存储位置"></a>数据存储位置</h3><p>Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中</p><h3 id="查询语言"><a href="#查询语言" class="headerlink" title="查询语言"></a>查询语言</h3><p>由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。</p><h3 id="数据更新"><a href="#数据更新" class="headerlink" title="数据更新"></a>数据更新</h3><p>由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO …  VALUES 添加数据，使用 UPDATE … SET修改数据。</p><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><p>Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于 MapReduce 的引入， Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询。</p><h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><p>Hive中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。而数据库通常有自己的执行引擎。</p><h3 id="执行延迟"><a href="#执行延迟" class="headerlink" title="执行延迟"></a>执行延迟</h3><p>Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。</p><h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><p>由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的（世界上最大的Hadoop 集群在 Yahoo!，2009年的规模在4000 台节点左右）。而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库 Oracle 在理论上的扩展能力也只有100台左右。</p><h3 id="数据规模"><a href="#数据规模" class="headerlink" title="数据规模"></a>数据规模</h3><p>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。</p><h1 id="Hive安装"><a href="#Hive安装" class="headerlink" title="Hive安装"></a>Hive安装</h1><h2 id="Hive安装地址"><a href="#Hive安装地址" class="headerlink" title="Hive安装地址"></a>Hive安装地址</h2><p>1．Hive官网地址<br><a href="http://hive.apache.org/">http://hive.apache.org/</a><br>2．文档查看地址<br><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a><br>3．下载地址<br><a href="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a><br>4．github地址<br><a href="https://github.com/apache/hive">https://github.com/apache/hive</a></p><h2 id="Hive安装部署"><a href="#Hive安装部署" class="headerlink" title="Hive安装部署"></a>Hive安装部署</h2><p>1．Hive安装及配置<br>（1）把apache-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下<br>（2）解压apache-hive-1.2.1-bin.tar.gz到/opt/module/目录下面</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 software]$ tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>（3）修改apache-hive-1.2.1-bin.tar.gz的名称为hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ mv apache-hive-1.2.1-bin/ hive</span><br></pre></td></tr></table></figure><p>（4）修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ mv hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure><p>​    （5）配置hive-env.sh文件<br>​    （a）配置HADOOP_HOME路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure><p>​    （b）配置HIVE_CONF_DIR路径(hive配置文件的路径)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_CONF_DIR=/opt/module/hive/conf</span><br></pre></td></tr></table></figure><p>2．Hadoop集群配置<br>（1）必须启动hdfs和yarn</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[shangbaishuyao@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><p>（2）在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir /tmp</span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir -p /user/hive/warehouse</span><br><span class="line"></span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /tmp</span><br><span class="line">[shangbaishuyao@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_245.png" alt="blog: www.xubatian.cn"></p><h2 id="Hive基本操作"><a href="#Hive基本操作" class="headerlink" title="Hive基本操作"></a>Hive基本操作</h2><p>（1）启动hive</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/hive</span><br></pre></td></tr></table></figure><p>（2）查看数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure><p>（3）打开默认数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; use default;</span><br></pre></td></tr></table></figure><p>（4）显示default数据库中的表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure><p>（5）创建一张表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table student(id int, name string);</span><br></pre></td></tr></table></figure><p>（6）显示数据库中有几张表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure><p>（7）查看表的结构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc student;</span><br></pre></td></tr></table></figure><p>（8）向表中插入数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; insert into student values(1000,&quot;ss&quot;);</span><br></pre></td></tr></table></figure><p>（9）查询表中数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br></pre></td></tr></table></figure><p>（10）退出hive</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; quit;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_246.png" alt="blog: www.xubatian.cn"></p><h2 id="将本地文件导入Hive案例"><a href="#将本地文件导入Hive案例" class="headerlink" title="将本地文件导入Hive案例"></a>将本地文件导入Hive案例</h2><p>需求<br>将本地/opt/module/datas/student.txt这个目录下的数据导入到hive的student(id int, name string)表中。<br>1．数据准备<br>在/opt/module/datas这个目录下准备数据<br>（1）在/opt/module/目录下创建datas</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ mkdir datas</span><br></pre></td></tr></table></figure><p>（2）在/opt/module/datas/目录下创建student.txt文件并添加数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 datas]$ touch student.txt</span><br><span class="line">[shangbaishuyao@hadoop102 datas]$ vi student.txt</span><br><span class="line">1001zhangshan</span><br><span class="line">1002lishi</span><br><span class="line">1003zhaoliu</span><br></pre></td></tr></table></figure><p>注意以tab键间隔。<br>2．Hive实际操作<br>（1）启动hive</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/hive</span><br></pre></td></tr></table></figure><p>（2）显示数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure><p>（3）使用default数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; use default;</span><br></pre></td></tr></table></figure><p>（4）显示default数据库中的表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure><p>（5）删除已创建的student表 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop table student;</span><br></pre></td></tr></table></figure><p>（6）创建student表, 并声明文件分隔符’\t’</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED</span><br><span class="line"> BY &#x27;\t&#x27;;      我在创建表的时候就指定将来hive将来切割数据是如何切割,不然后面加载本地数据的时候出现这种情况</span><br><span class="line"> ROW FORMAT  :  行格式化</span><br><span class="line"> DELIMITED  : 每个字段的间隔</span><br><span class="line"> FIELDS TERMINATED BY &#x27;\t&#x27; : 通过’\t’形式切割 </span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_247.png" alt="blog: www.xubatian.cn"></p><p>本地文件导入hive案例<br>（7）加载/opt/module/datas/student.txt 文件到student数据库表中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data local inpath &#x27;/opt/module/datas/student.txt&#x27; into table student;</span><br></pre></td></tr></table></figure><p>将文件映射成student表<br>（8）Hive查询结果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br><span class="line">OK</span><br><span class="line">1001zhangshan</span><br><span class="line">1002lishi</span><br><span class="line">1003zhaoliu</span><br><span class="line">Time taken: 0.266 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure><p>3．遇到的问题<br>再打开一个客户端窗口启动hive，会产生java.sql.SQLException异常。</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_248.png" alt="blog: www.xubatian.cn"></p><p>所以不能同时打开多个hive连接,如何解决呢?就是不用Derby数据库,使用mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.RuntimeException: java.lang.RuntimeException:</span><br><span class="line"> Unable to instantiate</span><br><span class="line"> org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</span><br><span class="line">        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)</span><br><span class="line">        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)</span><br><span class="line">        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)</span><br><span class="line">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">        at java.lang.reflect.Method.invoke(Method.java:606)</span><br><span class="line">        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)</span><br><span class="line">        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</span><br><span class="line">Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</span><br><span class="line">        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:86)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)</span><br><span class="line">        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)</span><br><span class="line">        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)</span><br><span class="line">        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)</span><br><span class="line">... 8 more</span><br></pre></td></tr></table></figure><p>原因是，Metastore默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore;</p><h2 id="MySql安装"><a href="#MySql安装" class="headerlink" title="MySql安装"></a>MySql安装</h2><h3 id="安装包准备"><a href="#安装包准备" class="headerlink" title="安装包准备"></a>安装包准备</h3><p>1．查看mysql是否安装，如果安装了，卸载mysql<br>    （1）查看</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 桌面]# rpm -qa|grep mysql</span><br><span class="line">mysql-libs-5.1.73-7.el6.x86_64</span><br></pre></td></tr></table></figure><p>​    （2）卸载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@Hadoop102 mysql-libs]$ sudo rpm -e mysql-libs-5.1.73-7.el6.x86_64    删不掉,因为有依赖,所以加上--nodeps</span><br><span class="line">error: Failed dependencies:</span><br><span class="line">libmysqlclient.so.16()(64bit) is needed by (installed) postfix-2:2.6.6-6.el6_7.1.x86_64</span><br><span class="line">libmysqlclient.so.16(libmysqlclient_16)(64bit) is needed by (installed) postfix-2:2.6.6-6.el6_7.1.x86_64</span><br><span class="line">mysql-libs is needed by (installed) postfix-2:2.6.6-6.el6_7.1.x86_64</span><br><span class="line">[shangbaishuyao@Hadoop102 mysql-libs]$ </span><br><span class="line">[root@hadoop102 桌面]# rpm -e --nodeps mysql-libs-5.1.73-7.el6.x86_64</span><br></pre></td></tr></table></figure><p>2．解压mysql-libs.zip文件到当前目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# unzip mysql-libs.zip</span><br><span class="line">[root@hadoop102 software]# ls</span><br><span class="line">mysql-libs.zip</span><br><span class="line">mysql-libs</span><br></pre></td></tr></table></figure><p>3．进入到mysql-libs文件夹下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# ll</span><br><span class="line">总用量 76048</span><br><span class="line">-rw-r--r--. 1 root root 18509960 3月  26 2015 MySQL-client-5.6.24-1.el6.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 root root  3575135 12月  1 2013 mysql-connector-java-5.1.27.tar.gz</span><br><span class="line">-rw-r--r--. 1 root root 55782196 3月  26 2015 MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure><h3 id="安装MySql服务器"><a href="#安装MySql服务器" class="headerlink" title="安装MySql服务器"></a>安装MySql服务器</h3><p>1．安装mysql服务端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@Hadoop102 mysql-libs]$ rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br><span class="line">error: can&#x27;t create transaction lock on /var/lib/rpm/.rpm.lock (权限不够)</span><br><span class="line">[root@hadoop102 mysql-libs]# sudo rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure><p>2．查看产生的随机密码</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_249.png" alt="blog: www.xubatian.cn"></p><p>所以必须修改随机密码:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# cat /root/.mysql_secret</span><br><span class="line">OEXaQuS8IWkG19Xs</span><br></pre></td></tr></table></figure><p>如果无法进入则:切换用户来进入</p><p>3．查看mysql状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# service mysql status</span><br></pre></td></tr></table></figure><p>启动mysql和查看状态:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@Hadoop102 ~]$ su - root</span><br><span class="line">密码：</span><br><span class="line">[root@Hadoop102 ~]# service mysql start</span><br><span class="line">Starting MySQL..                                           [确定]</span><br><span class="line">[root@Hadoop102 ~]# service mysql status</span><br><span class="line">MySQL running (5912)                                       [确定]</span><br><span class="line">[root@Hadoop102 ~]# </span><br></pre></td></tr></table></figure><p>4．启动mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# service mysql start</span><br></pre></td></tr></table></figure><h3 id="安装MySql客户端"><a href="#安装MySql客户端" class="headerlink" title="安装MySql客户端"></a>安装MySql客户端</h3><p>1．安装mysql客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure><p>2．链接mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# mysql -uroot -pOEXaQuS8IWkG19Xs</span><br></pre></td></tr></table></figure><p>3．修改密码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash">SET PASSWORD=PASSWORD(<span class="string">&#x27;000000&#x27;</span>);</span></span><br></pre></td></tr></table></figure><p>4．退出mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"><span class="built_in">exit</span></span></span><br></pre></td></tr></table></figure><p>删除mysql上不干净怎么办?一定要把这个删了</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_250.png" alt="blog: www.xubatian.cn"></p><h3 id="MySql中user表中主机配置"><a href="#MySql中user表中主机配置" class="headerlink" title="MySql中user表中主机配置"></a>MySql中user表中主机配置</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@Hadoop102</span> mysql<span class="operator">-</span>libs]# mysql <span class="operator">-</span>uroot <span class="operator">-</span>pshangbaishuyao</span><br><span class="line">Warning: <span class="keyword">Using</span> a password <span class="keyword">on</span> the command line interface can be insecure.</span><br><span class="line">Welcome <span class="keyword">to</span> the MySQL monitor.  Commands <span class="keyword">end</span> <span class="keyword">with</span> ; <span class="keyword">or</span> \g.</span><br><span class="line">Your MySQL connection id <span class="keyword">is</span> <span class="number">2</span></span><br><span class="line">Server version: <span class="number">5.6</span><span class="number">.24</span> MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) <span class="number">2000</span>, <span class="number">2015</span>, Oracle <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its affiliates. <span class="keyword">All</span> rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle <span class="keyword">is</span> a registered trademark <span class="keyword">of</span> Oracle Corporation <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its</span><br><span class="line">affiliates. Other names may be trademarks <span class="keyword">of</span> their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type <span class="string">&#x27;help;&#x27;</span> <span class="keyword">or</span> <span class="string">&#x27;\h&#x27;</span> <span class="keyword">for</span> help. Type <span class="string">&#x27;\c&#x27;</span> <span class="keyword">to</span> clear the <span class="keyword">current</span> input statement.</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> databses ;</span><br><span class="line">ERROR <span class="number">1064</span> (<span class="number">42000</span>): You have an error <span class="keyword">in</span> your <span class="keyword">SQL</span> syntax; <span class="keyword">check</span> the manual that corresponds <span class="keyword">to</span> your MySQL server version <span class="keyword">for</span> the <span class="keyword">right</span> syntax <span class="keyword">to</span> use near <span class="string">&#x27;databses&#x27;</span> <span class="keyword">at</span> line <span class="number">1</span></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> databases ;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> Database           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> information_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mysql              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> performance_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> test               <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="number">4</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.05</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> use mysql ;</span><br><span class="line">Reading <span class="keyword">table</span> information <span class="keyword">for</span> completion <span class="keyword">of</span> <span class="keyword">table</span> <span class="keyword">and</span> <span class="keyword">column</span> names</span><br><span class="line">You can turn off this feature <span class="keyword">to</span> <span class="keyword">get</span> a quicker startup <span class="keyword">with</span> <span class="operator">-</span>A</span><br><span class="line"></span><br><span class="line">Database changed</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> tables ;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="operator">|</span> Tables_in_mysql           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="operator">|</span> columns_priv              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> db                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> event                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> func                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> general_log               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_category             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_keyword              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_relation             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_topic                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> innodb_index_stats        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> innodb_table_stats        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> ndb_binlog_index          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> plugin                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> proc                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> procs_priv                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> proxies_priv              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> servers                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_master_info         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_relay_log_info      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_worker_info         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slow_log                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tables_priv               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_leap_second     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_name            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_transition      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_transition_type <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">user</span>                      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="number">28</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">desc</span> <span class="keyword">user</span> ；</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">desc</span> <span class="keyword">user</span> ;</span><br><span class="line">ERROR <span class="number">1064</span> (<span class="number">42000</span>): You have an error <span class="keyword">in</span> your <span class="keyword">SQL</span> syntax; <span class="keyword">check</span> the manual that corresponds <span class="keyword">to</span> your MySQL server version <span class="keyword">for</span> the <span class="keyword">right</span> syntax <span class="keyword">to</span> use near <span class="string">&#x27;desc user&#x27;</span> <span class="keyword">at</span> line <span class="number">3</span></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">desc</span> <span class="keyword">user</span></span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">desc</span> <span class="keyword">user</span>;</span><br><span class="line">ERROR <span class="number">1064</span> (<span class="number">42000</span>): You have an error <span class="keyword">in</span> your <span class="keyword">SQL</span> syntax; <span class="keyword">check</span> the manual that corresponds <span class="keyword">to</span> your MySQL server version <span class="keyword">for</span> the <span class="keyword">right</span> syntax <span class="keyword">to</span> use near <span class="string">&#x27;desc user&#x27;</span> <span class="keyword">at</span> line <span class="number">2</span></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">desc</span> <span class="keyword">user</span> ;</span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------+-----------------------------------+------+-----+-----------------------+-------+</span></span><br><span class="line"><span class="operator">|</span> Field                  <span class="operator">|</span> Type                              <span class="operator">|</span> <span class="keyword">Null</span> <span class="operator">|</span> Key <span class="operator">|</span> <span class="keyword">Default</span>               <span class="operator">|</span> Extra <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------+-----------------------------------+------+-----+-----------------------+-------+</span></span><br><span class="line"><span class="operator">|</span> Host                   <span class="operator">|</span> <span class="type">char</span>(<span class="number">60</span>)                          <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span> PRI <span class="operator">|</span>                       <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">User</span>                   <span class="operator">|</span> <span class="type">char</span>(<span class="number">16</span>)                          <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span> PRI <span class="operator">|</span>                       <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Password               <span class="operator">|</span> <span class="type">char</span>(<span class="number">41</span>)                          <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span>                       <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Select_priv            <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Insert_priv            <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Update_priv            <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Delete_priv            <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Create_priv            <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Drop_priv              <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Reload_priv            <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Shutdown_priv          <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Process_priv           <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> File_priv              <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Grant_priv             <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> References_priv        <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Index_priv             <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Alter_priv             <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Show_db_priv           <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Super_priv             <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Create_tmp_table_priv  <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Lock_tables_priv       <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Execute_priv           <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Repl_slave_priv        <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Repl_client_priv       <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Create_view_priv       <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Show_view_priv         <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Create_routine_priv    <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Alter_routine_priv     <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Create_user_priv       <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Event_priv             <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Trigger_priv           <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Create_tablespace_priv <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> ssl_type               <span class="operator">|</span> enum(<span class="string">&#x27;&#x27;</span>,<span class="string">&#x27;ANY&#x27;</span>,<span class="string">&#x27;X509&#x27;</span>,<span class="string">&#x27;SPECIFIED&#x27;</span>) <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span>                       <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> ssl_cipher             <span class="operator">|</span> <span class="type">blob</span>                              <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> x509_issuer            <span class="operator">|</span> <span class="type">blob</span>                              <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> x509_subject           <span class="operator">|</span> <span class="type">blob</span>                              <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> max_questions          <span class="operator">|</span> <span class="type">int</span>(<span class="number">11</span>) unsigned                  <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="number">0</span>                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> max_updates            <span class="operator">|</span> <span class="type">int</span>(<span class="number">11</span>) unsigned                  <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="number">0</span>                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> max_connections        <span class="operator">|</span> <span class="type">int</span>(<span class="number">11</span>) unsigned                  <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="number">0</span>                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> max_user_connections   <span class="operator">|</span> <span class="type">int</span>(<span class="number">11</span>) unsigned                  <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="number">0</span>                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> plugin                 <span class="operator">|</span> <span class="type">char</span>(<span class="number">64</span>)                          <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> mysql_native_password <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> authentication_string  <span class="operator">|</span> text                              <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> password_expired       <span class="operator">|</span> enum(<span class="string">&#x27;N&#x27;</span>,<span class="string">&#x27;Y&#x27;</span>)                     <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> N                     <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------+-----------------------------------+------+-----+-----------------------+-------+</span></span><br><span class="line"><span class="number">43</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.21</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> host,<span class="keyword">user</span>,password <span class="keyword">from</span> <span class="keyword">user</span> ;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------+-------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> host      <span class="operator">|</span> <span class="keyword">user</span> <span class="operator">|</span> password                                  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------+-------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> localhost <span class="operator">|</span> root  <span class="operator">|</span> <span class="operator">*</span><span class="number">0877</span>ABF89C5BE70C33A37E82834E8A9D03060F71 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hadoop102 <span class="operator">|</span> root <span class="operator">|</span> <span class="operator">*</span>FD17D48A578685F9749FDF93CACE07FE9D22C41D <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> <span class="operator">|</span> root  <span class="operator">|</span> <span class="operator">*</span>FD17D48A578685F9749FDF93CACE07FE9D22C41D <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> ::<span class="number">1</span>       <span class="operator">|</span> root <span class="operator">|</span> <span class="operator">*</span>FD17D48A578685F9749FDF93CACE07FE9D22C41D <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------+-------------------------------------------+</span></span><br><span class="line">可以看到只有这几台机器能连上,所以我们要修改,我们只需要第一个,其他几个都不需要,因为只有第一个我们修改了密码是:shangbaishuyao,其他都不是</span><br><span class="line"><span class="number">4</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> host <span class="operator">!=</span>&quot;localhost&quot;;</span><br><span class="line">Query OK, <span class="number">3</span> <span class="keyword">rows</span> affected (<span class="number">0.06</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> host,<span class="keyword">user</span>,password <span class="keyword">from</span> <span class="keyword">user</span> ;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------+-------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> host      <span class="operator">|</span> <span class="keyword">user</span> <span class="operator">|</span> password                                  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------+-------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> localhost <span class="operator">|</span> root <span class="operator">|</span> <span class="operator">*</span><span class="number">0877</span>ABF89C5BE70C33A37E82834E8A9D03060F71 <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------+-------------------------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> update <span class="keyword">user</span> <span class="keyword">set</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">where</span> host<span class="operator">=</span><span class="string">&#x27;localhost&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> host,<span class="keyword">user</span>,password <span class="keyword">from</span> <span class="keyword">user</span> ;</span><br><span class="line"><span class="operator">+</span><span class="comment">------+------+-------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> host <span class="operator">|</span> <span class="keyword">user</span> <span class="operator">|</span> password                                  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+------+-------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">%</span>    <span class="operator">|</span> root <span class="operator">|</span> <span class="operator">*</span><span class="number">0877</span>ABF89C5BE70C33A37E82834E8A9D03060F71 <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+------+-------------------------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> </span><br></pre></td></tr></table></figure><p>如图可以看到:任意机器都可以访问了</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_251.png" alt="blog: www.xubatian.cn"></p><p>一定要记得刷新权限:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_252.png" alt="blog: www.xubatian.cn"></p><p>配置只要是root用户+密码，在任何主机上都能登录MySQL数据库。<br>1．进入mysql</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# mysql -uroot -pshangbaishuyao</span><br></pre></td></tr></table></figure><p>2．显示数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;show databases;</span><br></pre></td></tr></table></figure><p>3．使用mysql数据库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;use mysql;</span><br></pre></td></tr></table></figure><p>4．展示mysql数据库中的所有表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;show tables;</span><br></pre></td></tr></table></figure><p>5．展示user表的结构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;desc user;</span><br></pre></td></tr></table></figure><p>6．查询user表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;select User, Host, Password from user;</span><br></pre></td></tr></table></figure><p>7．修改user表，把Host表内容修改为%</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;update user set host=&#x27;%&#x27; where host=&#x27;localhost&#x27;;</span><br></pre></td></tr></table></figure><p>8．删除root用户的其他host</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;delete from user where Host=&#x27;hadoop10&#x27;;</span><br><span class="line">mysql&gt;delete from user where Host=&#x27;127.0.0.1&#x27;;</span><br><span class="line">mysql&gt;delete from user where Host=&#x27;::1&#x27;;</span><br></pre></td></tr></table></figure><p>9．刷新</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;flush privileges;</span><br></pre></td></tr></table></figure><p>10．退出</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;quit;</span><br></pre></td></tr></table></figure><h2 id="Hive元数据配置到MySql"><a href="#Hive元数据配置到MySql" class="headerlink" title="Hive元数据配置到MySql"></a>Hive元数据配置到MySql</h2><h3 id="驱动拷贝"><a href="#驱动拷贝" class="headerlink" title="驱动拷贝"></a>驱动拷贝</h3><p>1．在/opt/software/mysql-libs目录下解压mysql-connector-java-5.1.27.tar.gz驱动包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# tar -zxvf mysql-connector-java-5.1.27.tar.gz</span><br></pre></td></tr></table></figure><p>2．拷贝/opt/software/mysql-libs/mysql-connector-java-5.1.27目录下的mysql-connector-java-5.1.27-bin.jar到/opt/module/hive/lib/</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-connector-java-5.1.27]# cp mysql-connector-java-5.1.27-bin.jar  /opt/module/hive/lib/</span><br></pre></td></tr></table></figure><p>2.5.2 配置Metastore到MySql<br>1．在/opt/module/hive/conf目录下创建一个hive-site.xml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ touch hive-site.xml</span><br><span class="line">[shangbaishuyao@hadoop102 conf]$ vi hive-site.xml</span><br></pre></td></tr></table></figure><p>2．根据官方文档配置参数，拷贝数据到hive-site.xml文件中</p><p><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration">https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!--使用JDBC连接那个库 ,当发现不存在直接创建出来--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;000000&lt;/value&gt;      这个密码一定要改为:shangbaishuyao</span><br><span class="line">  &lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>3．配置完毕后，如果启动hive异常，可以重新启动虚拟机。（重启后，别忘了启动hadoop集群）</p><h3 id="多窗口启动Hive测试"><a href="#多窗口启动Hive测试" class="headerlink" title="多窗口启动Hive测试"></a>多窗口启动Hive测试</h3><p>1．先启动MySQL</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao<span class="variable">@hadoop102</span> mysql<span class="operator">-</span>libs]$ mysql <span class="operator">-</span>uroot <span class="operator">-</span>p000000</span><br><span class="line">查看有几个数据库</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> Database           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> information_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mysql             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> performance_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> test               <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br></pre></td></tr></table></figure><p>2．再次打开多个窗口，分别启动hive</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/hive</span><br></pre></td></tr></table></figure><p>3．启动hive后，回到MySQL窗口查看数据库，显示增加了metastore数据库</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> Database           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> information_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> metastore          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mysql             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> performance_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> test               <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br></pre></td></tr></table></figure><h2 id="HiveJDBC访问"><a href="#HiveJDBC访问" class="headerlink" title="HiveJDBC访问"></a>HiveJDBC访问</h2><p>为什么使用hiveJDBC形式连接呢?</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_253.png" alt="blog: www.xubatian.cn"></p><p>准备1:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_254.png" alt="blog: www.xubatian.cn"></p><p>准备2:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_255.png" alt="blog: www.xubatian.cn"></p><h3 id="启动hiveserver2服务-CDH中不用启动这个"><a href="#启动hiveserver2服务-CDH中不用启动这个" class="headerlink" title="启动hiveserver2服务,CDH中不用启动这个"></a>启动hiveserver2服务,CDH中不用启动这个</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/hiveserver2</span><br></pre></td></tr></table></figure><h3 id="启动beeline-在CDH中任何目录下直接beeline"><a href="#启动beeline-在CDH中任何目录下直接beeline" class="headerlink" title="启动beeline,在CDH中任何目录下直接beeline"></a>启动beeline,在CDH中任何目录下直接beeline</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/beeline</span><br><span class="line">Beeline version 1.2.1 by Apache Hive</span><br><span class="line"><span class="meta">beeline&gt;</span><span class="bash"></span></span><br><span class="line"><span class="bash">2.6.3 连接hiveserver2</span></span><br><span class="line"><span class="meta">beeline&gt;</span><span class="bash"> !connect jdbc:hive2://hadoop102:10000（回车）</span></span><br><span class="line">Connecting to jdbc:hive2://hadoop102:10000</span><br><span class="line">Enter username for jdbc:hive2://hadoop102:10000: shangbaishuyao（回车）</span><br><span class="line">Enter password for jdbc:hive2://hadoop102:10000: （直接回车）</span><br><span class="line">Connected to: Apache Hive (version 1.2.1)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://hadoop102:10000&gt; show databases;</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default        |</span><br><span class="line">| hive_db2       |</span><br><span class="line">+----------------+--+</span><br></pre></td></tr></table></figure><p>上白书妖补充完成图片:<br>Hive账号密码:</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_256.png" alt="blog: www.xubatian.cn"></p><h3 id="Hive常用交互命令"><a href="#Hive常用交互命令" class="headerlink" title="Hive常用交互命令"></a>Hive常用交互命令</h3><p>他不是连到hive客户端里面用的,他是连接到</p><p>正常情况下,我们要连接到hive客户端里面才能操作.如下:</p><p>1．“-e”不进入hive的交互窗口执行sql语句</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/hive -e &quot;select id from student;&quot;</span><br></pre></td></tr></table></figure><p>2．“-f”执行脚本中sql语句<br>    （1）在/opt/module/datas目录下创建hivef.sql文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 datas]$ touch hivef.sql</span><br></pre></td></tr></table></figure><p>​        文件中写入正确的sql语句<br>​        select *from student;<br>​    （2）执行文件中的sql语句</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql</span><br></pre></td></tr></table></figure><p>（3）执行文件中的sql语句并将结果写入文件中</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql  &gt; /opt/module/datas/hive_result.txt</span><br></pre></td></tr></table></figure><h3 id="Hive其他命令操作"><a href="#Hive其他命令操作" class="headerlink" title="Hive其他命令操作"></a>Hive其他命令操作</h3><p>1．退出hive窗口：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash"><span class="built_in">exit</span>;</span></span><br><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">quit;</span></span><br></pre></td></tr></table></figure><p>在新版的hive中没区别了，在以前的版本是有的：<br>exit:先隐性提交数据，再退出；<br>quit:不提交数据，退出；<br>2．在hive cli命令窗口中如何查看hdfs文件系统</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;dfs -ls /;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_257.png" alt="blog: www.xubatian.cn"></p><p>3．在hive cli命令窗口中如何查看本地文件系统</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;! ls /opt/module/datas;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_258.png" alt="blog: www.xubatian.cn"></p><p>4．查看在hive中输入的所有历史命令<br>    （1）进入到当前用户的根目录/root或/home/shangbaishuyao<br>    （2）查看. hivehistory文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 ~]$ cat .hivehistory</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_259.png" alt="blog: www.xubatian.cn"></p><h2 id="Hive常见属性配置"><a href="#Hive常见属性配置" class="headerlink" title="Hive常见属性配置"></a>Hive常见属性配置</h2><h3 id="Hive数据仓库位置配置"><a href="#Hive数据仓库位置配置" class="headerlink" title="Hive数据仓库位置配置"></a>Hive数据仓库位置配置</h3><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_260.png" alt="blog: www.xubatian.cn"></p><p>因为他就是我们默认default这个库的最原始位置.<br>    1）Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse路径下。<br>    2）在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹。<br>    3）修改default数据仓库原始位置（将hive-default.xml.template如下配置信息拷贝到hive-site.xml文件中）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;location of default database for the warehouse&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>配置同组用户有执行权限</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure><h3 id="查询后信息显示配置"><a href="#查询后信息显示配置" class="headerlink" title="查询后信息显示配置"></a>查询后信息显示配置</h3><p>1）在hive-site.xml文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.cli.print.header&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--当前是哪一个databases--&gt; </span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>上白书妖补充效果图:<br>这种方式只是在当时有用,退出重进之后就没用了,所以我们要进行上面配置<br>文件的配置</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_261.png" alt="blog: www.xubatian.cn"></p><p>2）重新启动hive，对比配置前后差异。<br>（1）配置前，如图所示</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_262.png" alt="blog: www.xubatian.cn"></p><p>（2）配置后，如图所示</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_263.png" alt="blog: www.xubatian.cn"></p><h3 id="Hive运行日志信息配置"><a href="#Hive运行日志信息配置" class="headerlink" title="Hive运行日志信息配置"></a>Hive运行日志信息配置</h3><p>1．Hive的log默认存放在/tmp/shangbaishuyao/hive.log目录下（当前用户名下）</p><p>因为放在tmp目录下,他是会定期清除的,所以要修改<br>2．修改hive的log存放日志到/opt/module/hive/logs<br>    （1）修改/opt/module/hive/conf/hive-log4j.properties.template文件名称为<br>hive-log4j.properties</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ pwd</span><br><span class="line">/opt/module/hive/conf</span><br><span class="line">[shangbaishuyao@hadoop102 conf]$ mv hive-log4j.properties.template hive-log4j.properties</span><br></pre></td></tr></table></figure><p>   （2）在hive-log4j.properties文件中修改log存放位置</p><p>hive.log.dir=/opt/module/hive/logs</p><h3 id="参数配置方式"><a href="#参数配置方式" class="headerlink" title="参数配置方式"></a>参数配置方式</h3><p>Hive的底层是hadoop,所以就逃不开hadoop四个默认的配置文件以及四个自定义文件如:core-site.xml等,所以说我们在启动hive的时候,这八个文件肯定会去加载的,这个我们hadoop的时候学过,所以我们一下配置就没加入,但是如果你hadoop都没有配置的话,你得先从hadoop开始配置</p><p>1．查看当前所有的配置信息<br>hive&gt;set;</p><h3 id="参数的配置三种方式"><a href="#参数的配置三种方式" class="headerlink" title="参数的配置三种方式"></a>参数的配置三种方式</h3><p>  三种方式中: 修改xml文件是永久生效,其他只是当前生效<br>（1）第一种:配置文件方式<br>默认配置文件：hive-default.xml<br>用户自定义配置文件：hive-site.xml<br>    注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。<br>（2）第二种:命令行参数方式<br>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。<br>例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;</span><br></pre></td></tr></table></figure><p>设置参数<br>注意: 如果只是set mapred.reduce.tasks;就是查看<br>               set mapred.reduce.tasks = 10; 这是设置成10个<br>注意：仅对本次hive启动有效</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_264.png" alt="blog: www.xubatian.cn"></p><p>查看参数设置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure><p>shangbaishuayo补充:默认是-1</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_265.png" alt="blog: www.xubatian.cn"></p><p>（3）第三种:参数声明方式<br>可以在HQL中使用SET关键字设定参数<br>例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks=100;</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_266.png" alt="blog: www.xubatian.cn"></p><p>注意：仅对本次hive启动有效。<br>查看参数设置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure><p>上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。配置文件的方式是永久有效的</p><h1 id="Hive数据类型"><a href="#Hive数据类型" class="headerlink" title="Hive数据类型"></a>Hive数据类型</h1><h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><table><thead><tr><th>Hive数据类型</th><th>Java数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT  (tinyint)</td><td>byte</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT (smalint)</td><td>short</td><td>2byte有符号整数</td><td>20</td></tr><tr><td>INT    (int)</td><td>int</td><td>4byte有符号整数</td><td>20</td></tr><tr><td>BIGINT  (bigint)</td><td>long</td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN (boolean)</td><td>boolean</td><td>布尔类型，true或者false</td><td>TRUE  FALSE</td></tr><tr><td>FLOAT   (float)</td><td>float</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td>DOUBLE  (double)</td><td>double</td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td>STRING  (string)</td><td>string</td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP (timestamp)</td><td></td><td>时间类型</td><td></td></tr><tr><td>BINARY  (binary)</td><td></td><td>字节数组</td><td></td></tr></tbody></table><p>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</p><h2 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h2><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct()</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map()</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array()</td></tr></tbody></table><p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p><h2 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h2><p>1）假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;songsong&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;friends&quot;</span>: [<span class="string">&quot;bingbing&quot;</span> , <span class="string">&quot;lili&quot;</span>] ,       <span class="comment">//列表Array, </span></span><br><span class="line">    <span class="attr">&quot;children&quot;</span>: &#123;                      <span class="comment">//键值Map,</span></span><br><span class="line">        <span class="attr">&quot;xiao song&quot;</span>: <span class="number">18</span> ,</span><br><span class="line">        <span class="attr">&quot;xiaoxiao song&quot;</span>: <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">&quot;address&quot;</span>: &#123;                      <span class="comment">//结构Struct,</span></span><br><span class="line">        <span class="attr">&quot;street&quot;</span>: <span class="string">&quot;hui long guan&quot;</span> ,</span><br><span class="line">        <span class="attr">&quot;city&quot;</span>: <span class="string">&quot;beijing&quot;</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2）基于上述数据结构，我们在Hive里创建对应的表，并导入数据。<br>创建本地测试文件test.txt   在../datas/test.txt</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure><p>注意：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。<br>3）Hive上创建测试表test<br>_</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test(</span><br><span class="line">name string,</span><br><span class="line">friends <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>,</span><br><span class="line">children map<span class="operator">&lt;</span>string, <span class="type">int</span><span class="operator">&gt;</span>,</span><br><span class="line">address struct<span class="operator">&lt;</span>street:string, city:string<span class="operator">&gt;</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">collection items terminated <span class="keyword">by</span> <span class="string">&#x27;_&#x27;</span></span><br><span class="line">map keys terminated <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span></span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br></pre></td></tr></table></figure><p>字段解释：<br>row format delimited fields terminated by ‘,’  – 列分隔符<br>collection items terminated by ‘_’      –MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)<br>map keys terminated by ‘:’                – MAP中的key与value的分隔符<br>lines terminated by ‘\n’;                    – 行分隔符<br>4）导入文本数据到测试表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath ‘/opt/module/datas/test.txt’into table test</span><br></pre></td></tr></table></figure><p>5）访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> friends[<span class="number">1</span>],children[<span class="string">&#x27;xiao song&#x27;</span>],address.city <span class="keyword">from</span> test</span><br><span class="line"><span class="keyword">where</span> name<span class="operator">=</span>&quot;songsong&quot;;</span><br><span class="line">OK</span><br><span class="line">_c0     _c1     city</span><br><span class="line">lili    <span class="number">18</span>      beijing</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.076</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_267.png" alt="blog: www.xubatian.cn"></p><h2 id="类型转化"><a href="#类型转化" class="headerlink" title="类型转化"></a>类型转化</h2><p>Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p><h3 id="1．隐式类型转换规则如下"><a href="#1．隐式类型转换规则如下" class="headerlink" title="1．隐式类型转换规则如下"></a>1．隐式类型转换规则如下</h3><p>（1）任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。<br>（2）所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。<br>（3）TINYINT、SMALLINT、INT都可以转换为FLOAT。<br>（4）BOOLEAN类型不可以转换为任何其它的类型。</p><h3 id="2．可以使用CAST操作显示进行数据类型转换"><a href="#2．可以使用CAST操作显示进行数据类型转换" class="headerlink" title="2．可以使用CAST操作显示进行数据类型转换"></a>2．可以使用CAST操作显示进行数据类型转换</h3><p>例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;涓涓不塞，是为江河；源源不断，是为奋斗；生生不息，是为中国。——人民日报                &lt;/p&gt;
&lt;p&gt;​                                              &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="hive" scheme="http://xubatian.cn/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper简介</title>
    <link href="http://xubatian.cn/zookeeper%E7%AE%80%E4%BB%8B/"/>
    <id>http://xubatian.cn/zookeeper%E7%AE%80%E4%BB%8B/</id>
    <published>2022-01-18T06:51:38.000Z</published>
    <updated>2022-01-23T02:58:21.703Z</updated>
    
    <content type="html"><![CDATA[<p>我们经常在说命运，但我觉得，命是自己的，运却和整个国家相关联。历史的洪流滚起，即便微如沙粒，也能奔腾入海，也能汇成史诗。 ——人民日报                                              </p><span id="more"></span><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_215.jpg" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="Zookeeper入门"><a href="#Zookeeper入门" class="headerlink" title="Zookeeper入门"></a>Zookeeper入门</h1><h2 id="zookeeper概述"><a href="#zookeeper概述" class="headerlink" title="zookeeper概述"></a>zookeeper概述</h2><p>Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。多作为集群提供服务的中间件.<br>    分布式系统: 分布式系统指由很多台计算机组成的一个整体！这个整体一致对外,并且处理同一请求，系统对内透明，对外不透明！内部的每台计算机，都可以相互通信，例如使用RPC 或者是WebService！客户端向一个分布式系统发送的一次请求到接受到响应，有可能会经历多台计算机!<br>    Zookeeper从设计模式角度来理解，是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生了变化，Zookeeper就负责通知已经在Zookeeper上注册的那些观察者做出相应的反应.</p><p>Zookeeper = 文件系统 + 通知机制</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_216.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p>Zookeeper集群上每台存的数据都是一模一样的</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_220.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_219.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_221.png" width = "" height = "" alt="xubatian的博客" align="center" /><p>域名通过DNS域名解析器解析成ip地址,所以在互联网行业,光有域名是没啥用的</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_222.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_223.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_224.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_225.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h2><h3 id="官网首页"><a href="#官网首页" class="headerlink" title="官网首页"></a>官网首页</h3><p><a href="https://zookeeper.apache.org/">https://zookeeper.apache.org/</a></p><h3 id="下载截图"><a href="#下载截图" class="headerlink" title="下载截图"></a>下载截图</h3><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_226.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_227.png" width = "" height = "" alt="xubatian的博客" align="center" /><h1 id="Zookeeper安装"><a href="#Zookeeper安装" class="headerlink" title="Zookeeper安装"></a>Zookeeper安装</h1><h2 id="本地模式安装部署"><a href="#本地模式安装部署" class="headerlink" title="本地模式安装部署"></a>本地模式安装部署</h2><p>1．安装前准备<br>（1）安装Jdk<br>（2）拷贝Zookeeper安装包到Linux系统下<br>（3）解压到指定目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>2．配置修改<br>    （1）将/opt/module/zookeeper-3.4.10/conf这个路径下的zoo_sample.cfg修改为zoo.cfg；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ mv zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure><p>​    （2）打开zoo.cfg文件，修改dataDir路径：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ vim zoo.cfg</span><br><span class="line">修改如下内容：</span><br><span class="line">dataDir=/opt/module/zookeeper-3.4.10/zkData</span><br></pre></td></tr></table></figure><p>​    （3）在/opt/module/zookeeper-3.4.10/这个目录上创建zkData文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ mkdir zkData</span><br></pre></td></tr></table></figure><p>3．操作Zookeeper<br>（1）启动Zookeeper</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start</span><br></pre></td></tr></table></figure><p>（2）查看进程是否启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ jps</span><br><span class="line">4020 Jps</span><br><span class="line">4001 QuorumPeerMain</span><br></pre></td></tr></table></figure><p>（3）查看状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: standalone</span><br></pre></td></tr></table></figure><p>（4）启动客户端：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ bin/zkCli.sh</span><br></pre></td></tr></table></figure><p>（5）退出客户端：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] quit</span><br></pre></td></tr></table></figure><p>注意:</p><p>ZookeeperMain是客户端进程<br>QuorumPeerMain是服务端进程  </p><p>如果通过close关闭的话,他并不会把客户端的进程杀掉,只是把你们间的连接对象给关掉了</p><p>   (6) 停止Zookeeper</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh stop</span><br></pre></td></tr></table></figure><h2 id="Zookeeper的四字命令"><a href="#Zookeeper的四字命令" class="headerlink" title="Zookeeper的四字命令"></a>Zookeeper的四字命令</h2><p>​    Zookeeper支持某些特定的四字命令(The Four Letter Words) 与其进行交互，它们大多是查询命令，用来获取Zookeeper服务的当前状态及相关信息，用户在客户端可以通过telnet<br>或nc 向Zookeeper提交相应的命令。<br>​    Zookeeper常用四字命令主要如下:</p><table><thead><tr><th>ruok</th><th>测试服务是否处于正确状态，如果确实如此，那么服务返回 imok ,否则不做任何响应。</th></tr></thead><tbody><tr><td>conf</td><td>3.3.0版本引入的，打印出服务相关配置的详细信息</td></tr><tr><td>cons</td><td>列出所有连接到这台服务器的客户端全部会话详细信息。包括 接收/发送的包数量，会话id，操作延迟、最后的操作执行等等信息</td></tr><tr><td>crst</td><td>重置所有连接的连接和会话统计信息</td></tr><tr><td>dump</td><td>列出那些比较重要的会话和临时节点。这个命令只能在leader节点上有用</td></tr><tr><td>envi</td><td>打印出服务环境的详细信息</td></tr></tbody></table><p>注: 使用之前，需要先安装nc，可以使用yum方式进行安装.</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_228.png" width = "" height = "" alt="xubatian的博客" align="center" /><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_229.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="配置参数解读"><a href="#配置参数解读" class="headerlink" title="配置参数解读"></a>配置参数解读</h2><p>Zookeeper中的配置文件zoo.cfg中参数含义解读如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1．tickTime =2000：通信心跳数，Zookeeper服务器与客户端心跳时间，单位毫秒</span><br><span class="line">Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。</span><br><span class="line">它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime)</span><br><span class="line">2．initLimit =10：LF初始通信时限</span><br><span class="line">集群中的Follower跟随者服务器与Leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。</span><br><span class="line">3．syncLimit =5：LF同步通信时限</span><br><span class="line">集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。</span><br><span class="line">4．dataDir：数据文件目录+数据持久化路径</span><br><span class="line">主要用于保存Zookeeper中的数据。</span><br><span class="line">5．clientPort =2181：客户端连接端口</span><br><span class="line">监听客户端连接的端口。</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="Zookeeper内部原理"><a href="#Zookeeper内部原理" class="headerlink" title="Zookeeper内部原理"></a>Zookeeper内部原理</h1><h2 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h2><p>1）半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器。<br>2）Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。<br>3）以一个简单的例子来说明整个选举的过程。<br>假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么，如图所示。</p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_230.png" width = "" height = "" alt="xubatian的博客" align="center" /><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">（1）服务器1启动，此时只有它一台服务器启动了，它发出去的报文没有任何响应，所以它的选举状态一直是LOOKING状态。</span><br><span class="line">（2）服务器2启动，它与最开始启动的服务器1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以id值较大的服务器2胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3)，所以服务器1、2还是继续保持LOOKING状态。</span><br><span class="line">（3）服务器3启动，根据前面的理论分析，服务器3成为服务器1、2、3中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的Leader。</span><br><span class="line">（4）服务器4启动，根据前面的分析，理论上服务器4应该是服务器1、2、3、4中最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以它只能接收当小弟的命了。</span><br><span class="line">（5）服务器5启动，同4一样当小弟。</span><br></pre></td></tr></table></figure><p>xubatian补充解析:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">假如需要启动5台zookeeper, 第一台启动投票给自己, 第二台也是, 第三台也是. 到了第三台就已经是半数以上了. 如果你的myId大,则你就是Leader. 这个时里面没有数据,我们启动的情况. 但是假如里面有数据了, 但是我的Leader,即myid=3的挂掉了.那有如何重新选举呢? 原本5台zookeeper数据都是一样的. 但是最后leader,即myid=3的机器在挂掉的瞬间将数据给了myid=1的. 那这个时候其他的还没来得及给. 这个时候我们也是需要考虑谁的数据最全.即, 谁的数据最全, 谁就是大哥,即leader.</span><br><span class="line">    总结: zookeeper的选举机制, 最重要的就是myid和谁的数据量最全.</span><br></pre></td></tr></table></figure><h2 id="节点类型"><a href="#节点类型" class="headerlink" title="节点类型"></a>节点类型</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_231.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="Stat结构体"><a href="#Stat结构体" class="headerlink" title="Stat结构体"></a>Stat结构体</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_232.png" width = "" height = "" alt="xubatian的博客" align="center" /><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1）czxid-创建节点的事务zxid</span><br><span class="line">每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。</span><br><span class="line">事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。</span><br><span class="line">2）ctime - znode被创建的毫秒数(从1970年开始)</span><br><span class="line">3）mzxid - znode最后更新的事务zxid</span><br><span class="line">4）mtime - znode最后修改的毫秒数(从1970年开始)</span><br><span class="line">5）pZxid-znode最后更新的子节点zxid</span><br><span class="line">6）cversion - znode子节点变化号，znode子节点修改次数</span><br><span class="line">7）dataversion - znode数据变化号</span><br><span class="line">8）aclVersion - znode访问控制列表的变化号</span><br><span class="line">9）ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。</span><br><span class="line">10）dataLength- znode的数据长度</span><br><span class="line">11）numChildren - znode子节点数量</span><br></pre></td></tr></table></figure><h2 id="监听器原理"><a href="#监听器原理" class="headerlink" title="监听器原理"></a>监听器原理</h2><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_233.png" width = "" height = "" alt="xubatian的博客" align="center" /><h2 id="写数据流程"><a href="#写数据流程" class="headerlink" title="写数据流程"></a>写数据流程</h2><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_234.png" alt="blog: www.xubatian.cn"></p><h1 id="Zookeeper实战"><a href="#Zookeeper实战" class="headerlink" title="Zookeeper实战"></a>Zookeeper实战</h1><h2 id="分布式安装部署"><a href="#分布式安装部署" class="headerlink" title="分布式安装部署"></a>分布式安装部署</h2><p>1．集群规划<br>在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper。<br>2．解压安装<br>（1）解压Zookeeper安装包到/opt/module/目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>（2）同步/opt/module/zookeeper-3.4.10目录内容到hadoop103、hadoop104</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 module]$ xsync zookeeper-3.4.10/</span><br></pre></td></tr></table></figure><p>3．配置服务器编号<br>（1）在/opt/module/zookeeper-3.4.10/这个目录下创建zkData</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ mkdir -p zkData</span><br></pre></td></tr></table></figure><p>（2）在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zkData]$ touch myid</span><br></pre></td></tr></table></figure><p>添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码<br>（3）编辑myid文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zkData]$ vi myid</span><br></pre></td></tr></table></figure><p>​    在文件中添加与server对应的编号：(编号是选举的时候用)<br>2<br>（4）拷贝配置好的zookeeper到其他机器上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zkData]$ xsync myid</span><br></pre></td></tr></table></figure><p>并分别在hadoop102、hadoop103上修改myid文件中内容为3、4<br>4．配置zoo.cfg文件<br>（1）重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ mv zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure><p>（2）打开zoo.cfg文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ vim zoo.cfg</span><br></pre></td></tr></table></figure><p>修改数据存储路径配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/opt/module/zookeeper-3.4.10/zkData</span><br><span class="line">增加如下配置</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">######################cluster##########################</span></span></span><br><span class="line">主机名  交换数据端口号  选举数据端口号  </span><br><span class="line"></span><br><span class="line">server.2=hadoop102:2888:3888</span><br><span class="line">server.3=hadoop103:2888:3888</span><br><span class="line">server.4=hadoop104:2888:3888</span><br></pre></td></tr></table></figure><p>（3）同步zoo.cfg配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 conf]$ xsync zoo.cfg</span><br></pre></td></tr></table></figure><p>（4）配置参数解读</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">server.A=B:C:D。</span><br></pre></td></tr></table></figure><p>A是一个数字，表示这个是第几号服务器；<br>集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。<br>B是这个服务器的ip地址；<br>C是这个服务器与集群中的Leader服务器交换信息的端口；<br>D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</p><p>bin/zkServer.sh start 开启<br>bin/zkServer.sh status 查看状态<br>不能用,因为你已经是一个集群了,只是开了一个zookeeper是不能用的,但是他是起来了的,只不过是不能干活的    </p><p>4．集群操作<br>（1）分别启动Zookeeper</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start</span><br><span class="line">[shangbaishuyao@hadoop103 zookeeper-3.4.10]$ bin/zkServer.sh start</span><br><span class="line">[shangbaishuyao@hadoop104 zookeeper-3.4.10]$ bin/zkServer.sh start</span><br></pre></td></tr></table></figure><p>（2）查看状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh status</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line">[shangbaishuyao@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh status</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: leader</span><br><span class="line">[shangbaishuyao@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh status</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure><h2 id="客户端命令行操作"><a href="#客户端命令行操作" class="headerlink" title="客户端命令行操作"></a>客户端命令行操作</h2><table><thead><tr><th>命令基本语法</th><th>功能描述</th></tr></thead><tbody><tr><td>help</td><td>显示所有操作命令</td></tr><tr><td>ls path [watch]</td><td>使用 ls 命令来查看当前znode中所包含的内容</td></tr><tr><td>ls2 path [watch] 相当于  ls + stat</td><td>查看当前节点数据并能看到更新次数等数据</td></tr><tr><td>create 创建节点<img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_235.png" alt="img"></td><td>普通创建-s  含有序列-e  临时（重启或者超时消失） 上白书妖补充:<img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_236.png" alt="img"> 你在创建节点的时候要指定一下这个节点要存写什么数据,不然不给你创建节点,如下图:<img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_237.png" alt="img"></td></tr><tr><td>get path [watch]</td><td>获得节点的值</td></tr><tr><td>set</td><td>设置节点的具体值,修改节点的值</td></tr><tr><td>stat</td><td>查看节点状态</td></tr><tr><td>delete</td><td>删除节点</td></tr><tr><td>rmr</td><td>递归删除节点</td></tr></tbody></table><h2 id="连接客户端"><a href="#连接客户端" class="headerlink" title="连接客户端"></a>连接客户端</h2><p>1．启动客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shangbaishuyao@hadoop103 zookeeper-3.4.10]$ bin/zkCli.sh</span><br></pre></td></tr></table></figure><p>2．显示所有操作命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] help</span><br></pre></td></tr></table></figure><p>3．查看当前znode中所包含的内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /</span><br><span class="line">[zookeeper]</span><br></pre></td></tr></table></figure><p>4．查看当前节点详细数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls2 /</span><br><span class="line">[zookeeper]</span><br><span class="line">cZxid = 0x0</span><br><span class="line">ctime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">mZxid = 0x0</span><br><span class="line">mtime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">pZxid = 0x0</span><br><span class="line">cversion = -1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure><p>5．分别创建2个普通节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] create /sanguo &quot;jinlian&quot;</span><br><span class="line">Created /sanguo</span><br><span class="line">[zk: localhost:2181(CONNECTED) 4] create /sanguo/shuguo &quot;liubei&quot;</span><br><span class="line">Created /sanguo/shuguo</span><br></pre></td></tr></table></figure><p>6．获得节点的值</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 5] get /sanguo</span><br><span class="line">jinlian</span><br><span class="line">cZxid = 0x100000003</span><br><span class="line">ctime = Wed Aug 29 00:03:23 CST 2018</span><br><span class="line">mZxid = 0x100000003</span><br><span class="line">mtime = Wed Aug 29 00:03:23 CST 2018</span><br><span class="line">pZxid = 0x100000004</span><br><span class="line">cversion = 1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 7</span><br><span class="line">numChildren = 1</span><br><span class="line">[zk: localhost:2181(CONNECTED) 6]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 6] get /sanguo/shuguo</span><br><span class="line">liubei</span><br><span class="line">cZxid = 0x100000004</span><br><span class="line">ctime = Wed Aug 29 00:04:35 CST 2018</span><br><span class="line">mZxid = 0x100000004</span><br><span class="line">mtime = Wed Aug 29 00:04:35 CST 2018</span><br><span class="line">pZxid = 0x100000004</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 6</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure><p>7．创建短暂节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 7] create -e /sanguo/wuguo &quot;zhouyu&quot;</span><br><span class="line">Created /sanguo/wuguo</span><br></pre></td></tr></table></figure><p>（1）在当前客户端是能查看到的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] ls /sanguo </span><br><span class="line">[wuguo, shuguo]</span><br></pre></td></tr></table></figure><p>（2）退出当前客户端然后再重启客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 12] quit</span><br><span class="line">[shangbaishuyao@hadoop104 zookeeper-3.4.10]$ bin/zkCli.sh</span><br></pre></td></tr></table></figure><p>（3）再次查看根目录下短暂节点已经删除</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /sanguo</span><br><span class="line">[shuguo]</span><br></pre></td></tr></table></figure><p>8．创建带序号的节点<br>    （1）先创建一个普通的根节点/sanguo/weiguo</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] create /sanguo/weiguo &quot;caocao&quot;</span><br><span class="line">Created /sanguo/weiguo</span><br></pre></td></tr></table></figure><p>​    （2）创建带序号的节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] create -s /sanguo/weiguo/xiaoqiao &quot;jinlian&quot;</span><br><span class="line">Created /sanguo/weiguo/xiaoqiao0000000000</span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] create -s /sanguo/weiguo/daqiao &quot;jinlian&quot;</span><br><span class="line">Created /sanguo/weiguo/daqiao0000000001</span><br><span class="line">[zk: localhost:2181(CONNECTED) 4] create -s /sanguo/weiguo/diaocan &quot;jinlian&quot;</span><br><span class="line">Created /sanguo/weiguo/diaocan0000000002</span><br></pre></td></tr></table></figure><p>如果原来没有序号节点，序号从0开始依次递增。如果原节点下已有2个节点，则再排序时从2开始，以此类推。<br>9．修改节点数据值</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 6] set /sanguo/weiguo &quot;simayi&quot;</span><br></pre></td></tr></table></figure><p>10．节点的值变化监听<br>    （1）在hadoop104主机上注册监听/sanguo节点数据变化</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 26] [zk: localhost:2181(CONNECTED) 8] get /sanguo watch</span><br></pre></td></tr></table></figure><p>​    （2）在hadoop103主机上修改/sanguo节点的数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] set /sanguo &quot;xisi&quot;</span><br></pre></td></tr></table></figure><p>​    （3）观察hadoop104主机收到数据变化的监听</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected type:NodeDataChanged path:/sanguo</span><br></pre></td></tr></table></figure><p>11．节点的子节点变化监听（路径变化）<br>    （1）在hadoop104主机上注册监听/sanguo节点的子节点变化</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls /sanguo watch</span><br><span class="line">[aa0000000001, server101]</span><br></pre></td></tr></table></figure><p>​    （2）在hadoop103主机/sanguo节点上创建子节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] create /sanguo/jin &quot;simayi&quot;</span><br><span class="line">Created /sanguo/jin</span><br></pre></td></tr></table></figure><p>​    （3）观察hadoop104主机收到子节点变化的监听</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/sanguo</span><br></pre></td></tr></table></figure><p>12．删除节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 4] delete /sanguo/jin</span><br></pre></td></tr></table></figure><p>13．递归删除节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 15] rmr /sanguo/shuguo</span><br></pre></td></tr></table></figure><p>14．查看节点状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 17] stat /sanguo</span><br><span class="line">cZxid = 0x100000003</span><br><span class="line">ctime = Wed Aug 29 00:03:23 CST 2018</span><br><span class="line">mZxid = 0x100000011</span><br><span class="line">mtime = Wed Aug 29 00:21:23 CST 2018</span><br><span class="line">pZxid = 0x100000014</span><br><span class="line">cversion = 9</span><br><span class="line">dataVersion = 1</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 4</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure><h1 id="API应用"><a href="#API应用" class="headerlink" title="API应用"></a>API应用</h1><h2 id="案例代码"><a href="#案例代码" class="headerlink" title="案例代码"></a>案例代码</h2><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/zookeeper/TestZookeeperAPI/TestZookeeper.java">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/zookeeper/TestZookeeperAPI/TestZookeeper.java</a></p><h2 id="IDEA环境搭建"><a href="#IDEA环境搭建" class="headerlink" title="IDEA环境搭建"></a>IDEA环境搭建</h2><p>1．创建一个Maven工程<br>2．添加pom文件</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;RELEASE&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;log4j-core&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;<span class="number">2.8</span><span class="number">.2</span>&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- https:<span class="comment">//mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --&gt;</span></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;zookeeper&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;<span class="number">3.4</span><span class="number">.10</span>&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure><p>2．添加pom文件<br>3．拷贝log4j.properties文件到项目根目录<br>需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger=INFO, stdout  </span><br><span class="line">log4j.appender.stdout=org.apache.log4j.ConsoleAppender  </span><br><span class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout  </span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  </span><br><span class="line">log4j.appender.logfile=org.apache.log4j.FileAppender  </span><br><span class="line">log4j.appender.logfile.File=target/spring.log  </span><br><span class="line">log4j.appender.logfile.layout=org.apache.log4j.PatternLayout  </span><br><span class="line">log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n </span><br></pre></td></tr></table></figure><h2 id="创建ZooKeeper客户端"><a href="#创建ZooKeeper客户端" class="headerlink" title="创建ZooKeeper客户端"></a>创建ZooKeeper客户端</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String connectString =</span><br><span class="line"> <span class="string">&quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</span><br><span class="line"><span class="keyword">private</span> ZooKeeper zkClient = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">zkClient = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 收到事件通知后的回调函数（用户的业务逻辑）</span></span><br><span class="line">System.out.println(event.getType() + <span class="string">&quot;--&quot;</span> + event.getPath());</span><br><span class="line"></span><br><span class="line"><span class="comment">// 再次启动监听</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">zkClient.getChildren(<span class="string">&quot;/&quot;</span>, <span class="keyword">true</span>);</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="创建子节点"><a href="#创建子节点" class="headerlink" title="创建子节点"></a>创建子节点</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建子节点</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 参数1：要创建的节点的路径； 参数2：节点数据 ； 参数3：节点权限 ；参数4：节点的类型</span></span><br><span class="line">String nodeCreated = zkClient.create(<span class="string">&quot;/shangbaishuyao&quot;</span>, <span class="string">&quot;jinlian&quot;</span>.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="获取子节点并监听节点变化"><a href="#获取子节点并监听节点变化" class="headerlink" title="获取子节点并监听节点变化"></a>获取子节点并监听节点变化</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取子节点</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getChildren</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">List&lt;String&gt; children = zkClient.getChildren(<span class="string">&quot;/&quot;</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">System.out.println(child);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 延时阻塞</span></span><br><span class="line">Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="判断Znode是否存在"><a href="#判断Znode是否存在" class="headerlink" title="判断Znode是否存在"></a>判断Znode是否存在</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 判断znode是否存在</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">exist</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">Stat stat = zkClient.exists(<span class="string">&quot;/eclipse&quot;</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">System.out.println(stat == <span class="keyword">null</span> ? <span class="string">&quot;not exist&quot;</span> : <span class="string">&quot;exist&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="修改znode数据-获得znode数据-删除znode节点"><a href="#修改znode数据-获得znode数据-删除znode节点" class="headerlink" title="修改znode数据,获得znode数据,删除znode节点"></a>修改znode数据,获得znode数据,删除znode节点</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    获取Znode数据</span></span><br><span class="line"><span class="comment">    修改znode数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testznodeData</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">byte</span>[] getdatas = zkClient.getData(<span class="string">&quot;/shangbaishuyao&quot;</span>,<span class="keyword">false</span> , <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">    Stat setdatas = zkClient.setData(<span class="string">&quot;/shangbaishuyao&quot;</span>,<span class="string">&quot;xww&quot;</span>.getBytes(),<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    System.out.println(<span class="string">&quot;得到shangbaishuyao节点的数据&quot;</span>+<span class="keyword">new</span> String(getdatas));</span><br><span class="line">    System.out.println(<span class="string">&quot;修改shangbaishuyao节点的数据&quot;</span> + setdatas);</span><br><span class="line"></span><br><span class="line">zkClient.delete(<span class="string">&quot;/shangbaishuyao&quot;</span>,<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="监听服务器节点动态上下线案例"><a href="#监听服务器节点动态上下线案例" class="headerlink" title="监听服务器节点动态上下线案例"></a>监听服务器节点动态上下线案例</h2><p>1．需求<br>某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线。<br>2．需求分析，如图所示</p><p><img src="https://xubatian.oss-cn-hangzhou.aliyuncs.com/xubatian_blogs_img2/aliyun/www.xubatian.cn_238.png" alt="blog: www.xubatian.cn"></p><h2 id="案例代码-1"><a href="#案例代码-1" class="headerlink" title="案例代码"></a>案例代码</h2><p><a href="https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/zookeeper/zookeeperCase/">https://github.com/ShangBaiShuYao/bigdata/blob/master/src/main/java/com/shangbaishuyao/zookeeper/zookeeperCase/</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;我们经常在说命运，但我觉得，命是自己的，运却和整个国家相关联。历史的洪流滚起，即便微如沙粒，也能奔腾入海，也能汇成史诗。 ——人民日报                                              &lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="http://xubatian.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="zookeeper" scheme="http://xubatian.cn/tags/zookeeper/"/>
    
  </entry>
  
</feed>
